  
  
Abstract—This paper proposes and implements a framework 
for human-robot collaboration in a Mobile Visual Sensor 
Network (MVSN). A collaborative architecture for the 
proposed human-integrated MVSN was developed to allow the 
human operator and robots to collaborate to perform 
surveillance tasks. We successfully implemented the MVSN so 
the user can control the deployment of the mobile sensors 
through his head movement. We also explored using computer 
vision techniques and navigation techniques on the robot nodes 
to conduct active human target detection. The robot nodes, 
therefore, are able to detect human faces while exploring the 
unknown environment, and then relay the face images to the 
operator for target recognition. In this way, humans and robots 
can complement each other to accomplish surveillance tasks. 
Our experimental results validated the proposed framework. 
I. INTRODUCTION 
In recent years, Mobile Visual Sensor Networks 
(MVSNs) which consist of multiple robots equipped with 
visual sensors have been used for numerous applications, 
such as surveillance for security, reconnaissance for military 
or police operations, or search and rescue in emergency 
response. While stationary visual sensor networks (VSNs) 
have limited sensing ability and coverage, MVSNs have 
some advantages, such as adaptation to environmental 
changes, re-configurability for better sensing performance, 
and larger coverage compared with a single robot or a 
stationary sensor network. MVSNs enable users to explore 
dangerous areas or areas that do not support human life, such 
as the ocean or outer space. Furthermore, a user can team up 
with multiple robots, which can greatly reduce the time it 
takes to explore an area and gather intelligence or locate the 
desired object. 
In traditional surveillance applications, the mobile visual 
sensor nodes independently and continuously send video 
streams to a central processing server, where the video could 
be analyzed by software or human operators. In this work we 
propose that humans and mobile robots can work together to 
accomplish a task. In other words, humans and robots can 
form a team with each having complementary skills and 
being committed to a common goal through collaboration 
[1]. Our work focuses on the application of detecting and 
recognizing a human target through human-robot 
collaboration. 
 
This work is partially supported by the NSF grant CISE/IIS 1231671 and 
National Natural Science Foundation of China under Grants 61328302 and 
61222310. 
Ha Manh Do, Craig Mouser, and Weihua Sheng are with the Laboratory 
for Advanced Sensing, Computation and Control (ASCC Lab), School of 
Electrical and Computer Engineering, Oklahoma State University, 
Stillwater, OK, 74078, USA (e-mails: ha.do@okstate.edu, 
cmouser@okstate.edu, weihua.sheng@okstate.edu); Meiqiu Liu is with the 
College of Electrical Engineering, Zhejiang University, Hangzhou 310027, 
China (e-mail: liumeiqin@zju.edu.cn). 
Fong and Nourbakhsh [2] pointed out that to reduce 
human workload, fatigue induced error and risk, intelligent 
robotic systems need to be a significant part of mission 
design. It is relatively easy for robot sensors to detect targets. 
However, current mobile robots have poor performance of 
target recognition in changing and unstructured 
environments. Furthermore, dealing with the variability in 
shape, texture, color, and size of natural objects leads to more 
complicated algorithms for robust target recognition and 
results in an MVSN that is more difficult and expensive to 
develop and operate [3]. On the other hand, humans have 
superior recognition capabilities, can easily adapt to changing 
environments [4], and are good at sensor deployment. 
However, human operators are inconsistent, suffer from 
distraction [5], and quickly become tired and overloaded in 
stressful conditions. By combining the advantages of human 
perception and recognition skills with the autonomous 
robots’ accuracy and consistency, a human–robot team can 
increase the performance of target recognition and reduce the 
complexity of the algorithms, even in unpredictable 
conditions that autonomous systems are incompetent to deal 
with [6].  
This paper proposes and implements a framework for 
human-robot collaboration in an MVSN, in which humans 
and mobile robots work together in surveillance tasks to 
explore an area as well as to detect and recognize a target. 
This paper is organized as follows. Section II covers previous 
work done in this field. Section III presents the design of the 
human-integrated MVSN and its implementation as an open 
platform. Section IV describes human-robot collaboration for 
surveillance. Section V gives the experimental results. 
Section VI concludes the paper and also discusses the 
potential future work. 
II. PREVIOUS WORK 
This section presents related works in the field of human-
robot collaboration in surveillance. Burke et al. [7] used 
wearable arm trackers and a tablet to immerse a human node 
in a multiple-robot team for remote exploration and 
surveillance. They focused on the development of a 
multimodal interaction system including naturalistic human 
gestures, voice commands, and a tablet interface to provide 
the human operator with multiple interaction modes given the 
situational demands. However, their surveillance system did 
not send video back to the operator and the robots did not 
share detection and recognition task with the operator. While 
Lewis et al. [8] stated that beside human-robots interaction 
the robots need to be equipped with autonomous algorithms 
to mitigate some of the challenges and complexity the 
operators face in controlling multiple robots. Iocchi et al. [9] 
proposed a system including human security operators 
carrying sensors, mobile robots and network of fixed cameras 
to collaboratively monitor public environments. However, the 
paper mainly discussed the system architecture and the data 
Human-Robot Collaboration in a Mobile Visual Sensor Network 
Ha Manh Do, Craig Mouser, Meiqin Liu, and Weihua Sheng 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2203
  
 
Figure 1.  Human-Robot Team Configuration in the Mobile Visual 
Sensor Network. 
 
Figure 2.  Collaborative Architecture of the Human-integrated MVSN 
 
Service 
Layer 
Target 
Detection 
Robot 1 
Sensors Sensors 
Human 
Onbody Wearable 
Devices 
… 
Remote Environment Human Operator Environment 
Sensor Driver GUI 
Robot 
Driver 
Nav. SLAM 
Human Action & 
Intention 
       Data/controls                   Human-robot collaborative communications      Nav. (Navigation) 
Physical 
Layer 
Device 
Layer 
Communication 
Layer 
Collaborative 
Layer 
Task 1 
Robot 
State 
Target 
Detection 
Sensor Driver 
Robot 
Driver 
Nav. SLAM 
Task N 
Robot 
State 
Robot N 
Sensor Driver 
Task Manager 
Human Assisted 
Deployment 
Collaborative Target 
Recognition 
    Communication Framework 
fusion approach for future development to locate and track 
targets through cooperation between sensor nodes. Tkach et 
al. [10] presented four human–robot collaboration levels for 
target recognition tasks in unstructured environments. They 
proposed algorithms that are developed for real-time dynamic 
switching between collaboration levels in a human–robot 
target recognition task. Their algorithms can increase the 
target identification rate and reduce the complexity of the 
robotic system. Those examples also provide an empirical 
proof of the advantages of such collaborations in a target 
recognition task.  
MVSNs were deployed for urban search and rescue 
response at the World Trade Center [11]. During that 
mission, it is found that the most pressing concern is to allow 
humans to efficiently control more robots when the robot-to-
human ratio is high. Another concern is to create intelligent 
and intuitive human-robot interfaces. In subsequent research, 
Nourbakhsh et al. [12] developed a multirobot search and 
rescue system. They used a multi-agent architecture to create 
a scalable system that could integrate multiple types of robots 
as well as humans into a network. By creating separate nodes 
in their multi-agent system for information gathering, 
communication, exploration, and human interaction, their 
system becomes scalable to a level that allows a human to 
efficiently manage multiple agents.   
The above works mainly focused on solving separate 
problems, such as human-robot interaction, human-robot 
cooperation or coordination, and adjustable-autonomy for 
multiple robots, human-robot collaboration in exploration or 
target detection/ recognition, etc. In this paper, we aim to 
develop an open platform of a human-integrated MVSN 
based on a new collaborative architecture that efficiently 
allows a human and multiple robotic sensors to collaborate in 
a surveillance task. This platform mainly addresses two 
issues: human-assisted robot deployment and human-robot 
collaborative target recognition. Our platform allows the 
robots and a human to perform the work in which they are 
best suited. This makes our framework scalable and simple, 
which leads to increased robot-to-human ratio without 
increasing the complexity of the interface. 
III. HUMAN-INTEGRATED MOBILE VISUAL SENSOR 
NETWORK 
The proposed human-integrated MVSN consists of two 
parts: the robot nodes and the human node. The robot nodes 
are essentially mobile robots equipped with cameras. Human 
wearing on-body devices and a minicomputer with a 
multimedia graphical user interface (GUI) acts as a human 
node in the MVSN. Those nodes are wirelessly connected in 
a configuration as shown in Fig. 1. This configuration allows 
the human to act as one additional node connected to the 
wireless network as the robots do and to collaborate with the 
robots on surveillance tasks. Human-robot collaboration in 
MVSN requires significant researches to address principal 
challenges such as human-robot communication of intention, 
action planning and coordination. Efficient solutions to these 
problems require communication and collaborative models 
which has been partially addressed by existing multiagent 
system (MAS) architectures such as RETSINA [13]. In the 
human-integrated MVSN, two essential tasks, remote 
deployment of robots and target search and recognition, need 
to be implemented based on mobile robot service such as 
SLAM (Simultaneous Localization and Mapping), 
navigation, and object detection.  
A. Collaborative Architecture of Human-integrated MVSN 
As shown in Fig. 2, the five-layer collaborative archi-
tecture of human-integrated MVSN includes physical layer, 
device layer, service layer, communication layer, and 
collaborative layer. The physical layer consists of the 
hardware of the nodes. The robot nodes are built based on 
mobile robots equipped with sensors such as a RGB camera 
or RGB-D camera, a laser range finder (LRF), and encoders. 
The human operator wears on-body devices and a mini-
computer with graphical user interface. The device layer 
provides the device drivers for the sensors and actuators. 
The service layer contains basic services each node provides. 
The collaborative layer contains high level functions for 
human-robot collaboration and robot-robot cooperation. 
Based on the Robot Operating System (ROS) network [14], 
the communication layer provides a seamless connection 
between the different modules in the service layer and the 
collaborative layer. 
The collaborative layer is developed for human-robot 
collaboration and robot-robot cooperation in the MVSN. 
Currently, this layer is proposed with three main functions 
including human-assisted deployment, collaborative target 
2204
  
 
Figure 3.  A network of three mobile sensors and the human node. 
recognition, and task manager. Human-assisted deployment 
provides methods to enable the human operator to actively 
guide multiple robots to explore the unknown environment 
while searching for targets and to assist the robots to solve 
problems during autonomous exploration and search. 
Collaborative target recognition enables the human operator 
and robots to work together to detect and identify a target. 
Task manager coordinates the different functions and 
dispatches the right tasks to each individual robot. 
The above architecture was applied to implement the 
software of the proposed MVSN. In this paper, we focus on 
human-assisted deployment and collaborative target 
recognition. As we mentioned, human is good at deploying 
the robots at a higher level while the robots are good at 
detecting targets through vision sensors. Based on 
commands estimated from the head motion and a GUI, the 
robots are easily deployed to explore the remote location. 
While exploring the remote location, the robots can detect 
the targets and send the image of the target to the human 
operator for recognition. 
B. Open Platform for Human-Integrated MVSN 
The open platform built for testing the proposed human-
integrated MVSN consists of multiple mobile robot nodes 
and the human node, which are shown in Fig. 3. This section 
describes the hardware and software structure of this 
platform. 
1) Hardware 
 a) Robot node 
The robot node, an ASCCBot [15] developed in our lab, 
is a mobile visual sensor platform built on an iRobot Create 
base. Mounted on it are an omnidirectional camera, a laser 
range finder (LRF), and a fit-pc2 minicomputer. It also has 
batteries on board to power these devices. The 
omnidirectional camera is a Mobotix Q24 360° IP camera. It 
can display the images in various modes from a full 360° 
view to a simple pinhole camera style view. The LRF is a 
Hokuyo URG-04LX-UG01. It is a low-power laser 
rangefinder with a wide range up to 5600mm x 240°, and an 
accuracy of ±30mm.  
b) Human node 
As shown in Fig. 3, an operator wearing a head-mounted 
display (HMD), the Vuzix iWear VR920, acts as a human 
node in the MVSN. In this node, the HMD is connected to a 
fit-pc2 minicomputer running Windows 7. The HMD is 
capable of sensing head rotation using built-in 3 degree of 
freedom head-tracker. Therefore it can be used for the 
deployment of robots through human head motion. It can 
send rotational feedback to allow the operator to control the 
mobile visual network with the rotations of his head. For 
instance, looking left can cause the robot to turn left. 
Moreover, the HMD allows the operator to have both see-
through mode and immersive mode when navigating the 
robots. In the immersive mode, this HMD can allow the user 
to remain more focused. The video from the cameras on the 
robots can be streamed back to the user and displayed on the 
HMD. Therefore the user can locate and recognize targets of 
interest by collaborating with multiple robot nodes. 
2) Software 
The software consists of two main parts: the robot node 
software and the human node software. They were both 
developed based on ROS. 
a) Robot node software 
ROS Electric [14] was installed on Ubuntu 10.04 for 
running the robot node's software. For the basic functions in 
the robot node, we utilized exiting packages from ROS 
repositories to develop the device layer for interfacing with 
the robot base, and Hokuyo LRF. In service layer, two main 
services including SLAM and navigation were developed 
based on existing ROS packages. SLAM was based on Rao-
Blackwellized particle filters [16]. Motion planning and 
autonomous navigation were based on the particle filter based 
localization method [17], and the adaptive (or KLD-
sampling) Monte Carlo localization approach [18]. We 
implemented the driver to receive the images from the Q24 
camera. 
b) Human Node Software 
The software of the human node allows a human node to 
be integrated into the network. This node can publish and 
subscribe message through ROS network, read data from 
Yaw, Pitch, and Roll sensors on the HMD. Those data were 
fused by Human Action & Intention function to recognize 
the operator’s commands through his head motion. The GUI 
developed based on OpenCV allows the operator to view the 
map and the image from each robot node on the HMD.  
IV.  HUMAN-ROBOT COLLABORATION IN SURVEILLANCE 
A. Human-assisted deployment  
Using head motions, the human operator can deploy the 
robot nodes in different environments. First, the operator 
determines the initial locations of robots and assigns each 
robot a goal point on the 2D map. The operator can also 
drive any robot by watching what the robot sees through the 
HMD, while the other robots autonomously move to their 
goal points. After the timeout, if any robot cannot reach the 
goal point, it sends a request to the operator for assistance. 
Therefore by assistance provided by the operator, the robots 
can be deployed in complex environments. In the future, the 
robot-robot cooperative SLAM, collaborative navigation, 
and autonomous exploration and search would be fully 
developed for faster and more efficient exploration of the 
unknown environments. 
2205
  
     
confidence : a)  0.0         b) 0.67             c) 0.73              d) 0.94 
Figure 4.  Shapshots from different viewpoints with different 
confidences. 
 
(a)                    (b)                        (c)                       (d) 
Figure 5.  Sequence of robot motion shown in the map and face 
detection results of AHTD in corresponding frames   
B. Collaborative Target Recognition 
The surveillance task consists of target detection and 
target recognition. In particular, in the proposed MVSN, 
human detection was conducted by robot nodes, and human 
recognition was performed by the human node.  
1) Active Human Target Detection (AHTD) 
In human target detection, body parts such as head, face, 
legs, arms, upper body, lower body, etc. can provide cues for 
human detection, but a face usually plays a major role in 
human recognition. Face detection is the first step to all facial 
analysis algorithms and it is easier than detection of other 
body parts because the human face has a distinct structure. 
Face detection proposed by Viola and Jones [19] is the most 
popular approach. We adopted this approach for face 
detection. In general, a frontal view of the human face can 
make it easier for the operator to recognize. In this sense, a 
good viewpoint should be found to obtain a frontal face 
image. Therefore, active human target detection (AHTD) was 
implemented on our system. 
The OpenCV’s HaarDetectObjects function was first 
utilized with different trained classifiers to get the initial face 
region, then the Good Features to Track [20] function was 
applied to compute the feature points in the face region for 
tracking and evaluating the face. To find a better viewpoint, 
we need define a measure to assess its suitability for face 
detection. The viewpoint was evaluated by a confidence 
measure based on the distribution of feature points in the face 
region as follows: 
 = 2min	(






,






)                (1) 
Here N R is the number of feature points in the right half 
of the face region. N L is the number of feature points in the 
left one. If it is a profile face, the feature points mostly locate 
on the left half or the right half, so the confidence is much 
less than 1.0. While the frontal face gives a high confidence 
of close to 1. As shown in Fig. 4, the profile face obtained 
from the viewpoint (b) gives a confidence of 0.67, while the 
frontal face obtained from the viewpoint (d) gives a 
confidence of 0.94. 
The robot runs the AHTD method according to the 
following steps: 
Step 1 - Face detection: The robot performs face 
detection on the whole image with deferent cascades of 
frontal faces and profile faces. 
Step 2 - Face evaluation: We compute the feature points 
in the face region using Good Features To Track function 
and then the confidence measure according to (1). If the 
confidence is above certain threshold (0.85), the face region 
is highlighted and sent to the human node, otherwise the 
robot is guided to a new position to achieve a better 
viewpoint in next steps. 
Step 3 - Goal point prediction: The goal point with the 
best viewpoint is predicted through the angle of face (AOF) 
from the camera, the angle of human (AOH) in the panorama 
image, and the distance (D) between the human target and the 
robot.  
 Step 4 - Robot navigation and face tracking: Move the 
robot and predict the new IROI based on robot motion. The 
face detection is done on this IROI to reduce computation. 
The confidence and the new goal point are updated for each 
image frame. If the confidence is above certain threshold, the 
face region is highlighted and sent to the human node. If the 
face is lost in two consecutive frames, go back Step 1.  
Fig. 5 presents the sequence of robot motion shown in the 
map and face detection results of AHTD in corresponding 
frames. In the map, the robot and its moving direction are 
represented by a red circle and a blue arrow, respectively. A 
profile face was detected in frame (b). As shown in Fig. 5-c, 
a new goal point (represented by a red arrow) and a path 
(represented by a green curve) were generated to navigate the 
robot to the location with the best viewpoint. The robot 
stopped at frame (d) when the confidence reached 0.94 at the 
position very close to the goal point. 
2) Collaborative target recognition 
Once a face is detected, the robot can autonomously send 
the image to the HMD and ask the human node to recognize 
the face. The operator is usually able to recognize the 
highlighted faces sent from the robot. In case the image is not 
clear, the operator could drive the robot around the human 
target to get a better viewpoint for recognition. After that the 
operator could assign a new goal point to the robot and get 
back to the previous task of collaborating with other robots. 
C. Task Manager 
The task manager was developed as a state machine. It 
allows the human operator to control the network, to assign 
tasks to robot nodes, to switch the map view and the image 
view. The system mainly works in two phases including: 
deployment phase and recognition phase. The task manager 
monitors not only the whole MVSN but also each node in 
autonomous tasks. It allows the system to smoothly switch 
2206
  
 
a) Human can deploy robot nodes in different rooms and select the goal points in the maps. 
 
b) Robot node can autonomously transmit the face image to the HMD and ask the operator to recognize the detected face. 
Figure 6.  Shapshots from the graphical user interface of the software on the human node. 
between those phases. It also ensures the robot navigates to 
the goal point. 
V. EXPERIMENTS AND RESULTS 
In this section, we present our experimental results. 
Through the experiments, the network was configured with 
three robot nodes and a human node as our human-integrated 
MVSN. Each of these robot nodes could detect faces, stream 
images, publish the number of faces detected, create a 2D 
map using SLAM, autonomously navigate to a goal point, 
and autonomously reach the best viewpoint to detect faces. 
Working as a master node in the network, the human operator 
wearing the HMD could see what robots were seeing, deploy 
and control the network, and collaborate with robot nodes to 
recognize human targets. The graphical user interface of the 
software on the human node is shown in Fig. 6. 
In our testing, we were able to successfully deploy the 
robots in a lab environment. These nodes were connected to 
a wireless network though a Wi-Fi router, which provides 
sufficient bandwidth. The MVSN was tested for both 
human-assisted deployment and collaborative surveillance. 
A. Robot Deployment 
There are three different modes of operation during robot 
deployment. 
• Mode 1: A user can view images from all 3 robot nodes 
and can switch display mode: double panorama (360
0
 view) 
mode which is good for exploring a remote location; 
panorama (180
0
) mode which is good for face detection. 
• Mode 2: A user can view maps from all 3 robot nodes. 
The robot’s position was updated on the map and the operator 
can select the goal point by selecting the goal position they 
want the robot to reach. The robots can then automatically 
move to that location. 
• Mode3: The human node controls an active robot, 
subscribes to both the map and the image of this robot, and 
drives the robot to explore the environment. Meanwhile, 
other robots can autonomously navigate to their goal points 
as shown in Fig. 6-a. 
The robots correctly built and displayed a 2D map. The 
map is streamed at 480x480 at more than 6 frames per second 
with a delay less than 0.5 s. The video was also successfully 
streamed at 640x480 at more than 3 frames per second. There 
is also a slight delay of approximately 0.6 s between the robot 
video and the map feed when it is displayed on the HMD, 
resulting in Table I. 
B. Collaborative Target Recognition  
The operator could listen to requests for help from other 
robots while controlling one robot to explore the remote 
location. The robot nodes were able to successfully and 
accurately detect faces and draw a rectangle around them. 
They then sent the detected faces back to the human node 
where the operator recognizes them. The robot that has a 
higher number of detected faces is autonomously switched 
into the HMD. The operator can recognize the faces and send 
back the results to the robot as shown in Fig. 6-b. 
2207
  
The face detection was able to detect all faces within 4 
feet of the robot when the face was looking at 90° or less 
angle from the camera. At 6 feet we saw a significant drop in 
performance, only detecting faces correctly in 67% of the 
frames. By the time we reached 10 feet, the algorithm could 
no longer detect faces. These results can be seen in Table II. 
It was also able to detect faces from only 0° to 90° off the 
horizontal plane of the camera. 
VI. CONCLUSION AND FUTURE WORK 
In this research, we proposed the collaborative 
architecture to develope a human-integrated mobile visual 
sensor network for surveillance applications. Three aspects of 
the system architecture were implemented: human-assisted 
robot deployment, human-robot collaborative surveillance 
based on the AHTD, and task manager. This case study of 
human-robot collaboration proves that it is possible to 
develop an MVSN that combines the complementary 
capacities of human and robots. The collaborative 
architecture was validated in a lab environment for human 
target recognition. Overall, this system could be deployed 
into the real world and have impact on applications such as 
surveillance, reconnaissance, search and rescue, and 
exploration. The future work will improve the following two 
aspects. First, the software should be able to detect other 
objects, depending on the application at hand. Second, in 
order to enhance the efficiency of collaboration, robot nodes 
should be able to understand and predict the human 
intentions through multimodal interaction that incorporates 
gestures, and gazes. 
REFERENCES 
[1] A. Bauer, D. Wollherr, and M. Buss, “Human-Robot Collaboration: A 
Survey,” International Journal of Humanoid Robotics, vol. 05, no. 01, 
p. 47, 2007. 
[2] T.W. Fong and I. Nourbakhsh, "Interaction Challenges in Human-
Robot Space Exploration," Interactions, Vol. 12, No. 2, March 2005, 
pp. 42-45. 
[3] R. Kleihorst, B. Schueler, A. Danilin, and M. Heijligers, “Smart 
camera mote with high performance vision system”, Proceedings of 
ACM SenSys Workshop on Distributed Smart Cameras (DSC ’06), 
2006. 
[4] G. Rodriguez and C. R. Weisbin, “A new method to evaluate human-
robot system performance.,” Autonomous robots, vol. 14, no. 2–3, pp. 
165-78, 2003. 
[5] J. B. F. Van Erp and H. a. H. C. Van Veen, “Vibrotactile in-vehicle 
navigation system,” Transportation Research Part F: Traffic 
Psychology and Behaviour, vol. 7, no. 4–5, pp. 247-256, Jul. 2004. 
[6] A. Bechar and Y. Edan, “Human–robot collaboration for improved 
target recognition of agricultural robots,” Ind. Robot, vol. 30, no. 5, 
pp. 432–436, 2003. 
[7] D. Burke, N. Schurr, J. Ayers, J. Rousseau, J. Fertitta, A. Carlin, and 
D. Dumond, “Multimodal interaction for human-robot teams,” SPIE 
Defense, Secur. Sens., vol. 8741, no. International Society for Optics 
and Photonics, p. 87410E, May 2013. 
[8] M. Lewis, M. Goodrich, K. Sycara, and M. Steinberg, “Human 
Factors issues for Interaction with Bio-Inspired Swarms,” in 
Proceedings of the Human Factors and Ergonomics Society Annual 
Meeting, 2012, vol. 56, no. 1, pp. 61–64. 
[9] L. Iocchi, D. N. Monekosso, D. Nardi, M. Nicolescu, P. Remagnino, 
and M. V. Espina, “Smart Monitoring of Complex Public Scenes 
Collaboration between Human Guards , Security Network and 
Robotic Platforms,” in 2011 AAAI Fall Symposium, 2011, pp. 14–19. 
[10] I. Tkach, A. Bechar, and Y. Edan, “Switching Between Collaboration 
Levels in a Human--Robot Target Recognition System,” IEEE 
Transactions on Systems, Man, and Cybernetics, Part C: Applications 
and Reviews and, vol. 41, no. 6, pp. 955-967, 2011. 
[11] J. Casper and R. R. Murphy, “Human-robot interactions during the 
robot-assisted urban search and rescue response at the World Trade 
Center.,” IEEE transactions on systems, man, and cybernetics. Part B, 
Cybernetics, vol. 33, no. 3, pp. 367-85, Jan. 2003. 
[12] Nourbakhsh, I.R., K. Sycara, M. Koes, M. Yong, M. Lewis, and S. 
Burion. "Human-Robot Teaming for Search and Rescue." IEEE 
Pervasive Computing 4.1 (2005): 72-78. Print. 
[13] K. Sycara, J. A. Giampapa, B. Langley, and M. Paolucci, “The 
RETSINA MAS , a Case Study,” Spinger-Verlag, pp. 232–250, 2003. 
[14] ”ROS wiki,” Webpage, http://www.ros.org/wiki/, accessed Jun. 2012. 
[15] Gang Li, Jianhao Du, Chun Zhu and Weihua Sheng, "A cost-effective 
and open mobile sensor platform for networked surveillance", Proc. 
SPIE 8137, 813713 (2011). 
[16] G. Grisetti, C. Stachniss, W. Burgard,  "Improved Techniques for Grid 
Mapping With Rao-Blackwellized Particle Filters," Robotics, IEEE 
Transactions on , vol.23, no.1, pp.34-46, Feb. 2007. 
[17] S. Thrun, D. Fox, W. Burgard, and F. Dellaert, “Robust monte carlo 
localization for mobile robots,” Artificial Intelligence, 128(1-2):99–
141, 2000. 
[18] D. Fox, “Adapting the Sample Size in Particle Filters Through KLD- 
Sampling,” International Journal of Robotics Research, 22, 2003. 
[19] P. Viola and M. Jones, “Robust real-time object detection,” 
International Journal of Computer Vision, 57(2), 137-154, 2004. 
[20] Shi, J.; Tomasi, C., "Good features to track," Computer Vision and 
Pattern Recognition, 1994. Proceedings CVPR '94., 1994 IEEE 
Computer Society Conference on, vol., no., pp.593,600, 21-23 Jun 
1994. 
 
TABLE I 
TRANSMISSION DELAY BETWEEN ROBOTS AND HUMAN NODE 
Mode Image Map 
Mode 1 2.2 s x 
Mode 2 x 0.5 s 
Mode 3 0.6 s 0.15 s 
 
TABLE II 
ACCURACY OF THE FACE DETECTION  
Viewing Angle 4 feet 6 feet 8 feet 10 feet 
0° 98% 67% 5% 0% 
30° 95% 47% 2% 0% 
45° 93% 43% 3% 0% 
90° 98% 67% 5% 0% 
100° 35% 21% 0% 0% 
110° 0% 0% 0% 0% 
 
2208
