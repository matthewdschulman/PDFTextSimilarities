Active Sensing for Dynamic, Non-holonomic, Robust Visual Servoing
Avik De
?
, Karl S. Bayer
y
and Daniel E. Koditschek
?
Abstract— We consider the problem of visually servoing a
legged vehicle with unicycle-like nonholonomic constraints sub-
ject to second-order fore-aft dynamics in its horizontal-plane.
We target applications to rugged environments characterized
by complex terrain likely to signiﬁcantly perturb the robot’s
nominal dynamics. At the same time, it is crucial that the cam-
era avoid “obstacle” poses where absolute localization would be
compromised by even partial loss of landmark visibility. Hence,
we seek a controller whose robustness against disturbances
and obstacle avoidance capabilities can be assured by a strict
global Lyapunov function. Since the nonholonomic constraints
preclude smooth point stabilizability we introduce an extra
degree of sensory freedom, afﬁxing the camera to an actuated
panning axis on the robot’s back. Smooth stabilizability to the
robot-orientation-indifferent goal cycle no longer precluded, we
construct a controller and strict global Lyapunov function with
the desired properties. We implement several versions of the
scheme on a RHex robot maneuvering over slippery ground
and document its successful empirical performance.
I. INTRODUCTION
In GPS-denied settings, visual servoing—closed-loop po-
sition control by reference to ﬁxed visual landmarks [1]—
offers an attractive approach to self-localization, particularly
over complex terrain where broken [2], unstable [3], and
ﬂowing [4] substrates preclude odometry. Advances in com-
puter vision, processing power, and algorithmic insight [5]
lend ever more speed and reliability to the extraction and
tracking of natural features from successive camera images.
We by-pass that problem in favor of a structured vision
solution [6], focusing on the servo problem only.
In that context, range and ﬁeld-of-view limits pose a
fundamental problem: getting too close or far from a near-
ﬁeld scene will quickly degrade its efﬁcacy as a visual
landmark. Thus, our ﬁrst major focus, following [7] is visual-
obstacle avoidance: we require that our closed-loop vector
ﬁelds not only guarantee convergence to a goal set but avoid
straying out of the landmark’s visibility set along the way.
A second major focus places our paper within the volumi-
nous and ever-growing literature on active sensing. It is well-
known [8], [9], [10], [11] that simultaneous, coupled control
of an actuated sensory subsystem in coordination with the
robot’s body dynamics can remediate the stabilizability of
underactuated base platforms. Following suit by recourse
to a panning camera, our unicycle-like base, the dynamical
legged XRHex [12] re-implementation of RHex [13], raises
?
Electrical and Systems Engineering, University of Pennsylvania,
Philadelphia, PA, USA.favik,kodg@seas.upenn.edu.
y
Mechanical Engineering, Columbia University, New York, NY , USA.
ksb2153@columbia.edu.
This work was supported in part by AFOSR MURI FA9550-10-1-0567
and in part by NSF CDI-II 1028237.
Fig. 1. Our experimental apparatus with XRHex [12] and visual ﬁducials
[6] as a landmark (Section III) using the proposed controller (Section II) to
move to a goal location while avoiding visual obstacles.
two related new considerations. First, we seek to run the
robot over considerably more complex terrain than in [14]
and at considerably higher speeds known to necessitate a
second-order unicycle model [15] for reasonable ﬁdelity to
horizontal plane dynamics. Second, our preoccupation with
unstable terrain raises the prospect of signiﬁcant perturba-
tions, well beyond the inﬁdelities of any unicyle model. The
paper’s central contribution is to show how the active sensor’s
relaxation of the goal set from a point into a cycle attractor
affords a controller with a strict global Lyapunov function
and its accompanying guarantee of both obstacle avoidance
and ISS [16] defense against perturbations.
A. Related Literature
Fully-actuated ﬁrst-order visual servo control is largely a
solved problem [1] and, in particular, smooth stabilizability
opens the door to the full suite of navigation function (NF)
methods [7], including the lift to second-order dynamics [17]
that can preserve all the ﬁrst-order guarantees—not merely
convergence, but obstacle avoidance [18], and robustness
against disturbances as well [19]. Recent work incorporating
visual servo methods into global landmark-based localiza-
tion, navigation and mapping [20], [21] suggests the powerful
role that effectively stabilized visual servo loops can achieve
in complex task settings.
When smooth state-feedback point stabilizability fails,
e.g. for nonholonomically constrained kinematic carts [22],
an important and longstanding remediation is the introduc-
tion of systematic time-variation into the feedback signal
[23], restoring exponential asymptotic point stability and
the disturbance robustness it confers (cf. [24]). While this
approach has been successfully applied in the broad context
of visual servoing including the explicit construction of a
strict Lyapunov function [10], [21], it is not clear to us
how to extend that method to the present setting requiring
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6192
obstacle avoidance and disturbance rejection in the context
of a second-order plant. The alternative of introducing an
explicitly nonsmooth (hybrid) point stabilizing law [14] can
avoid obstacles but does not extend straightforwardly to
second-order dynamics and cannot confer the robustifying
ISS properties [16] (i.e., no single strict Lyapunov function
can exist) in view of Brockett’s condition [22].
Instead, we take recourse to a third approach that also
enjoys a long tradition in the visual servoing literature: the
relaxation of the required task from point to submanifold
stabilization via the introduction of an actively panned cam-
era [10], [25], [21]. For example, in [25] this is arranged
by locating the camera on a single d.o.f. boom actuated
so as to rotate around the robot’s base. Now, placing the
camera at a speciﬁed point in SE(2) (the group of planar
rigid transformations) corresponds via the inverse image
of the camera map to an entire cycle (the circle, S
1
) in
the robot’s four d.o.f. conﬁguration space. In [25], [21]
this additional freedom is exploited by composing three
distinct task controllers to (i) localize the camera frame, (ii)
avoid both image plane occlusions, and (iii) avoid obstacles
encountered in the corporeal world. No formal analysis of the
composition of these controllers is furnished, whereas with
an eye toward the robustness our application requires, we aim
for a controller that admits a strict global Lyapunov function.
Because it is not presently clear how to encompass all three
of the desiderata within the control Lyapunov framework, we
conﬁne attention to the ﬁrst two (camera frame convergence
and positive invariance of the visible set) with the beneﬁt of
introducing (and achieving) the third goal of robustness.
The paper makes one further new contribution to the
active visual servoing problem. The prior literature generally
separates the camera’s optical center from its panning axis,
thereby conferring full rank upon the Jacobian of the camera
map. But we observe that there are many practical situations
which mitigate against the enlarged workspace envelope
associated with a “panning arm” of ﬁnite length.
1
At zero
length, the camera map Jacobian (the interaction matrix
of [25]) becomes singular and stabilizability of even the
relaxed task speciﬁcation, the circle, S
1
, requires passage
to a second-order plant model (in a manner analogous to
the method of approximate input-output linearization [26]).
More generally, this Jacobian will be very poorly conditioned
when the servo target is much farther away than the length
of the camera-wielding manipulator (for instance, in an
aerial surveillance vehicle), and we presume that successful
physical implementation will require a model with singular
camera map Jacobian, as we assume here.
B. Organization and Contributions of the Paper
The primary contribution of this paper is the development
of a new, smooth, state-feedback-based visual servo con-
troller using the second-order lift of a family of navigation
functions along with a formal proof that the controller
1
For instance, we are able to protect the camera against the dangers of
shocks and impacts as RHex scrambles through its treacherous environment
by mounting it so as to allow only pure rotation around the optical axis.
achieves the three objectives: convergence (stabilization to
any landmark conﬁguration in the visibility set); obstacle
avoidance (repulsion from the boundary of the visibility
set); and robustness (a guarantee that bounded-input distur-
bances result in bounded servo errors along with explicit
computation of controller gain values relative to disturbance
magnitudes guaranteeing the preservation of obstacle avoid-
ance). These results are achieved by the analysis of a global
Lyapunov function that is strict (i.e. positive deﬁnite with
negative deﬁnite derivative) relative to a relaxed goal set
(the circle, S
1
, arising as the inverse image of the speciﬁed
camera frame under the camera map), and of uniform height
along the zero section over the boundary of the visibility set.
The second contribution of the paper is an empirical imple-
mentation of (a) the provably correct version and (b) a higher
performing variant (for which the full correctness proof is
in progress) of this controller on the XRHex robot [12],
[13], providing experimental evidence of its efﬁcacy with
emphasis on its strong disturbance rejection characteristics.
II. STABILIZING A UNICYCLE TOS
1
The most basic model of the horizontal-plane dynamics of
a RHex with conﬁguration (p;)2R
2
S
1
 SE(2) is that
of a kinematic unicycle,
h
_ p
_

i
=

cos() 0
sin() 0
0 1

[
u1
u2
] =:B() [
u1
u2
]; (1)
where u
1
is the forward speed and u
2
is the turning rate.
In instantiations of this model where the body inertia or the
speed levels are low enough that viscous effects dominate
inertial effects, it is reasonable (and standard practice [24])
to assume that (u
1
;u
2
) can be directly controlled.
However, at the target speeds we wish to reach, a ﬁrst-
order model is a poor ﬁt to the dynamics of RHex [15];
we will need to respect the robot’s signiﬁcant inertial lags
in this operating regime and impose a number of regularity
conditions on any feedback controller u = k(q) we intro-
duce. Adding an integrator and assuming our controlled input
is v
1
= _ u
1
imposes the inertial model (and automatically
ensures that u(t) is C
1
). The framework we propose here
can further guarantee that (a) v
1
is a smooth function of the
state (so u
1
(t) is C
2
), and (b)kv
1
k =k _ u
1
k is bounded.
A. Second-order Control Scheme
We append the panning d.o.f. as a conﬁguration variable
2 S
1
, deﬁned in world coordinates for convenience, and
assume that it is a fully actuated double integrator
  =v
3
: (2)
Now q := (p;;)2Q :=R
2
T
2
is the full conﬁguration
of the system, while z := (p;)2 Z := R
2
S
1
are the
“task coordinates” that we are interested in.
In some of the relevant prior unicycle servoing literature
[10], the Jacobian relating the control inputs to (in effect) _ z
is nonsingular due to the lever arm of the panning boom,
i.e. the camera’s position and orientation in any desired
world frame can now be smoothly approached from any
6193
inﬁnitesimal direction by appropriately coordinating the body
and eye degrees of freedom. However, with our reluctance
to commit (and be hostage) to the conditioning problems
for a distant target, the camera optical center is modeled to
be coincident with the panning axis, and a bit more work
must be done to exploit the new actuated d.o.f. to achieve
the desired stabilization to a cycle.
At this point, we assume (ensured by our construction in
Section III-A) that the task can be expressed with respect to
a smooth non-degenerate NF, ', (possessing a unique local
minimum at a point and attaining the exact height of unity
at the obstacle boundaries) [27] over the task space Z, with
the additional properties of convexity
2
and bounded gradient.
For convenience, we will refer to the task positions “below”
the '-unity height as the visible set
V :=fz2Z : 0'(z) 1g
and detail the application-speciﬁc interpretation in Section
III. Let f := D' deﬁne the smooth, stable and uniformly
bounded reference dynamics, _ r =f(r) for r2Z.
1) Plant model: Let b = B
?
. Collecting and differentiat-
ing once the ﬁrst two components of (1) and appending (2)
yields the system dynamics,
h
B
T
b
T
i
 z =
h
v1
v3
u1u2
i
; (3)
_
 =u
2
; (4)
where u
1
= B
T
1
_ z is the forward velocity of the unicycle.
Note that I =BB
T
+bb
T
.
2) Feedback Controller: The incorporation of an actuated
panning d.o.f. effectively affords us a partially decoupled
plant model whereby the base orientation  excites the
task variables of interest, z purely through a feedforward
cascade
3
. The task state, z, is still non-holonomically con-
strained (b
T
_ z 0), but our control idea here is to align so
that there is no component of the reference acting in the con-
strained motion direction, i.e. we try to (a) minimizejb
T
fj,
thereby mitigating harmful consequences of underactuation
as long as (b) e := _ z f is small.
a) Orientation Controller: Let  := [
1 0 0
0 1 0
], let \ be
the function that returns the angle of a non-zero vector on
the plane, and deﬁne the saturating angle function,
\
s
(p) :=
kpk
2
kpk
2
+"
2
\
\p; (5)
which returns 0 when p = 0, and is smooth. Let  :=
\
s
(f) and 
r
:= . Deﬁne the orientation error,

0
= 1  cos
r
: (6)
2
Convexity—a property available to our model space,Y, to be introduced
in Section III—is not essential to the controller. Its chief virtue here is
to afford an immediate extension of the ISS framework [16] to goal sets
which are compact manifolds rather than points [28] without the need for
more intricate reasoning about the existence and nature of Control-Lyapunov
Functions relative to non-goal components of the invariant set that would
otherwise be required [29].
3
However, in (11), that we will ﬁnd it necessary to feed the task variables
back into the  dynamics, thereby imposing a coupling in the closed-loop.
Let us deﬁne s := sin
r
, and pick
u
2
=
_
 k
0
s =) _ 
0
= k
0
s
2
= k
0

0
(2 
0
): (7)
Lastly, note that the parasitic component of the reference
which we wish to minimize satisﬁes (proof in Appendix I)
jb
T
fjkfk(s +); (8)
where the magnitude of (z) = "
\
o(kfk) (an artifact of
the saturation in\
s
) can be controlled directly by "
\
.
In Section IV, we present and use a modiﬁed orientation
controller with a multistability property, which demonstrates
empirically superior performance.
b) Graph Error Controller: To control the velocity
error e, we adopt established methods [17] of lifting ﬁrst-
order reference dynamics to second-order plants by using the
graph error as a “kinetic energy,” and we brieﬂy summarize
them here. In our notation, let
d(z; _ z) :=
_
f  D' k
1
e (9)
be the desired graph error control, where e := _ z  f
and D is the differential operator. Note that B
T
d can be
matched exactly with avaliable control inputs, i.e. we can
make B
T
( z d) 0.
3) Closed-loop dynamics: Summarizing from the previ-
ous section, our feedback controller sets
h
v1
v3
u2
i
=FB(z; _ z;) :=
h
B
T
d
_
 k0s
i
: (10)
Inserting this into the plant dynamics, we obtain the
closed-loop dynamics
_
 =g
0
(z; _ z;) := D
z
 _ z k
0
s(z;) (11)
 z =g
1
(z; _ z;) :=BB
T
d +
_
bB
T
1
_ z; (12)
where B
1
is deﬁned to be the ﬁrst column of B.
The state space takes the form of a cross product, S
1

TZ, and we will typically use the coordinates (;z; _ z) for
computation purposes. The following propositions express
the stability properties of the proposed control scheme.
Proposition 1 (Stability). Denote by G :='
 1
(0)f0g2
TZ the task goal set, comprising the zero section over the
desired position in the task variables. Under the closed-loop
dynamics (11), (12), the base-indifferent goal set, S
1
G is
asymptotically stable and its basin of attraction includes all
of the visibility set, VZ.
Proof. Please refer to Appendix II for the deﬁnitions and
detailed derivations of the computations in this proof.
We will show that := 
2

0
+
1
, where
1
:='+
1
2
e
T
e,
is a strict global Lyapunov function for the goal set S
1
G.
Clearly,  is positive deﬁnite with respect to this goal set.
It now sufﬁces to show that _  is negative deﬁnite along the
motions of the closed-loop dynamics. We have,
_ 
1
= _ ' +e
T
_ e = D'
T
f kkek
2
 f
T
bb
T
( z d); (13)
where the last summand can be thought of as “noise” owed
to our underactuation. However,jf
T
bj is controlled by our
orientation controller, for which we have already presented
6194
the Lyapunov function
0
(6). As shown in Appendix II, the
“noise” terms are sub-quadratic in the velocity errorkek or
jsj, with coefﬁcients that are bounded functions of z (recall
z2V, a compact domain), only perturbed by our saturation
by "
\
of Section II-A.2.a. The derivative of  is
_  
~
k
1
(kek a
8
)
2
 
kfk
2
~
k0
(
~
k
0
  
3
) +"
\
o(kfk
2
); (14)
where 
3
is bounded and so
~
k
0
can simply be made large
enough to make the last parenthesized term positive, and the
magnitude of the last term can be made arbitrarily small.
Moreover,kfk = 0 =)a
8
= 0.
The sum of the ﬁrst two terms above is negative semidef-
inite, and the sum is zero only if both of the following hold:
1) kfk = 0 =) z2 '
 1
(0) by our assumptions on ',
and that a
8
= 0,
2) kek = 0 =) _ z =f(z), which, along with 1), in turn
implies _ z = 0.
Since "
\
is arbitrarily small,  is nonpositive, vanishing
only when (z; _ z)2G, and the result follows from Lasalle’s
Invariance Principle [30].
Proposition 2 (Obstacle Avoidance). If the initial (rest)
conﬁguration satisﬁes (0)  1, the closed-loop system
avoids '-modeled obstacles by never leaving the visible
states, TV.
Proof. Proposition 1 ensures that _  0, and consequently
(t)< 1 fort> 0. Using the deﬁnition of,'< 1, and
from the deﬁnition of ', we get the desired conclusion.
In addition to the properties above, the existence of the
strict (with respect to the compact goal set, S
1
G [28])
Lyapunov function  implies that the closed-loop system
(11), (12) can handle bounded disturbance vectors injected
into the dynamics (3), (4) and converge to “small” (instead
of 0, in the disturbance-free situation) asymptotic error.
Disturbance vectors could be unmodeled perturbations from
terrain interactions (see Section IV), or a time-varying refer-
ence system f(z;t) with bounded D
t
f. We emphasize here
that (for e.g.) a hybrid controller that cannot have a strict
Lyapunov function could not provide this guarantee.
Lastly, we note that (although not considered here) a
second-order steering version of the -dynamics (a parsi-
monious model for RHex at higher speeds [15]) could be
handled by a very similar control methodology, by altering
the orientation controller in Section II-A.2.a.
III. VISUAL SERVOING WITH NAVIGATION FUNCTIONS
Since our conﬁgurations are strictly in the horizontal
plane, a constellation of three identiﬁable collinear feature
points provides a diffeomorphic “sensor map” (section III-A
in [7]). We note here that our methods and results extend di-
rectly to any scenario where there is a diffeomorphism from
the sensorium to the robot task coordinates, not restricted to
planar robots or indeed to visual sensing.
Deﬁne the sensor space asY = [ 1; 1]
3
, where the bounds
are imposed by the limited ﬁeld-of-view of the camera.
Adapted from the deﬁnition of the “camera map” in [7]
? ? ??
?
???
?
? ? ??
?
???
?
? ? ??
?
???
?
?
?
Fig. 2. A simulation comparing the task-space behavior of the NF con-
struction in motivating prior work [7] (left) and the proposed construction in
III-A with the smooth orientation controller of Section II-A.2.a (middle), and
the non-smooth orientation controller of Section III-B (right). The beacon
constellation is drawn as the triple of black dots, the contour lines are the
level sets of ' at the slice z
3
= e
T
3
'
 1
(0) (where e
3
is the third basis
vector), and the red dot on the right is the goal location. The rightmost two
simulations are deliberately underdamped, to demonstrate the “preferred
direction” side-effect of the smooth orientation controller.
(which we shall refer to as c
C
), we deﬁne a sensor map
c : SE(2)! Y such that y = c(z) as c = c
C
R, where
R : SE(2)! SE(2) is the rigid body transformation
R(p;) =

 p1 cos p2 sin
p1 sin p2 cos
 

: (15)
In the “visible set” (conﬁgurations z that face the camera
and are such that z
2
< 0), det(Dc) 6= 0. Let us deﬁne
(for notational brevity) J(y) := Dc
 1
j
c
 1
(y)
. For our im-
plementation, we assume that our sensor measures (y; _ y).
Using the same notation as [7], the setIY is the compact
domain of interest in the image space, and V = c
 1
(I) is
the corresponding (also compact) region in the task space.
The implementation of visual servo control is a straight-
forward adaptation of our controller synthesis in Section
II. Instead of specifying reference dynamics in the task
coordinates, we specify them directly in image coordinates
4
(we construct ' explicitly in Section III-A).
We wish to emphasize here that all the calculations we
perform are done completely in the image-space. Instead
of algebraically solving for z = c
 1
(y) and using this in
z-reference dynamics (this method is sensitive to camera
calibration errors, which affect c
 1
), we are solving these
image-task equations dynamically. Of course, the stability
and obstacle avoidance results of Propositions 1 and 2 hold
in whatever coordinates the closed-loop system is executed.
It should be noted thatJ(y) is non-trivial to compute. The
approximation used in [7] was accurate enough for initial
conditions close to the goal, however we wish to perform
large-scale motions which preclude local approximations.
The world x-coordinates of our beacon (three collinear
visual ﬁducials, each pair equidistant) are assumed to be
[ 1; 0; 1] without loss of generality. If v := (y
1
y
2
+y
2
y
3
 
2y
1
y
3
; y
1
+ 2y
2
 y
3
) and v
?
:=

0 1
1 0

v, then
c
 1
(y) = 

1
kvk
[
v v
?
]

y2(y3 y1)
y3 y1

 \v

: (16)
We can use this analytical form for c
 1
to compute J(y).
4
It should be mentioned that the nonlinear change of coordinatesc intro-
duces a “Coriolis-like” term, because when pulled back to z-coordinates,
 z =
_
J _ y + J  y. Empirically, we found that the
_
J terms were small in
magnitude in our energetic regime, and ignored them in our experiments.
6195
A. Navigation Function Construction
Here we construct a NF [27] for servoing to a point while
avoiding image-space obstacles. Our construction is a slight
variant of the construction in [31], which is provably correct
over a number of free-space models including the present
one [31]. Furthermore, it enjoys the additional property of
convexity over the (convex) space Y [31] of interest in this
application, hence it supports the hypotheses of Proposition
1, and we will rely upon it for the formally instantiated
construction of ' in that Proposition. Our application to
visual servoing closely follows [7], including an exact copy
of the visual obstacle encoding, , and a slight variation on
the goal encoding, .
Following [31], we construct a strictly convex (except at
the goal) objective function  ' : Y! R
+
, such that ' =

k
  ' is a NF, where 
k
: R
+
! [0; 1] : x7!
 
x
1+x

1=k
.
In this paper, we propose the following variations to this
classical construction:
1) Given the aforementioned “good”  = 0 and “bad”
 = 0 sets, instead of the classical “task encoding”
function  ' =

k

, we use the modiﬁed  ' =

k
(=")
,
which is intuitively designed (for small values of ")
so that ' when  is large, and D' is dominated
by D and repels away from the obstacle when  is
small. The tuning parameter " controls the trade-off
between steep (approaching logical) obstacle “walls”
and torque requirements. Note that this modiﬁcation
retains the basic task-encoding as well as the convexity
properties of the original, while enabling a more faith-
ful reproduction of the-deﬁned goal-seeking behavior
far from any obstacles.
2) We encode goal-tracking in the task space, Z, instead
of the image space,Y, by (a) setting =
z
c
 1
:Y!
R
+
, where
z
(z) =
1
2
kz z
0
k
2
, andz
0
is the goal point
in the task coordinates, and (b) using the modiﬁed
reference dynamics f(y) =  J(y)
 1
J(y)
 T
D' in
our implementation. These modiﬁcations from (11),
(14) of [7] give rise to relatively more desirable tra-
jectories (metrically) inZ (see Figure 2). Note that we
sacriﬁce the guarantee of convexity of  ' by composing
withc
 1
, however we conjecture (proof is presently in
progress) that the NF properties can be veriﬁed without
it.
5
B. Simulations
We use our simulations to test relaxations of some of
the conditions required for our analytical guarantees in
Proposition 1:
1) We use a smaller k
0
than the conservative bound.
2) We introduce a new “tuning knob” by using separate
proportional and damping gains to form a modiﬁed
version of (9) and drop the feedforward
_
f to get
d(z; _ z) := k
p
D' k
d
_ z:
5
Even though we deﬁne the goal and obstacle in different coordinates,
they remain topologically coupled, disallowing a “cross-product” composi-
tion [32] of the two.
Fig. 3. A single trial of our algorithm running on RHex. The top left plot
shows level sets of a slice of ' at the goal . The bottom left plot shows
the beneﬁt of the active sensor: since is free from the task coordinates, the
robot can orient itself independently of the task-space obstacle, eliminating
the need for “parallel-parking” maneuvers.
Fig. 4. A visual illustration of performance degradation when the robot
dynamics get worse (due to shifting terrain). In some cases on loose terrain
the perturbations are large enough to cause failure (Proposition 1 can be
used to generate conservative bounds on the magnitude of perturbations
allowed relative to the available force and power limits), a quantitative
characterization of which is deferred to future work.
3) We drop the feedforward
_
 term in (7) and substitute
a non-smooth, Lipschitz version which does not have
a preferred “forward direction”,
u
NS
2
=k
0
sin
r
;
with the “ ” sign chosen whenj
r
j < =2 and vice
versa, whose Lyapunov function is 
NS
0
= 1 cos
r
.
This function is inspired by the multistable potentials
in [33] and has stable attractors at 
r
= 0 and . In
constrast, (7) causes the simulated robot to reverse its
heading after the initial overshoot (Figure 2).
These relaxations invalidate the conditions in the proof
of Proposition 1, but Figure 2 provides evidence that the
conditions are not necessary, which is encouraging for robot
implementation in the face of restrictions imposed by compu-
tational resource scarcity and desire for higher performance.
IV. ROBOT EXPERIMENTS
Our experiments were performed on XRHex [12] equipped
with a panning camera (Point Grey DragonFly 2). The
“beacon” comprised three distinguishable visual ﬁducials
6196
[6]. The provably correct scheme [7] was implemented, but
suffered from previously alluded-to metric inconveniences
(Section III-A). In the interests of space we will only show
data for the newer, higher-performing variant whose formal
proof of correctness is in progress. We used three testing
environments: (a) indoors with good traction, (b) indoors
with shifting terrain
6
, and (c) outdoors. We include results
from each of these in the video attachment.
Figure 3 shows a representative trial where the task-space
behavior of the robot is plotted. Figure 4 contains results
from comparative tests between relatively predictable RHex
dynamics, and loose, shifting terrain. Note from the left plot
that the robot roughly travels along gradient ﬂows of ',
indicating that lifting a ﬁrst-order (gradient) reference ﬁeld,
f, to a viable second-order trackable form [17] gets the
second-order system to essentially perform gradient descent.
V. CONCLUSIONS AND FUTURE WORK
We develop a theoretical framework targeting robust con-
troller design for a dynamic underactuated system by adding
sensor DOF’s which enable task-point stabilization accompa-
nied by a strict global Lyapunov function in the task-space—
even though the dynamics are still non-holonomically con-
strained. We apply this framework to the synthesis and im-
plementation of visual servo control on a RHex, documenting
the robustness by tests over substrates where the dynamics
are unpredictable. Notwithstanding the speciﬁc applications
focus on a cart (albeit in a regime where a kinematic model is
a poor ﬁt), it generalizes easily to truly second-order systems
such as UA Vs, AUVs, or sagittal-plane hopping robots (the
last of which we wish to examine carefully in future work).
We did not consider the problem of world-space obstacle
avoidance, though it is straightforward to incorporate mod-
eled obstacles with the mature NF framework [27] that we
have adopted (Section III-A). We are intrigued to suspect
that an extension of the control methods we presented here
might be applicable to that setting.
Lastly, note that while we used a visual system, the
framework in Section III is easily extensible to any sensing
model which admits a local diffeomorphism between the
robot pose and measurements in the sensorium—an avenue
we wish to explore in the future.
APPENDIX I
DERIVATION OF (8)
We will use a;x;y as dummy variable names in this sec-
tion. First, note that b
T
f = (b)
T
f. Using the deﬁnition
(5), when f6= 0,
jb
T
fj =kfkj sin( \(f))j =kfkj sin
 

r
+
"
2
\
kfk
2

j
=kfk


s cos
"
2
\
kfk
2
+ cos
r
sin
"
2
\
kfk
2


kfkjsj +kfk sin
jj"
2
\
kfk
2
:
6
This was constructed by placing multiple layers of slippery plastic sheets
on the ground such that the inter-layer static friction was far lower than that
required to propel the robot, causing the pieces to shift constantly.
TABLE I
LIST OF APPENDIX II SYMBOLS
Name Deﬁnition
a
1
(z) kb
T
Dfk
a
2
(z;k
1
) k b
T
D' k
1
kfjsj
a
3
(z;k
1
) kB
1
Dzk
a
4
(z;k
0
) k2DzfB
T
1
 k
0
sB
T
1
k
a
5
(z) kB
T
1
fDzfk
a
6
(z;k
0
) a
1
+a
4
a
7
(z;k
1
) a
2
+a
5
a
8
(z;k
1
)
a
6
kfk(jsj+"
\
)
2
~
k
1
~
k
1
k
1
 kfk(s +"
\
)a
3
(must be > 0)
~
k
0
 

2
kfk
  1

k
0
 
a
2
6
4
~
k
1
(must be > 0)

0
max
z2V
kfk
2
4
~
k
1

1
max
z2V
kfka
3
(z)

2
max
z2V
kfk
2

3
(k
1
) max
z2V
a
2
7
4
The “error term” (last summand) is of the form m(x) :=
x sin
 
a
x
2

for x;a> 0. It is easy to see that m(x)x.
Also, m(x)
a
x
. To see this, let y =
a
x
2
, and note that
lim
y!0
q
a
y
siny =
p
a lim
y!0
siny
y
p
y =
p
ay;
and other than at the limit, sin(y)y fory 0, and so this
is an upper bound: m(x)
a
x
.
Putting these together,
m(x) minfx;
a
x
g =x minf1;
a
x
2
g:
The maximum value is at x =
p
a, where m(
p
a)
p
a.
Returning to b
T
f with a =jj"
2
\
;x =kfk,
jb
T
fjkfk
 
jsj +
p
2"\
kfk
2

=:kfk(jsj +"
\
(z));
where we deﬁne  for notational brevity, and note that
kfk"
\
(z) 1 from the bound obtained above.
APPENDIX II
PROOF OF PROPOSITION 1
a) Derivation of (13): From the deﬁnition of 
1
,
_ 
1
= _ ' +e
T
_ e = _ ' +e
T
( z 
_
f)
= _ ' +e
T
( z d) +e
T
(d 
_
f)
= _ ' +e
T
BB
T
( z d) +e
T
bb
T
( z d) +e
T
( D' k
1
e)
where we use the fact that I = BB
T
+bb
T
. For the next
steps, recall that (10) ensuresB
T
( z d) = 0, and thatb
T
_ z =
0, resulting in
_ 
1
= D'
T
f k
1
kek
2
 f
T
bb
T
( z d):
b) Bounding the “Actuation Noise” Terms: Note that
b
T
d =b
T
(
_
f  D' +k
1
f)a
1
kek +a
2
; and
b
T
 z =u
1
u
2
=u
1
(D
z
(e +f) k
0
s)
= (B
T
1
e +B
T
1
f)(D
z
(e +f) k
0
s)
a
3
kek
2
+a
4
kek +a
5
+k
0
jsjkfk:
6197
Adding together all the terms,
_ 
1
 D'
T
f k
1
kek
2
+kfk(jsj +"
\
)
 
a
3
kek
2
+a
6
kek
+a
7
+k
0
jsjkfk

 D'
T
f 
~
k
1
(kek a
8
)
2
+kfkjsj(a
7
+k
0
jsjkfk)
+
a
2
6
kfk
2
jsj
2
4
~
k1
+"
\
kfk(z)a
9
(z);
wherea
9
(z) contains boundedz-terms which are not explic-
itly deﬁned here.
c) Combining 
0
and 
1
: Adding the two summand
derivative terms,

2
_ 
0
+ _ 
1
 D'
T
f 
~
k
1
(kek a
8
)
2
+
 
k
0
+
a
2
6
4
~
k1

kfk
2
jsj
2
+kfkjsja
7
+"
\
kfk(z)a
9
(z)  
2
k
0
s
2
= kfk
2
 
~
k
1
(kek a
8
)
2
 
~
k
0
 
jsj 
kfka7
2
~
k0

2
+
kfk
2
a
2
7
4
~
k0
+"
\
kfk(z)a
9
(z)
 
~
k
1
(kek a
8
)
2
 
kfk
2
~
k0
(
~
k
0
  
3
) +"
\
kfko(z);
where we catch all the bounded functions ofz ino(z). Now
note that (0) = 0, and a linearization yields that for small
kzk,(z)o(kzk). Non-degeneracy of' implies that close
to the goal,kzko(kfk), and including this, the last term
is also o(kfk
2
) and proportional to "
\
.
REFERENCES
[1] F. Chaumette and S. Hutchinson, “Visual servo control. I. Basic
approaches,” IEEE Robotics Automation Magazine, vol. 13, no. 4, pp.
82–90, 2006.
[2] R. Murphy, “Human-robot interaction in rescue robotics,” IEEE Trans-
actions on Systems, Man, and Cybernetics, Part C: Applications and
Reviews, vol. 34, no. 2, pp. 138–153, 2004.
[3] D. Vergano, “Nasa’s spirit robot stuck in sand on mars,” USA Today,
Jan. 2010.
[4] C. Li, P. B. Umbanhowar, H. Komsuoglu, D. E. Koditschek, and D. I.
Goldman, “Sensitive dependence of the motion of a legged robot on
granular media,” Proceedings of the National Academy of Sciences,
vol. 106, no. 9, pp. 3029–3034, 2009.
[5] E. Marchand and F. Chaumette, “Feature tracking for visual servoing
purposes,” Robotics and Autonomous Systems, vol. 52, no. 1, pp. 53–
70, 2005.
[6] E. Olson, “AprilTag: a robust and ﬂexible visual ﬁducial system,”
in Robotics and Automation (ICRA), 2011 IEEE International
Conference on, 2011, pp. 3400–3407.
[7] N. Cowan, J. Weingarten, and D. Koditschek, “Visual servoing
via navigation functions,” IEEE Transactions on Robotics and
Automation, vol. 18, no. 4, pp. 521–533, Aug. 2002.
[8] R. Zhang, P.-S. Tsai, J. Cryer, and M. Shah, “Shape-from-shading: a
survey,” IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, vol. 21, no. 8, pp. 690–706, Aug. 1999.
[9] Y .-M. Wei, L. Kang, B. Yang, and L.-d. Wu, “Applications of
structure from motion: a survey,” Journal of Zhejiang University,
vol. 14, no. 7, pp. 486–494, July 2013.
[10] D. P. Tsakiris, P. Rives, and C. Samson, “Extending visual servoing
techniques to nonholonomic mobile robots,” in The conﬂuence of
vision and control, ser. Lecture Notes in Control and Information
Sciences. Springer London, Jan. 1998, no. 237, pp. 106–117.
[11] A. De and D. E. Koditschek, “Toward dynamical sensor management
for reactive wall-following,” in Proceedings of the 2013 IEEE Inter-
national Conference on Robotics and Automation, May 2013.
[12] K. C. Galloway, G. C. Haynes, B. D. Ilhan, A. M. Johnson, R. Knopf,
G. Lynch, B. Plotnick, M. White, and D. E. Koditschek, “X-RHex: A
highly mobile hexapedal robot for sensorimotor tasks,” University of
Pennsylvania, Tech. Rep., 2010.
[13] U. Saranli, M. Buehler, and D. E. Koditschek, “Rhex: A simple and
highly mobile hexapod robot,” The International Journal of Robotics
Research, vol. 20, no. 7, p. 616, 2001.
[14] G. A. D. Lopes and D. E. Koditschek, “Visual servoing for
nonholonomically constrained three degree of freedom kinematic
systems,” The International Journal of Robotics Research, vol. 26,
no. 7, pp. 715–736, July 2007.
[15] S. Skaff, G. Kantor, D. Maiwand, and A. A. Rizzi, Inertial navigation
and visual line following for a dynamical hexapod robot, 2003,
vol. 2, pp. 1808–1813.
[16] E. Sontag, “Input to state stability: Basic concepts and results,”
Nonlinear and Optimal Control Theory, pp. 163–220, 2008.
[17] D. E. Koditschek, “Adaptive techniques for mechanical systems,”
in Proc. 5th. Yale Workshop on Adaptive Systems, May 1987, pp.
259–265.
[18] D. C. Conner, A. Rizzi, and H. Choset, “Construction and automated
deployment of local potential functions for global robot control and
navigation,” Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-
TR-03-22, Nov. 2003.
[19] S. Revzen, B. Ilhan, and D. Koditschek, “Dynamical trajectory replan-
ning for uncertain environments,” in IEEE Conference on Decision and
Control, 2012, pp. 3476–3483.
[20] F. Bonin-Font, A. Ortiz, and G. Oliver, “Visual navigation for mobile
robots: A survey,” Journal of Intelligent and Robotic Systems, vol. 53,
no. 3, pp. 263–296, Nov. 2008.
[21] A. D. Petiteville, S. Hutchinson, V . Cadenat, and M. Courdesses,
“2D visual servoing for a long range navigation in a cluttered
environment,” in IEEE Conference on Decision and Control and
European Control Conference, 2011, pp. 5677–5682.
[22] R. Brockett, “Asymptotic stability and feedback stabilization,”
Differential geometric control theory, vol. 27, pp. 181–191, 1983.
[23] C. Samson, “Velocity and torque feedback control of a nonholonomic
cart,” in Advanced robot control. Springer, 1991, pp. 125–151.
[24] J. Laumond, Robot motion planning and control, ser. Lecture notes
in control and information sciences. Springer, 1998.
[25] D. Folio and V . Cadenat, “A controller to avoid both occlusions
and obstacles during a vision-based navigation task in a cluttered
environment,” in IEEE Conference on Decision and Control, 2005,
pp. 3898–3903.
[26] J. Hauser, S. Sastry, and P. Kokotovic, “Nonlinear control via approx-
imate input-output linearization: the ball and beam example,” IEEE
Transactions on Automatic Control, vol. 37, no. 3, pp. 392–398, 1992.
[27] D. E. Koditschek and E. Rimon, “Robot navigation functions on
manifolds with boundary,” Advances in Applied Mathematics, vol. 11,
no. 4, pp. 412–442, Dec. 1990.
[28] E. Sontag and Y . Wang, “On characterizations of input-to-state stability
with respect to compact sets,” in IFAC Non-Linear Control Systems
Design Symposium, 1995, pp. 226–231.
[29] D. Eﬁmov, “Global lyapunov analysis of multistable nonlinear sys-
tems,” SIAM Journal on Control and Optimization, vol. 50, no. 5, pp.
3132–3154, Jan. 2012.
[30] H. K. Khalil, Nonlinear systems. Prentice Hall Upper Saddle River,
NJ, 2002.
[31] D. E. Koditschek, “An approach to autonomous robot assembly,”
Robotica, vol. 12, no. 02, pp. 137–155, 1994.
[32] N. Cowan, “Navigation functions on cross product spaces,” IEEE
Transactions on Automatic Control, vol. 52, no. 7, pp. 1297–1302,
2007.
[33] G. C. Haynes, A. A. Rizzi, and D. E. Koditschek, “Multistable
phase regulation for robust steady and transitional legged gaits,” The
International Journal of Robotics Research, 2012.
6198
