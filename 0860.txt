A New Hierarchical Binaural Sound Source Localization Method Based
on Interaural Matching Filter
Hong Liu
1;2
, Jie Zhang
2
and Zhuo Fu
3
Abstract—Binauralsoundsourcelocalizationisanimportant
technique in friendly Human-Robot Interaction (HRI) for its
easy-implementation with only two microphones. This paper
develops a robust method based on a hierarchical probabilistic
model. Reliable frequency sub-bands are used to PHAT???
method in the ﬁrst layer to obtain a priori crude Interaural
Time-Delay (ITD) and the probabilistic distribution of candi-
date azimuths. The second layer utilizes Interaural Intensity
Difference (IID) to reduce matching time and reﬁne candidate
azimuthsaswellaselevations.AnovelfeaturenamedInteraural
Matching Filter (IMF), which can eliminate the difference
between ITDs and IIDs, is proposed in the third layer. The
probability of sound source location is acquired by computing
the similarity between the IMF of received binaural signal and
the IMFs in templates. Finally, combined with the former ITDs
and IIDs, the similarity matrix is used to make a decision of
sound source location based on a Bayes rule. Our innovation
liesinaddingselectingreliablefrequencycomponentsintotime-
delay estimation and foremost taking IMF as a feature of sound
source. Compared with several state-of-the-art algorithms, ex-
perimental results show our approach has a better performance
eveninnoisyenvironmentswithoutincreasingstorages,andalso
has less time complexity.
I. INTRODUCTION
Binaural sound source localization is an important tech-
nique in friendly Human-Robot Interaction (HRI) only by
two microphones as the human auditory localization with
the capability of pinpointing the sound source swiftly and
accurately. There are three important and difﬁcult issues
concerning binaural localization: First, how to accurately
localize any kind of speech or sound source. Second, how
to localize several different sound sources at the same time.
Third, how to track one or several moving sound sources [1].
There are two signiﬁcant binaural (interaural) cues based
on differences in time and level of the sound arriving at two
ears called interaural time difference (ITDs) and interaural
intensity differences (IIDs). Since “Duplex Theory” [2] and
This work is supported by National Natural Science Foundation of
China (NSFC, No. 61340046, 60875050, 60675025), National High
Technology Researchand Development Program of China (863 Pro-
gram, No.2006AA04Z247), Scientiﬁc and Technical Innovation Com-
mission of Shenzhen Municipality (No. JCYJ20120614152234873, CX-
C201104210010A, JCYJ20130331144631730, JCYJ20130331144716089).
1
H. Liu is with Faculty of Key Laboratory of Machine perception and
Intelligence, Peking University, Shenzhen Graduate School, Beijing, 100871
CHINA. hongliu@pku:edu:cn
2
J. Zhang is with the Engineering Lab on Intelligent Perception for
Internet of Things(ELIT), Shenzhen Graduate School of Peking University,
Shenzhen, 518055 CHINA. zhang
?
jie827@163:com
3
Z. Fu has graduated from the Engineering Lab on Intelligent Percep-
tion for Internet of Things(ELIT), Shenzhen Graduate School of Peking
University, Shenzhen, 518055 CHINA.
cochlear model [3] were proposed, a large amount of bin-
aural localization algorithms have been developed in various
experimental environments [4][5].
Most traditional methods are based on ITD or IID and
seldomconsidertheinﬂuenceoneachother[5-7].Intuitively,
with the inﬂuence of ITD, the signals received by two ears
have different starting points with respect to sound source,
which affects the extraction of IID. In addition, these meth-
ods usually match new obtained features with templates to
assurethedirectionofsoundsource,thatis,higherresolution
needs more templates, and more time will be consumed for
overall searching to fulﬁl practical application.
For these reasons, several new algorithms have been
proposed recently. For example, Li et al introduced a Bayes-
Rulebasedthree-layerhierarchicalsystemforbinauralsound
source localization [8]. Along with the similar hierarchical
architectures like Finger et al [9], experiments show that
hierarchical system can reduce time consumption effectively.
Willert et al proposed a biologically inspired and technical-
ly implemented binaural sound localization system, where
binaural cues are extracted using cochleagrams generated by
a cochlear model [10]. Meanwhile, noise and reverberation
woulddegradetheperformanceofsoundlocalization.Jeubet
al proposed an interaural coherence model of room impulse
response (RIR) and dual-channel Wiener ﬁlter for binaural
cues dereverberation preserving [11]. Jacob et al presented
a multichannel widely linear Wiener ﬁltering approach to
binaural noise reduction using a microphone array [12].
Actually, although most algorithms mentioned above may
solve a particular problem well, the time or space complexity
will be a vital limitation in the usage for real sound localiza-
tion systems directly. Willert et al presented two-dimensional
frequency versus time-delay representations of binaural cues,
so-called activity maps, and we have improved this idea
and proposed the concept of Time-Delay Compensation for
binaural localization [13]. However, there is a common
hypothesis that the noises received by two ears are zero-
mean Gaussian, which is not always correct, especially for
thenoisesfromfansofrobotandburstinterferences.Besides,
Time-Delay Compensation assumes that the disparity of
binaural signals is only reﬂected in time-delay and linear
attenuation of energy.
Accordingly, this work foremost proposes IMF as binaural
localization feature in order to avoid considering the type or
content of noises and satisfy the requirements of complexity.
This idea can be applied to eliminate the inﬂuence of time-
delay on IID without making any assumptions. The three-
layer probabilistic model is presented to reduce the matching
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1598
times and improve accuracy.
In details, a modiﬁed spectral weighting function of GCC
named PHAT-?? [14] based on selecting reliable frequency
sub-bands is presented to obtain a priori crude ITDs as well
as the probabilistic distribution of azimuths in the ﬁrst layer,
becausedifferentfrequencybandssharedifferenttime-delays
and reliability for computing ITDs.
In the second layer, IIDs are exploited to reduce the
searching space of azimuth and elevation for the prior
obtained in the ﬁrst stage. Theoretically, different elevation
of the same azimuth should share same IIDs regarding dual-
microphone sound localization, but the regularity of head
leads to discrimination existing, that is to say, IIDs can be
utilizedinthisstagetoreﬁnecandidateazimuthandelevation
fortunately [7][15].
In the last layer, the newly-developed IMF feature for
soundlocalizationisintroduced.Takethesignalofleft(right)
ear as the input of a system and the other as the target signal,
we can design a ﬁlter to eliminate the disparity between
binaural signals and this is what Interaural Matching Filter
means. Once the coefﬁcients of IMF in all directions are
stored, the process of localization is simpliﬁed as calculating
similarity between unknown coefﬁcients and templates.
Indeed, IMF also compensates one signal to the other
ignoring the cues of lags and energy, but considering the
disparity comprehensively. For binaural sound localization,
the noises received by the two ears are usually of the same
type despite of different contents. Therefore, IMF is more
reﬂectiveoftherelationshipbetweenITDandIIDthanTime-
Delay Compensation as well as noisy tolerance.
The rest of this paper is organized as follows: Section II
gives an overlook of this paper. Section III shows the details
of our method. Experiments and discussions are shown in
section IV. At last, the conclusions are drawn in Section V.
II. OVERALL ARCHITECTURE
An overlook of our hierarchical binaural sound source
localizationmethodconsistingofsignalmodelandalgorithm
structure is introduced in this section.
A. Localization Modeling
Let s(n) denote a sound source signal, and the received
signals as x
l
(n) and x
r
(n) on the left and right ears, respec-
tively (see Fig.1(a)):
x
l
(n)=h
l
(?;?;r)?s(n)+n
l
(n)
x
r
(n)=h
r
(?;?;r)?s(n)+n
r
(n)
(1)
where h
l
(?;?;r) and h
r
(?;?;r) are the Head Related Trans-
fer Functions (HRTFs) of the direct paths from source to
the two ears, which heavily rely on azimuth, elevation and
distance. In Eq.(1), n
l
(n) and n
r
(n) are the interferences, and
?;?;r are the azimuth, elevation and distance, respectively.
As with HRI systems, azimuth and elevation represent
the direction of sound source and the distance is neglected.
Besides in CIPIC database of head-related impulse responses
(HRIRs), all sounds used share the same distance of 1m [16].
Therefore, the task of this work is to obtain the azimuth ?
Fig. 1. (a) Signal model of binaural sound localization. For far ﬁeld,
the propagation path of sound source to two eras are thought to be parallel.
(b)Theinteraural-polarcoordinatesystem.Theazimuthistheanglebetween
a vector to the sound source and the midsaggital or vertical median plane,
andvariesfrom?90
o
to+90
o
.Theelevationistheanglefromthehorizontal
plane to the projection of the source into the midsaggital plane, and varies
from?90
o
to +270
o
.
and elevation ? from x
l
(n) and x
r
(n), which are measured
in a head-centered interaural-polar coordinate system (see
Fig.1(b)). Then Eq.(1) can be simpliﬁed as follows:
x
l
(n)=h
l
(?;?)?s(n)+n
l
(n)
x
r
(n)=h
r
(?;?)?s(n)+n
r
(n)
(2)
In the frequency domain, it can be easily obtained:
X
l
(?)=H
l
(?;?;?)S(?)+N
l
(?)
X
r
(?)=H
r
(?;?;?)S(?)+N
r
(?)
(3)
B. Framework
The hierarchical binaural sound localization method con-
sists of four consecutive parts and the framework is shown
in Fig.2 (? is the location of source):
• Interaural Time Dif ference: This is the ﬁrst layer,
which mainly computes a priori crude ITDs of input
signal x
l
(n) and x
r
(n). The ofﬂine training provides the
probabilistic distribution of azimuth p(?
i
) for localiza-
tion. Moreover, selecting reliable frequency sub-bands
is applied to time-delay estimation. The output of this
layer is candidate azimuths.
• Interaural IntensityDif ference:ThispartexploitsIIDs
to reduce the searching space of azimuth and elevation
based on the prior obtained in the ﬁrst layer, which is
a reﬁne unit to achieve a more accurate probabilistic
distribution of azimuth ? and elevation ?. The input
and output are similar to these of former layer.
• Interaural Matching Filter: This unit designs a Inter-
aural Matching Filter (IMF). Taking the signal of left
(right) ear as the input of IMF and the other as the
target signal, with the Minimum Mean Square Error
(MMSE) criterion we can obtain the impulse response
of IMF. The input of this part is impulse response of all
directions and the output is the cosine similarity, that is,
the probability of sound source located.
• Decision?making: A Bayes rule method is used to
make the ﬁnal decision in this stage and the output is
the results of localization.
III. HIERARCHICAL BINAURAL SOUND LOCALIZATION
A. Interaural Time Difference
The average time-delays are illustrated in Fig.3. It can
be easily obtained that different elevations with the same
1599
Fig. 2. Block diagram of this framework. The upper training is an ofﬂine process providing templates for localization.
0
20
40
60
0
10
20
30
?60
?40
?20
0
20
40
elevation
Average time?delay of all 1250 directions
azimuth
Interaural Time?delay
Fig. 3. The average time-delay of all 1250 directions (25 azimuths?
50 elevations) in CIPIC database. The received signals of two ears for
computation are the convolution between HRTFs and a certain sound.
azimuth share the same time-delay or lags. In other words,
whenazimuthisbeingcalculated,theimpactofelevationcan
be ignored. Thereby, in this stage x
l
and x
r
can be expressed
as:
x
l
(n)=h
l
(?)?s(n)+n
l
(n)
x
r
(n)=h
r
(?)?s(n)+n
r
(n)
(4)
Assume that binaural signals are counterparts of sound
source with time-delay and attenuation, it can be attained:
x
l
(n)=a
l
s(n??
l
)+n
l
(n)
x
r
(n)=a
r
s(n??
r
)+n
r
(n)
(5)
where a
l
and a
r
denote the attenuation factors, ?
l
and ?
r
are time factors from the sound source to the two acoustic
sensors, respectively. Deﬁne interaural time-delay ∆? as:
∆? =?
r
??
l
(6)
Withrespecttocalculatingtime-delay∆?,themostwidely
used method is Generalized Cross Correlation (GCC) [17].
Its improved version, a weighting function of GCC named
PHAT???, is used in this paper. The cross-correlation be-
tweenthesignalsreceivedbythetwoearscanberepresented
as follows:
R
x
l
;x
r
(n)=
∫
π
?π
W(?)X
l
(?)X
r
?
(?)e
?j?n
d?
W(?)=
1
|G(?)|
?
+|?
2
(?)|
G(?)=X
l
(?)X
r
?
(?)
(7)
whereW(?) denotes the weighting function used to sharpen
the peak of GCC function, and G(?) is the cross power
spectrum. In Eq.(7), ? is reverberation factor determined
by signal-to-noise ratio (SNR) in environment and ? is a
coherence function. Then the time-delay can be obtained by
maximizing cross-correlation function R
x
l
;x
r
(n),
∆? =argmax
n
R
x
l
;x
r
(n) (8)
In fact, GCC-PHAT does not consider the reliable fre-
quency channels of a certain sound source, but calculates the
time-delay over time instead. However, the signals received
by two ears are not always narrow-band and there is a fact
thatlow-frequencysoundstravelmoreeasilyaroundthehead
and the time differences or lags are not affected by ?.
Therefore, this paper proposes a method for selecting
reliable frequency channels to improve interaural time-delay
estimation. The time-delays of elevation ? =?45
o
in all
azimuths and frequency sub-bands are illustrated in Fig.4,
from which it can be seen that there are only several lower
frequency channels available to compute time-delay.
0
20
40
60
0
10
20
30
?40
?20
0
20
40
60
frequency sub?band
Time?delay of 42 frequency sub?bands
azimuth
time?delay
Fig. 4. The time-delay of frequency sub-band in direction of elevation
? =?45
o
. The original signal is decomposed into 42 frequency channels.
As a consequence, a cochlea model is used to decompose
the original signals received by two ears into K frequency
channels with respective frequency centered by f
c
and band-
width b
c
. The ﬁlterbank achievement is based on equivalent
rectangular bandwidth (ERB)-ﬁlters, where f
c
and b
c
can be
computed by using the Glasberg and Moore parameters [17].
Therefore, Eq.(8) can be rewritten as:
∆?
m
=argmax
n
R
m
x
l
;x
r
(n); m=1;2;···;K (9)
1600
where∆?
m
representsthetime-delayofmfrequencychannel.
Then the ﬁnal time-delay of a certain direction can be
calculated by weighted averaging as:
∆? =
1
k
K
∑
m=1
b
m
∆?
m
(10)
b
m
=
{
0 if channel m is unreliable;
1 if channel m is reliable:
(11)
where k is the number of selected frequency channels used
to evaluate time-delay, and b
m
is a binary mask as the sign
of whether the frequency channel m is reliable and selected.
Then,consideringthegeometricalrelationshipbetweenthe
time-delayandazimuth(seeFig.1(a)),theazimuth? iseasily
evaluated from ? as Eq.(12) shows:
?=sin
?1
(
∆d
d
)
=sin
?1
(
∆?c
df
s
)
(12)
where c is the speed of sound in the air (344 m/s), ∆d is
the distance difference from the sound source to two ears, d
is the distance between the two ears, and f
s
is the sampling
frequency. As with CIPIC database, the azimuth is divided
into 25 intervals centered by?80
o
;?65
o
;?55
o
;?45
o
:5
o
:
45
o
; 55
o
; 65
o
; 80
o
. When a new source appears, its azimuth
will be calculated and denoted the nearest center azimuth
?. Due to the existence of noises in the environment and
the possibility of error in calculation, azimuth localization
inaccuracy may happen [13].
Since each time-delay corresponds to an only azimuth ?
i
,
so the probability of azimuth ?
i
by P(?
i
|?) can be trained
andstoredbeforelocalization.Therefore,whenthecandidate
azimuth ?
i
is obtained in this layer, all possible ? are tested
for each ?. Then the mean value ?
i
and standard deviation
?
i
are obtained. The probability of azimuth ?
i
and available
interval are shown as:
P(?
i
|?)=P(?
i
|∆?)?N(?
i
;?
2
i
)
∆??(?3?
i
+?
i
;3?
i
+?
i
) when ? =?
i
(13)
0 5 10 15 20 25
0
5
10
15
20
25
theoretical azimuth
evaluated azimuth
Result of azimuth in the first stage
 
 
Fig. 5. The evaluated result of azimuth in the ﬁrst stage. The solid dots
represent the possible azimuths, for example, if the actual azimuth ? is
?80
o
, the evaluated azimuths ? are?80
o
;?65
o
;?55
o
;?45
o
;?40
o
;?35
o
with different possibilities, and the possibility at ? =?80
o
is maximum..
Accordingly, a crude result in this stage can be obtained
and shown in Fig.5, which is the so-called candidate az-
imuths for the following operations.
B. Interaural Intensity Difference
Undertheidealconditionandignoringthebackgroundand
reverberation noises, the energy spectrums of the received
signals at two ears are:
E
l
(?)=X
l
(?)
2
=S(?)|H
l
(?)|
2
E
r
(?)=X
r
(?)
2
=S(?)|H
r
(?)|
2
(14)
From an engineering point of view and in intensity domain,
the relation between the sound source and HRTFs in Eq.(14)
will be simpliﬁed as addition [8]. Hereby the logarithmic
energy intensities are formulated as:
I
l
(?)=10logE
l
(?)=10logS(?)+20log|H
l
(?)|
I
r
(?)=10logE
r
(?)=10logS(?)+20log|H
r
(?)|
(15)
Then, interaural intensity difference can be deﬁned as:
∆I(?)=I
l
(?)?I
r
(?)
=20log|H
l
(?)|?20log|H
r
(?)|
=20log
|H
l
(?)|
|H
r
(?)|
(16)
Hence, sound source is irrelevant to IIDs, which only relys
on HRTFs instead. For a certain subject, the distribution of
IIDs is stationary that can be trained before localization.
As for dual-microphone localization, according to the so-
called inverse-square-law [7], the IIDs vary from radius. But
reﬂection, diffraction and the regularity of head make IIDs
vary from azimuths and elevations, which means IIDs can be
utilized to reﬁne the candidate azimuth and elevation. The
IIDs of all 1250 directions are illustrated in Fig.6 and they
arestoredintemplates.Itcanbeseenthatdifferentdirections
share different IIDs in the gross.
0
20
40
60
0
10
20
30
?20
?15
?10
?5
0
5
10
elevation
Interaural Intensity Difference of all 1250 directions
azimuth
Interaural Intensity Difference
Fig. 6. Interaural Intensity Difference of all 1250 directions. Here IIDs
denote the feature of original signals without frequency division.
Here based on the result of ﬁrst layer, candidate elevations
can be assured by matching the IID of undeﬁned location
with IIDs within templates using Bayes rule expressed as:
P(?|?
i
)=
P(?
i
;?)
P(?
i
)
(17)
where p(?
i
) is obtained in the previous layer. Similarly, let
iid
j
and ?
j
denote the mean value and standard deviation
respectively when the candidate azimuth is ?
i
. Then the
1601
probability of elevation ? and available interval of iid are
shown as follow:
P(?
i
;?)=P(?
i
;iid)?N(iid
j
;?
2
j
)
iid?(?3?
j
+iid
j
;3?
j
+iid
j
) when (?;?)=(?
i
;?
j
)
(18)
C. Interaural Matching Filter
In order to eliminate the disparity between binaural signal
x
l
(n);x
r
(n), it is easy to compose an Interaural Matching
Filter (IMF) [19][20] shown in Fig.7, whose task is to press
y(n) on towards x
r
(n):
Fig. 7. Linear discrete time Interaural Matching Filter, which includes
delay and attenuation. Taking x
l
(n) as the input of IMF and x
r
(n) as the
expectation is equivalent to the contrast situation in theory.
Let w w w = [w
0
;w
1
;:::;w
M?1
] be the impulse response of
IMF, and the frame length of x
l
(n);x
r
(n) is M, the output of
IMF is expressed as:
y(n)=
M?1
∑
i=0
w
?
i
x
l
(n?i); n=0;1;:::;M (19)
where? denotes conjugate. Since the estimation of expec-
tation response x
r
(n) always has error, thus deﬁne the error
function as:
e(n)=x
r
(n)?y(n) (20)
At the same time, the cost function is deﬁned as follow:
J(n)=E{|e(n)|
2
}=E{e(n)e
?
(n)} (21)
where E is the expectation operator. Here considering the
MMSE criterion to solve the vector w w w, we can obtain the
Wiener-Hopf equation:
∞
∑
i=0
w
i
R
x
l
;x
l
(i?k)=R
x
l
;x
r
(?k); k=0;1;:::;M?1 (22)
whereR
x
l
;x
l
istheautocorrelationofx
l
andR
x
l
;x
r
isthecross-
correlationfunctioncalculatedintheﬁrstlayeralready.Ifthe
signal received by left ear is set as:
x x x
l
(n)=[x
l
(n);x
l
(n?1);:::;x
l
(n?M+1)]
T
(23)
Then the autocorrelation matrix of x
l
(n) is:
R R R={x x x
l
(n)x x x
H
l
(n)}
=
?
?
?
?
?
R
x
l
;x
l
(0) R
x
l
;x
l
(1) ::: R
x
l
;x
l
(M?1)
R
?
x
l
;x
l
(1) R
x
l
;x
l
(0) ::: R
x
l
;x
l
(M?2)
.
.
.
.
.
.
.
.
.
.
.
.
R
?
x
l
;x
l
(M?1) R
?
x
l
;x
l
(M?2) ::: R
x
l
;x
l
(0)
?
?
?
?
?
(24)
Similarly, the cross-correlation vector between input x x x
l
and
expectation response x
r
(n) is calculated as:
r r r =E{x x x
l
(n)x
?
r
(n)}
=[R
x
l
;x
r
(0);R
x
l
;x
r
(?1);:::;R
x
l
;x
r
(?M+1)]
(25)
Therefore, the vector of IMF coefﬁcients can be formulat-
ed as:
w w w=R R R
?1
r r r (26)
So far, the impulse response w w w in all directions have been
obtained, which includes the information of ITDs and IIDs,
and our next goal is to use it for the ﬁnal localization.
Here use cosine similarity between two IMFs to describe
the probability of sound source location like:
?
w w w
1
w w w
2
=
<w w w
1
;w w w
2
>
?w w w
1
??w w w
2
?
(27)
where <;>;?? denote the inner product of vectors and 2nd
order norm. If w w w
1
is the coefﬁcients from templates and w w w
2
is the coefﬁcients of received sound, ?
w w w
1
w w w
2
can bewrite the
similarity between the two. The mean value and standard
deviation of estimation error e(n) in all 1250 directions can
be quantized and shown in Fig.8.
0
20
40
60
0
10
20
30
0
50
100
mean value of e(n)
0
20
40
60
0
10
20
30
0
0.5
1
elevation
standard deviation of e(n) 
azimuth
(b)
(a)
Fig. 8. The mean value and standard deviation of estimation error of
IMF (The two sub-ﬁgures (a) and (b) depict the mean value and standard
deviation, respectively.).
A similarity matrix ?
w w w
1
w w w
2
is illustrated in Fig.9,
when the sound source is located in the direction of
? =?45
o
;? =5:625
o
. It can be seen the coordination of
the brightest dot is (10;5) enclosed by a circle, where the
cosine similarity is largest.
Similarity distribution between templates and candidate IMF
elevation
azimuth
 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
15
5
10
20
25
10 20 30 40 50
Fig. 9. The similarity matrix when the sound source is located in the
direction of? =?45
o
;? =5:625
o
. The more deeper of the pixel, the bigger
thesimilarityis.Itcanbeseenthecoordinationofthebrightestdotis (10;5)
enclosed by a circle. The dots of similarity less than zero are set black.
Therefore, the cosine similarity can be used to represent
the probability of sound source location. Based on the results
of the previous two layers, what we need to do is to calculate
1602
the similarity between the IMFs of candidate directions in
templates with the IMF of received signals as the following
equation shows:
P(?
ij
|?
i
;?
j
)=
P(?
i
;?
j
;?
ij
)
P(?
i
;?
j
)
=argmax
?
ij
?
w w w
ij
w w w
template
|
(?
i
;?
j
)
(28)
D. Decision-Making
Once templates including ITDs, IIDs and IMFs are stored,
the process of localization is simpliﬁed as matching candi-
date features with templates and Algorithm 1 illustrates the
procedures of localization. Here, the Bayes rule is used to
make the ﬁnal decision expressed mathematically as (Let ?
denotes the sound source location):
? =argmax
?
P(?|?;?;?)
=argmax
?
P(?|?)P(?|?;?)P(?|?;?;?)P(?)
(29)
where P(?|?);P(?|?;?);P(?|?;?;?) are obtained in the
previous layers, and P(?) is the prior information same to
all locations.
Algorithm 1: Hierarchical Binaural Sound Localization
Input: x
l
(n), x
r
(n)
Output: azimuth ?, elevation ?
1 Templates: ?
i
, ?
i
, iid
j
, ?
j
, IMFs ;
2 while x
l
(n), x
r
(n) available do
3 decompose x
l
(n);x
r
(n) into K frequency bands;
4 for m?1 to K do
5 ∆?
m
?argmax
n
R
m
x
l
;x
r
(n) ;
6 if frequency band m is reliable then
7 b
m
?1 ;
8 end
9 end
10 ∆??
1
k
∑
K
m=1
b
m
∆?
m
;
11 if ∆??(?3?
i
+?
i
;3?
i
+?
i
) then
12 ?
i
?arcsin(
?
i
c
df
s
) ;
13 candidate azimuths??
i
;
14 P(?
i
)?N(?
i
;?
2
i
)|
∆?
;
15 end
16 while ?
i
exists do
17 iid?I
l
(?)?I
r
(?) ;
18 if iid?(?3?
j
+iid
j
;3?
j
+iid
j
) then
19 transform iid
j
into elevation ?
j
;
20 candidate elevations??
j
;
21 P(?
j
|?
i
)?N(iid
j
;?
2
j
)|
(∆?;iid)
;
22 end
23 while (?
i
;?
j
) exist do
24 w w w
ij
?R R R
?1
x
l
;x
l
r r r
x
l
;x
r
;
25 ?
ij
?
<w w w
ij
;I I IM M MF F F
template
>
?w w w
ij
??I I IM M MF F F
template
?
;
26 end
27 end
28 (?;?)?argmax
(?;?)
P(?)P(?|?)P(TDC|?;?) ;
29 return (?;?)
30 end
IV. EXPERIMENTS AND DISCUSSIONS
The CIPIC database used in our experiments is measured
bytheU.C.DavisCIPICInterfaceLaboratory,whichincludes
head-related impulse responses (HRIRs) for 45 different
subjects (including 27 males, 16 females, and KENAR with
large and small pinna). The HRIRs are tested at 1m distance
with 25 different azimuths, 50 different elevations, totally
1250directionsforeachsubject.Thesoundsourceismusical
signal. The period of each sound for training and localization
is 2 seconds and the sampling frequency is 44.1kHz. The
result will be compared with Hierarchical System [8], Online
Calibration [9] and Time-Delay Compensation (TDC) [13],
and this method is IMF for short.
In this paper, the results for test sets are based on different
signal parts at 45?1250?100?128?5, which means 45
subjects, 1250 directions, 100 sound signals and 5 sound
activities processed over 128 sample points, which is also
the length of window. This algorithm of IMF is validated in
different SNRs and with several different sound activities.
A. Azimuth
The accuracy of azimuth ? is shown in TABLE I. It can
be seen that in noisy environment our method reaches the
highest performance among the four methods clearly.
Speciﬁc say, the performances among these four algo-
rithms have small gaps with each other in the experimental
environment without noise. All the accuracies are over 89%
with the error tolerance 0
o
, and over 99% with the error
tolerance 10
o
, that is, these four algorithms have satisﬁed the
real requirement in quite environments. It can be found that
Hierarchical System has the best performance while Online
Calibrationhastheworstperformance,whichisprobablydue
tothedifferentcuesusedindifferentalgorithms,forexample
Hierarchical System utilizes the spectral differential cues.
0 10 20 30 40
40
50
60
70
80
90
100
SNR/dB
Accuracy/%
Localization accuracy of azimuth in different SNRs
 
 
IMF
TDC
Online Calibration
Hierachical System
Fig. 10. Localization accuracy of azimuth in different SNRs with 5
o
tolerance.
A comparison of the four algorithms with 5
o
tolerance in
different SNRs is illustrated in Fig.10. It can be acquired our
algorithm has a considerable superiority to the other three in
noisy environments. With the effect of noise, our algorithm
can extract more accurate time-delay because selecting re-
liable frequency sub-bands is used for PHAT???, which
can reject unreliable frequency components to obtain a more
robust time-delay.
1603
TABLE I
THE ACCURACY OF? IN DIFFERENT SNRS
SNR Environment without noise 20dB 10dB
Tolerance =0
o
≤5
o
≤10
o
=0
o
≤5
o
≤10
o
=0
o
≤5
o
≤10
o
ICTDC 93.16% 98.52% 99.56% 88.32% 98.20% 99.13% 70.24% 84.96% 92.72%
TDC 90.28% 98.48% 99.84% 87.56% 97.56% 98.96% 62.64% 78.43% 83.04%
Online Calibration 89.12% 96.76% 99.24% 84.26% 95.92% 98.24% 58.94% 67.52% 75.23%
Hierarchical System 93.90% 98.70% 99.87% 85.64% 97.21% 98.72% 63.64% 79.50% 84.13%
TABLE II
THE ACCURACY OF? IN DIFFERENT SNRS
SNR Environment without noise 20dB 10dB
Tolerance =0
o
≤5:625
o
≤11:25
o
=0
o
≤5:625
o
≤11:25
o
=0
o
≤5:625
o
≤11:25
o
ICTDC 83.48% 91.32% 94.80% 52.56% 71.44% 78.08% 28.88% 46.96% 55.44%
TDC 70.48% 92.13% 94.65% 20.38% 45.96% 61.05% 10.56% 26.21% 35.98%
Online Calibration 63.61% 90.25% 95.82% 17.22% 39.49% 56.92% 9.53% 20.84% 29.16%
Hierarchical System 64.77% 92.47% 95.23% 20.34% 43.62% 60.92% 10.73% 25.46% 32.10%
B. Elevation
In our experiments, the localization space is divided into
50 elevations ranging from?45
o
to +230:625
o
in steps of
5:625
o
. The localization accuracy of elevation ? is shown
in TABLE II. Obviously, IMF has acquired the best result
compared to the other three algorithms.
In the environments without noise, though the accuracy
of IMF is highest, the others do not lag far behind. If
the tolerance is more than 5:625
o
, all methods can achieve
more than 90% satisfying the fundamental system roughly.
However, in noisy environments the accuracy of other three
algorithms drops rapidly and our method has an obvious
advantage. While with no tolerance, the accuracy of IMF
has even achieved more than two times of other three’s.
The main reason lies in that ITD offers little help
for elevation localization, which depends crucially on I-
ID algorithm. Among these four algorithms, IMF, TDC,
and Hierarchical System have considered the inﬂuence
of IID, so they have obtained a more better result than
Online Calibration. TDC has considered the relation be-
tween ITD and IID, which makes it more robust than
Hierarchical System. Compared with TDC, IMF has not
assumed that the disparity of binaural signals is only re-
ﬂected in time-delay and linear attenuation of energy, but
considering overall, which makes it have more universality.
The performance of the four algorithms in different S-
NRs with 5:625
o
tolerance is illustrated in Fig.11. It can
be seen that in different noisy environments, TDC and
Hierarchical System have little disparity in accuracy, but
IMF has achieved a visible localization accuracy, because
apart from the layer of Interaural Intensity Dif ference,
IMF can be effectively extracted for localization whatever
the type of noises, which is important to the estimation of
elevation. Accordingly, as localization feature IMF is more
robust than spectral cues proposed in Hierarchical System.
C. Sound Activity Localization
Inordertoverifytheeffectivenessanduniversalityforreal
sound localization system, ﬁve different sound activities are
expected to evaluate the IMF method in this paper. The ﬁve
0 10 20 30 40
0
20
40
60
80
100
Localization accuracy of elevation in different SNRs
SNR/dB
Accuracy/%
 
 
IMF
TDC
Online Calibration
Hierachical System
Fig. 11. Localization accuracy of elevation in different SNRs with 5:625
o
tolerance.
TABLE III
THE LOCALIZATION ACCURACY OF DIFFERENT SOUND ACTIVITIES
direction Azimuth Elevation
Tolerance =0
o
≤10
o
=0
o
≤11:25
o
clapping 86.24% 94.24% 80.81% 93.24%
knocking 93.12% 97.44% 84.88% 94.16%
telephone 90.72% 98.40% 79.44% 96.08%
screaming 88.04% 97.44% 73.36% 94.88%
smashing 85.60% 96.08% 66.32% 89.92%
sound activities are all very common in life, including clap-
pinghands,knockingonadoor,telephoneringing,screaming
and glass smashing, which are recorded in quite ofﬁcial
environment. Table III shows the localization accuracy. It
can be seen that these activities are well localized for both
azimuth and elevation, for example, when the tolerance is
0
o
, the accuracy of azimuth can achieve more over 85:6%.
However, it’s worth noting that the accuracies of glass
smashing are slightly lower than the others’. This phe-
nomenon greatly depends on the sounding principle, because
glass smashing has blank sounding time slot and the energy
mainly distributes in high frequency channels, which make
selecting reliable frequency channels more difﬁcult, but the
localization results are fully suitable for HRI systems.
D. Complexity Analysis
Let N
a
;N
e
and N
c
denote the number of azimuth, elevation
and the channels of ﬁlterbank (approximately equivalent to
1604
TABLE IV
THE SPACE COMPLEXITY OF THE FOUR ALGORITHMS
space storage order
IMF N
a
N
e
N
c
+2N
a
N
e
O(N
a
N
e
N
c
)
TDC N
a
N
e
N
c
+N
a
N
e
O(N
a
N
e
N
c
)
Online Calibration N
a
N
e
N
c
+N
a
N
e
O(N
a
N
e
N
c
)
Hierarchical System N
a
N
e
N
c
+2N
a
N
e
O(N
a
N
e
N
c
)
TABLE V
THE TIME COMPLEXITY OF THE FOUR ALGORITHMS
time times of comparison order
IMF N
a
+N
a
(N
e
+N
c
) O(N
a
N
e
)
TDC N
a
+N
a
N
e
N
c
O(N
a
N
e
N
c
)
Online Calibration N
a
+N
a
N
e
+N
a
N
e
N
c
O(N
a
N
e
N
c
)
Hierarchical System N
a
+N
a
(N
e
+N
c
) O(N
a
N
e
)
the order of ﬁlter), respectively. Considering the algorithms
mentioned above deﬁnitely concluding the process of train-
ing, and the templates of ITD, IID should be stored before
localization.
The space complexity of the four algorithms is shown
in TABLE IV. It can be observed that the storage of both
IMF and Hierarchical System is N
a
N
e
bigger than TDC or
Online Calibration, for IMF needs to store the information
ofIIDsinalldirections,sodoesHierarchical System.Butall
the orders of storage of the four algorithms are O(N
a
N
e
N
c
),
which means the space complexity is acceptable.
When taking the comparison as the basic operation, the
time complexity of the four algorithms are shown as TABLE
V. It is clearly to be seen that IMF and Hierarchical System
have achieved the lowest time complexity than the other
two algorithms, because these two methods have the similar
matchingmechanism,withwhichthepreviouslayerprovides
candidates for the following layers. Compared with IDC,
IMF adds a layer of IID, which can effectively reduce
the matching times of elevation. That’s why IMF and
Hierarchical System achieve the best time complexity.
V. CONCLUSIONS
In this paper, a new Hierarchical binaural sound localiza-
tion method based on Interaural Matching Filter has been
proposed. Our algorithm utilizes ITD, IID and IMF as
the features of sound source. As for time-delay estimation,
binaural signals possess prodigious sparsity and selecting
reliable frequency components is necessary to improve ac-
curacy. Compared with ITD, IID has little regularity with
directions, which means it can be used to reduce searching
space and it’s successfully used in this work. The newly-
proposed IMF has not made any assumptions of the type
of noise and relationship between binaural signals reﬂected
in simple time-delay and attenuation, and experiments show
that IMF is more robust than traditional features. At last,
we would like to emphasize that our method has achieved
a favourable localization accuracy for both azimuth and
elevation, especially in the noisy environments, and needs
less time complexity without requiring more storages for
templates. So to speak, we have provided a more appropriate
choice for real HRI systems. Our future work may be
fastened on studying the factors inﬂuencing the accuracy of
IMF.
REFERENCES
[1] N. Roman and D. Wang, “Binaural tracking of multiple moving
sources”, IEEE Transactions on Audio, Speech, and Language Pro-
cessing (TASL), vol.16, no.4, pp.728-739, May. 2008.
[2] L. A. Jeffress, “A place theory of sound localization”, Journal of
comparative and physiological psychology, vol.61, pp.468-486, 1948.
[3] R. F. Lyon, and C. Mead, “An analog electronic cochlea”, IEEE
Transactions on Acoustics, Speech and Signal Processing (ASSP),
vol.36, pp.1119-1134, 1988.
[4] H. Liu and X. F. Li, “Time Delay Estimation for Speech Signal Based
on FOC-Spectrum”, in Proceedings of International Conference on
INTERSPEECH, Portland, USA, pp.1732-1735, 2012.
[5] X. F. Li, H. Liu and X. S. Yang, “Sound Source Localization for
Mobile Robot Based on Time Difference Feature and Space Grid
Matching”, in Proceedings of IEEE/RSJ International Conference
on Robotics and Systems (IROS), San Francisco, California, USA,
pp.2879-2886, 2011.
[6] M. D. Gillette and H. F. Silverman, “A linear closed-form algorithm
for source localization from time-differences of arrival”, IEEE Signal
Processing Letters, vol.15, pp.1-4, 2008.
[7] W. Cui, Z. Cao and J. Wei, “Dual-microphone source location method
in 2-D space”, in Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP’06), vol.4, pp.845-
848, May. 2006.
[8] D. Li and S. E. Levinson, “A bayes-rule based hierarchical system
for binaural sound source localization”, in Proceedings of IEEE
International Conference on Acoustics, Speech, and Signal Processing
(ICASSP’03), vol.5, pp.521-524, Apr. 2003.
[9] H. Finger and Paul Ruvolo, “Approaches and Databases for Online
Calibration of Binaural Sound Localization for Robotic Heads”, in
Proceedings of IEEE/RSJ International Conference on Intelligent
Robotics and Systems (IROS’10), pp.4340-4345, Oct. 2010.
[10] V. Willert, J. Eggert, J. Adamy, R. Stahl, and E. Korner, “A proba-
bilistic model for binaural sound localization”, IEEE Transaction on
Systems, Man, and Cybernetics, Part B: vol.36, no.5, pp.982-994, Oct.
2006.
[11] M. Jeub, M. Schafer, T. Esch and P. Vary, “Model-Based Dereverbera-
tion Preserving Binaural Cues”, IEEE Transactions on Audio, Speech,
and Language Processing (TASL), vol.18, pp.1732-1745, Sept. 2010.
[12] J. Benesty and J. D. Chen, “A Multichannel Widely Linear Approach
to Binaural Noise Reduction Using an Array of Microphones”, in
Proceedings of IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp.313-316, 2012.
[13] H. Liu, Z. Fu and X. F. Li, “A Two-Layer Probabilistic Model
Based on Time-Delay Compensation Binaural Sound Localization”,
in Proceedings of IEEE International Conference on Robotics and
Automation (ICRA), pp.2690-2697, May. 2013.
[14] H.LiuandM.Shen,“ContinuousSoundSourceLocalizationbasedon
Microphone Array for Mobile Robots”, in Proceedings of IEEE/RSJ
International Conference on Intelligent Robotics and Systems (IROS),
pp. 4332-4339, Oct. 2010.
[15] B. G. Shinn-Cunningham, S. Santarelli and N. Kopco, “Tori of
confusion: Binaural localization cues for sources within reach of a
listener”, The Journal of the Acoustical Society of America, vol.107,
no.3, pp.1627-636, March. 2000.
[16] V. Algazi, R. Duda, D. Thompson, and C. Avendano, “The CIPIC
HRTF database”, IEEE Workshop on Applications of Signal Process-
ing to Audio and Acoustics, Mohonk, New York, pp.99-102, Oct.
2001.
[17] C. Knapp and G. Carter, “The generalized correlation method for
estimation of time delay”, IEEE Transactions on Acoustics, Speech
and Signal Processing (ASSP), vol.24(4), pp.320-327, 1976.
[18] B. Glasberg and B. Moore, “Derivation of auditory ﬁlter shapes from
notched noise data”, Hearing research, vol.47(1), pp.103-138, 1990.
[19] A. Hyv¨ arinen and E. Oja, “Independent component analysis: algo-
rithms and applications”, Neural networks, vol.13(4), pp.411-430,
2000.
[20] S. S. Haykin, “Adaptive Filter Theory”, 4/e[M], Pearson Education
India, 2005.
1605
