Extrinsic Calibration of 2D Laser Sensors
Dong-Geol Choi, Yunsu Bok, Jun-Sik Kim and In So Kweon
Abstract—This paper describes a new methodology for esti-
mating a relative pose of two 2D laser sensors. Two dimensional
laser scan points do not have enough feature information for
motion tracking. For this reason, additional image sensors or
artiﬁcial landmarks have been used to ﬁnd a relative pose.
We propose the method to estimate a relative pose of 2D laser
sensors without any additional sensor or artiﬁcial landmark. By
scanning two orthogonal planes, we utilize only the coplanarity
of the scan points on each plane and the orthogonality of the
plane normals. Experiments with both synthetic and real data
show the validity of the proposed method. To the best of our
knowledge this works provides the ﬁrst solution for the problem.
I. INTRODUCTION
A laser range sensor has been one of the most important
sensors in many robotic applications, mainly due to its
accuracy and robustness at measuring distances. Wolf et al.
[1] showed its usefulness for simultaneous localization and
mapping (SLAM) successfully using one laser scanner in a
2D environment. With the evolution of robotic applications
from two-dimensions to three-dimensions, demands to use
range information in three dimensional applications have
been growing. Many works [2] [3] [4] has been presented 6D
SLAM using a camera and a laser range ﬁnder in an outdoor
environment. An unmanned autonomous vehicle is usually
equipped extraordinarily many sensors for such tasks. Thrun
et al. [5] [6] successfully achieved important tasks such as
vehicle motion estimation and target detection using several
laser sensors, cameras and 3D sensors in the 2005 DARPA
Grand Challenge and in the 2007 Urban Challenge. Bok et
al. [7] showed large scale 3D reconstruction in an outdoor
environment using 2 laser sensors and 2 cameras.
In these systems using multiple sensors, registration or ex-
trinsic calibration of those sensors is critical, and thus, many
efforts have been made for accurate registration between
various sensors. However, there have been surprisingly few
works on registering multiple laser range sensors. Because
of the difﬁculties in searching for correspondences between
scan data captured by moving laser range sensors, most of
the previous approaches have utilized a camera attached to
This work was supported by the National Research Foundation of
Korea(NRF) grant funded by the Korea government(MSIP) (No. 2010-
0028680) and the National Research Foundation, Korea, under the NRF-
ANR joint research program (No. 2011-0031920).
D.-G. Choi is a Ph.D. student of the Robotics Program, KAIST, Daejeon,
Korea dgchoi@rcv.kaist.ac.kr
Y. Bok is a research assistant professor of Division of Future Vehicle,
KAIST, Daejeon, Korea ysbok@rcv.kaist.ac.kr
J.-S. Kim is with Interaction and Robotics Research Center, Korea
Institute of Science and Technology, Hwarangno 14-gil 5, Seongbuk-gu,
Seoul, Korea junsik.kim@kist.re.kr
I. S. Kweon is a professor of Department of Electrical Engineering,
KAIST, Daejeon, Korea iskweon@kaist.ac.kr
the set of laser scanners. Zhang et al. [8] used image sensors
to calculate camera-laser extrinsic parameters when a planar
calibration pattern was posed in the views of both the camera
and the laser range sensor. Geo et al. and Underwood et
al. [9] [10] proposed extrinsic calibration methods using
artiﬁcial landmarks. The artiﬁcial landmarks with different
reﬂectance are attached at known positions so that a user
can determine the exact pose between the laser sensors and
the mobile platform. Maddern et al. [11] proposed extrinsic
calibration between 2D and 3D LIDAR sensors with respect
to a vehicle base frame. Levinson et al. [12] used surface
normals generated from local neighborhoods to ﬁnd extrinsic
and intrinsic calibration parameters for a Velodyne HDL-64E
on a moving vehicle platform. All of these methods require
complicated steps for calibration (eg. laser-camera-laser), or
specially designed landmarks for laser-laser calibration. A
3D laser scanner is too expensive to be easily used for
calibration.
We propose a new method for registering two laser
scanners without any extra sensor or known motion infor-
mation. The proposed method uses only scan data on two
orthogonal planes. At each scan, each plane has two scan
lines from the two laser scanners, and the coplanarity of
the scan points provides a linear constraint to an unknown
vector composed by the extrinsic parameters. The extrinsic
parameters are extracted from the unknown vector using
orthogonality between the two planes. Finally, we apply a
nonlinear optimization based on those two scene constraints,
coplanarity and orthogonality, to reﬁne the solution.
II. EXTRINSIC CALIBRATION
OF TWO SINGLE LINE LASER SENSORS
This section provides how to solve for the extrinsic param-
eters between two 2D laser sensors. We will derive a linear
constraint on the relative pose between two laser sensors, and
explain how to extract physically meaningful relative poses
from the linearly constrained solutions. Reﬁnement of the
solution with nonlinear optimization will also be discussed
in this section.
Assume that we have two laser range ﬁnders L
1
and L
2
,
and a plane ?
1
as shown in Fig. 1 (b). Each laser scan plane
intersects ?
1
and for L
1
, there are two points q
1
and q
2
deﬁning the intersecting line in 3D. Similarly, for L
2
,we
can deﬁne two points q
3
and q
4
. Note that all the points
are represented in a common coordinate system, which is
the coordinate system of L
1
. The points represented in the
coordinate system of L
1
and those in L
2
coordinate system
will be denoted byq
n
=[x
n
y
n
z
n
]

andq

n
=[x

n
y

n
z

n
]

,
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3027
(a) System overview (b) Coplanarity (c) Orthogonality
Fig. 1: Proposed calibration setup and constraints for two laser sensors using two planes. (a) At each scan, each plane has
two scan lines from the two laser scanners. (b) The coplanarity of the scan points provides a linear constraint to the unknown
vector composed by the extrinsic parameters. (c) The extrinsic parameters are extracted from the linear system solution by
using orthogonality between the two planes.
respectively. They can be transformed as
q
n
= Rq

n
+t. (1)
where R and t are a rotation matrix and a translation
vector that transforms points in L
2
coordinate to L
1
co-
ordinate. The element of R at the i-th row and the j-
th column is denoted by r
ij
, and let t =[t
x
t
y
t
z
]

.
Difference between two points will also be denoted by
?q
ij
= q
i
?q
j
=[?x
ij
?y
ij
?z
ij
]

and ?q

ij
= q

i
?q

j
=
[?x

ij
?y

ij
?z

ij
]

.
Fig. 1 (a) shows the overview of the proposed calibration
method. We use the coplanarity and orthogonality between
the two orthogonal planes to estimate extrinsic parameters.
A. Linear constraint using coplanarity
The normal vector n
?
of the plane ?
1
is represented with
three points q
1
, q
2
and q
3
as
n
?
? n
1
=(q
2
?q
1
)?(q
3
?q
1
) (2)
and with q
1
, q
2
and q
4
,
n
?
? n
2
=(q
2
?q
1
)?(q
4
?q
1
). (3)
Those two normal vectors n
1
and n
2
should be parallel, and
it gives
((q
2
?q
1
)?(q
3
?q
1
))?((q
2
?q
1
)?(q
4
?q
1
)) = 0. (4)
The fact that ?q
21
=(q
2
?q
1
) =0 makes (4) as
?q
21
·((q
3
?q
1
)?(q
4
?q
1
)) = 0, (5)
which means that (q
3
? q
1
) ? (q
4
? q
1
) is orthogonal to
?q
21
, geometrically.
Expansion of the second part (q
3
?q
1
)?(q
4
?q
1
) is
q
3
?q
4
?q
3
?q
1
?q
1
?q
4
+q
1
?q
1
. (6)
The point q
3
is equal to Rq

3
+t, where q

3
is represented
in the L
2
local coordinate system as mentioned above. The
ﬁrst term is expanded as
q
3
?q
4
=(Rq

3
+t)?(Rq

4
+t)
= R(q

3
?q

4
)+t?R?q

43
(7)
The very next two terms are
?q
3
?q
1
?q
1
?q
4
= ?q
1
?R?q

43
. (8)
The last term q
1
? q
1
diminishes obviously, and (5)
becomes,
?q
21
·(R(q

3
?q

4
)+(t?q
1
)?R?q

43
)=0 (9)
Equation (9) is expanded as
? ? ? ? ? ? ? ? ? ? ? ? ?x
21
(z

3
x

4
?x

3
z

4
)
?x

43
(z
1
x
2
?x
1
z
2
)
?z

43
(z
1
x
2
?x
1
z
2
)
?z
21
(z

3
x

4
?x

3
z

4
)
?x
21
?x

43
?x
21
?z

43
?z
21
?x

43
?z
21
?z

43
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? r
12
r
21
r
23
r
32
r
31
t
y
?r
21
t
z
r
33
t
y
?r
23
t
z
r
21
t
x
?r
11
t
y
r
23
t
x
?r
13
t
y
? ? ? ? ? ? ? ? ? ? ? ? =0,
(10)
which is a linear constraint equation on the extrinsic param-
eters R and t. By stacking measurement vectors, a linear
system Ax = 0 is built.
At least seven frames (sets of concurrent scan data) are
required to solve the equation via singular value decomposi-
tion (SVD). Let v be the right singular vector corresponding
to the smallest singular value, and let v
i
be the i-th element
of v. The solution v is obtained up to scale: the ﬁnal solution
is ?v with an unknown scale factor ?. Now, the remaining
problems are two-folds: 1) determining the true scale ?, and
2) extracting the valid relative transformation R and t from
the vector ?v.
3028
B. Pose parameters from the least square solution
The solution vector?v is composed of the pose parameters
R andt as shown in (10), and we extract the parameters from
v and from additional constraints. From the orthonormality
of R, r
2
12
+r
2
22
+r
2
32
=1 and it gives the following equations
from the form of the vector ?v.
r
22
(?)= ±

1??
2
k
0
(11)
v
2
1
+v
2
4
= v
2
2
+v
2
3
(12)
v
4
(v
3
v
5
?v
2
v
6
)= v
1
(v
3
v
7
?v
2
v
8
) (13)
Parameters k
n
are known constants determined by the
vector v. From (10), (12), and (13), the rotation matrix R is
expressed as a function of ? as
R(?)=
? ? ?k
5
r
22
(?)±k
6
?k
1
?k
8
r
22
(?)?k
7
?k
2
r
22
(?) ?k
3
?k
7
r
22
(?)?k
8
?k
4
?k
6
r
22
(?)±k
5
? ? (14)
t(?)=
? ? ?k
12
r
22
(?)+k
13
??k
9
±k
10
r
22
(?)?k
11
? ? (15)
Equations (14) and (15), show that the extrinsic parameters
R and t are ﬁnally expressed as functions of ?. The
deﬁnitions of the constant k
n
are given in the Appendix.
C. Scale candidates using orthogonality
An additional equation is required to determine the scale?.
In this paper, we utilize a pair of perpendicular planes. Scan
data on both planes can be used to compute v because they
share common R and t. Normal vectors of the planes are
functions of ?. The scale is determined using the orthogonal
property of the vectors. Let n
1
and n
3
be the normal vectors
of two orthogonal planes ?
1
and ?
2
, respectively as shown
in Fig. 1 (c).
Scanning the two planes, two lines can be extracted from
range data. Lines on planes ?
1
and ?
2
scanned by L
1
are
deﬁned by (q
1
,q
2
) and (q
5
,q
6
), and those scanned by L
2
are deﬁned by (q
3
,q
4
) and (q
7
,q
8
), respectively, as shown
in Fig. 1 (c). Normal vectors n
1
and n
3
are computed using
these points. The constants k
n
from v are derived by tedious
manipulations, whose details appear in the Appendix.
n
1
=?q
21
??q
31
=
? ? ?k
14
k
15
r
22
(?)+k
16
?k
17
? ? (16)
n
3
=?q
65
??q
75
=
? ? ?k
18
k
19
r
22
(?)+k
20
?k
21
? ? (17)
The dot product of the two normal vectors n
1
and n
3
must be equal to zero.
0 5 10 15 20 25 30
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Frame number
Scale value
Fig. 2: Scale candidate values for all frames. Red dots denote
scale candidates from each pair of orthogonal planes and blue
circles the ground truth scale value.
(a) Top view (b) Side view
Fig. 3: Sensor conﬁgurations using the true scale value and
its ‘mirrored’ scale value (a) top view and (b) side view.
n
1
·n
3
=
? ? ?k
14
k
15
r
22
(?)+k
16
?k
17
? ? ·
? ? ?k
18
k
19
r
22
(?)+k
20
?k
21
? ? = k
22
?
2
+k
23
r
22
(?)+k
24
= k
22
?
2
±k
23
(

??
2
k
0
+1)+k
24
(18)
Substituting the third term and computing the squares of
both sides, we obtain
k
2
22
?
4
+(2k
22
k
24
+k
2
23
k
0
)?
2
?k
2
23
+k
2
24
=0 (19)
which is a special form of a 4-th order polynomial equation
of ?. The candidates of the scale ? are computed by sub-
stituting ? = ?
2
, solving a quadratic equation and selecting
positive solutions for ?. Equation (19) is a quadratic form
and its coefﬁcients contain double signs in the same order.
Therefore we can obtain up to eight candidates for the scale
per frame. The deﬁnitions of the constants k
n
are provided
in the Appendix.
D. Scale selection
As shown in Sect. II-C, each pair of orthogonal planes
yields 8 solutions. Fig. 2 shows positive real scale values
among all candidates. To ﬁnd a unique solution, each one is
veriﬁed using the equations derived from the other frames.
In an ideal case, the equations must be equal to zero if the
right solution is adopted. We select one candidate for each
3029
frame which minimizes the equation evaluation values. The
ﬁnal solution for the scale is determined using a Gaussian
kernel density estimation. Since the scale ? is substituted
by ? = ?
2
, both signs of the ﬁnal solution satisfy (19).
The other solution with the opposite sign is referred to as
a ‘mirrored scale value’ in the rest of this paper. Fig. 3
shows an example of two different sensor conﬁgurations
derived from the solution pair. Gray and red lasers indicate
L
1
and L
2
, respectively. The solution using the mirrored
scale value is displayed in blue denoted by L

2
. All sensors
are transformed into the coordinate system of L
1
. L
2
and L

2
are placed symmetrically fromL
1
. Both of the conﬁgurations
satisfy all of the constraints proposed, and thus, it is not
possible to choose the right solution between them. However,
it is trivial to choose the right one by comparing them to the
actual sensor conﬁguration, because those two possible poses
have different signs in translation with respect to L
1
.
E. Nonlinear optimization
The initial solution computed in Sect. II-D is reﬁned via
non-linear optimization. We assumed that the two planes are
perpendicular to each other in Sect. II-C. However, planes
in real environments usually do not satisfy the assumption
perfectly. We include the angle between the planes as an
unknown parameter with an initial value of 90 degrees.
Moreover, we estimate the pose of the sensor rig in each
frame with respect to the planes in the optimization. Initial
values of the pose for each frame must be computed ﬁrst.
The pose of the sensor rig with respect to the planes has
ﬁve degrees of freedom because the translation along an
intersection line of the two planes cannot be estimated (i.e.,
translation vector has two degrees of freedom). Without loss
of generality, we set the intersection line of the two planes
as y-axis (i.e., the both planes contain the y-axis), and the
normal vector n
1
of ?
1
as x-axis of the world coordinate
system. For each frame, scan data of L
2
are transformed
into L
1
coordinate system and the normal vectors n
1
and
n
3
of the two planes are computed by plane ﬁtting. Since
the normal vectors are computed up to scale, we ﬁx their
directions so that the signs of n
1
·(q
6
?q
5
) andn
3
·(q
1
?q
2
)
are must be positive, when the scanned surfaces are concave.
(see Fig. 1(c)). After computing the normal vectors and their
directions, we set three orthonormal vectors v
x
, v
y
and v
z
which correspond to x, y and z axes of the world coordinate
system.
v
x
=
n
1
n
1

(20)
v
y
=
n
3
?n
1
n
3
?n
1

(21)
v
z
= v
x
?v
y
(22)
Rotation part
ˆ
R
i
of the transformation from L
1
to the
world coordinate in i-th scan data frame is computed as
ˆ
R
i
=
	
v
x
v
y
v
z



. (23)
Fig. 4: Deﬁnition of two planes in the world coordinate
system. The intersecting line of the planes is set as y-axis.
The normal vector n
1
of ?
1
is set as x-axis. The normal
vector n
3
of ?
2
is a function of ?, the angle between two
planes.
We have mentioned that the translation vector
ˆ
t
i
has only
two degrees of freedom. Since we set the intersection line as
y-axis,y element of the translation can be any value. Without
loss of generality, we set its value to zero. The other elements
are computed transforming the intersecting point q
int
of the
two lines q
1
q
2
and q
5
q
6
into the world coordinate system
using the fact that q
int
must lie on the y-axis.
q
int
? (q
1
?q
2
)?(q
5
?q
6
) (24)
ˆ
t
i
=
	
?v
x

q
int
0 ?v
z

q
int



(25)
In the optimization process, the angle ? between the two
planes affects their normal vectors. In our implementation,
we ﬁx ˆ n
1
equal to x-axis and adjust ˆ n
3
using (27) (see Fig.
4).
ˆ n
1
=
	
100



(26)
ˆ n
3
=
	
cos? 0sin?



(27)
The cost function f for optimization is the squared sum
of the Euclidean distances between the two planes and the
scan data in the world coordinate system. Constant terms of
the planes are equal to zero because the origin of the world
coordinate system is on both planes. n is the number of scan
data frame that used in the optimization. Note that ˆ n
3
is a
function of ?, as shown in (27).
3030
0 2 4 6 8 10
?0.5
0
0.5
1
1.5
Laser data noise (mm)
Rotation error (deg)
Linear initial
Refined by optimization
(a) Rotation error using noisy range data
84 86 88 90 92 94 96
?2
0
2
4
6
8
10
12
Angle between two planes (deg)
Rotation error (deg)
Linear initial
Refined by optimization
(b) Rotation error using angle uncer-
tainty between two planes
0 10 20 30 40
0
0.1
0.2
0.3
0.4
Frame number
Rotation error (deg)
Linear initial
Refined by optimization
(c) Rotation error estimated with differ-
ent frame numbers
0 2 4 6 8 10
?0.01
0
0.01
0.02
0.03
0.04
0.05
0.06
Laser data noise (mm)
Rotation error (deg)
Refined by optimization
(d) Rotation error using noisy range data
84 86 88 90 92 94 96
?0.06
?0.04
?0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
Angle between two planes (deg)
Rotation error (deg)
Refined by optimization
(e) Rotation error using angle uncer-
tainty between two planes
0 10 20 30 40
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Frame number
Rotation error (deg)
Refined by optimization
(f) Rotation error estimated with differ-
ent frame numbers
Fig. 5: Performance analysis of initial results (blue) and reﬁned results (red) from three aspects: (a, d) measurement noise,
(b, e) errors in the orthogonality assumption, and (c, f) the number of frames used in the estimation. (a, b and c) are shown
comparision results between the initial and reﬁned results. (d, e and f) show more details of the performance after the
reﬁnement.
f(R,t,
ˆ
R
i
,
ˆ
t
i
,?)=
n

i=1
(

q?(L1,?1)
(ˆ n

1
(
ˆ
R
i
q+
ˆ
t
i
))
2
+

q?(L1,?2)
(ˆ n

3
(
ˆ
R
i
q+
ˆ
t
i
))
2
+

q

?(L2,?1)
(ˆ n

1
(
ˆ
R
i
(Rq

+t)+
ˆ
t
i
))
2
+

q

?(L2,?2)
(ˆ n

3
(
ˆ
R
i
(Rq

+t)+
ˆ
t
i
))
2
)
(28)
III. EXPERIMENTAL RESULTS
A. Simulation using synthetic data
We generated a set of synthetic data to verify the per-
formance of the proposed algorithm. The two planes in
the world coordinate system x =0 and z =0 were
used as orthogonal planes scanned by laser sensors. The
positions of the two sensors are generated randomly within
100< t
x
<500, -500< t
y
<500 and 100< t
z
<500, and
a vector representing their relative rotation is generated in
(?30, 30)(deg).
We analyzed three aspects of the proposed algorithm:
performance for measurement noise, errors in orthogonality
assumption of the two planes and the number of used frames.
The experimental results are shown in Fig. 5. Gaussian noise
N(0,30) is added to the range data used in Fig. 5(a). The
angle between the two planes is changed from 85 to 95(deg)
in Fig. 5(b) to demonstrate the performance of the proposed
algorithm if the assumption of orthogonality is broken. The
number of frames used for calibration is changed from 8 to
40 in Fig. 5(c) under the additive Gaussian noise of N(0,5)
to the range data.
In the experiment, the initial and reﬁned values of the
extrinsic parameters computed by the proposed algorithm are
compared to the ground truth. The difference in rotation is
measured by the angle of the difference ?R between the
ture rotation R
true
and the measured rotation R
measured
as
?R = R
true
?R
measured
?1
. The angle of ?R expressed in
the angle-axis representation and the L
2
norm of translation
are computed as rotation and translation errors, respectively.
In Fig. 5, we performed 100 trials and computed the mean
and standard deviation of the rotation and translation errors.
Blue and red lines indicate the performance of initial and
reﬁned results, respectively. As shown in Fig. 5(a) and (b),
the rotation error of initial results increases smoothly as the
noise increases. Fig. 5(b) shows that the proposed algorithm
is very sensitive to the errors in the orthogonality of the two
planes. The nonlinear optimization resolves this sensitivity
issue because it optimizes the angle between the planes as
well. Fig. 5(c) shows that changing frame numbers does not
give much effect to the accuracy of the initial results. The red
lines shows that the proposed optimization method reduces
3031
Fig. 6: Sensor conﬁguration for real data experiment. Red,
green and blue arrows denote x,y and z axes of each sensor
coordinate system, respectively.
TABLE I: Estimated extrinsic parameters using real data
Rotation (rad)
Conventional -0.397912, -0.030092, -0.395114
Proposed -0.383018, -0.009843, -0.369669
Translation (mm)
Conventional 122.712734, -276.401163, -372.751889
Proposed 112.350861, -261.344662, -361.813052
the rotation error signiﬁcantly in all cases. Fig. 5(d), (e) and
(f) show the reﬁned results in detail.
Translation errors are not displayed because their distribu-
tion is similar to that of the rotation errors.
B. Experiment using real data
For real-data experiments, we designed a sensor system
which consists of two laser sensors (Hokuyo UTM-30LX)
and a CCD camera (PointGrey Flea2) as shown in Fig. 6.
Using real data, calibration result by the proposed method
is compared to that by the conventional method with an
extra camera by Zhang and Pless [8]. In order to compute
the relative pose using the conventional method, the relative
poses between the two laser sensors and the camera are
multiplied; the extra camera “bridges” between the laser
sensors.
Table I shows the results estimated by the proposed
method and those by the conventional method. The rotation
parameters are transformed into a 3?1 vector as mentioned
in Sect. III-A. The rotation and translation differences be-
tween the conventional and the proposed method estimated as
0.909108 (deg) and 6.682265 (mm), respectively. The angle
value ? between two planes is estimated to be 89.751313
(deg).
Fig. 7(a) shows the accumulated laser scan data on two
planes from the two sensors L
1
(red) and L
2
(blue) with
the estimated relative pose. The top-view shown in Fig. 7(b)
veriﬁes that the scan points in the world coordinate are on
the two planes containing the y-axis, as we assumed, and it
conﬁrms that the proposed method successfully register the
two laser sensors.
(a) Accumulated laser data
0 500 1000 1500 2000
0
200
400
600
800
1000
1200
1400
1600
1800
2000
Z (mm)
X (mm)
(b) Top view
Fig. 7: An example of accumulated laser data (red : L
1
, blue
: L
2
) : (a) scanned laser data on two planes and (b) top view.
IV. CONCLUSIONS
In this paper, we present a novel method for extrinsic
calibration of two 2D laser sensors. The proposed algorithm
uses only range data scanned on two orthogonal planes
without any additional sensor or information about motion
of the laser sensors. Coplanarity of scan data from the two
laser sensors gives a linear equation of their relative pose.
Two ‘mirrored’ solutions are computed from the solution
vector of the equation and its scale is determined by the
orthogonality of two planes. We demonstrated the accuracy
and robustness of the proposed algorithm using synthetic data
with noise. The experiment using real data veriﬁes that the
proposed method provides a solution comparable to that of
the conventional method, which uses an extra camera.
V. APPENDIX
1. Rotation parameters
r
12
= ?v
1
= ?k
1
r
21
= ?v
2
= ?k
2
r
23
= ?v
3
= ?k
3
r
32
= ?v
4
= ?k
4
r
22
= ±

1??
2
(v
2
2
+v
2
3
)= ±

1??
2
k
0
3032
r
11
=
?v
1
v
2
r
22
±v
3
v
4
v
2
1
+v
2
4
= ?k
5
r
22
±k
6
r
31
=
?v
2
v
4
r
22
?v
1
v
3
v
2
1
+v
2
4
= ?k
7
r
22
?k
8
r
13
=
?v
1
v
3
r
22
?v
2
v
4
v
2
1
+v
2
4
= ?k
8
r
22
?k
7
r
33
=
?v
3
v
4
r
22
±v
1
v
2
v
2
1
+v
2
4
= ?k
6
r
22
±k
5
2. Translation parameters
t
x
=
?v
1
(?v
3
v
7
+v
2
v
8
)r
22
+v
4
(v
2
v
7
+v
3
v
8
)
v
4
(v
2
2
+v
2
3
)
= ?k
12
r
22
+k
13
t
y
= ?
(v
3
v
5
?v
2
v
6
)
?v
1
= ??k
9
t
z
=
±v
4
(v
3
v
5
?v
2
v
6
)r
22
?v
1
(v
2
v
5
+v
3
v
6
)
v
1
(v
2
2
+v
2
3
)
= ±k
10
r
22
?k
11
3. k
n
values
k
14
= ??z
21
(k
2
x

3
+k
3
z

3
?k
9
)
k
15
=?z
21
(?k
1
x

3
?k
4
z

3
?k
12
)
??x
21
(?k
3
x

3
?k
2
z

3
±k
10
)
k
16
=?z
21
(±k
2
x

3
?k
3
z

3
+k
13
?x
1
)
??x
21
(?k
4
x

3
±k
1
z

3
?k
11
?z
1
)
k
17
=?x
21
(k
2
x

3
+k
3
z

3
?k
9
)
k
18
= ??z
65
(k
2
x

7
+k
3
z

7
?k
9
)
k
19
=?z
65
(?k
1
x

7
?k
4
z

7
?k
12
)
??x
65
(?k
3
x

7
?k
2
z

7
±k
10
)
k
20
=?z
65
(±k
2
x

7
?k
3
z

7
+k
13
?x
5
)
??x
65
(?k
4
x

7
±k
1
z

7
?k
11
?z
5
)
k
21
=?x
65
(k
2
x

7
+k
3
z

7
?k
9
)
k
22
= k
14
k
18
+k
17
k
21
?k
15
k
19
k
0
k
23
= k
15
k
20
+k
16
k
19
k
24
= k
15
k
19
+k
16
k
20
The detailed derivation can be found in the sup-
plementary material at http://rcv.kaist.ac.kr/
˜
dgchoi/icra2014derivation.pdf
REFERENCES
[1] D. F. Wolf and G. S. Sukhatme, “Mobile robot simultaneous local-
ization and mapping in dynamic environments,” Autonomous Robots,
vol. 19, no. 1, pp. 53–65, 2005.
[2] P. Newman, D. Cole, and K. Ho, “Outdoor slam using visual ap-
pearance and laser ranging,” in Robotics and Automation, 2006. ICRA
2006. Proceedings 2006 IEEE International Conference on. IEEE,
2006, pp. 1180–1187.
[3] A. Nuchter, K. Lingemann, J. Hertzberg, and H. Surmann, “6d slam
with approximate data association,” in Advanced Robotics, 2005.
ICAR’05. Proceedings., 12th International Conference on. IEEE,
2005, pp. 242–249.
[4] D. M. Cole and P. M. Newman, “Using laser range data for 3d slam
in outdoor environments,” in Robotics and Automation, 2006. ICRA
2006. Proceedings 2006 IEEE International Conference on. IEEE,
2006, pp. 1556–1563.
[5] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron,
J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, et al., “Stanley:
The robot that won the darpa grand challenge,” Journal of ﬁeld
Robotics, vol. 23, no. 9, pp. 661–692, 2006.
[6] M. Montemerlo, J. Becker, S. Bhat, H. Dahlkamp, D. Dolgov, S. Et-
tinger, D. Haehnel, T. Hilden, G. Hoffmann, B. Huhnke, et al., “Junior:
The stanford entry in the urban challenge,” Journal of Field Robotics,
vol. 25, no. 9, pp. 569–597, 2008.
[7] Y. Bok, Y. Jeong, D.-G. Choi, and I. S. Kweon, “Capturing village-
level heritages with a hand-held camera-laser fusion sensor,” Interna-
tional Journal of Computer Vision, vol. 94, no. 1, pp. 36–53, 2011.
[8] Q. Zhang and R. Pless, “Extrinsic calibration of a camera and laser
range ﬁnder (improves camera calibration),” in Intelligent Robots and
Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ Interna-
tional Conference on, vol. 3. IEEE, 2004, pp. 2301–2306.
[9] J. Underwood, A. Hill, and S. Scheding, “Calibration of range sensor
pose on mobile platforms,” in Intelligent Robots and Systems, 2007.
IROS 2007. IEEE/RSJ International Conference on. IEEE, 2007, pp.
3866–3871.
[10] C. Gao and J. R. Spletzer, “On-line calibration of multiple lidars on a
mobile vehicle platform,” in Robotics and Automation (ICRA), 2010
IEEE International Conference on. IEEE, 2010, pp. 279–284.
[11] W. Maddern, A. Harrison, and P. Newman, “Lost in translation (and
rotation): Rapid extrinsic calibration for 2d and 3d lidars,” in Robotics
and Automation (ICRA), 2012 IEEE International Conference on.
IEEE, 2012, pp. 3096–3102.
[12] J. Levinson and S. Thrun, “Unsupervised calibration for multi-beam
lasers,” in Experimental Robotics. Springer, 2014, pp. 179–193.
3033
