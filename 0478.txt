High Level Landmark-Based Visual Navigation Using Unsupervised
Geometric Constraints in Local Bundle Adjustment
Yan Lu, Dezhen Song, and Jingang Yi
Abstract? We present a high level landmark-based visual
navigation approach for a monocular mobile robot. We utilize
heterogeneous features, such as points, line segments, lines,
planes, and vanishing points, and their inner geometric con-
straints as the integrated high level landmarks. This is managed
through a multilayer feature graph (MFG). Our method extends
local bundle adjustment (LBA)-based framework by explicitly
exploiting different features and their geometric relationships
in an unsupervised manner. The algorithm takes a video stream
as input, initializes and incrementally updates MFG based on
extracted key frames; it also renes localization and MFG
landmarks through the LBA. Physical experiments show that
our method can reduce the absolute trajectory error of a
traditional point landmark-based LBA method by up to 63:9%.
I. INTRODUCTION
Visual navigation using low cost cameras, such as cam-
eras in mobile devices like cell phones and tablets, has
gained more research attention due to the increasing needs
for navigation assistance in indoor and/or GPS challenged
environments. Visual navigation is often conducted under the
simultaneous localization and mapping (SLAM) framework.
Despite its great success, visual SLAM still suffers from
technical issues such as scale drift and robustness to dynamic
environment. Besides the limitations of camera itself (as a
bearing-only sensor), another possible reason is that most
systems use low level features (e.g. salient points) as sole
landmarks. This may not be the best choice when higher level
landmarks are available in man-made environments where
abundant lines in parallel directions and the salient building
facades exist. These higher level landmarks not only enable
the possibility of higher level tasks such as object recognition
and human-robot interaction, but also can potentially help
improve navigation performance.
We utilize heterogeneous visual features, including points,
line segments, lines, planes, and vanishing points, and their
inner geometric constraints as the integrated high level
landmarks to assist robot navigation (see Fig. 1). This is
managed through a multilayer feature graph (MFG), an open
data structure containing geometric relationships, such as
parallelism and coplanarity. Our method extends local bundle
adjustment (LBA)-based framework by explicitly exploiting
different features and their geometric relationships in an
unsupervised manner. The algorithm takes a video stream
as input, initializes and incrementally updates and expands
Y . Lu and D. Song are with the Department of Computer Science and
Engineering, Texas A&M University, College Station, TX 77843, USA.
Emails:fylu, dzsongg@cse.tamu.edu.
J. Yi is with the Department of Mechanical and Aerospace Engineering,
Rutgers University, Piscataway, NJ 08854, USA. E-mail:jgyi@rutgers.edu.
Fig. 1. Output of our algorithm, and a Google Earth
TM
view of the same
site from a similar perspective. Coplanar landmarks (points and lines) are
coded in the same color, while general landmarks are in gray color. The
dotted line is the estimated camera trajectory.
MFG based on extracted key frames, and renes localization
and MFG landmarks through the LBA. Physical experiments
show that our method can reduce the mean absolute trajec-
tory error of a traditional point landmark-based LBA method
by up to 63:9%.
II. RELATED WORK
Our work is an extension of SLAM problems, and visual
SLAM in particular [1].
There are two prevalent methodologies in visual SLAM:
the bundle adjustment (BA) approaches (e.g., [2]) rooted in
the structure from motion (SFM) area in computer vision,
and the ltering methods (e.g. [3]) originated from the
traditional SLAM eld of robotics research. Strasdat et al.
have analyzed the advantages of each method in [4]. For both
methods, various camera types/modalities have been studied,
such as a monocular camera [5], [6], a stereo camera [7], an
omnidirectional camera [8], and an RGB-D camera [9].
Besides methodology and sensor conguration, another
critical issue in visual SLAM is environment representation.
For example, point cloud [10] and sparse feature points [11]
are often employed as landmarks in a map. Recently, many
researchers have realized that landmark selection is an im-
portant factor in visual odometry and SLAM performance.
Lower level landmarks such as Harris corner and SIFT point,
are relatively easy to use due to their geometric simplicity,
which shares many properties with traditional point clouds
generated from laser range nders. However, point features
are merely mathematical singularities in color, texture, and
geometric space. They can also be easily inuenced by
lighting and shadow conditions. To overcome these short-
comings, higher level landmarks have received more and
more attention for visual SLAM, such as line segments [12]
and planes [13].
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1540
These works have demonstrated the advantages of higher
level landmarks in robustness and accuracy, but they either
treat these landmarks as isolated objects, or partially explore
the inner relationship between them. This treatment simpli-
es the SLAM problem formulation but cannot fully utilize
the power of high level landmarks. Very recently, Tretyak et
al. present an optimization framework for geometric parsing
of image by jointly using edges, line segments, lines, and
vanishing points [14]. However, this method limits itself to
a single image for now. At almost the same time, Li et al.
propose the initial MFG concept based on two views [15],
which is then applied to building exterior mapping under an
EKF framework [16]. Inspired by [4], we present an LBA-
based approach to constructing MFG from a video stream.
III. BACKGROUND AND PROBLEM FORMULATION
A. Assumptions and Notations: Consider a monocular robot
navigating in a previously unknown environment. Assume:
a.1 The robot operates in a largely static environment with
rectilinear structures, a characteristic of typical man-
made environments, and
a.2 The camera is calibrated with radial distortion removed.
Let us dene the following notations,
V Input camera video,
fWg 3D Cartesian world coordinate system,
I
k
A key frame extracted fromV, I
k
2V;k2N,
fC
k
g Camera coordinate system at I
k
,
K Camera calibration matrix,
R
k
Camera rotation matrix at I
k
,
t
k
Camera translation vector at I
k
,
P
k
Camera projection matrix, P
k
= K [R
k
j t
k
],
X
i:j
Collection dened as X
i:j
=fX
k
;ikjg,
M
k
MFG constructed based on key frames I
0:k
,
E
n
n-dimensional Euclidean space,
P
n
n-dimensional projective space, and
X A homogeneous vector, X = [
~
X
T
; 1]
T
, where
~
X denotes the inhomogeneous, counterpart of X.
X2P
n
)
~
X2E
n
.
We abuse ?=? to denote real and up-to-scale equalities for
inhomogeneous and homogeneous vectors, respectively.
B. Review of Multilayer Feature Graph: Introduced in [15],
MFG is the key data structure for organizing landmarks.
Fig. 2 shows the structure of MFG, composed of ve types
of features in separate layers with four kinds of geometric
constraints. Let us briey review MFG for completeness.
1) A key point node represents a 3D point landmark. We
denote its 3D position by P
i
2P
3
and its image observation
in I
k
by p
i;k
2P
2
.
2) A line segment node represents a 3D segment object.
We denote it in 3D by S
i
= [D
T
i1
; D
T
i2
]
T
, where D
i1
and D
i2
are the two endpoints. Its observation in I
k
is then s
i;k
=
[d
T
i1;k
; d
T
i2;k
]
T
.
3) An ideal line node represents an innite 3D line. We
denote an ideal line in 3D by L
i
= [Q
T
i
; J
T
i
]
T
, where Q
i
is a nite 3D point locating on L
i
and J
i
is an innite
3D point dening the direction of L
i
. The image of L
i
in
É
É
É
É É
Key Point
*
Vanishing Point
Line Segment
Ideal Line
Primary Plane
*
*
*
Parallelism
Coplanarity
Collinearity
Adjacency
Fig. 2. MFG structure. All nodes exist in 3D space, and the shaded
regions indicate nodes which also exist in 2D image. Geometric relationships
between nodes are represented by edges of different line types.
I
k
is denoted by l
i;k
. Image ideal line l
i;k
is detected by
identifying collinear image line segment(s). Thus, an ideal
line has a set of supporting line segments.
4) A primary plane node represents a planar 3D object
(e.g., a wall). We denote a primary plane by 
i
= [n
T
i
;d
i
]
T
in 3D, where n
i
2E
3
and d
i
2R, such that X
T

i
= 0 for
any point X on the plane.
5) A vanishing point node represents a particular 3D
direction. We denote a vanishing point by V
i
in 3D and
its observation in I
k
by v
i;k
.
Besides, the geometric relationships between these nodes are
represented by MFG edges connecting them, including par-
allelism, coplanarity, collinearity and adjacency (see Fig. 2).
C. Problem Formulation: I
0
and I
1
are given along with an
initial MFGM
1
from [15]. For k 2, the problem is:
Denition 1: Given videoV, MFGM
k 1
, and historical
camera posesfR
0:k 1
; t
0:k 1
g, select key frameI
k
, estimate
camera posefR
k
; t
k
g, and update the nodes and edges of
M
k 1
to attainM
k
.
IV. SYSTEM DESIGN
Our system architecture is illustrated in Fig. 3, where the
main blocks are shaded and explained in this section.
A. Key Frame Selection: Given a video input, it is necessary
to select a set of key frames for motion estimation. This is
to guarantee sufcient baseline distance between two frames
and avoid ill-posed epipolar geometry problems. The basic
principle is to nd a good balance between two needs:
a) large camera movement to provide sufcient motion
parallax and b) sufcient overlap of scene. Based on existing
methods [2], [5], [8], we make the following criteria for key
frame selection. Supposing I
k 1
andM
k 1
are given, a
video frame is chosen as key frameI
k
if it satises: 1) there
are as many video frames between I
k 1
and I
k
as possible,
2) the number of SIFT point correspondences betweenI
k 1
and I
k
is no less than N
2d
(N
2d
= 50 here), and 3) the
number of MFG key points that are observable in I
k
is not
less thanN
3d
(N
3d
= 5 here). SinceI
0
andI
1
are manually
selected by user, the selection criteria only apply for k 2.
B. Image Feature Processing: Once I
k
is selected, we
proceed to extract image features from it in the image
feature processing step. 2D key points and line segments are
extracted from I
k
using SIFT and LSD [17], respectively,
and 2D ideal lines and vanishing points are detected based
on the resulting line segments. The correspondences of these
1541
Fig. 3. System
Diagram
2.1) Feature point 
detection
3.2) Current 
camera pose R k , t k
4.1) New point 
correspondences
4.2) 3D point 
Triangulation
2.3) Line segment 
detection
5) 3D line 
Triangulation
On existing 
plane?
8) Local bundle 
adjustment
Yes
2.4) Ideal line 
estimation
2.5) Ideal line 
matching
On existing 
plane?
Yes
No
2.6) Vanishing 
point detection
2.7) Vanishing 
point matching
Any new 3D 
plane?
7) Compute 3D 
plane equations
Yes
Exist in 
MFG?
2.2) Feature point 
matching with I k-1
I k
Yes
No
3.1) Relative 
camera pose R, t
Exist in 
MFG?
No Yes
No
1) Key frame 
selection
video
Image Feature Processing
Key Point Update
Camera Pose Estimation
Ideal Line Update
Primary Plane Update
R 0:k-1 , t 0:k-1
R 0:k , t 0:k
M k-1
M k
Exist in 
MFG?
6) Establish new 
vanishing point
Vanishing Point Update Yes
No
image features are then found betweenI
k 1
andI
k
. Detailed
methods in this process can be found in [15].
Remark 1: All features detected fromI
k
only exist in the
image space so far, and they will be associated with 3D
landmarks, or used to establish new 3D landmarks in the
MFG update step.
C. Camera Pose Estimation: With image feature correspon-
dences obtained, estimating the 6 degrees of freedom (DoF)
camera pose R
k
and t
k
for I
k
is a key step for inferring
3D information and updating MFG. Existing methods (e.g.
[18]) usually solve this problem using 3-point algorithms
[19] based on the 3D-2D correspondences fP
i
$ p
i;k
g
between known 3D points and their observations inI
k
. This
method omits the 2D-2D correspondences betweenI
k 1
and
I
k
whose 3D positions are unknown yet. To fully use both
kinds of information, Tardif et al. propose [8] decoupling the
estimation of R
k
from t
k
. We modify the method as below.
Step 1: Based on SIFT point correspondences between
I
k 1
and I
k
, compute essential matrix E using the 5-point
algorithm in RANSAC [20]. Decompose E to recover the
relative rotation R and translation t, withktk unknown.
Step 2: Compute ktk using 3D-2D correspondences
through a RANSAC process using only one correspondence
for a minimal solution. This completes the 6 DoF estimation.
In the Step 2 of [8], Tardif et al. estimate the full 3 DoFs of
t using two 3D-2D correspondences for a minimal solution.
This difference can be justied by the different cameras we
use - an omnidirectional camera in [8] with 360

horizontal
eld of view (HFOV) vs. a regular camera we use with 40


80

HFOV . Narrower HFOV results in fewer observable 3D
landmarks in view and thus fewer 3D-2D correspondences,
especially in a turning situation. Therefore, we choose to
reduce the problem dimension in Step 2 to t our needs.
D. MFG Update and LBA: MFG update includes key point
update, ideal line update, vanishing point update, and primary
plane update, as shown in Fig. 3. Updating MFG basically
means associating existing MFG nodes with their latest
observations if available, and establishing new MFG nodes
and edges based on image features. The update algorithms
for each type of feature are presented in Sections V-A.1, V-
A.2, V-A.3, and V-A.4, respectively. Following MFG update,
an LBA is performed to jointly rene recent camera poses
and MFG nodes using a window of key frames. Due to the
uniqueness of MFG, we propose a new LBA formulation that
integrates the heterogeneous features in MFG along with the
embedded geometric constraints in Section V-B.
V. ALGORITHMS
We begin with the MFG update algorithms for each type
of feature, and then details the MFG-based LBA formulation.
A. MFG Update
Updating MFG involves associating image features with
existing 3D landmarks and augmenting MFG by setting up
new nodes (landmarks) and edges (geometric relationships),
which is detailed below.
1) Key Point Update: Key point update is similar to tra-
ditional point-based SFM methods. We briey describe our
algorithm for completeness. For a 2D point correspondence
x
i;k 1
$ x
i;k
between I
k 1
and I
k
,
 if it is a re-observation of key point P
j
, let p
j;k
= x
i;k
.
 if it is a newly discovered point, compute its motion
parallax (x
i;k 1
; x
i;k
) using (1). If (x
i;k 1
; x
i;k
) is
greater than a threshold
p
, we compute its 3D position
and add it toM
k
as a new key point node. Otherwise,
we start a new image point trackQ
j
=fx
i;k 1
; x
i;k
g
to keep track of it in future frames.
 if it is an observation of an image point track Q
j
,
append it to the trackQ
j
=Q
j
[fx
i;k
g, and check
whetherQ
j
can be converted to a key point node. To
do this, we compute the motion parallax between each
pair of points inQ
j
, and if anyone is larger than 
p
, a
new key point node is established and added toM
k
.
2) Ideal Line Update: Before presenting the ideal line
update algorithm, we need to dene the motion parallax for
ideal lines rst. It is well known that the motion parallax of
a point correspondence x
i;k 1
$ x
i;k
can be dened as
(x
i;k 1
; x
i;k
):=hK
 1
H
r
x
i;k 1
; K
 1
x
i;k
i; H
r
=KRK
 1
(1)
where H
r
represents a rotational homography [21], h;i
indicates the angle between two vectors, and R is the relative
1542
y i,k-1
y i,k
y' i,k
x j,k-1 x' j,k
I k-1 I k
H r
Camera center
?
k j,
x
) (
, 1
?
k j j,k-
x , x ?
Fig. 4. Illustration of parallax computation for image ideal lines. Hr is
a rotational homography mapping dened in (1). Bold lines are supporting
line segments of the underlying (thin) ideal line.(x
j;k 1
;x
+
j;k
) is dened
as the motion parallax for point pair x
j;k 1
$x
+
j;k
.
rotation between I
k 1
and I
k
. Generally speaking, motion
parallax has not been clearly dened for lines. Here we
propose a heuristic motion parallax measurement for ideal
lines by leveraging the line segment endpoints on them. For
an image ideal line correspondence y
i;k 1
$ y
i;k
, dene
%(y
i;k 1
; y
i;k
) :=
1
n
n
X
j=1
(x
j;k 1
; x
+
j;k
); (2)
wherefx
j;k 1
;j = 1; ;ng are the endpoints of image
line segments that support y
i;k 1
, and x
+
j;k
is the perpen-
dicular foot of x
0
j;k
:= H
r
x
j;k 1
on y
i;k
in I
k
(see Fig. 4).
The rationale is that we want to reward line correspondences
which have larger distance in their perpendicular direction. If
y
0
i;k
overlap with y
i;k
, their motion parallax should be zero.
With the motion parallax dened, the ideal line update can
be performed in a similar fashion to the key point case, and
thus skipped here.
Remark 2: Line segment nodes are also updated in this
process. Since a line segment always has an ideal line parent,
when an image ideal line is converted to a node, its associated
line segments are also converted to line segment nodes. Their
3D positions are computed from the 3D ideal line parameters.
3) Vanishing Point Update: Updating vanishing point
nodes is straightforward. Given an image vanishing point
correspondence 
i;k 1
$ 
i;k
, if it is a re-observation of
existing node V
j
, let v
j;k
= 
i;k
. Otherwise, establish a
new vanishing point node V
j
= [
T
i;k
R
k
; 0]
T
. It is trivial
but important to update the edges between ideal lines and
vanishing points whenever a new ideal line or vanishing point
node is added.
4) Primary Plane Update: Finding new primary planes
relies on detecting coplanar key points and ideal lines. Here
we detect primary planes directly from 3D key points and
ideal lines using RANSAC. To be specic, let C be the
collection of 3D key points and ideal lines which are not
yet associated with any primary plane. We brief two key
steps of the RANSAC process below.
1) Compute a plane candidate   from a minimal solution
set, which could include either 3 key points, or 2
parallel ideal lines, or 1 key point plus 1 ideal line.
2) 8c2C, compute score f( ;c). If c is a key point,
f( ;c) is the perpendicular distance from c to  ; if c
is an ideal line,f( ;c) is the average of the distances
from its associated line segment endpoints to  .
If the size of the largest consensus set is greater than a
thresholdN
plane
, we add the corresponding plane candidate
toM
k
as a primary plane node, and establish edges between
it and the key points and ideal lines in the consensus set.
Moreover, when new key point or ideal line nodes are
established, we check if they belong to existing primary
planes similarly.
B. Local Bundle Adjustment
After MFG is updated, we want to rene the estimated
camera pose and MFG nodes simultaneously using LBA.
Along the lines of [8], we usew latest key frames to bundle
adjust m latest camera poses and MFG nodes established
since I
k m+1
, with w  m usually. To account for the
various feature types and geometric constraints in MFG, we
need to dene cost functions accordingly.
1) Key Point: Denote the re-projection of key point P
i
in
I
k
by ^ p
i;k
:= P
k
P
i
. We assume zero-mean Gaussian noise
for image point measurement, i.e.,
~
^ p
i;k
N (0; 
p
). Dene
the cost function for P
i
in I
k
to be
C
pt
(P
i
;k) = (
~
^ p
i;k
  ~ p
i;k
)
T

 1
p
(
~
^ p
i;k
  ~ p
i;k
): (3)
2) Ideal Line & Collinearity: Denote the re-projection
of ideal line L
i
in I
k
by
^
l
i;k
:= P
k
Q
i
 P
k
J
i
. Since
the observation of L
i
in I
k
, i.e. l
i;k
, is estimated from its
supporting line segmentsfs
;k
j = 1;g, we directly treat
these line segments as its observations for cost function
denition. The measurement noise of image line segment can
be modeled in various ways. Here we adopt a simple but
well-accepted modeling, which assumes each line segment
endpoint is subject to a zero-mean Gaussian noise, i.e.,
~
d
j;k
N (0;
2
d
I
2
), wherej = 1; 2, and I
2
is a 22 identity
matrix. Dene the cost function for L
i
in I
k
as
C
ln
(L
i
;k) =
X

2
X
j=1
 
d
?
(
~
d
j;k
;
^
l
i;k
)

d
!
2
; (4)
where d
?
(;) denotes the perpendicular distance from a
point to a line in image. This cost function effectively
captures the collinearity constraint between ideal lines and
line segments.
3) Vanishing Point & Parallelism: Let the re-projection
of vanishing point V
i
in I
k
be ^ v
i;k
:= P
k
V
i
. The observa-
tion of V
i
in I
k
is v
i;k
which is the intersection of image
line segments from the same parallel group. Since v
i;k
is
estimated from line segments, its estimation covariance 
v
i;k
can be easily derived as well [22]. Dene the cost function
for V
i
in I
k
by
C
vp
(V
i
;k) = (^ v
i;k
  v
i;k
)
T

 1
v
i;k
(^ v
i;k
  v
i;k
): (5)
In particular, for all ideal linesfL
j
g connected to V
i
in
MFG, we enforce L
j
= [Q
T
j
; V
T
i
]
T
such that these lines are
strictly parallel. Recall that Q
j
is a nite point on L
j
. This
parameterization and cost function together account for the
parallelism relationship in MFG.
4) Primary Plane & Coplanarity: Primary plane 
i
has
neither re-projection nor direct observation in image space.
Therefore, we dene its cost function by leveraging 3D key
points and ideal lines, respectively. For key point P
j
and
primary plane 
i
, dene
1543
C
pl
(P
j
; 
i
) =
(
[
?
(P
j
; 
i
)]
2
if P
j
2 
i
0 otherwise
(6)
where 
?
(;) denotes the perpendicular distance from a
point to a plane in 3D, and P
j
2 
i
indicates that P
j
is
connected with 
i
in MFG.
For ideal line L
j
and primary plane 
i
, dene
C
pl
(L
j
; 
i
) =
(
1
n
P
n
=1
[
?
(D

; 
i
)]
2
if L
j
2 
i
0 otherwise
(7)
wherefD

; = 1; ;ng denote the endpoints of all the
line segments that support L
j
. Eqs. (6) and (7) represent the
coplanarity constraint in MFG.
5) Overall Metric: Denote the last m camera poses by
S
k
cp
=fR
i
; t
i
ji = k m + 1; ;kg, and the last m key
frames byI
k
=fI
i
ji =k m + 1; ;kg. The key points
to be rened in LBA are those that can be observed in at
least one frame ofI
k
, and we denote them byS
k
pt
. Similarly
we deneS
k
ln
andS
k
vp
for ideal lines and vanishing points,
respectively. The primary planes to be rened are those that
have edges connected to key points fromS
k
pt
or ideal lines
fromS
k
ln
, and we denote them byS
k
pl
. Then the total cost
function is dened in (8) in next page.
The MFG-LBA problem at time k is
min
S
k
cp
;S
k
pt
;S
k
ln
;S
k
vp
;S
k
pl
C
total
(M
k
): (9)
This problem can be solved using the Levenberg-Marquardt
algorithm [21], and the solution provides rened camera
posesS
k
cp
and MFG nodes including key pointsS
k
pt
, ideal
linesS
k
ln
, vanishing pointsS
k
vp
, and primary planesS
k
pl
.
VI. EXPERIMENTS
We have implemented our algorithm using C++ in Win-
dows 7. To validate the algorithm, we have compared it with
state-of-the-art methods on different datasets. We choose
the 1-Point RANSAC-based EKF-SLAM method [1] for
comparison as it is state of the art in monocular visual
navigation. Its MATLAB
R 
code is available at the author's
webpage [23]. In the following, we will refer to it as
1Point-EKF for brevity. The second method for comparison
only uses SIFT points as landmarks and performs LBA as
well, named Point-LBA. This method can be regarded as a
degenerate version of MFG. Our proposed method is named
MFG-LBA. It is worth mentioning that no loop closing is
performed.
A. Indoor Experiments
1) Evaluation Metric: To evaluate the localization ac-
curacy, we adopt the widely used absolute trajectory error
(ATE) [1]. Since the ground truth and the estimation of
camera poses are usually represented in different coordinate
systems, we need to align them before computing ATE. Let
g
W
0
k
be the ground truth of camera position at time k in a
coordinate systemfW
0
g and r
W
k
the estimated one infWg.
We need to nd a similarity transformation that maps r
W
k
to
fW
0
g: r
W
0
k
:=sR
W
0
W
r
W
k
+t
W
0
W
; where the transformation is
dened by rotation matrix R
W
0
W
, translation vector t
W
0
W
and
0 5 10 15 20 25
0
5
10
15
20
 
X [m]
 
Y [m]
Ground truth
Point?LBA
MFG?LBA
1Point?EKF
(a)
0 50 100 150 200 250
0
50
100
150
200
250
300
350
400
Key frame 
Cost function value
 
 
Key point
Ideal line
Vanishing point
Primary plane
(b)
Fig. 5. (a) Estimated robot trajectories by different methods. (b) Cost
function values of each component in MFG-LBA.
scaling factor s. Part of the transformation parameters (e.g.
t
W
0
W
) could be known a priori, and the unknown part would
be obtained by minimizing
P
k
kr
W
0
k
  g
W
0
k
k
2
. The ATE"
k
at time k is then dened as the metric distance between
the estimation and the ground truth of camera position:
"
k
=kr
W
0
k
  g
W
0
k
k:
2) Datasets: We have tested on two datasets: HRBB
dataset and Bicocca dataset. The HRBB dataset is collected
on the 4-th oor of H. R. Bright building at Texas A&M
University using a PackBot. A Nikon 5100 camera with 60

HFOV is mounted on the PackBot. The dataset consists of
12000 raw frames with 1920 1080 resolution and 30 fps
frame rate. In our experiments, we down-sample the images
to 640 360 for faster computation. The robot trajectory
covers around 70 meters. The ground truth of camera poses
is obtained using articial landmarks with error of1 cm.
The Bicocca dataset used here is an image sequence from
the publicly available Rawseeds datasets [24]. The image
resolution is 320240, and ground truth of camera positions
is also provided. A sequence of 2000 frames are used for
experiments, describing a trajectory of around 77 meters.
3) Results: Tabs. I(a) and I(b) show the ATE's for each
method on two datasets. We can see that Point-LBA performs
better than 1Point-EKF and this complies with the observa-
tion of [4]. Our algorithm MFG-LBA outperforms both of
the other methods, achieving a relative mean ATE of 1:09%
and 3:29% on each dataset, respectively. This implies that the
mean ATE of our algorithm is 63:9% and 29:1% less than
that of Point-LBA on each dataset, respectively. The larger
ATE on the Bicocca dataset is due to lower image resolution.
Fig. 5(a) shows the estimated trajectories on the HRBB
dataset. We can see that our algorithm suffers the smallest
scale and angular drift. Fig. 5(b) illustrates the cost function
values of each component of (8) over key frames, from which
we see how each type of feature and constraint contributes
to the LBA process.
B. Outdoor Test
We have collected an outdoor dataset containing 3240
frames with a hand-held camera (Nikon 5100). The camera
trajectory covers around 150 meters on campus. The images
are down-sampled to resolution 640360. Although we do
not have true camera trajectory, we have measured the plane
normal directions of building facades on Google Maps
TM
1544
C
total
(M
k
)=
k
X
=k w+1
2
4
X
P2S
k
pt
C
pt
(P;)+
X
L2S
k
ln
C
ln
(L;)+
X
V2S
k
vp
C
vp
(V;)
3
5
+
X
2S
k
pl
2
4
X
P2S
k
pt
C
pl
(P; ) +
X
L2S
k
ln
C
pl
(L; )
3
5
(8)
TABLE I
(a) ATE of HRBB Dataset
Method Mean Std. of Max Mean ATE over
ATE (m) ATE (m) ATE (m) trajectory length
1Point-EKF 4.37 2.01 8.25 6.24%
Point-LBA 2.11 1.06 3.52 3.01%
MFG-LBA 0.76 0.51 2.02 1.09%
(b) ATE of Bicocca Dataset
Method Mean Std. of Max Mean ATE over
ATE (m) ATE (m) ATE (m) trajectory length
1Point-EKF 3.64 2.34 9.56 4.73%
Point-LBA 3.57 2.12 9.62 4.64%
MFG-LBA 2.53 1.69 8.77 3.29%
(c) Plane Normal Error (in

)
Method 
1

2

3
Point-LBA 1.10 1.71 4.38
MFG-LBA 1.09 1.56 3.70
and use them for evaluation. For the Point-LBA algorithm,
3D planes are detected from the resulting 3D points by
nding coplanar points using RANSAC; the found planes
are then re-estimated by optimization. Tab. I(c) shows the
plane normal errors of reconstructed building facades. Our
method produces smaller plane normal errors than Point-
LBA, implying a smaller angular drift (see Fig. 1).
VII. CONCLUSIONS AND FUTURE WORK
We presented a method utilizing heterogeneous visual fea-
tures and their inner geometric constraints as the integrated
high level landmarks to assist robot navigation. This was
managed through a multilayer feature graph. Our method
extended LBA framework by explicitly exploiting different
features and their geometric relationships in an unsupervised
manner. Physical experiments showed that our algorithm
outperformed state of the art in localization and mapping
accuracy. In the future, we will use MFG to facilitate
loop closure detection and consider incorporating appearance
information to enhance robustness.
ACKNOWLEDGMENT
We would like to acknowledge the insightful thoughts from Y . Xu, A.
Perera, and S. Oh in Kitware. We also thank W. Li, M. Hielsberg, J. Lee,
Z. Gui, M. Hirami, S. Mun, S. Jacob, and P. Peelen for their inputs.
REFERENCES
[1] J. Civera, O. G. Grasa, A. J. Davison, and J. Montiel, ?1-point
RANSAC for extended Kalman ltering: Application to real-time
structure from motion and visual odometry,? Journal of Field Robotics,
vol. 27, no. 5, pp. 609?631, 2010.
[2] E. Mouragnon, M. Lhuillier, M. Dhome, F. Dekeyser, and P. Sayd,
?Generic and real-time structure from motion using local bundle
adjustment,? Image and Vision Computing, vol. 27.
[3] A. Davison, ?Real-time simultaneous localisation and mapping with a
single camera,? in Computer Vision, 2003. Proceedings. Ninth IEEE
International Conference on, oct. 2003, pp. 1403 ?1410 vol.2.
[4] H. Strasdat, J. Montiel, and A. J. Davison, ?Real-time monocular
SLAM: Why lter?? in Robotics and Automation (ICRA), 2010 IEEE
International Conference on. IEEE, 2010, pp. 2657?2664.
[5] E. Royer, M. Lhuillier, M. Dhome, and J.-M. Lavest, ?Monocular
vision for mobile robot localization and autonomous navigation,?
International Journal of Computer Vision, vol. 74, no. 3, pp. 237?
260, 2007.
[6] W. Li and D. Song, ?Toward featureless visual navigation: Simulta-
neous localization and planar surface extraction using motion vectors
in video streams,? in IEEE International Conference on Robotics and
Automation (ICRA), Hong Kong, China, May-June 2014.
[7] G. Sibley, C. Mei, I. Reid, and P. Newman, ?Vast-scale outdoor nav-
igation using adaptive relative bundle adjustment,? The International
Journal of Robotics Research, vol. 29, no. 8, pp. 958?980, 2010.
[8] J.-P. Tardif, Y . Pavlidis, and K. Daniilidis, ?Monocular visual odometry
in urban environments using an omnidirectional camera,? in Intelligent
Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Con-
ference on. IEEE, 2008, pp. 2531?2538.
[9] P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox, ?RGB-D
mapping: Using kinect-style depth cameras for dense 3D modeling of
indoor environments,? The International Journal of Robotics Research,
vol. 31, no. 5, pp. 647?663, 2012.
[10] J.-S. Gutmann, M. Fukuchi, and M. Fujita, ?3D perception and
environment map generation for humanoid robot navigation,? The
International Journal of Robotics Research, vol. 27, no. 10, pp. 1117?
1134, 2008.
[11] G. Klein and D. Murray, ?Parallel tracking and mapping for small AR
workspaces,? in Mixed and Augmented Reality, 2007. ISMAR 2007.
6th IEEE and ACM International Symposium on. IEEE, 2007, pp.
225?234.
[12] J. Zhang and D. Song, ?Error aware monocular visual odometry using
vertical line pairs for small robots in urban areas,? in Special Track
on Physically Grounded AI (PGAI), AAAI Conference on Articial
Intelligence (AAAI), Atlanta, Georgia, USA, July 2010.
[13] A. Flint, C. Mei, I. Reid, and D. Murray, ?Growing semantically
meaningful models for visual SLAM,? in Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp.
467?474.
[14] E. Tretyak, O. Barinova, P. Kohli, and V . Lempitsky, ?Geometric
image parsing in man-made environments,? International Journal of
Computer Vision, vol. 97, no. 3, pp. 305?321, 2012.
[15] H. Li, D. Song, Y . Lu, and J. Liu, ?A two-view based multilayer feature
graph for robot navigation,? in Robotics and Automation (ICRA), 2012
IEEE International Conference on. St. Paul, MN, USA: IEEE, May
2012, pp. 3580?3587.
[16] Y . Lu, D. Song, Y . Xu, A. G. A. Perera, and S. Oh, ?Automatic building
exterior mapping using multilayer feature graphs,? in Automation Sci-
ence and Engineering (CASE), 2013 IEEE International Conference
on, 2013, pp. 162?167.
[17] R. von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall, ?LSD: A fast
line segment detector with a false detection control,? Pattern Analysis
and Machine Intelligence, IEEE Transactions on, vol. 32, no. 4, pp.
722 ?732, april 2010.
[18] E. Mouragnon, M. Lhuillier, M. Dhome, F. Dekeyser, and P. Sayd,
?Real time localization and 3D reconstruction,? in Computer Vision
and Pattern Recognition, 2006 IEEE Computer Society Conference
on, vol. 1. IEEE, 2006, pp. 363?370.
[19] B. M. Haralick, C.-N. Lee, K. Ottenberg, and M. N¬ olle, ?Review and
analysis of solutions of the three point perspective pose estimation
problem,? International Journal of Computer Vision, vol. 13, no. 3,
pp. 331?356, 1994.
[20] D. Nist« er, ?An efcient solution to the ve-point relative pose prob-
lem,? Pattern Analysis and Machine Intelligence, IEEE Transactions
on, vol. 26, no. 6, pp. 756?770, 2004.
[21] R. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision. Cambridge Univ Pr, 2003.
[22] Y . Xu, S. Oh, and A. Hoogs, ?A minimum error vanishing point
detection approach for uncalibrated monocular images of man-made
environments,? in Computer Vision and Pattern Recognition (CVPR),
IEEE Conference on, 2013, pp. 1376?1383.
[23] J. Civera, O. G. Grasa, A. J. Davison, and J. M. M. Montiel, ?1-
Point RANSAC Inverse Depth EKF Monocular SLAM Matlab Code.?
http://webdiis.unizar.es/

jcivera/code/1p-ransac-ekf-monoslam.html.
[24] G. F. M. M. D. G. S. Andrea Bonarini, Wolfram Burgard and J. D.
Tardos, ?Rawseeds: Robotics advancement through web-publishing of
sensorial and elaborated extensive data sets,? in In proceedings of
IROS'06 Workshop on Benchmarks in Robotics Research, 2006.
1545
