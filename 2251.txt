A New Flexible Controller for a Humanoid Robot That Considers
Visual and Force Information Interaction*
Gan Ma
12
, Qiang Huang
123
, Zhangguo Yu
12
, Xuechao Chen
12
,
Weimin Zhang
12
, Junyao Gao
12
, Libo Meng
12
and Yun-Hui Liu
14
Abstract? To enhance the safety of a humanoid robot when
it is operating a complex environment, a number of methods
that combine visual and force information have been presented.
These methods are generally divided into two approaches. The
rst approach is to coordinate the visual controller and force
controller in a parallel way, and the second approach is to
coordinate them in series. However, these two approaches do not
consider the interaction between the visual controller and force
controller. Specically, the rst approach does not consider the
interaction between the controllers. The second approach only
considers the effect of the output of the visual controller on the
force controller, while the effect of the force controller on the
visual controller is not considered. This study presents a design
for a new exible controller for a humanoid robot that considers
the interaction of visual and force information. The advantages
of the proposed method are that it simultaneously incorporates
the functions of a visual servo controller and a exible controller
as well as its ability to consider the interaction of visual and
force information when a humanoid robot is operating.
I. INTRODUCTION
In contrast to other robots, humanoid robots have the
advantage of being able to adapt to the environment of
humans because of their human-like appearance and struc-
ture [1]. Therefore, humanoid robots are more suited to
serve people. In recent decades, an increasing number of
researchers have been engaged in the study of humanoid
robots, especially in the area of dynamic walking [2] and
balance control [3]. However, there are a number of key
technical issues concerning humanoid robots that remain to
be solved, including the ability to accurately sense and safely
operate in a complex environment.
An image capture method employing a camera, which
can gather large amounts of information from the envi-
ronment, is very useful for the operation of a robot. A
number of researchers have proposed methods to obtain
target information from the camera image and subsequently
use it to control the movement of a robot [4][5]. Meanwhile,
other researchers are dedicated to the study of using visual
information in the dynamic feedback loop [6]-[11]. This
*This work was supported by the National Natural Science Foundation
of China under Grants 61320106012, 61375103, 61273348 and 61175077,
and ?111 Project? under Grant B08043
1
IRI, School of Mechatronical Engineering, Beijing Institute of Tech-
nology, Beijing, China fmagan,qhuang,yuzgg@bit.edu.cn,
yhliu@mae.cuhk.edu.hk
2
Key Laboratory of Biomimetic Robots and Systems, Ministry of Edu-
cation, China
3
Key Laboratory of Intelligent Control and Decision of Complex System,
China
4
The Department of Mechanical and Automation Engineering, The
Chinese University of Hong Kong, Hong Kong
technology is referred to as visual servoing [12][13]. The
above-mentioned methods generally utilize a visual feedback
method to directly control the movement of a humanoid
robot. However, when a humanoid robot is operating in a
complex environment, it tends to come into contact with
objects it may not have noticed. In this case, the robot or
the environment may become damaged if a stiffness control
algorithm is adopted. Thus, pure visual servoing control
cannot guarantee the safety of a humanoid robot operating
in a complex environment.
To enhance the safety of humanoid robots during op-
eration, a number of methods have been proposed. These
methods combine the visual and force information to control
the robot, and they are mainly divided into two approaches.
The rst approach coordinates the visual controller and
force controller in a parallel way. Specically, a controller
coordinator is employed to coordinate the values from the
visual controller and force controller. The nal control values
are obtained from the controller coordinator [14][15]. The
second approach coordinates the visual servo controller and
the force controller in series. Specically, this approach
integrates the visual servo controller and the force controller
into a control loop, and the output of the visual servo
controller is used as the input of the force controller. The
nal control values are obtained from the force controller
[16]-[18]. These two approaches provide exible methods of
controlling a robot's interaction with a known environment.
However, the rst approach does not consider the interaction
of the visual controller and force controller. In contrast, in the
second approach, the visual controller is the main controller,
and this approach simply considers the effect of the visual
controller output on the force controller. Thus, both methods
do not consider the interaction of the two controllers. In fact,
when a humanoid robot performs a complex task, such as
opening a door, the force controller data should also be used
as feedback for the visual controller, which is essential to
enhancing the accuracy of the control system.
This study focuses on the sensing and operating problem
of a humanoid robot in a complex environment. In this study,
a new exible controller that considers the interaction of
the visual and force information for a humanoid robot was
designed. The proposed method simultaneously functions as
both a visual servo controller and a exible controller, and
it can solve the above-mentioned interaction problem caused
by other approaches.
The remainder of this paper is structured as follows.
First, a high-precision, image-based visual servo algorithm
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1036
for an eye-in-hand camera system is designed. Next, a new
exible controller that considers the interaction of visual and
force information for a humanoid robot is designed. Finally,
the task of a humanoid robotic arm approaching a certain
point and drawing a circle on a board is performed by our
humanoid robot platform to verify the proposed methods.
II. MODELING OF A HUMANOID ROBOTIC ARM
This section addresses the modeling of a humanoid robotic
arm. Fig. 1 shows the basic structure and coordinate frames
of the robotic arm. Because this study mainly focuses on the
control of a humanoid robotic arm, the body of the humanoid
robot is simplied as a xed rigid base; the robotic arm
is attached to the rigid base, which consists of three rigid
links, and an eye-in-hand camera is mounted at the tip of
the robotic arm.
The base frame 
b
is placed at a base point of the robotic
arm. The end-effector frame 
e
is xed at the tip of the
robotic arm. 
c
is the camera frame. In this section, the
kinematic model, dynamic model and pinhole model of the
robotic arm are established.
A. Kinematic Modeling of the System
The kinematic model describes motion without consider-
ing the forces that cause it. The forward kinematics of the
robotic arm are
x
e
=f(q); (1)
whereq denotes the vector of the joint angles andx
e
denotes
the position and attitude of the end-effector frame with
respect to the base frame. In this study, the orientation of the
robotic arm is denoted as three parameters using the X-Y-Z
xed angles rotation method [19], where the rotation order
is R
x
(), R
y
() and R
z
(). In this case, x
e
is dened as
x
e
= [x y z   ]
T
: (2)
The velocity kinematics describe the relation between the
joint velocity _ q and the Cartesian velocity _ x
e
, which is
dened as
_ x
e
=J
m
_ q; (3)
where J
m
denotes the Jacobian matrix of the robotic arm.
B. Dynamic Modeling of the System
The dynamic model of a robot describes the relation
between the motion of the robot, and the forces that cause
the motion. In this section, the Lagrangian method is used
to model the robotic system. The robot's dynamics equations
are described as follows:
M(q) q +H(q; _ q) +G(q) =J
T
m
F
ext
+
m
; (4)
where the left three terms denote the inertia force, Coriolis
force and gravity, respectively; F
ext
denotes external force
applied to in the end-effector; and 
m
denotes the input
torque of the robotic arm joints.
Base Frame
Humanoid 
Robotic Arm
b ?
c ?
, q?
Eye-in-hand 
Camera
e ?
Fig. 1. Model of a humanoid robotic arm
Target Object End-effector
c ?
u
v
Eye-In-Hand 
Camera
t ?
e ?
Fig. 2. Eye-in-hand vision system
C. Pinhole Model of the System
In this section, an eye-in-hand visual tracking model is
constructed (Fig. 2). This paper assumes that N (N >=
3) feature points are mounted on the target and that the
coordinates of the feature points are already known with
respect to the target frame 
t
.
It is essential to capture the position and attitude of a
target in real-time as the camera is constantly moving when
the robot is manipulating an object. In this section, the
perspective projection method is employed to model the eye-
in-hand camera projection. In this case, the camera is a
pinhole camera with a perspective projection. The 2D image
frame is denoted byfu;vg. Under this model,
y =
1
z
P
c
x
t
; (5)
where
c
x
t
is the homogeneous coordinates of the feature
point with respect to the camera frame, z is the depth of
the feature points, y = [u;v]
T
is the projection coordinate
of the feature points on the image plane, and P is a matrix,
composed of the rst two rows of the 3 4 perspective
projection matrix M. The matrix M is dependent on the
intrinsic and extrinsic parameters of the camera [20].
1037
III. EYE-IN-HAND VISUAL SERVO CONTROLLER
This section proposes an algorithm for an accurate visual
servo controller that can be used to control the movement
of a robotic arm without external forces being applied. The
objective of the control algorithm is to lock the projection
of all the feature points at a desired xed positiony
d
on the
image plane.
The relation between the image-plane velocity of a feature
point and the velocity of the point with respect to the camera
frame can be described by the image Jacobian matrix
_ y =J
image

c
v
t
c
w
t

(6)
where [
c
v
t
;
c
!
t
]
T
is the velocity of the feature point with
respect to the camera frame and J
image
denotes the image
Jacobian matrix of the camera, which is dened as
J
image
=
"

z
0  
u
z
 
uv


2
+u
2

 v
0

z
 
v
z
 
2
 v
2

uv

u
#
: (7)
When N feature points are used, the extended image Jaco-
bian is denoted by
J
images
=
2
6
6
4
J
image1
:
:
J
imageN
3
7
7
5
: (8)
Because of the hypothesis that the desired position on
the image plane y
d
is constant, the desired velocity and
acceleration on the image plane should be zero, which leads
to the nominal reference
^
_ y
d
= _ y
d
 y = y; (9)
where is positive scalar and y is 21 image error vector
of the feature point.
In this study, we also assume that the desired end-effector
of the robotic arm is moving at a constant velocity. This
assumption leads to the nominal depth
^ z
d
=z 
v
ez

; (10)
where v
ez
denotes the third row of [v
e
;!
e
]
T
. In this case,
the estimation of J
image
is
^
J
image
=
"

^ z
0  
u
^ z
 
uv


2
+u
2

 v
0

^ z
 
v
^ z
 
2
 v
2

uv

u
#
: (11)
The estimation of the velocities of the target relative to
the camera frame are deduced from (6)

c
^ v
t
c
^ !
t

=K
^
J
 1
image
^
_ y
d
; (12)
where K is the gain matrix,
^
J
 1
image
denotes the inverse
matrix of the estimation of J
image
, and [
c
^ v
t
;
c
^ !
t
]
T
denotes
the estimation of the target velocities with respect to the
camera frame.
We denote the desired target velocities in the camera frame
as [
c
v
td
;
c
!
td
]
T
. To reach the desired target velocities, the
estimation error of the end-effector of the robotic arm is
denoted by [v
e
; !
e
]
T
, where

v
e
!
e

=

c
^ v
t
c
^ !
t

 

c
v
td
c
!
td

: (13)
IV. THE NEW FLEXIBLE CONTROLLER FOR A ROBOT
This section discusses the proposed new exible controller,
which considers the interaction of visual and force informa-
tion for a humanoid robot. When the robot is operating in
a complex environment, it tends to come into contact with
objects it may not have noticed. In this case, a pure visual
servo controller cannot guarantee the safety of the robot.
To simultaneously enhance the accuracy and the safety of
the arm during operation, a model that utilizes a virtual mass-
spring-damper in the end-effector of the arm is established
(Fig. 3). Here, the relation between the measured interaction
effort F
ext
and the motion about the end-effector reference
trajectory of the arm x
e
is
J  x
e
+B _ x
e
+Kx
e
=F
ext
; (14)
where x
e
is the displacement of the end-effector position
from the reference point andJ2R
66
,B2R
66
andK2
R
66
are the desired inertia, damping and stiffness values,
respectively. For an anthropomorphic robotic arm to interact
with the environment, the impedance characterization of the
human arm [21] and biomechanical data [22] can help us to
select the correct inertia, damping and stiffness parameters.
The controlled acceleration
^
 x of the system, which is based
on (14), is
^
 x
e
=J
 1
fF
ext
 B _ x
e
 Kx
e
g: (15)
By adding (15) to (13), the total velocities of the end-
effector of the robotic arm can be denoted by [v
e
; !
e
]
T
,
where

v
e
!
e

=

v
ed
!
ed

+

v
e
!
e

+
^
 x
e
T; (16)
[v
ed
; !
ed
]
T
are the desired velocities of the end-effector, and
T is the control period of the system.
Substituting (16) into (3) gives the joint velocities
^
_ q as
^
_ q =J
 1
m

v
ed
!
ed

+J
 1
m

v
e
!
e

+J
 1
m
^
 x
e
T: (17)
Based on the differentiation of (17), the controller is dened
as

m
=M(q)
^
 q +H(^ q;
^
_ q) +G(^ q); (18)
where 
m
is the joint control torques of the robot.
Thus, by properly selecting the constant matrices, the arm
can be controlled by the proposed controller, and the end-
effector of the arm can behave as a mass-damper-spring
system.
The proposed approach is different from [14][15] and [16]-
[18] in terms of its consideration of the interaction of visual
and force information. Specically, the previous approaches
simply present methods to coordinate a visual controller and
a force controller to enhance the safety of the robot while
1038
End-effector
e ?
Virtual Mass-Damper-Spring
c
?
Fig. 3. Flexible model of the robotic arm
 Visual Based 
Flexible Control
Manipulator
Force 
Sensor
m
?
ext
F
0
X
Motion 
Planner
Target Points
e
X
Encoder
y
Fig. 4. The new exible control structure
performing a task. In the proposed approach, the controller
simultaneously functions as a visual servo controller and a
exible controller. The projection coordinate of the target
points on the image plane y, the external forces F
ext
, and
the joint angles detected by the encodersX
e
are the inputs of
the controller, and these inputs are processed simultaneously
in the controller. The joint control torques
m
are the outputs
of the controller. Fig. 4 shows the overall structure of the
proposed exible control method.
V. EXPERIMENTAL VERIFICATION
A. Experimental Platform
To validate the effectiveness of the proposed method, an
experiment is performed using our humanoid robot BHR-
4 (Fig. 5). This robot consists of a torso, head, waist, two
arms and two legs. The height of the robot is 1.65 m, and it
weights approximately 60 kg. This robot has 51 degrees of
freedom (DOF), including 13 facial joints that produce vivid
facial expressions.
Its arms have six degrees of freedom, and they are
designed in an anthropomorphic way: three DOFs in the
shoulder, two DOFs in the elbow and one DOF in the
wrist. The length of the arm is 0.733 m, and it weights
approximately 7.4 kg. The arm is divided into three parts:
the upper arm with a length of 0.33 m, the lower arm with
a length of 0.22 m and the end-effector with a length of
0.183 m. Table I lists the dynamic parameters of the arm.
To perform our drawing task, a pen tip is designed as the
Force-Torque 
Sensor
Target 
Points
Eye-in-hand 
camera
Drawing 
Board
Humanoid 
Robot
b ?
c ?
t ?
Fig. 5. The experimental platform
TABLE I
DYNAMIC PARAMETERS FOR THE ROBOTIC ARM
Mass [kg] Ixx [kgm
2
] Iyy [kgm
2
] Izz [kgm
2
]
Upper Arm 3:807 0:004865 0:001686 0:005318
Lower Arm 2.515 0.002401 0.001043 0.002732
Hand 1.078 0.001673 0.001656 0.002087
end-effector of the arm. In addition, a 6-axis force/torque
sensor is mounted on the end-effector and is used to detect
the forces and torques caused by the drawing board. A SVS
camera with a sampling rate of 15 fps is mounted on the
end-effector of the arm. The base frame 
b
is xed at the
shoulder of the arm, and the target frame 
t
is xed at the
center of the target object. The camera frame 
c
is xed on
the camera.
The robot is controlled by a main control computer and by
joint controllers on every joint. The main control computer
uses the Quick UNIX (QNX) [23] operating system to
control the robot. Communication between the main control
computer and the joint controllers is based on Ethernet for
Control Automation Technology (EtherCAT) communica-
tion, which was developed by Bechhoff. The main control
computer is the master for EtherCAT communication, while
the motor controllers are the slave nodes. The control loop
is set at 250 Hz.
B. Tracking Experiment
To verify the effectiveness of the proposed controller, the
humanoid robotic arm is directed to approach a target board
and subsequently draw a circle on the board. Five target
1039
2
3
4
5
1
Fig. 6. Distribution of feature points
points, which are detected using computer vision techniques,
are used to determine the position and attitude of the target
and are distributed as shown in Fig. 6.
To perform the drawing task, this experiment is divided
into two parts. First, the robotic arm adjusts its position and
attitude to track the starting point of the drawing task. In
this part, no external forces exist; the controller functions as
a pure visual servo controller. When the tip of the robotic arm
comes into contact with the drawing board, the robotic arm
attempts to draw a circle. In this case, the proposed controller
would simultaneously manage the information from both the
camera and the 6-axis force sensor.
In the tracking experiment, the initial position and attitude
of the end-effector are
X
e0
= [0; 0:403; 0:33; 90

; 0

; 0

]: (19)
The nominal position and attitude of the target are
X
t0
= [0; 0:38; 0:23; 88

; 1

; 1

]: (20)
Fig. 7 shows the variations in the position and attitude
of the end-effector. Fig. 8 shows the variations in the line
velocities and the angular velocities of the end-effector. Fig.
7 indicates that both the position errors and attitude errors
decrease during this phase. Fig. 7 also shows that the position
and attitude of the end-effectorX
e
gradually approach those
to the starting drawing point X
t0
. Fig. 8 indicates that the
velocities are continuous and that the uctuation is minimal.
C. Drawing Experiment
When the robotic arm is tracked to the starting draw-
ing point X
t0
on the drawing board, the drawing task
is performed. In this experiment, the controller receives
information from both the camera and the force sensor.
TABLE II
LENGTH ERRORS ON THE DRAWING BOARD
Numbers 1 2 3 4 5 6 7 8 9 10
L [mm] 3:2 2:4 1:8 2:7 0:4 1:9 1:5 1:8 2:3 0:7
Fig. 7. Position and attitude of the end-effector during tracking
Fig. 8. Velocities of the end-effector during tracking
In this experiment, the robot attempts to draw a circle with
a radius of 0.04 m on the drawing board. Fig. 9 shows the
variations in the position and attitude of the end-effector,
and Fig. 10 shows the variations in the velocities of the end-
effector during the drawing task.
As shown in Fig. 9 and Fig. 10, the position and attitude of
the tip of the arm change during the task but nally converge
to the starting drawing point. Fig. 9 shows that the attitude
of the arm remains approximately constant during this phase.
The attitude remains approximately constant because the
robot is controlled to maintain the same attitude vertical to
the drawing board during the drawing task. Fig. 10 indicates
that the arm is moving smoothly.
To validate the accuracy of the proposed method, the ex-
periments are performed ten times, and each time, the target
drawing board is placed in different positions with different
attitudes. This process creates different camera information
and force sensor information inputs for the controller. Table
II shows the length errors L between the desired nal point
and the actual nal point on the drawing board. A statistical
analysis shows that the system has relatively low error, which
indicates that the proposed controller performs well.
VI. CONCLUSION
This study focused on the problem of accurate sensing
and safe operation of a humanoid robot in a complex
environment. The contributions of this paper are as follows:
1040
Fig. 9. Position and attitude of the end-effector during the drawing task
Fig. 10. Velocities of the end-effector during the drawing task
1) A new exible controller that considers the interaction
of visual and force information for a humanoid robot is
designed.
2) The advantages of the proposed method are that it
simultaneously functions as a visual servo controller and
exible controller, and it also considers the interaction of
visual and force information when a humanoid robot is
operated.
3) The proposed method was validated on a humanoid
robotic platform.
For a humanoid robot to operate when walking, it is
crucial to guarantee the dynamic stability of the robot. The
whole-body motion dynamics should be considered [24][25].
This is a difcult task because the robot's balance control
is a challenging problem when planning the whole-body
movement of a humanoid robot on-line. Our future study
will focus on the whole-body movement of a humanoid robot
when operating in an unpredictable environment.
REFERENCES
[1] D. Sakamoto, T. Kanda, and T. Ono, ?Cooperative embodied com-
munication emerged by interactive humanoid robots,? International
JournalofHuman-ComputerStudies, vol. 62, no. 2, pp. 247-265, 2005.
[2] M. F. Silva, and J. A. T. Machado, ?A literature review on the
optimization of legged robots,? Journal of Vibration and Control, vol.
18, no.12, pp. 1753-1767, 2012.
[3] C. Y . Chen, and P. H. Huang, ?Review of an autonomous humanoid
robot and its mechanical control,? Journal of Vibration and Control,
vol. 18, no.7, pp. 973-982, 2012.
[4] J. Kim, I. Park, J. Lee, and J. Oh, ?Experiments of vision guided
walking of humanoid robot, KHR-2,? IEEE International Conference
on Humanoid Robots, pp. 135-140, 2005.
[5] N. Oda, and J. Yoneda, ?Visual feedback control based on optical ow
vector eld for biped walking robot,? IEEE International Conference
on Mechatronics, pp. 635-640, Vicenza, VI, Italy, 2013.
[6] P. Michel, J. Chestnutt, S. Kagami, K. Nishiwaki, J. Kuffner, T.
Kanade, ?Motion Planning Using predicted Perceptive Capability,?
International Journal of Humanoid Robotics (IJHR), vol. 6, no. 3,
pp. 435-457, 2009.
[7] P. Michel, J. Chestnutt, S. Kagami, K. Nishiwaki, J. Kuffner, T.
Kanade, ?GPU-accelerated real-time 3D tracking for humanoid lo-
comotion and stair climbing,? IEEE International Conference on
Intelligent Robots and Systems (IROS), pp. 463-469, 2007.
[8] C. Dune, A. Herdt, O. Stasse, P.-B. Wieber, K. Yokoi, E. Yoshida,
?Canceling the sway motion in visual servoing for dynamic walking in
visual servoing,? IEEE International Conference on Intelligent Robot
and Systems (IROS), pp. 3175-3180, 2010.
[9] N. Mansard, O. Stasse, F. Chaumette, K. Yokoi, ?Visually-Guided
Grasping while Walking on a Humanoid Robot,? IEEE International
ConferenceonRoboticsandAutomation(ICRA), pp. 3041-3047, 2007.
[10] O. Stasse, B. Verrelst, A. Davison, N. Mansard, F. Saidi, B. Verborght,
C. Esteves, K. Yokoi, ?Integrating Walking and Vision to increase
Humanoid Autonomy,? International Journal of Humanoid Robotics,
vol. 5, no. 9, 2008.
[11] G. Taylor, L. Kleeman, ?Hybrid position-based visual servoing with
online calibration for a humanoid robot,? IEEE International Confer-
ence on Intelligent Robots and Systems (IROS), pp. 686-691, 2004.
[12] F. Chaumette, S. Hutchinson, ?Visual servo control, Part I: Basic
approaches,? IEEE Robotics and Automation Magazine, vol. 13, no.
4, pp. 82-90, 2006.
[13] S. Hutchinson, G. D. Hager, and P. I. Corke, ?A Tutorial on Visual
Servo Control,? IEEE Transactions On Robotics and Automation, vol.
12, pp. 651-670, 1996.
[14] B. J. Nelson, J. D. Morrow, P. Khosla, ?Improved Force Control
Through Visual Servoing,? American Control Conference, pp. 380-
386, 1995.
[15] K. Hosoda, K. Igarashi, M. Asada, ?Adaptive Hybrid Visual Ser-
voing/Force Control in Unknown Environment,? IEEE International
Conference on Intelligent Robots and Systems (IROS), pp. 1097-1103,
1996.
[16] G. Morel, E. Malis, S. Boudet, ?Impedance based coimbination of
visual and force control,? IEEE International Conference on Robotics
and Automation (ICRA), pp. 1743-1748, 1998.
[17] V . Lippiello, B. Siciliano, L. Villani, ?Robot Interaction Control
Using Force and Vision,?IEEEInternationalConferenceonIntelligent
Robots and Systems (IROS), pp. 1470-1475, 2006.
[18] V . Lippiello, B. Siciliano, L. Villani, ?A Position-Based Visual
Impedance Control for Robot Manipulators,? IEEE International Con-
ference on Robotics and Automation (ICRA), pp. 2068-2073, 2007.
[19] J. J. Craig, Introduction to Robotics: Mechanics and Control, 3rd
Edition, Pearson Prentice Hall, 2005.
[20] D. Forsyth, J. Ponce, Computer Vision: A Modern Approach, Upper
Saddle River, NJ: Prentice Hall, 2003.
[21] J. M. Dolan, M. B. Friedman, and M. L. Nagurka, ?Dynamic and
Loaded Impedance Components in the Maintenance of Human Arm
Posture,? IEEE Transactions on Systems, Man, and Cyberbetucs, vol.
23, no. 3, pp. 698-109, 1993.
[22] M. Kallmann, ?Analytical inverse kinematics with body posture con-
trol,? Computer Animation and Virtual Worlds, vol. 19, pp. 79-91,
2008.
[23] A. Kavas and D. G. Feitelson, ?Comparing Windows NT, Linux, and
QNX as the basis for cluster systems,? Concurrency Computation
Practice and Experience, vol. 13 (Compendex), pp. 1303-1332, 2001.
[24] L. Sentis and O. Khatib, ?Synthesis of Whole-body Behaviors Through
Hierarchical Control of Behavioral Primitives,? International Journal
of Humanoid Robotics (IJHR), vol. 2, no. 4, pp. 505-518, 2005.
[25] N. Mansard, ?A Dedicated Solver for Fast Operational-Space Inverse
Dynamics,? IEEE International Conference on Robotics and Automa-
tion (ICRA), pp. 4943-4949, 2012.
1041
