Audio-based Localization for Swarms of Micro Air Vehicles
Meysam Basiri
1;2
, Felix Schill
1
, Dario Floreano
1
and Pedro U.Lima
2
Abstract— Localization is one of the key challenges that
needs to be considered beforehand to design truly autonomous
MA V teams. In this paper, we present a cooperative method to
address the localization problem for a team of MA Vs, where
individuals obtain their position through perceiving a sound-
emitting beacon MA V that is ﬂying relative to a reference point
in the environment. For this purpose, an on-board audio-based
localization system is proposed that allows individuals to mea-
sure the relative bearing to the beacon robot and furthermore
to localize themselves and the beacon robot simultaneously,
without the need for a communication network. Our method is
based on coherence testing among signals of a small on-board
microphone array, to obtain the relative bearing measurements,
and an estimator, to fuse these measurements with sensory
information about the motion of the robot throughout time,
to estimate robustly the MA V positions. The proposed method
is evaluated both in simulation and in real world experiments.
I. INTRODUCTION
Employing a swarm of autonomous robots, for achieving
tasks in a collaborative manner, has been of great interest in
the ﬁeld of robotics. Teams of micro air vehicles (MA Vs)
can accomplish aerial coverage tasks more robustly and
more efﬁciently compared to a single ﬂying robot. Tasks
such as security patrols or searching for victims inside a
disaster area can beneﬁt from several autonomous MA Vs
operating in parallel. In addition, by employing multiple
MA Vs and by sharing resources among them, it is potentially
possible to use teams of low cost, lightweight and safe
Micro Air Vehicles (MA Vs) instead of a large and expensive
aerial platform. Other possible applications of MA V swarms
include rapidly-deployable communication networks [1], en-
vironmental monitoring, aerial surveillance and mapping,
trafﬁc monitoring and search and rescue [2].
A key challenge in designing truly autonomous MA Vs
is the robot localization problem, that is the problem of
estimating the MA V’s location relative to its environment.
Individual’s knowledge about their 3D position is essential
for allowing MA Vs to navigate autonomously to different
points in space and to achieve aerial coverage tasks such
as exploration and mapping. This information could also be
used in multi-MA V systems to avoid inter robot collisions by
priori spatial separation of individuals at different altitudes
or locations [3]. Furthermore, by sharing their position with
other team members, individuals can obtain the relative
*This work was supported by a doctoral grant from FCT
(SFRH/BD/51070/2010), EC FP-7 research funding mechanism under
grant agreement no. 266470 and FCT project [PEst-OE/EEI/LA0009/2013].
1
Laboratory of Intelligent Systems, Ecole Polytechnique Federale de
Lausanne, CH-1015 Lausanne, Switzerland (e-mail: meysam.basiri, fe-
lix.schill, dario.ﬂoreano@epﬂ.ch)
2
Institute for Systems and Robotics, Instituto Superior Tecnico, Lisboa,
Portugal (e-mail: pal@isr.ist.utl.pt)
position of their neighbouring robots and use this information
to form formations [4] [5] and avoid collisions with other
MA Vs without the need of priori spatial separation [6].
A localization system for a MA V must satisfy strict
constraints in terms of weight, size, power consumption, pro-
cessing power, three-dimensional coverage and price. These
constraints limit the use of many successful localization
systems, used on ground robots or large aerial vehicles, for
MA Vs. Inspired by some animal groups [7], [8], which use
sound for localization, we propose an audio-based local-
ization system that allows individuals in a MA V group to
obtain their absolute location, by only measuring the relative
bearing to a beacon MA V that is emitting a bird-like chirp
sound throughout time. An audio-based localization system
satisﬁes the imposed constraints on MA Vs and furthermore
have the advantage of being independent of illumination,
weather conditions, such as fog, smoke and dust, and pos-
sible occlusions caused by obstacles or other MA Vs. In
previous works we showed the success of using sound to
locate acoustic targets on the ground [2] and to obtain the
relative position between members of a multi-MA V system
[9]. Such systems could potentially be exploited to perceive
other non-cooperative noise emitting aerial platforms.
This paper is organized as follows: Section II describes
the related work on localization systems for MA Vs. Section
III describes our proposed method for MA V localization in
a Multi-MA V system, by ﬁrstly explaining the estimator that
localizes the MA Vs when bearing-only measurements are
available, in Subsection III-A, and furthermore explaining the
on-board audio-based relative bearing measurement system,
in Subsection III-B. Section IV provides the results of sim-
ulation and real world experiments of the proposed method.
II. STATE OF THE ART
In general, robot localization methods that are addressed
in the literature can be divided into two main categories:
1) Global Localization methods
2) Local Localization methods
Methods based on the former approach determine the
absolute position of robots relative to a global reference
frame with the assist of an external system. Using external
Colour vision cameras or infrared 3-D motion tracking
cameras [10], for indoor aerial robots, and using Global
Positioning System (GPS) and wireless positioning beacons
[11], for outdoor aerial robots, are examples of methods in
this category. The advantage with solutions based on this
approach is the accuracy that they usually provide while
having a low computational complexity. The main drawback
is their dependency on an external system that is not always
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4729
available. Deployment of wireless positioning beacons in the
environment and in advance of each mission, if not impos-
sible, is both costly and time-consuming. Furthermore, GPS
technologies are vulnerable to jamming and interferences,
have low resolution, and are impossible to use in cluttered
environments where there is no direct line of sight with the
transmitting satellites [12]. GPS vulnerability is considered
as one of the main problems that need to be solved before
allowing MA Vs to operate in civilian airspace [13].
Due to disadvantages of the ﬁrst approach, effort has been
put into the design of local localization methods, where
the position of robots are obtained locally, using onboard
sensors, and independent of any external systems. In this
group of methods, localization is achieved using probabilistic
techniques and by only employing on-board proprioceptive
and exteroceptive sensor information. The most common ex-
amples of this approach are the vision based SLAM (Simul-
taneous Localization and Mapping) algorithms that mainly
use an onboard camera to map features in the environment
and to localize the robot [14], [15], [16]. A drawback with the
local localization methods is that they mainly require a high
computational power and a high data storage for operation.
This might not always be available, specially on small scale
micro air vehicles. The need for real-time processing of
high resolution and high frame-rate images, the dependency
on illumination, visual contrast, weather conditions and the
limited ﬁeld of view of vision sensors, the errors caused due
to the high or insufﬁcient number of features in the images,
the long displacement between loop closings and the fast
dynamic nature of MA Vs, are some of the major drawbacks
of the visual SLAM methods for aerial robots [16].
III. PROPOSED METHOD
This section explains our method for localizing MA Vs in
a Multi-MA V system. This method can be considered as a
combination between the two class of approaches, as dis-
cussed in Section II, to employ some of the advantages from
both classes. It allows a group of MA Vs to cooperatively
localize themselves using only their onboard sensors, and
without the need for any external systems, while avoiding
the high complexity nature of the local localization methods.
The idea here is that a single MA V in the group starts to ﬂy
in a circular pattern around a static reference point and fur-
thermore acts as a positioning beacon attracting the attention
of other MA Vs in the group. Other MA Vs then measure the
relative bearing to this beacon robot and estimate the position
of both the beacon robot and themselves, throughout time,
relative to the reference point. No communication between
the robots is required and only prior knowledge of the
behaviour of the beacon robot is used in the estimations.
As illustrated in Figure 1, the beacon robot is controlled
to circle around a desired reference point, while trying to
maintain a previously deﬁned altitude, speed and circling
radius. Many control strategies for guiding a MA V on a
circular path exists [17], [18]. In this work, a vector ﬁeld
based controller, similar to the one proposed in [18], was
used to control the motion of the beacon MA V around the
Fig. 1. Diagram illustrating the positions of two MA Vs, beacon MA V (p
b
)
and observing MA V (po), for two successive time steps.
reference point. The beacon MA V can consider a static point
on the ground, detected by an onboard camera [19], or a
static acoustic target on the ground, detected by an onboard
microphone array [2], as the reference point.
A. Bearing-only localization
In this section, an Extended Kalman Filtering (EKF) based
estimator is derived to provide an observing MA V with a
robust estimate of its location. This is achieved by fusing
the noisy relative bearing measurements with information
about the motion of the MA V through out time, given by its
onboard proprioceptive sensors, and taking into account the
prior knowledge about the behaviour of the beacon robot.
The estimator is recursive and consists of an Initialization
step and two iterative steps, Prediction and Update, that are
explained in the following subsections.
1) Extended Kalman Filtering (EKF):
At time instant k, the position of the beacon MA V and
an observing MA V , relative to the reference point, is given
by position vectorsp
b
(k) andp
o
(k) respectively, wherep
b
is
deﬁned in Cylindrical coordinate system byp
b
= (
b
;
b
;z
b
)
and p
o
is deﬁned in Cartesian coordinate system by p
o
=
(x
o
;y
o
;z
o
). The combination of both position vectors is
considered as the state vector X for the EKF:
X =

p
b
p
o

=


b

b
z
b
x
o
y
o
z
o

T
(1)
Furthermore, a66 covariance matrixP(k) deﬁnes the state
error covariance matrix at time instant k.
Initialization: In this work, an initial state estimation
strategy is proposed in order to obtain a good initial guess
of the state vector X(0) to have a faster convergence in the
state estimation. The EKF is initialized after the ﬁrst reli-
able bearing measurement is obtained, by using the MA V’s
orientation and altitude sensor values:
p
b
(0) = (R
b
;0;Z
b
) (2)
p
o
(0) = Z
b
  !
j  `(R
G
O
  !
b
0
) (3)
` =
(
(Z
b
 Zo)
sign(Z
b
 Zo(0))
  !
jR
G
O
(0)
  !
b0
Z
b
6=Z
o
D
M
2
Z
b
=Z
o
where, Z
b
and R
b
are the prior knowledge of the beacon
MA V’s altitude and circling radius respectively,
  !
j is a unit
vector along the positive z axis of the global coordinate
systemG, () is the vector dot product,Z
o
(0) is the measured
altitude of the observer MA V at time zero and D
M
is the
4730
maximum detection range of the bearing measuring sensor.
Vector
  !
b
0
is a unit vector in the observer MA V’s body
ﬁxed coordinate system O pointing along the direction of
the initial relative bearing measurement.R
G
O
(0) is a rotation
matrix that rotates vectors from coordinate system O to G:
R
G
O
(k) =R
z
( 
o
(k))R
y
( 
o
(k))R
x
( 
o
(k)) (4)
(
o
(k);
o
(k);
o
(k)) are the yaw, roll and pitch orientation
measurements of the observer robot and (R
z
;R
y
;R
x
) are
basic rotation matrices that rotate vectors about the local
z;y;x axis respectively. In the case of ideal measurements
and when Z
b
6= Z
o
, equation (3), obtained using basic
vector operations, calculates the center point of a circle of
radiusR
b
that have the observer MA V on its circumference.
Furthermore, a covariance matrix P(0) is initialized:
P(0) =diag(
2

b
(0)
;
2

b
(0)
;
2
z
b
(0)
;
2
xo(0)
;
2
yo(0)
;
2
zo(0)
)
where 
2
x(0)
is the initial covariance of the state variable
x that are chosen in accordance to the reliability of sensor
readings and the uncertainties in the initial state estimation.
Prediction: In the prediction step, the current state of the
system
~
X(k) is predicted from X(k 1). For the observer
MA V , a probabilistic motion model and the onboard sensor
information, providing the speed and orientation of the MA V ,
is used to predict the position vector ~ p
o
(k) from p
o
(k 1).
~ p
o
(k) =p
o
(k 1)+R
G
O
(k 1)
2
4
V
o
(k 1)dt
0
0
3
5
(5)
where V
o
(k) is the speed sensor reading and dt is the time
interval between the two time steps. The motion model (5) is
derived by assuming that, at every iteration, the MA V has a
forward motion along the x-axis of its body ﬁxed coordinate
system, followed by a three dimensional rotation.
If communication between the robots were available, the
speed and orientation values of the beacon MA V along with
conversions between Cylindrical and Cartesian coordinate
systems could also be used to predict the beacon MA V’s
position vector ~ p
b
(k). However, as we are interested in a
solution that does not depend on a communication network,
only the prior knowledge about the speed V
b
and circling
radius R
b
is used to obtain ~ p
b
(k):
~ p
b
(k) =p
b
(k 1)+
h
0
V
b
R
b
dt 0
i
T
(6)
Furthermore, a prediction of the state covariance matrix,
~
P(k), is obtained by assuming that the uncertainty in state
predictors (5) and (6) is a zero mean multivariate Gaussian.
Update: In the Update step, the relative bearing measure-
ment, presented by vector
~
b
k
, is used to update the state
prediction
~
X(k). For this, a measurement model to predict
the relative bearing from the state predictions is deﬁned:
 = tan
 1

ry
rx

' = tan
 1

rz
p
rx
2
+ry
2

(7)
where
~ r = (r
x
;r
y
;r
z
) = (
b
cos
b
 x
o
;
b
sin
b
 y
o
;z
b
 z
o
)
is a vector in the global coordinate system G that starts at
the position p
o
and ends at the position p
b
.  and ' are the
azimuth and elevation of vector~ r. The predicted bearing (
~
,
~ ') is found by substituting the state predictions
~
X(k), from
equations (5) and (6), into equation (7).
Furthermore, an innovation (k) is deﬁned as the dif-
ference between the predicted bearing (
~

k
; ~ '
k
) and the
measured bearing (
^

k
; ^ '
k
):
(k) =

^

k
 
~

k
^ '
k
  ~ '
k

T
(8)
where
^

k
and ^ '
k
are the azimuth and elevation of vector
~
b
k
expressed in the coordinate system G, i.e R
G
O
(k)
~
b
k
. The
innovation covariance matrix S(k) is computed by:
S(k) =H
~
P(k)H
T
+D (9)
whereD is the error covariance of bearing measurements and
is found empirically. H is the Jacobian of the measurement
model (7) with respect to the states:
H =

@
@X
@'
@X




~
X(k)
=

H
11
::: H
16
H
21
::: H
26






~
X(k)
(10)
where
H
11
= (y
o
cos'
b
 x
o
sin'
b
)/<
1
H
12
=
b
(
b
 x
o
cos'
b
 y
o
sin'
b
)/<
1
H
14
= ( y
o
+
b
sin'
b
)/<
1
H
15
= (x
o
+
b
cos'
b
)/<
1
H
13
=H
16
= 0
H
21
= ((z
o
 z
b
)(
b
 x
o
cos'
b
 y
o
sin'
b
))/<
3
H
22
= 
b
(z
o
 z
b
)(y
o
cos'
b
 x
o
sin'
b
)/<
3
H
24
= (z
o
 z
b
)(x
o
 
b
cos'
b
)/<
3
H
25
= (z
o
 z
b
)(y
o
 
b
sin'
b
)/<
3
H
23
= H
26
=
p
<
1

<
2
<
1
=
b
2
+x
o
2
+y
o
2
 2
b
(x
o
cos'
b
+y
o
sin'
b
)
<
2
=<
1
+(z
o
 z
b
)
2
<
3
=<
2
q
(x
o
 
b
cos'
b
)
2
+(y
o
 
b
sin'
b
)
2
Finally the states are updated:
X(k) =
~
X(k)+K(k)(k)
P(k) =
~
P(k) K(k)H
~
P(k)
where K(k) is the Kalman gain at time k derived by:
K(k) =
~
P(k)H
T
S(k)
 1
B. Audio-based relative bearing measurement
The previous section described a method of estimating
the 3D position of members in a swarm of MA Vs when
only relative-bearing measurements to a ﬂying beacon MA V
is available. In this section, the proposed sensor suite for
obtaining these measurements is described. Figure 2 shows
the schematic diagram of the on-board audio-based relative
bearing measurement system, describing its key units.
The beacon robot is equipped with a small piezo trans-
ducer and is programmed to continuously generate periodic
chirps of predeﬁned frequency. To generate a loud sound
wave, that would result in a higher detection range of the
4731
Chirp 
Generator 
Beacon Robot 
Piezo 
Relat
Me
 
Microphone  
Array 
 Realative  
Bearing 
Observer Robot 
Chirp  
Detector  
Chirp  
Extractor 
Coherence 
measuring 
Direction  
Search 
Fig. 2. Schematic diagram of the proposed Audio-based relative bearing
measurement system illustrating the key parts of the system.
beacon MA V , a band-limited chirp with frequencies close
to the resonance frequency of the piezo element is used. A
chirp is used instead of a pure tone to avoid the problem of
ambiguous measurements caused due to the repetitive nature
of narrowband sounds. Figure 4(a) shows the frequency
spectrogram of a single chirp, recorded in ﬂight by an
observer MA V , in presence of a chirping beacon MA V .
Observing MA Vs are equipped with a microphone array,
shown in ﬁgure 3, to measure the sound waves at different
points in space. An incoming chirp is picked up by the
spatially separated microphones at different time instances.
The existing delay between the microphone signals is used
to measure the direction of arrival of the chirp. A minimum
number of four microphones is needed to obtain the direction
of arrival in 3D without ambiguity. Here, four microphones
are used to minimize the hardware and computational loads.
A tetrahedral microphone array geometry is used to obtain
equivalent localization performance in all directions [20].
Microphone signals are continuously checked by the Chirp
Detector for existence of the desired chirp in the sound
mixture. The presence of a chirp is detected by template
matching, where a template of the chirp, stored in the
memory, is continuously cross-correlated with the signals.
Upon detection of a chirp, the time window holding the entire
chirp, for all microphones, is passed to the Chirp Extractor.
The Chirp Extractor ﬁlters the chirp from other sounds
that might exist in the signal segment. For this, the time
window containing the chirp is passed through a band pass
ﬁlter to remove the unwanted low frequency wind noise and
other high frequency noises (see Figure 4(b)). Furthermore,
Fig. 3. Picture of the MA V platform [21] used in the experiments. A
Microphone array of four microphones and a digital sound recorder is used
for recording sounds during ﬂight.
Fig. 4. In-ﬂight sound of a chirping MA V recorded by an observing
MA V . (a) Spectrogram of a detected chirp (b)Spectrogram of the signal
after band-pass ﬁltering (c) FRFT transform of the band-passed chirp and
the corresponding passband region (d) Spectrogram of the ﬁnal ﬁltered chirp.
the Fractional Fourier transform (FRFT) [22] of the ﬁltered
signal is computed with a FRFT order of  obtained by:
 =
2

tan
 1
(af
s
) (11)
where f
s
is the sampling frequency and a is the chirp rate
used by the beacon MA V . The computed FRFT contains a
single impulse-shaped peak that corresponds to the chirp.
The chirp is ﬁltered out from other sounds that have made it
through the band-pass ﬁlter, by only retaining the bin with
the highest peak along with its few nearby bins and setting
all other bins to zero (see Figure 4(c)). The ratio of the peak
value to the mean value of all zeroed bins prior to zeroing
provides a good measure for the quality of the perceived
chirp. This measure is computed and used as a measure
of reliability of the obtained bearing measurement and only
measurements that satisfy a predeﬁned reliability level are
used in the update step of the EKF estimator. The ﬁltered
chirp in the FRFT domain is then transformed back to the
time domain by computing the inverse FRFT.
After ﬁltering, a measure of similarity between the chirps
of every microphone pair is obtained by computing the
inverse Fourier transform of the cross spectrum:
R
ij
() =
N 1
X
k=0
P
i
[k]P

j
[k]e
i
2k
N
(12)
where P
i
(k) is the discrete Fourier transform of the signal
of microphone i, P

j
denotes the complex conjugate of P
j
and  is the correlation lag in samples in the range:
 
d
m
c
< <
d
m
c
where d
m
is the distance between the microphones and c is
the speed of sound.
A weighting function was introduced into equation (12)
by [23] to whiten the cross-spectrum of the signals and
allow equal contribution of all the frequencies in the cross
correlation. Although this results in sharper cross correlation
4732
?1
?0.5
0
0.5 1
?1
0
1
?1
?0.5
0
0.5
1
 
 
?0.1
0
0.1
0.2
0.3
0.4
0.5
Fig. 5. Grid search of the in-ﬂight chirp signal of Figure 4, showing the
chirp’s direction of arrival likelihood for all directions around the MA V .
Each cell is on the surface of a unit sphere and represents the end of a unit
vector starting from the origin that points towards a direction.
peaks for broadband sounds, it ampliﬁes the background
noise for narrowband sounds. A modiﬁed version of weight-
ing function [23] is used here instead to only allow equal
contribution of the desired frequencies:
R
ij
() =
N 1
X
k=0


P
i
P

j
jP
i
jjP
j
j

e
i
2k
N
(13)
 =

1 f
min
<f <f
max
0 otherwise
where f
min
and f
max
are the minimum and maximum
frequencies of the chirp.
Upon ﬁndingR
ij
from (13), for all microphone pairsij, a
search for the most likely source direction
  !
b
m
is performed.
  !
b
m
= argmax
  !
b
X
i;j
R
ij
(
~
bij
) (14)
where time delay 
~
bij
corresponds to direction
  !
b and is
computed from the coordinates of microphonesi andj in the
body ﬁxed coordinate system. In this work a full direction
grid search, for a spherical geodesic grid of 2562 points,
is performed. Other search methods for further reducing the
cost of this search is available [24]. Figure 5 shows the result
of a grid search for the perceived chirp described in Figure
4 illustrating the likelihood of all grid cells.
IV. EXPERIMENTS AND RESULTS
To verify the bearing-only position estimator, described
in section III-A, a group of MA Vs were modelled using
a computer simulation. Simulated MA Vs were presented
by a ﬁrst order 3D ﬂight model with three degrees of
freedom for the airspeed, turn rate and the altitude, all
controlled by PID controllers. The MA V’s airspeed, turn
rate and altitude dynamics have rate limitations and are
inﬂuenced by a uniform noise. Furthermore, the sensors
that provide the MA V’s orientations, speed, altitude and the
relative bearing to the beacon MA V were modelled to be
affected by a zero mean uniform noise while the relative
bearing sensor was limited in range. Model parameters were
tuned to best represent a simple MA V platform that was
0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
 
 
True path of observer MAV
True path of beacon MAV
Position estimation
Error covariance ellipsoid
50 100 150 200 250 300
0
10
20
30
40
50
60
70
80
Time Steps
Distance (m) / Angle ( 
o
 )
Absolute state estimation error
 
 
 x
o
y
o
z
o
?
b
?
b
z
b
Fig. 6. Results of simulation with 1 observer and 1 beacon MA V . A
uniform noise of 5
o
for relative-bearing/attitude sensors and 1m for
altitude/speed sensors is used. The bearing sensor’s detection range is150m.
top: Position estimates. bottom: Absolute estimation error of EKF states (1).
used throughout the real experiments [21]. A vector ﬁeld
controller is used to steer the motion of the MA Vs onto a
circular path around a point of interest. For a beacon robot
this point is always static, while for an observer robot random
points are generated sequentially to steer the robot between
random points. Figure 6 shows the results of a simulation run
involving a beacon MA V and an observer MA V . It shows
the gradual convergence of the position estimations to the
true position and the reduction in the error covariance.As
expected, the convergence speed depends on the relative
motion between the robots, where for the motions that result
in a faster change of the relative bearing a faster localization
is obtained. Upon localization, a good position tracking is
achieved by the estimator.
Multiple real experiments were performed to verify the
proposed audio-based relative bearing measurement system
and the localization performance of the EKF estimator. A
beacon MA V , equipped with an autopilot, was programmed
to ﬂy in circles around a GPS coordinate with constant
velocity and constant altitude while emitting chirps using
an on-board piezo. An observing MA V , shown in Figure 3,
was then ﬂown manually in proximity of the beacon MA V to
record the sound waves using the on-board microphone array.
The engine power of the observer MA V was occasionally
reduced or turned off to increase the chirp to noise ratio and
the detection range. The orientation, altitude, air-speed and
global positioning information of both MA Vs are measured
using on-board sensors and are transmitted and stored on to
4733
0.9 1 1.1 1.2 1.3 1.4 1.5
GPS position of observer MAV
GPS position of beacon MAV
Position estimation
Error covariance ellipsoid
0 5 10 15 20 25
-150
-100
-50
0
50
100
150
Time (seconds)
Angle ( 
o
 ) / Distance (m)
 
 
Audio based relative Azimuth
GPS based relative Azimuth
GPS based relative distance
Fig. 7. Result of a real experiment with two MA Vs. top: Audio-
based position estimates and error covariance ellipsoid along with path
of MA Vs provided by GPS sensors. bottom: Audio-based relative bearing
measurements along with the relative bearings and the relative distance
between the robots obtained by the GPS sensors.
a ground station. Figure 7 shows the audio-based relative
bearing measurements and the observer MA V’s position es-
timates, along with a comparison against GPS based values,
for an experiment. A good coherence between the GPS-based
and audio-based estimates is observed. A root mean square
error (rms) value of 6.9 degrees, as the error between the
GPS-based and Audio-based bearing measurements, were
obtained from 240 in-ﬂight bearing measurements.
V. CONCLUSION AND FUTURE WORK
A solution to the problem of MA V swarm localization was
presented. This solution consists of a single beacon MA V
that circles around a reference point in space while emitting
continuous linear chirps of predeﬁned frequency spectrum
to assist other MA Vs in localizing themselves. MA Vs are
equipped with an on-board audio-based relative positioning
system, to measure the bearing to the beacon MA V , and
on-board sensors, to obtain information about their motions
throughout time. The proposed EKF-based ﬁlter was shown
to be well-suited for the sensor fusion and achieving a robust
localization. No communication between the robots was
required and only prior knowledge about robot’s behaviour
were used in the estimations. Investigating different types
of MA V motions that could result in a faster localization,
employing multiple beacon MA Vs to improve localization
performance, and study of switching protocols to switch
MA Vs between beacon and observer states, for exploration
and reduction in the swarm’s overall localization error, are
some of the areas of work we are currently pursuing.
REFERENCES
[1] S. Hauert, S. Leven, J. Zufferey, and D. Floreano, “Communication-
based swarming for ﬂying robots,” in Proc. Intl. Conf. Robotics and
Automation Workshop on Network Science and Systems, 2010.
[2] M. Basiri, F. Schill, P. Lima, and D. Floreano, “Robust acoustic
source localization of emergency signals from micro air vehicles,” in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), oct. 2012, pp. 4737 –4742.
[3] D. T. Cole, A. H. G¨ oktoan, and S. Sukkarieh, “The demonstration
of a cooperative control architecture for uav teams,” in Experimental
Robotics. Springer, 2008, pp. 501–510.
[4] M. Basiri, A. Bishop, and P. Jensfelt, “Distributed control of triangular
formations with angle-only constraints,” Systems & Control Letters,
vol. 59, no. 2, pp. 147–154, 2010.
[5] N. Moshtagh, N. Michael, A. Jadbabaie, and K. Daniilidis, “Vision-
based, distributed control laws for motion coordination of nonholo-
nomic robots,” IEEE Transactions on Robotics, vol. 25, no. 4, pp. 851
–860, aug. 2009.
[6] R. Carnie, R. Walker, and P. Corke, “Image processing algorithms for
uav ”sense and avoid”,” in IEEE International Conference on Robotics
and Automation ICRA, may 2006, pp. 2848 –2853.
[7] A. Farnsworth, “Flight calls and their value for future ornithological
studies and conservation research,” The Auk, vol. 122, no. 3, 2005.
[8] P. Muller and D. Robert, “A shot in the dark: the silent quest of
a free-ﬂying phonotactic ﬂy,” Journal of Experimental Biology, vol.
204, no. 6, pp. 1039–1052, 2001.
[9] M. Basiri, F. Schill, D. Floreano, and P. Lima, “Audio-based relative
positioning system for multiple micro air vehicle systems,” in Pro-
ceedings of Robotics: Science and Systems, Berlin, June 2013.
[10] G. M. Hoffmann, S. L. Waslander, and C. J. Tomlin, “Quadrotor
helicopter trajectory tracking control,” in AIAA Guidance, Navigation
and Control Conference, Honolulu, Hawaii, 2008, pp. 1–14.
[11] L. Hu and D. Evans, “Localization for mobile sensor networks,” in
Proceedings of the 10th annual International Conference on Mobile
computing and networking. ACM, 2004, pp. 45–57.
[12] R. Siegwart and I. Nourbakhsh, Introduction to autonomous mobile
robots. MIT press, 2004.
[13] C. James et al., “Vulnerability assessment of the transportation in-
frastructure relying on the global positioning system,” Volpe National
Transportation Systems Center, US Department of Transportation,
Tech. Rep, 2001.
[14] M. Blosch, S. Weiss, D. Scaramuzza, and R. Siegwart, “Vision based
mav navigation in unknown and unstructured environments,” in IEEE
International Conference on Robotics and automation (ICRA), 2010.
[15] M. Bryson and S. Sukkarieh, “Building a robust implementation of
bearing-only inertial slam for a uav,” Journal of Field Robotics, vol. 24,
no. 1-2, pp. 113–143, 2007.
[16] J. Artieda, J. M. Sebastian, P. Campoy, J. F. Correa, I. F. Mondrag´ on,
C. Mart´ ınez, and M. Olivares, “Visual 3-d slam from uavs,” Journal of
Intelligent and Robotic Systems, vol. 55, no. 4-5, pp. 299–321, 2009.
[17] E. W. Frew, D. A. Lawrence, C. Dixon, J. Elston, and W. J. Pisano,
“Lyapunov guidance vector ﬁelds for unmanned aircraft applications,”
in American Control Conference. IEEE, 2007, pp. 371–376.
[18] D. R. Nelson, D. B. Barber, T. W. McLain, and R. W. Beard, “Vector
ﬁeld path following for miniature air vehicles,” IEEE Transactions on
Robotics, vol. 23, no. 3, pp. 519–529, 2007.
[19] B. Grocholsky, J. Keller, V . Kumar, and G. Pappas, “Cooperative air
and ground surveillance,” Robotics & Automation Magazine, IEEE,
vol. 13, no. 3, pp. 16–25, 2006.
[20] J.-S. Hu, C.-M. Tsai, C.-Y . Chan, and Y .-J. Chang, “Geometrical
arrangement of microphone array for accuracy enhancement in sound
source localization,” in Control Conference (ASCC). IEEE, 2011.
[21] S. Leven, J. Zufferey, and D. Floreano, “A simple and robust ﬁxed-
wing platform for outdoor ﬂying robot experiments,” in International
symposium on ﬂying insects and robots, 2007, pp. 69–70.
[22] V . Namias, “The fractional order fourier transform and its application
to quantum mechanics,” IMA Journal of Applied Mathematics, vol. 25,
no. 3, pp. 241–265, 1980.
[23] C. Knapp and G. Carter, “The generalized correlation method for
estimation of time delay,” IEEE Transactions on Acoustics, Speech
and Signal Processing, vol. 24, no. 4, pp. 320–327, 1976.
[24] J. Valin, F. Michaud, and J. Rouat, “Robust localization and tracking of
simultaneous moving sound sources using beamforming and particle
ﬁltering,” Robotics and Autonomous Systems, vol. 55, no. 3, pp. 216–
228, 2007.
4734
