Smart Tissue Anastomosis Robot (STAR): Accuracy Evaluation for
Supervisory Suturing Using Near-Infrared Fluorescent Markers
Simon Leonard, Azad Shademan, Yonjae Kim, Axel Krieger and Peter C. W. Kim
Abstract— This paper speciﬁes and evaluates the accuracy of
the Smart Tissue Anastomosis Robot (STAR). The STAR is a
proof of concept vision-guided robotic system equipped with an
actuated laparoscopic suturing tool and a multispectral vision
system. The STAR supports image-based suturing commands
and is capable of detecting near-infrared ﬂuorescent (NIRF)
markers that provide reliable visual segmentation and tracking.
The paper reports the best case scenario accuracy speciﬁcations
of the STAR as derived from its conﬁguration and calibration
parameters. We also evaluate experimentally the effects of
overlaying NIRF markers on the accuracy of the STAR when
these markers are used as the source of image-based commands
and we compare these results to the accuracy of the STAR with
image-based commands generated from plain color images. Our
results demonstrate that the STAR is able to place sutures on
a planar phantom with an average accuracy of 0.5 mm with a
standard deviation of 0.2 mm and that NIRF markers have no
statistically signiﬁcant adverse effect on the accuracy.
I. INTRODUCTION
Robotic assisted surgeries (RAS) have signiﬁcantly im-
proved the ergonomics for surgeons and contributed to faster
recoveries and shorter hospitalizations for patients. Despite
this progress, one of the most time-consuming and repetitive
stages during a laparoscopic surgery remains the suturing
or stapling of surgical incisions such as anastomoses. The
average duration for a RAS laparoscopic anastomosis ranges
from 30 to 90 minutes for intestinal [1] and 28 minutes for
vascular [2]. Thus, the motivations to partially or fully auto-
mate laparoscopic suturing are 1) the low suturing efﬁciency
of current surgical robots and 2) the variability of procedural
techniques between surgeons affects the consistency of vali-
dated metrics for anastomosis such as bursting strenght and
accuracy [3].
Several surgical robots have proposed to automate laparo-
scopic suturing but the challenges of sensing and tracking
tissues in a surgical environment have prevented most of
them from reaching clinical stages. These challenges mainly
stem from the poor visibility of the tissues, occlusions,
specularities and non-rigid deformations. Whereas several
RAS systems address these challenges by augmenting or
complementing human perception [4] or motor skills [5],
the Smart Tissue Anastomosis Robot (STAR) [6] aims to
substitute the motor skills of surgeons and sensing by using
actuated tools and sensing that are designed to address chal-
lenges of laparoscopic suturing. To address the challenges
of tissue sensing and tracking that are inherent to surgeries,
the STAR uses near-infrared ﬂuorescent (NIRF) markers in
The authors are with the Sheikh Zayed Institute for Pediatric Surgical
Innovation at the Children’s National Medical Center, Washington DC
(a) STAR system. (b) Circular
needle of the
Endo360

.
Fig. 1. a) The STAR hardware: A 7 DOF LWR mounted with an modiﬁed
Endo360

. b) The actuated needle of the STAR.
combination with a multispectral vision system that is ca-
pable of segmenting the markers reliably despite occlusions
and overlaying them on intraoperative color images. This is
possible because NIRF light penetrates through blood and
tissue. These markers are placed by a surgeon at the desired
stitching sites and the markers are segmented and tracked by
the vision system to guide the motion of the robot.
The ﬁrst contribution of this paper is to derive the open-
loop accuracy speciﬁcations of the STAR in a non-clinical
environment. These speciﬁcations are used to determine
which suturing tasks could be executed by the STAR and
to establish a baseline for more realistic experiments. The
second contribution is to evaluate the accuracy of the STAR
with and without NIRF markers and to determine if the
accuracy is within the speciﬁed bounds. Finally, we answer
the question as to whether the NIRF markers affect the STAR
accuracy in our experiments.
The main components of the STAR are a robot arm
equipped with an actuated suturing tool and a multispectral
vision system (Fig. 1a). The STAR tool is an actuated version
of the Endo360
 R 
(EndoEvolution, North Chelmsford, MA)
that is mounted on a 7 degrees of freedom (DOF) Light
Weight Robot (LWR) from Kuka (Kuka AG, Augsburg
Germany). The tool has two actuators, one to drive a circular
needle and one to drive the pitch axis located 46 mm above
the tip of the tool as illustrated in Fig. 1b. The multispectral
vision system enables to visualize intraoperative color and
infrared images. Infrared images are used to enhance visual
tracking by segmenting NIRF markers that can be segmented
when occluded by up to 2 mm of tissues [7]. The color
images are used to manually select the image coordinates
of each stitch and these coordinates are transformed to 3D
suturing commands to the robot that plans and executes the
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1889
motion.
II. PREVIOUS WORK
The ﬁeld of robotic assisted surgeries includes a broad
range of applications including orthopedics, neurology, urol-
ogy, ocular and general surgeries. In the case of minimally
invasive surgeries (MIS), one important challenge is to work
in a conﬁned space under the constraints imposed by the
tools insertion points. Robotic systems such as the da Vinci
Surgical System [8], Raven [9] and MiroSurge [10] are
teleoperated MIS robots that provide surgeons with a greater
dexterous workspace and improved visualization when com-
pared to manual laparoscopic instruments. It is important
to underline, however, that the purpose of these systems
is to enhance the skills of surgeons as opposed to fully
or partially automate a surgical procedure or part thereof.
Previous research has demonstrated the beneﬁts of actively
modifying the motion of these robots according to a speciﬁed
task or constraints. One example involve a system capable
to segment a surgeon’s motion during a suturing task with a
da Vinci [11] and to synchronize a similar motion segment
recorded by an expert. Various systems have proposed to
combine haptic feedback with virtual ﬁxtures [12] to guide
the motion of the surgeon along a path in the workspace [13],
[14] or to prevent the end-effector from entering speciﬁc
areas [15].
A different paradigm for RAS consists of shifting a
greater portion of autonomy on the robot system, sensors and
controllers. Several laparoscopic hand-held tools for suturing
or knot tying have been designed [16], [17], [18], [19]
and some master-slave robots provide features for suturing
such as force feedback [20], programmable modes [21] and
virtual ﬁxture for tool alignment [13]. None of these systems,
however, are able to close an incision without a “surgeon
in the loop”. Recently, a single arm robot equipped with
a laparoscopic needle holder demonstrated it could insert a
needle in a phantom by using circular markers [22]. The
accuracy criteria, however, consisted of successfully inserting
the needle within 3 mm surface markers and the results
varied according to light intensity. The STAR proposes a
similar supervised control approach that shifts the burden
of planning and executing the motion on the robotic system
while maintaining the role of the surgeon to decide where
to place each suture. The STAR, however, combines a
specialized suturing tool with vision system that enables to
reliably segment and track NIRF markers.
III. SMART TISSUE ANASTOMOSIS ROBOT
The STAR system is a combination of robot hardware,
multispectral imaging and control software. In this paper
we report the accuracy of the STAR for suturing on a
planar phantom. Although a planar phantom represents a
signiﬁcant assumption for any surgical task, similar con-
ditions have been reported in the literature for systems in
the pre-clinical development phase [13], [18], [22], [23],
[24]. Furthermore, given that this paper aims to evaluate
the accuracy of the STAR in a non-clinical environment, a
Fig. 2. Computer-aided design of the STAR tool.
planar phantom provides a reliable ground truth that enables
accurate measurements of non-biomechanical parameters.
A. Actuated Suturing Tool
We designed and built a single degree-of-freedom suturing
stage, motorizing pitch and needle actuation of a commer-
cially available manual suture tool Endo360

. Fig. 2 shows a
computer aided design (CAD) drawing of the automated su-
turing tool design. The manual Endo360

tool is able to drive
a disposable suture needle with attached 3-0 suture along a
circular path, enabling single handed laparoscopic suturing.
The manual handle of the Endo360

was disconnected and
the tool head, tool shaft, cables to actuate needle drive and
push-pull bar to actuate pitch were connected to an aluminum
bracket. Two DC brush motors (Maxon Motors, Sachseln,
Switzerland) drive pitch and needle actuation through two
bevel gears activating a cam drive for pitch actuation and
a bidirectional cable drive with pulley for needle actuation.
Two turnbuckles allow for adjusting the tension of the cables
for the needle actuation. Epos 2 (Maxon Motors, Sachseln,
Switzerland) controllers are used for motor control.
B. Near Infrared Fluorescence Imaging (NIRF)
NIRF imaging is a technique that uses the near-infrared
(NIR) wavelengths of the light spectrum (700 1000 nm) to
excite a ﬂuorophore agent, which emits its excited photons in
the NIR wavelengths. Excited photons propagate and pene-
trate soft tissue, enabling better signal-to-noise ratios in poor
visible-light conditions. NIRF imaging has been successfully
used for several biomedical imaging applications.The main
application has been oncology and the intraoperative visual-
ization of cancer cells during surgery [25]. The ﬁeld of NIRF
is rapidly growing and there has been recent examples of
its application in intraoperative image-guided surgery [26],
[25].
We developed a NIRF imaging system to be used for
supervisory suturing by a surgeon via a graphical user
interface. The proposed imaging system consists of a high
power light-emitting diode (LED) light source acting as the
NIRF excitation light, a NIR camera with emission ﬁlter to
capture the emitted NIR light, and a color camera to capture
the visible-light scene. The excitation light is a custom-
built eight-LED ring light with narrow-band high-power 760
nm LEDs (North Coast Technical Inc., Chesterland, OH).
The wavelength is chosen according to the peak absorption
1890
Fig. 3. NIRF Imaging System Diagram.
wavelength of the ﬂuorophore. The NIRF imaging sensor
is an acA-50gm 2000NIR gigabit Ethernet (GigE) camera
(Basler Inc., Exton, PA) equipped with the NIR-enhanced
CMOSIS CMV2000 2048 1088 pixel CMOS sensor and a
bandpass 845nm 55nm ﬁlter (Chroma Technology Corp.,
Bellows Falls, VT). The color image is captured with a
1288 964 Flea2 camera (Point Grey Research, Richmond,
BC) with a color (RGB) Sony ICX445 sensor. Both cameras
use Tamron 23FM08-L lenses with focal length of 8 mm. A
diagram of our NIRF imaging system is depicted in Fig. 3.
A typical ﬂuorophore, which re-emits light upon infrared
light excitation, is indocyanine green (ICG). We have previ-
ously developed NIRF markers based on the ICG [7]. The
details of the NIRF marker preparation and a feasibility
study of its applicability for visual tracking is studied in our
previous work [7].
The NIRF marker is delivered with a 20 ml Biohit Proline
Mechanical Pipette, which allows delivery of small droplets
of ICG on the accuracy handle (see Fig. 4, and 7).
C. Supervisory Control
As described in Section II, several RAS systems are de-
signed around the master-slave or shared control paradigms
that enable a surgeon to directly control the motion of a
robot. The STAR system is built around the supervisory
control paradigm that enables a surgeon to specify high
level suture commands and leave the details of implementing
the suturing motion to the robotic system. As presented in
Section III-B, the current version of the STAR provides
multispectral intraoperative images that overlays segmented
NIRF markers on the RGB images. Using a computer input
device, a surgeon generates image-based suture commands
by selecting the markers in a desired order. As illustrated in
Fig. 4, however, the segmented NIRF markers used in our
experiments are not consistent and some have a larger size
than others. This is caused by a range of factors including the
volume of the ICG deposit, the amount of light it receives
and the diffusion caused by surrounding tissues. It is thus
important to evaluate the effect, if any, of these markers
on the STAR accuracy and if they can be used reliably for
vision-based guidance. Several parameters must be consid-
ered to determine the 3D suturing accuracy of the STAR.
First, kinematics errors result from possible errors in the
model of the LWR, misalignment between the laparoscopic
tool shaft and the LWR ﬂange and errors in position of
Fig. 4. NIR markers are segmented from the NIR camera and overlaid on
top of the RGB image. The empty slot in the second row is caused by a
damage to the pad and not caused by a lack of ﬂuorescense.
Fig. 5. Schematic drawing of coordinate frames. X axes are in red, Y axes
are in green and Z axes are in blue. The plane normal n corresponds to the
Z axis of the plane.
the tool pitch axis. Second, hand-eye calibration and camera
calibration errors result in errors in robot commands. Third,
the manual selection of a selected 3D point can result in
errors of several pixels. Although we concede that kinematics
errors are unavoidable, we carefully assessed the kinematics
calibration of the LWR during the hand-eye calibration. The
measured deviation of the tool was in the order of 0.1mm
which is signiﬁcantly smaller than errors from the hand-eye
calibration and selection errors and was therefore discarded.
Several relationships must be determined in order to move
the tool control point (TCP) to a 3D position that corresponds
to a selected image point and these relationships are subject
to measurement errors. Given the planar surface of the
phantom, the homography between the suturing plane and the
image plane must be known and the transformation between
the cameras and the robot must also be known. As illustrated
in Fig. 5, the homography H between the suturing and image
planes does not constrain the origin of the coordinate frame
of the suturing plane and it does not constrain the orientation
of the plane about its normal axis. Thus, we impose the
relative orientation between the suturing plane and the color
camera to have zero rotation about Z such that the orientation
of the plane only requires rotations about X and Y axes.
Errors resulting from camera calibration are available from
the camera parameters and the reprojection errors. Let K
deﬁne a camera projection matrix
K =
2
4
f
x
0 c
x
0 f
y
c
y
0 0 1
3
5
(1)
where f
x
= s
x
f and f
y
= s
y
f are the scaled focal length and
1891
c
x
and c
y
are the coordinates of the optical center. Let
C
E
P
=

r
1
r
2
r
3
t
0 0 0 1

(2)
where r
1
, r
2
and r
3
are columns of a rotation matrix and t
is a translation vector that deﬁne the rigid transformation of
the suturing plane P with respect to the camera coordinate
frame C as illustrated in Fig. 5. A point on the suturing plane
is deﬁned by the coordinates
P
P =

P
X
P
Y 0

T
and is
projected to an image point with homogeneous coordinates
p=

x y 1

T
according to a homography
p H

P
X
P
Y 1

T
; (3)
where H is deﬁned up to a scale by
H = K

r
1
r
2
t

: (4)
If H is non-singular, the coordinates
P
X and
P
Y are deter-
mined by
w

P
X
P
Y 1

T
= H
 1
p (5)
where w is the scale factor. Furthermore,
P
P is transformed
to a 3D point in the coordinate system of the camera by
C
P=
C
E
P

P
X
P
Y 0 1

T
: (6)
We deﬁne the suturing plane normal by n and the distance
between the color camera and the suturing plane by d.
Substituting Equation (5) in (6) (taking care of normalizing
the homogeneous coordinates) we obtain that the 3D camera
coordinates of an image point p are given by
C
P=
d
n
T

f
y
(x  c
x
) f
x
(y  c
y
) f
x
f
y

T
2
4
f
y
(c
x
  x)
f
x
(c
y
  y)
  f
x
f
y
3
5
: (7)
Given that we constrain the orientation of the
plane with respect to the camera, we note that
n =

sin(a)  sin(a)cos(b) cos(a)cos(b)

T
, where a
and b are the rotation angles about X and Y respectively
and d = n
T
t.
Once the
C
P is computed, the coordinates are transformed
in the coordinate system of the robot with
R
P=
R
E
C
C
P: (8)
According to Equation (7), the coordinates of a point in the
camera frame depend on the following nine parameters: ﬁve
extrinsic camera parameters,four intrinsic camera parameters
and two image coordinates. Given the non-linear nature of
Equation (7), if
X=

t
T
a b f
x
f
y
c
x
c
y
x y

T
designates a random vector of the parameters with expected
value
¯
X and covariance S
X
, the covariance of a 3D point in
the camera coordinate frame is given byS
P
= J
¯
X
S
X
J
T
¯
X
where
J
¯
X
is the Jacobian of partial derivatives ¶
C
P=¶X evaluated
at
¯
X.
Fig. 6. Error from placement selection is used to estimate the selection
covariance S
selection
We decompose the 11 11 matrix S
X
in the following
block diagonal
S
X
=

S
camera
0
0 S
selection

where S
camera
= (J
T
C
S
 1
points
J
C
)
 1
is the 9 9 covariance of
the estimated camera, where J
C
is a N 9 camera Jacobian
that is evaluated from N measured points as described in [27]
and S
selection
is the 2 2 covariance caused by errors in the
selection of a placement as illustrated in Fig. 6. Given a
spherical marker with radius r and 3D coordinates
P
P, the
covariance S
selection
captures the error in selecting a pixel
that does not correspond to the center of the marker. Thus,
although S
selection
can be arbitrarily large, it determines how
accurate a surgeon must be in selecting a placement to
achieve a desired accuracy given a hand-eye conﬁguration.
Likewise, the covariance resulting from estimating the
hand-eye calibration transformation
R
E
C
is determined from
residuals during the calibration procedure. The STAR is
conﬁgured as an eye-to-hand system where the camera is
static and observes the tip of the robot laparoscopic tool.
Because of this conﬁguration, our hand-eye calibration pro-
cedure involves mounting a calibration rod on the robot and
moving the TCP to a few positions on a calibrating pattern
that is registered in the camera coordinate system. Using a set
of N measurements
P
P
i
(1 i N) in the robot coordinate
system and the corresponding measurements
C
P
i
in the
camera coordinate system, we use the standard least-squares
method presented in [28] to estimate the transformation
R
E
C
.
Using the errors resulting from this registration we estimate
the hand-eye covariance S
handeye
= J
H
S
H
J
T
H
where J
H
is the
Jacobian between the camera and robot frames andS
H
is the
covariance of measured errors.
IV. EXPERIMENTS
We ﬁrst performed a full calibration of the STAR from
which the accuracy speciﬁcations are derived. Then, we
performed two sets of experiments, one with NIRF markers
and one without, to validate the accuracy of the STAR and
to compare these results to the speciﬁcations. The LWR
manipulator is mounted upside down on the ceiling facing
an operating room table. Given that the STAR tool overall
length measures 680 mm, the height of the table was adjusted
at 1.7 m below the base of the robot to enable the tool control
point to reach the table while avoiding full extension. The
cameras were mounted side by side looking down with their
1892
Fig. 7. Handle used to measure STAR accuracy.
optical axis slightly looking down the Y axis of the robot
base. The RGB camera, which is used as the main video
source, framed an area of 1210 cm of the table (see Fig. 4
for a typical view).
We molded gelatin suturing pads to measure the accuracy
of the STAR. Each pad provides 25 suturing handles each of
size 1035 mm that are distributed evenly on a 1010 cm
ﬂat surface (see Fig. 4). For the NIRF markers experiment,
the top surface of each handle contains a recessed hemisphere
of 1 mm radius that is able to contain roughly 15 mL of ICG
as illustrated in Fig. 7.
A. Theoretical Accuracy Speciﬁcations
The images from the RGB camera were selected as the
main video source for the user interface. The main motivation
for this decision is the better calibration of the RBG camera
compared to the NIR camera. Thus, only image points
from the RGB camera are used to specify suture commands
and, therefore, only the parameters of the RGB camera are
relevant to estimate the accuracy of the STAR.
The calibration of the RGB camera was validated with a
calibration pattern and the root mean square (RMS) repro-
jection error was 0.2 pixel. This error was used to generate
the covariance matrix S
points
and, along with the Jacobian
J
C
also derived from the calibration validation, was used to
evaluate S
camera
.
The covariance S
selection
for selection errors was set to 5
pixels for both horizontal and vertical image coordinates.
This value roughly corresponds to average radius of the
projected markers in the image.
The hand-eye calibration procedure was described in Sec-
tion III-C. The procedure involves using the robot to touch a
few ﬁducial points on a calibration pattern and then estimate
the rigid transformation between the 3D coordinates mea-
sured with the robot and those measured with the calibrated
RGB camera. The hand-eye calibration resulted in a RMS
error of 0.23 mm. During the hand-eye calibration we also
validated the kinematics deviations of the STAR by ensuring
that the relative motion between each position was consistent
with those on the calibration pattern. The deviation of the
tool was measured at 0.1 mm and was neglected.
Using the covariance S
camera
and S
selection
, the 11 11
covariance matrix S
X
is computed and then converted to the
3D covariance matrix S
P
with the Jacobian J
¯
X
. Finally, S
P
,
which represents the uncertainty of the 3D coordinates in
the camera coordinate frame, is converted to the covariance
matrix S
H
which represents the uncertainty of the 3D coor-
dinates in the robot frame. A matrix S
H
was computed for
each image coordinate in the RGB camera and the average
errors are reported in Table I.
TABLE I
THEORETICAL AVERAGE ERRORS AND STANDARD DEVIATIONS
m
X
(s
X
) m
Y
(s
Y
) m
Z
(s
Z
)
1.4 mm (0.1 mm) 0.8 mm (0.1 mm) 1.0 mm (0.2 mm)
TABLE II
MEASURED AVERAGE ERRORS AND STANDARD DEVIATIONS
Red Markers NIRF Markers
0.4567 mm (0.1495 mm) 0.4721 mm (0.1695 mm)
We observe that the deviation along X is greater than the
deviation along Y . This is explained by the orientation of the
camera with respect to the robot where the optical axis of
the camera was slighly aligned with the X axis of the robot.
B. Ink Markers
In our ﬁrst experiment we only used RGB images to
determine the stitches placements. We used a red marker to
draw a small dot (roughly 1 mm radius) on the top surface of
each handle which resulted in dots of about 6 pixels radius in
the RGB images. The images were displayed on a computer
screen and the placement of each stitch was selected by
manually clicking in the middle of each dot. Each mouse
click was transformed to the coordinate system of the robot
using Equations (8) and (7). Given that the kinematics of the
STAR tool accounts for the needle tip, 1.5 mm was added
to the Z coordinate to enable the needle to bite the gelatin.
The normal orientation of the TCP was aligned opposite to
the normal of the plane.
The error of each suture was determined by
measuring the distance with a caliper between
the thread and the center of the marker while
looking through the gelatin as shown in Fig. 8.
Fig. 8. Measurement of error
for each handle.
The mean error and standard de-
viation for this experiment are
reported in Table II and are
within the margins of conﬁdence
reported in Table I.
C. NIRF Markers
We repeated the previous ex-
periment by replacing the ink
dots with NIRF markers. A small dose of ICG was dropped
into the cavity on the surface of each handle by using a
single channel micropipette. The NIR light source was placed
above the suturing plane and the markers were segmented
with image processing algorithms consisting of histogram
equalization, morphological operations and thresholding. The
resulting binary image was overlayed on the color image by
means of a homography which maps the view of the suturing
plane in the NIR images onto the view of the suturing plane
in the color images. The homography was computed during
the calibration of the cameras.
1893
After the completion of the suturing, we measured the
error between the center of the hemispheres and the thread.
The mean error and standard deviation for this experiment
are reported in Table II and are also within the margins of
conﬁdence reported in Table I.
To determine if the NIR markers affect the accuracy error
we tested the hypothesis that the mean error when using
NIRF markers is equal to the mean error without using NIRF
markers by performing two-sample t-test. The test result p=
0:7397 suggests that NIRF markers did not have a signiﬁcant
impact on the the accuracy of the STAR.
V. CONCLUSION AND FUTURE WORK
The STAR aims to relieve a surgeon from the burden
of manual suturing by partially automating laparoscopic
suturing. The system combines a robot arm with an actu-
ated suturing tool totalling 8 DOF with vision-guidance to
enable a surgeon to place stitches by selecting them from
intraoperative images. The STAR also provides multispectral
imaging to segment and track NIRF markers that are placed
on the surface of tissues. This paper reports the accuracy
speciﬁcations of the STAR and evaluates its accuracy in
practice by measuring the errors at several positions on
a planar phantom. Our experiments demonstrate that the
accuracy of the STAR is less than 0.5 mm and is within the
expected speciﬁcations. We also demonstrate that accuracy of
the STAR is not affected by NIRF markers and its accuracy
remains within the speciﬁcations. We are currently in the
process of extending the capabilities of the STAR to suture
3D structures by replacing the RGB camera with a light-ﬁeld
camera and by developing an actuated stapling tool.
ACKNOWLEDGEMENTS
The authors would like to thank Dr. Matthieu F. Dumont
for preparing the ICG ﬂuorophore.
REFERENCES
[1] J. Ruurda, I. Broeders, B. Pulles, F. Kappelhof, and C. van der Werken,
“Manual robot assisted endoscopic suturing - Time-action analysis in
an experimental model,” Surgical Endoscopy And Other Interventional
Techniques, vol. 18, no. 8, pp. 1249–1252, August 2004.
[2] P.
ˇ
St´ adler, L. Dvoˇ r´ aˇ cek, P. Vit´ asek, and P. Matouˇ s, “The application
of robotic surgery in vascular medicine,” Innovations, vol. 7, no. 4,
pp. 247–253, 2012.
[3] M. Waseda, N. Inaki, J. R. T. Bermudez, G. Manukyan, I. A. Gacek,
M. O. Schurr, M. Braun, and G. F. Buess, “Precision in stitches: Radius
surgical system,” Surgical Endoscopy, vol. 21, no. 11, pp. 2056–2062,
November 2007.
[4] K. K. Badani, A. Bhandari, A. Tewari, and M. Menon, “Comparison
of two-dimensional and three-dimensional suturing: Is there a differ-
ence in a robotic surgery setting?” Journal of Endourology, vol. 9,
December 2005.
[5] D. Nio, R. Balm, S. Maartense, M. Guijt, and W. A. Bemelman, “The
efﬁcacy of robot-assisted versus conventional laparoscopic vascular
anastomoses in an experimental model,” European Journal of Vascular
and Endovascular Surgery, vol. 27, pp. 283–286, 2004.
[6] S. Leonard, K. L. Wu, Y . Kim, A. Krieger, and P. C. Kim, “Smart
tissue anastomosis robot (STAR): A vision-guided robotics system for
laparoscopic suturing,” IEEE Transactions on Biomedical Engineering,
Preprint.
[7] A. Shademan, M. F. Dumont, S. Leonard, A. Krieger, and P. C. W.
Kim, “Feasibility of near-infrared markers for guiding surgical robots,”
in SPIE Proceedings Vol. 8840 Optical Modeling and Performance
Predictions VI, vol. 8840, 2013.
[8] G. T. Sung and I. S. Gill, “Robotic laparoscopic surgery: a comparison
of the da vinci and zeus systems,” Urology, vol. 58, no. 6, p. 893898,
December 2001.
[9] J. Rosen and M. Hannaford, “Doc at a distance,” IEEE Spectrum, pp.
34–39, October 2006.
[10] U. Hagn, R. Konietschke, A. Tobergte, M. Nickl, S. Jrg, B. Kbler,
G. Passig, M. Grger, F. Frhlich, U. Seibold, L. Le-Tien, A. Albu-
Schffer, A. Nothhelfer, F. Hacker, M. Grebenstein, and G. Hirzinger,
“DLR MiroSurge: a versatile system for research in endoscopic
telesurgery,” International Journal of Computer Assisted Radiology
and Surgery, vol. 5, pp. 183–193, March 2010.
[11] H. C. Lin, I. Shafran, D. Yuh, and G. D. Hager, “Towards automatic
skill evaluation: Detection and segmentation of robot-assisted surgical
motions,” Computer Aided Surgery, vol. 11, no. 5, pp. 220–230, 2006.
[12] J. J. Abbott, P. Marayong, and A. M. Okamura, “Haptic virtual ﬁxtures
for robot-assisted manipulation,” Robotic Research, vol. 28, pp. 49–64,
2007.
[13] A. Kapoor, M. Li, and R. Taylor, “Spatial motion constraints for robot
assisted suturing using virtual ﬁxtures,” in Medical Image Computing
And Computer-Assisted Intervention (MICCAI’05), vol. 3750, 2005,
pp. 89–96.
[14] P. Marayong, M. Li, A. Okamura, and G. Hager, “Spatial motion con-
straints: Theory and demonstrations for robot guidance using virtual
ﬁxtures,” in Proceedings of the 2003 IEEE International Conference
On Robotics And Automation, 2003, pp. 1954–1959.
[15] L. B. Rosenberg, “Virtual ﬁxtures: Perceptual tools for telerobotic ma-
nipulation,” in IEEE Virtual Reality Annual International Symposium,
1993, pp. 76–82.
[16] H. Wang, S. Wang, J. Ding, and H. Luo, “Suturing and tying knots
assisted by a surgical robot system in laryngeal mis,” Robotica, vol. 28,
pp. 241–252, March 2010.
[17] B. Brehmer, C. Moll, A. Markis, R. Kirschner-Hermanis, R. Kn¨ uchel,
and G. Jakse, “Endosew: A new device for laparoscopic running
sutures,” Journal Of Endourology, vol. 22, pp. 307–311, February
2008.
[18] T. Go¨ opel, F. Ha¨ artl, A. Schneider, M. Buss, and H. Feussner,
“Automation of a suturing device for minimally invasive surgery,”
Surgical Endoscopy, vol. 25, pp. 2100–2104, July 2011.
[19] J. F. Kuniholm, G. D. Buckner, W. Nifong, and M. Orrico, “Automated
knot tying for ﬁxation in minimally invasive, robot-assisted cardiac
surgery,” Journal of Biomechanical Engineering, vol. 127, pp. 1001–
1008, 2005.
[20] R. Bauernschmitt, E. U. Schirmbeck, A. Knoll, H. Mayer, I. Nagy,
N. Wessel, S. M. Wildhirt, and R. Lange, “Towards robotic heart
surgery: Introduction of autonomous procedures into an experimental
surgical telemanipulator system,” The International Journal of Medical
Robotics and Computer Assisted Surgery, vol. 1, pp. 74–79, 2005.
[21] H. Kang and J. T. Wen, “Robotic assistants aid surgeons during
minimally invasive procedures,” IEEE Engineering in Medecine and
Biology, vol. 20, no. 1, pp. 94–104, 2001.
[22] S. Iyer, T. Looi, and J. Drake, “A single arm, single camera system for
automated suturing,” in Proceedings of the IEEE 2013 International
Conference on Robotics an d Automation (ICRA’13), 2013.
[23] C. Staub, T. Osa, A. Knoll, and R. Bauernschmitt, “Automation of
tissue piercing using circular needles and vision guidance for com-
puter aided laparoscopic surgery,” in Proceedings of the IEEE 2010
International Conference on Robotics an d Automation (ICRA’10),
2010.
[24] C. E. Reiley, E. Plaku, and G. D. Hager, “Motion generation of robotic
surgical tasks: Learning from expert demonstrations,” in 32nd Annual
International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC), Sept 2010.
[25] S. L. Troyan, V . Kianzad, S. L. Gibbs-Strauss, S. Gioux, A. Matsuiand,
R. Oketokoun, L. Ngo, A. Khamene, F. Azar, and J. V . Frangioni, “The
FLARE
TM
intraoperative near-infrared ﬂuorescence imaging system:
A ﬁrst-in-human clinical trial in breast cancer sentinel lymph node
mapping,” Annals of Surgical Oncology, vol. 16, pp. 2943–2952, 2009.
[26] J. V . Frangioni, “In vivo near-infrared ﬂuorescence imaging,” Current
Opinion in Chemical Biology, vol. 7, pp. 626–634, 2003.
[27] R. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision. Cambridge University Press, 2003.
[28] K. Arun, T. Huang, and S. Blostein, “Least-squares ﬁtting of two 3-
d point sets,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 9, no. 5, pp. 698–700, September 1987.
1894
