Blind Collision Detection and Obstacle Characterisation
Using a Compliant Robotic Arm
Piyamate Wisanuvej, Jindong Liu, Ching-Mei Chen, and Guang-Zhong Yang
Abstract— This paper presents a novel blind collision de-
tection and material characterisation scheme for a compliant
robotic arm. By the incorporation of a simple MEMS ac-
celerometer at each joint, the robot is able to detect collision,
identify the material of an obstacle, and create a map of the
environment. Detailed hardware design is provided, illustrating
its value for building a compact and economical robot platform.
The proposed method does not require the additional use
of vision sensor for mapping the environment, and hence is
termed as ‘blind’ collision detection and environment mapping.
Based on the shock wave and vibration signals, the proposed
algorithm is able to classify a range of materials encountered.
Detailed laboratory evaluation was performed with controlled
obstacle collision from different orientation and locations with
varying force and materials. The proposed method has achieved
98% detection sensitivity while maintaining 77% speciﬁcity.
Furthermore, by using sound feature extraction and machine
learning techniques, the classiﬁer produces an accuracy of 98%
for classifying four different impact materials. In this paper, we
also demonstrate its use for detailed environment mapping by
using the proposed method.
I. INTRODUCTION
Research in compliant robots working safely in human
environment is an increasingly popular research topic be-
cause of the wide range of applications under develop-
ment. These cover human-robot cooperation, learning-from-
demonstration, and rehabilitation and healthcare applications.
The robot designed to work with human interaction must be
able to operate safely and cause no harm to human or damage
to properties. In particular, avoiding collision is one of the
crucial components for safe robot operation. Typically, this
requires vision- or laser-based perception systems. Occlusion
in the ﬁeld of view or very fast movement can cause visual
based collision avoidance to fail. The use of integrated
sensors for collision detection has many advantages, which
can also complement the commonly used vision sensor to
improve the overall sensitivity and robustness of the robot.
Thus far, extensive research has been carried out for equip-
ping a robot manipulator with low-level sensors to ensure
safe operation. By observing joint position and commanded
joint torque, collision detection can be achieved without addi-
tional sensor [1], [2], [3]. However, this requires calculations
of robot dynamics. Therefore, the method requires complex
structural analysis of the robotic arm. Another detection
system based on current sensing eliminates the needs of
structural analysis and dynamics calculations [4]. However,
Piyamate Wisanuvej, Jindong Liu, Ching-Mei Chen, and Guang-Zhong
Yang are with the Hamlyn Centre for Robotic Surgery, Imperial College
London, UK. fpiyamate.w12, j.liug@imperial.ac.uk
Materials presented in this paper are part of the completed Master’s thesis
at The Hamlyn Centre for Robotic Surgery, Imperial College London
the method still suffers from the drawback that it only works
when the arm is moving and there is no sense of localisation
of the collision point.
An alternative method achieves collision detection in
quadruped mobile robot by using an accelerometer to mon-
itor the robot movement [5]. The statistical analysis of the
signal in frequency domain enables the collision events to
be detected. However, location, magnitude, and direction
information of the impact are not detected.
In addition to the collision detection, being able to identify
obstacle characteristics is also important in practice. The
robot controller can store the object location as a static ob-
stacle and navigate around it in subsequent motion planning.
On the other hand, collision with human could be considered
as dynamic obstacle. Robot may temporarily remember the
collision point and avoid that with a certain safety distance.
We all know that humans can sense the hardness of an
object by tapping [6]. In the same way, research in object
identiﬁcation also relied on tapping with robotic arm/hand
with the obstacles. By capturing the vibration signal with an
accelerometer, tapping different objects can produce different
vibration patterns [7], [8], [9], [10]. As an example, a legged
robot has been proposed, which can identify its walking sur-
face from accelerometer data analysis with machine learning
[11]. Despite its high accuracy in surface prediction, this
method is based on a walking robot. The signal pattern
is different from tapping signal, and thus cannot be used
directly to perform object recognition. In addition to tac-
tile sensing, other sensory feedback can also be used. By
analysing contact sound from robotic manipulator, the object
material can also be identiﬁed [12], [13], [14]. Furthermore,
object identiﬁcation can be achieved by characterising its
thermal property using robotic manipulator with heat/thermal
sensors [15].
By attaching sensors onto the robotic manipulators, en-
vironment exploration can be realised. Although contact-
less approaches using laser range scanner [16] or time-of-
ﬂight camera [17] are commonly used, they require complex
hardware/software integration. Currently, some approaches
using tactile based sensor are being proposed. For example,
a robot hand with 256-point piezoresistive sensor array can
provide high spatial resolution for contact object identiﬁca-
tion [18]. Another method is to use a robotic arm with optical
waveguide tactile sensor to identify characteristic features on
test objects [19].
MEMS accelerometers are commonly used in aforemen-
tioned methods because of its compact size, low cost, high
bandwidth and high sensitivity. This paper proposes an
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2249
approach by using MEMS accelerometers integrated with the
robotic manipulator for:
1) Collision detection, impact location, direction, and
magnitude estimation
2) Material property (hardness) identiﬁcation
3) Unknown environment exploration and mapping
In this work, we modiﬁed an off-the-shelf robotic arm
by rebuilding all the control electronics. By adding current
sensor as a means of torque sensing, the arm becomes
compliant with a torque controller. The accelerometer on
board is used to capture the contact vibration of this com-
pliant arm. By processing the acceleration data, the arm can
detect the collision and estimates its location, direction, and
magnitude. Additionally, the signal is processed further by
using machine-learning algorithms to identify the material
characteristics of the object. Finally, a blind exploration
technique is implemented by using the proposed collision
detection method.
II. METHODOLOGY
This section describes the implementation of the robotic
arm. Details on hardware construction are presented along
with the software implementation considerations for solving
the blind collision detection, material identiﬁcation, and
exploration problems.
A. Robot Hardware
A 7 degree-of-freedom robotic arm Cyton Alpha 7D 1G
(Robai, Philadelphia, PA) is used as the basis of our robot
design, but with several major modiﬁcations. This is shown
in Fig. 1. Originally, each joint of the robot is equipped
with a hobby servo motor. The improved version of the
electronic boards for the motors are built to accommodate the
required sensors including the current sensor, accelerometer,
and magnetic encoder. The motor is driven by PID controllers
with current and position control loops. The motor current
is limited at a preset level to provide compliant control. The
complete electronics is ﬁtted inside each joint of the robot as
shown in Fig. 2. The 3-axis accelerometer (Analog Devices
ADXL346) is conﬁgured for 16g measurement range and
3200 Hz data rate.
B. Blind Collision Detection
Accelerometers inside the robot joints give detailed read-
ings of acceleration from each joint. In normal operating
condition, the only source of acceleration signal is from the
arm movement itself. This acceleration due to movement
usually has a low frequency less than 100 Hz. In case of
collision events, external force applied to the arm affects the
acceleration signiﬁcantly. The sudden impact with an exter-
nal object causes vibration with much higher frequencies.
Fig. 3 shows the acceleration signal due to arm movement
with collisions. An example of high frequency vibration from
the impact is shown in Fig. 4, the parameters are explained
in the following.
1) Detection: Due to the large difference in signal patterns
from movement and collision signals, the peak pulse from
collision can be easily detected using two criteria: Magnitude
ThresholdTm and Time ThresholdTt. The time window is
a threshold to accept only short peaks. This effectively ﬁlters
out the acceleration peak from jittery robot movement caus-
ing false positives. The magnitude threshold is for ﬁltering
out the low joint vibration from the motor and gear system
themselves.
Since each robot joint has its own sensor, a total of 9
acceleration readings are obtained in real-time. This provides
redundancy in the system, as each part of the arm is being
monitored. By having sensors on different parts throughout
the robot, the collision events can also be localised. When a
collision occurs, the vibration propagates from the point of
impact through the links and joints that connect together.
Consider the case of a collision occurred in the i
th
link,
the circuit board and sensor is rigidly connected to the i
th
joint casing andi
th
link. Whereas thei+1
th
andi 1
th
link
connect to the output shaft through the gearing system. The
vibration propagation via rigid connection loses less energy
than loose connection. The reduction gearing in the robot
joint has around 4-5 stages with the ratio in ranges of 1:30-
1:500. This causes the vibration magnitude to be greatly
decreased because of the reduced torque when transferred
through the gearing system, effectively damping the vibration
signal propagation. With this simpliﬁed model, it suggests
that the joint closest to the impact gets the highest signal.
Thus, by comparing the signal magnitude the link-level
localisation can be achieved. The magnitudes of the signal
(A
k
:A
x
;A
y
;A
z
) are obtained from the peak amplitude of
the collision signal.
m =
q
A
2
x
+A
2
y
+A
2
z
(1)
tip
Links:
L
0
L
1
L
2
L
3
L
4
L
5
Fig. 1: The experiment setup of a robotic arm and a solenoid
with interchangeable hardness tips (showing the white tip in
enlarged image). The arm is divided into 6 links (L0- L5)
for the purpose of location identiﬁcation of collisions.
2250
(a) Printed circuit boards
(b) Circuit assembly
Fig. 2: The circuit board assembly of the customised servo
motor for the robotic arm. The yellow circle indicates the lo-
cation of the accelerometer. The axes illustrate the alignment
of the sensor’s coordinate frame.
time (s) acceleration (g) 00 .20 .40 .6 0.8 11 . 21 . 4 ?15 ?10 ?5 0 5 10 15 still                                         movement
collisions with table
gripped by hand
Fig. 3: Acceleration signals of the arm’s end effector showing
hard collisions (with wooden table) and general collision
(with ﬁnger) along with an arm movement
2) Direction and magnitude estimation: Each accelerome-
ter sensor has 3 axes, arranged in XYZ Cartesian coordinates.
This gives directional information of the collision. Since
the sensor provides Cartesian output format, the absolute
magnitude m of the signal can be calculated by Root Mean
Square value. Additionally, the impact direction can be
obtained by the coordinate transformations of the sensor’s
coordinate frame inside the joint to the global reference
frame.
It is important to note that the accelerometer signal
contains earth’s gravitational acceleration component. This
measurement should be subtracted because it affects the
calculation by shifting the whole signal with this offset. From
the sensor’s point of view, this Baseline Acceleration B
i
changes according to the orientation of the sensor relative to
the ground, including its movement. This can be determined
from the kinematics calculation. A simpler approach can also
time (ms) acceleration (g) 0 10 20 30 40 50 ?8 ?6 ?4 ?2 0 2 4 6 8 +Th
m,k
-Th
m,k
Th
t,k
B
k
A
k
Fig. 4: Parameters of a collision signal in k
th
axis: Mag-
nitude A
k
, Baseline Acceleration B
k
, Amplitude Threshold
Tm
k
, and Time Threshold Tt
k
be used. Since the vibration from any impact is typically very
short (less than 20 milliseconds) compared to the change
of acceleration from the arm movement, the baseline grav-
itational acceleration can be determine temporarily in each
collision event. This signal baseline can then be obtained by
calculating the median of the signal, which takes into account
the sensor’s offset as well.
The direction of impact in sensor’s frame can be repre-
sented by Euler’s angles (;; ). These angles are obtained
from acceleration magnitudes using these equations derived
in [20].
 = arctan
0
@
A
x
q
A
2
y
+A
2
z
1
A
(2a)
 = arctan
 
A
y
p
A
2
x
+A
2
z
!
(2b)
 = arctan
0
@
q
A
2
x
+A
2
y
A
z
1
A
(2c)
C. Object Identiﬁcation
When different materials hit an object, varying sound
signatures are produced. Human can roughly identify the
material by the impact sound ourselves. This suggests that
impact vibration caused by collision has its own characteris-
tics based on the material. Since the acceleration from impact
vibration follows a similar pattern as in impact sound [21],
similar techniques of sound processing can be used. Sample
signal from collision with different materials are shown in
Fig. 5. The vibration signal can be processed using the
machine learning techniques. Using the previous detection
method, the collision signal can be segmented. By consider-
ing the vibration signal as a sound, its characteristics can be
2251
time (ms) time (ms)
Acceleration (g) Acceleration (g)
Material 4 Material 3
Material 2 Material 1
10 20 30 40 50 60 70 10 20 30 40 50 60 70
10 20 30 40 50 60 70 10 20 30 40 50 60 70
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
Fig. 5: Acceleration signals for collisions from different
materials used in experiments
described by comprehensive amount of sound features. These
features can be extracted by sound processing software. Then
classiﬁers are used to build a model of material classiﬁcation
based on the vibration features.
In order to process the signal with the classiﬁers, its
features need to be extracted. As the vibration signal is a
time-domain signal similar to the voice signal (with lower
frequency), the audio features extraction library is used to
obtain those features. The YAAFE – Yet Another Audio
Feature Extractor [22] is used. The list of features used are
shown in Table II.
The classiﬁers used are the following: J48 Decision Tree,
SVM, Neural Network (Multilayer Perceptron), Linear Re-
gression, and Naive Bayes.
D. Environment Exploration
As the manipulator has capability of collision detection,
this can be extended further to perform blind exploration
(without visual sensors). With the known robot kinematics,
each joint location in space can be calculated. When the
robot hits an unknown obstacle within the environment, its
position based on location of impact can be determined.
By simultaneously moving the robot around in space to
collect collision data, the point cloud of the environment can
be created. With the information of impact direction, each
collision point is tagged with a normal vector. The plane can
be estimated with this vector to build a surface mesh of the
environment.
III. EXPERIMENTS
To evaluate the accuracy of each of the proposed method-
ologies, detailed laboratory experiments have been per-
formed. The experiments conducted contain three parts. In
the ﬁrst two experiments, arm is conﬁgured to be stationary,
whereas the arm is manually controlled in the third experi-
ment.
A. Collision Detection
The experiment is carried out as shown in Fig. 1. A
solenoid is used as a tool to hit the robotic arm in different
locations. It is rigidly ﬁxed to the table with a clamp. The
tip of the solenoid is a round shaped rigid plastic. In each
hit, the experiment is conﬁgured with three variables: link
number, direction, hit magnitude. The robot has six links.
Each link is hit with two directions perpendicular to the
link’s surface. Each combination of location and direction is
hit with three different magnitudes and repeat for 10 times.
This experiment format collected a total of 360 collisions.
B. Object Identiﬁcation
The experiment is setup in a similar way as previous.
The only difference is that the tip of the solenoid is also
a controlled variable. Four different tips are built from a
3D printer using different materials. We call these material
M
1
;M
2
;M
3
;M
4
. The material properties are shown in Table
I. All tips have identical shape.
Mat. Tensile Shore Elongation Equivalent
strength(MPa) hardness at break (%) Material
1 0.8-1.5 26-28 (A) 170-220 Rubber band
2 2-4 55-65 (A) 80-100 Eraser
3 15-25 90-100 (A) 25-35 Tyre
4 50-65 83-86 (D) 10-25 Plastic helmet
TABLE I: Material properties of the solenoid tips used in
the object identiﬁcation experiment
In this experiment, each object hits the robotic arm repeat-
edly 100 times on the same spot. Together with 12 possible
combinations of variables, the total number of datasets are
1200. Each material is considered as a class, hence there are
total of 4 classes with 300 datasets for each class.
With extracted features, the total number of attributes for
each dataset is 397. All datasets are fed into 5 classiﬁers
to perform the classiﬁcation. The dataset is split into 90%
training set and 10% testing set. The data splitting is per-
formed using the cross-validation method so the splitting is
randomised and distributed equally. The classiﬁcation is done
using Weka – Waikato Environment for Knowledge Analysis
software [23]. The parameters used in the classiﬁers are
default values supplied by the software.
C. Environment Exploration
For environment exploration, the experiment setup in-
cludes two scenarios. For the ﬁrst scenario, the rectangular
box is placed to cover the arm workspace. Additional sloped
plastic plate is added as another obstacle for the second
scenario. The ﬂat surface is chosen for the simplicity for
data collection and result evaluation. To simplify the arm
trajectory generation, the arm is controlled manually by a
human operator. A Phantom Omni (Sensable, Wilmington,
MA) haptic device is used as a controller. It has 6 degrees
of freedom. All of the degrees of freedom are mapped to the
joint angle of the robotic arm. Scaling is applied to make the
operating range of the haptic device matches the joint range
2252
(a) Scenario 1 (b) Scenario 2
Fig. 6: Experiment setups of environment exploration, Sce-
nario 1: arm in an empty box, Scenario 2: an obstacle is
added into the box
0% 
20% 
40% 
60% 
80% 
100% 
Detection Location Magnitude Direction 
0 1 2 3 4 5 Average 
Fig. 7: Collision detection and estimations accuracy for each
robot’s link corresponding to Fig. 1
of the robot. Since the arm has an extra degree of freedom,
compared to the controller, one of the arm’s joint is kept
stationary.
The translucent plastic box is used. The inside dimension
of the box is62:73935:3 cm. Obstacle in the ﬁrst scenario
is the box itself. Due to workspace constrain, only 4 of the
sides are reachable. The second scenario involves one sloped
plane as an obstacle in addition to the box. This makes the
second scenario have 5 reachable sides. The data collection is
performed by capturing the joint angles and the acceleration
signal at the same time. The movement of the robotic arm
is determined by user’s decision, with best effort to reach
as many faces of the obstacle as possible. Fig. 6 shows the
experiment setup in two scenarios.
IV. RESULTS
A. Collision Detection
The experiment evaluations can be divided into 4 parts:
Collision Detection, Collided Link Location Identiﬁcation,
Magnitude Estimation, and Direction Estimation. These ex-
perimental results are shown in Fig. 7. Regarding the Colli-
sion Detection, the sensitivity of 98% and speciﬁcity of 77%
are achieved.
B. Object Identiﬁcation
Fig. 8 shows a comparison of the classiﬁcation accuracy.
Three of them give very accurate results, with the highest of
94% 
76% 
98% 94% 
62% 
0% 
20% 
40% 
60% 
80% 
100% 
Decision 
Tree 
SVM Neural 
Network 
Regression Naive 
Bayes 
Fig. 8: Object identiﬁcation accuracy
Ranking Feature
0.92 Auto Correlation
0.91 Energy
0.84 Temporal Shape Statistics
0.79 fp
0.68 Spectral Shape Statistics
0.51 Envelope Shape Statistics
0.50 Zero Crossing Rate
0.32 tp
0.23 Spectral Rolloff
TABLE II: InfoGain ranking values for the signal features
used, higher values mean higher contribution to classiﬁcation
accuracy
98%. With further analysis from the attribute selection tool in
Weka, it is shown that some of the features contribute a major
part of the accurate result, and some are not contributing at
all. By using Ranker attribute selection, the result shown
in Table II is the list of ranking value sorted by InfoGain
evaluation [23]. This gives a rough clue of which features
contribute most to the classiﬁcation accuracy.
C. Environment Exploration
Captured data points from two scenarios are shown in Fig.
9. It also shows the superimposed location of the actual
obstacle surface. Both scenarios have approximately 160
collision points. Each colour of the points represents each
surface of the learned environment. The Root Mean Square
Error of the collected data compared to the nearest obstacle’s
surface are 26.7mm and 14.9 mm for the ﬁrst and second
scenario respectively.
V. CONCLUSIONS
In this paper we have presented a blind approach of
collision detection for a robotic arm using accelerometer. The
result shows that the proposed method has accurate detection
while maintaining a reasonable speciﬁcity level. The impact
location can be also determined in terms of collided link,
with a considerable accuracy. However, the estimation of
impact direction and magnitude are not yet satisfying. This
deserves further investigation. We believe that the mechanical
conﬁguration of robot makes the vibration signal distorted.
2253
(a) Front (b) Side
Fig. 9: Exploration results, top plots show the 1
st
scenario,
bottom plots show the 2
nd
scenario, axes are in millimetres
Therefore, the magnitude and direction calculations (Equa-
tions 1 and 2) can become problematic.
Another possible improvement to the blind collision de-
tection problem is to localise the impact within the link.
Since the vibration signal is a wave that propagates through
the link material, collisions at different locations make the
signals arrive the sensors on both sides at different times. We
can possibly use time difference of arrival (TDOA) technique
to implement the localisation [24].
The accuracy of the experimental results in terms of
object identiﬁcation proves that accelerometer has very high
potential to capture the difference in vibration patterns from
different materials. In this experiment, only 4 prototype
materials have been used. This should be improved by adding
more material in everyday life to prove this approach in the
practical level.
The result from preliminary work on blind environment
exploration method is promising. This demonstrates one
possible application of using simple accelerometer to
implement a blind robot perception system. The current
method are only limited to collecting point cloud of the
environment since the impact direction estimation is not
accurate. If this estimation could be improved, however,
the normal vector of each point would be available.
Therefore, instead of collecting points, planes would be
collected. This could be merged into three dimensional mesh
and provide more realistic reconstruction of the environment.
REFERENCES
[1] A. D. Luca and R. Mattone, “Sensorless Robot Collision Detection and
Hybrid Force / Motion Control,” in IEEE International Conference on
Robotics and Automation (ICRA), no. April, 2005, pp. 999–1004.
[2] A. Luca, A. Albu-Schaffer, S. Haddadin, and G. Hirzinger,
“Collision Detection and Safe Reaction with the DLR-III Lightweight
Manipulator Arm,” in IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). Ieee, Oct. 2006, pp.
1623–1630.
[3] S. Haddadin, A. Albu-Schaffer, A. De Luca, and G. Hirzinger,
“Collision detection and reaction: A contribution to safe physical
human-robot interaction,” in IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), 2008, pp. 22–26.
[4] H.-W. Je, J.-Y . Baek, and M. C. Lee, “Current based compliance
control method for minimizing an impact force at collision of service
robot arm,” International Journal of Precision Engineering and
Manufacturing, vol. 12, no. 2, pp. 251–258, Apr. 2011.
[5] T. Meric ¸li, c. Meric ¸li, and H. Akn, “A robust statistical collision
detection framework for quadruped robots,” RoboCup 2008: Robot
Soccer World Cup XII, 2009.
[6] R. H. Lamotte, “Softness Discrimination With a Tool,” Journal of
Neurophysiology, vol. 83, no. 4, pp. 1777–1786, 2013.
[7] K. Kuchenbecker, J. Fiene, and G. Niemeyer, “Event-Based Haptics
and Acceleration Matching: Portraying and Assessing the Realism
of Contact,” First Joint Eurohaptics Conference and Symposium on
Haptic Interfaces for Virtual Environment and Teleoperator Systems,
pp. 381–387, 2005.
[8] J. Romano and K. Kuchenbecker, “Creating realistic virtual textures
from contact acceleration data,” Haptics, IEEE Transactions on,
vol. 5, no. 2, pp. 109–119, 2012.
[9] J. Windau, “An Inertia-Based Surface Identiﬁcation System,” in 2010
IEEE International Conference on Robotics and Automation. Ieee,
May 2010, pp. 2330–2335.
[10] H. Liu, X. Song, and T. Nanayakkara, “Friction estimation based
object surface classiﬁcation for intelligent manipulation,” in IEEE
ICRA 2011 workshop on autonomous grasping, Shanghai, 2011.
[11] D. Vail and M. Veloso, “Learning from Accelerometer Data on
a Legged Robot,” in 5th IFAC/EURON Symposium on Intelligent
Autonomous Vehicles, 2004.
[12] E. Torres-jara, L. Natale, and P. Fitzpatrick, “Tapping into Touch,”
in Fifth International Workshop on Epigenetic Robotics: Modeling
Cognitive Development in Robotic Systems, 2005, pp. 79–86.
[13] J. Richmond and D. Pai, “Active Measurement of Contact Sounds,”
in Robotics and Automation, 2000. Proceedings. ICRA’00. IEEE
International Conference on, vol. 3, 2000, pp. 2146–2152.
[14] S. Femmam, “Perception and characterization of materials using
signal processing techniques,” Instrumentation and Measurement,
IEEE Transactions on, vol. 50, no. 5, pp. 1203–1211, 2001.
[15] M. Campos, R. Bajcsy, and V . Kumar, “Exploratory procedures
for material properties: The temperature perception,” in Advanced
Robotics, 1991. ’Robots in Unstructured Environments’, 91 ICAR.,
Fifth International Conference on, 1991.
[16] G. Paul, D. K. Liu, N. Kirchner, and S. Webb, “Safe and Efﬁcient
Autonomous Exploration Technique for 3D Mapping of a Complex
Bridge Maintenance Environment,” in 24th International Symposium
on Automation and Robotics in Construction (ISARC 2007), 2007.
[17] T. Kunz, U. Reiser, M. Stilman, and A. Verl, “Real-time path
planning for a robot arm in changing environments,” in 2010
IEEE/RSJ International Conference on Intelligent Robots and
Systems. Ieee, Oct. 2010, pp. 5906–5911.
[18] P. Allen and P. Michelman, “Acquisition and interpretation of
3-D sensor data from touch,” IEEE Transactions on Robotics and
Automation, vol. 6, no. 4, pp. 397–404, 1990.
[19] A. M. Okamura, “Feature Detection for Haptic Exploration with
Robotic Fingers,” The International Journal of Robotics Research,
vol. 20, no. 12, pp. 925–938, Dec. 2001.
[20] M. Pedley and Freescale Semiconductor, “AN3461: Tilt Sensing Using
a Three-Axis Accelerometer,” 2013.
[21] W. McMahan, E. D. Gomez, L. Chen, K. Bark, J. C. Nappo,
E. I. Koch, D. I. Lee, K. R. Dumon, N. N. Williams, and
K. J. Kuchenbecker, “A practical system for recording instrument
interactions during live robotic surgery,” Journal of Robotic Surgery,
Apr. 2013.
[22] B. Mathieu, S. Essid, T. Fillon, J. Prado, and G. Richard, “YAAFE,
an Easy to Use and Efﬁcient Audio Feature Extraction Software,” in
ISMIR, no. Ismir, 2010, pp. 441–446.
[23] M. Hall, E. Frank, and G. Holmes, “The WEKA data mining software:
an update,” ACM SIGKDD Explorations Newsletter, vol. 11, no. 1,
pp. 10–18, 2009.
[24] K. R. Arun, E. Ong, and A. W. H. Khong, “Source localization
on solids using Kullback-Leibler Discrimination Information,”
International Conference on Information, Communications and
Signal Processing (ICICS), pp. 1–5, Dec. 2011.
2254
