Geometry Constrained Sparse Embedding for Multi-dimensional
Transfer Function Design in Direct Volume Rendering
Zhenzhou Shao
1
, Yong Guan
2
, Hongsheng He
1
and Jindong Tan
1
Abstract—Direct volume rendering (DVR) is commonly em-
ployedforthemedicalvisualization.Multi-dimensionaltransfer
functions are used in DVR to emphasize the region of interest
indetails.However,itisimpracticaltointeractdirectlywiththe
functions in more than three dimension. This paper proposes
a novel framework called geometry constrained sparse embed-
ding (GCSE) for dimensionality reduction (DR). GCSE allows
the conventional DR methods to be applied to a dictionary
with much smaller atoms instead. The mapping derived from
thedictionaryfeedstotheoriginalfeaturestoobtaintheonesin
thereduceddimension.Toobtainagooddictionary,theintrinsic
structure of features is encoded in the sparse embedding based
on a geometry distance. In addition, stochastic gradient descent
algorithm is employed to speed up the dictionary learning.
Various experiments have been conducted using both synthetic
and real CT data sets. Compared with conventional methods,
GCSE not only produces the comparable results, but also
performs well with the capability to handle the large data set
more powerfully. The rendering results using the real CT data
has demonstrated the effectiveness of GCSE.
I. INTRODUCTION
Efﬁcient interpretation and perception of 3-D medical data
is crucial to surgeons during the clinical intervention [1].
Commonly the direct volume rendering (DVR) technique is
employed for visualization by projecting the volume data in
a projected image without the data misclassiﬁcation in the
segmentation stage [2]. DVR is also used in other procedures,
such as surgical training, diagnosis and surgical planning.
In order to deﬁne the meaningful visualization and em-
phasize the regions of interest in the volume data, transfer
functions (TFs) are used for mapping from voxels with
the scalar information to color and optical properties [3].
Compared with the simple TF using the scalar value only,
multi-dimensional TFs are commonly designed based on
several features, such as gradient magnitude, curvature and
statistical measures, to enable more accurate perception for
surgeons.
As shown in Fig. 1, multiple features in TF domain are
extracted, and stored in a vector every voxel. However,
as the dimensionality of TF domain increases, the direct
interaction with the transfer function becomes impractical. In
this case, we have to ﬁnd a low-dimensional representation of
high-dimensional features to enable the effective interaction.
One solution to this problem is dimensionality reduction
1
Zhenzhou Shao, Hongsheng He and Jindong Tan are with the
Department of Mechanical, Aerospace, and Biomedical Engineer-
ing, The University of Tennessee, Knoxville, TN, 37996, USA
fzshao,hhe,tang@utk.edu
2
Yong Guan is with the Beijing Key Laboratory of Electronic System
Reliability Technology, Capital Normal University, Beijing, 100048, China
guanyong@mail.cnu.edu.cn
Fig. 1: Dimensionality reduction for transfer function design.
(DR) which is the transformation from the high-dimensional
data to a representation in the reduced dimension without
signiﬁcant information loss [4]. Another advantage of di-
mensionality reduction is that the redundant information can
be ﬁltered out simultaneously.
Conventional approaches of dimensionality reduction are
primarily divided into two categories: linear and non-linear
methods. Linear dimensionality reduction is characterized by
the linear mapping, e.g. principal component analysis (PCA)
[5], and the nonlinear methods, e.g. Isomap [6] and locally
linear embedding (LLE) [7], perform better by taking into
account the nonlinearity of the original data. However, both
methods incur the problem of heavy computational load on
the large data set. For example, assume the dimension of TF
domain is 5, and a CT volume with the size of 256 256
128 is given, the input to dimensionality reduction methods
is in R
58388608
. Nevertheless, the fast implementation of
transfer function design is required in both pre- (diagnosis
and surgical planning) and intra-operation.
Recently, the rapid development in the ﬁled of sparse
representation (SR) paves another way to the dimensionality
reduction. This method is achieved based on the assump-
tion that the signals are compressible, and have a sparse
representation in a basis set (also a.k.a dictionary). It has
been proven practical in many applications [8]–[10]. In [11],
Nguyen et al. proposed a sparse embedding (SE) framework
by combing dimensionality reduction and sparse learning
together. It enables the sparse representation (sparse coding
and dictionary learning) in the reduced dimension with lower
computational load. However, current SR based methods are
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1290
Fig. 2: Illustration of geometry constrained sparse embed-
ding.
still time-consuming as the mapping matrix is optimized
together with SR, especially for the medical data with the
large size.
The objective of this paper is to propose a dimensionality
reduction method with the capability to process the large
data set efﬁciently for the transfer function design in the
direct volume rendering. In this paper, a novel framework
of Geometry Constrained Sparse Embedding (GCSE) is pro-
posed. To improve the efﬁciency of dimensionality reduction,
the mapping matrix is separated from the framework of
SR in Nguyen’s work, and it is obtained by applying the
conventional method (i.e. PCA, Isomap and LLE) to the
dictionary under the assumption that if the features vectors
can be described well over an overcomplete dictionary, the
mapping derived from the dictionary can embed the entire
features in the target dimension. Therefore, the problem
is reduced to the optimization of dictionary. To this end,
the intrinsically geometrical structure based on a geometry
distance are taken into account. The geometry distance is
to measure the similarity and ﬁnd the geometry relationship
amongst data, while Euclidean distance just computes the
pairwise metric between data. Moreover, to achieve faster
implementation, the sparse representation in the proposed
method is implemented based on the stochastic gradient
descent [12].
II. GEOMETRY CONSTRAINED SPARSE EMBEDDING
As aforementioned, the transfer function design in DVR
is reduced to the dictionary learning. To obtain a better
dictionary, the intrinsically geometric structure of feature
vectors are taken into account in GCSE in this paper.
As shown in Fig. 2, for each datum in a high dimensional
data set, a group of neighbors are selected according to a
geometry distance of the datum to the hyperplane in the high
dimensional manifold, and a neighbor graph is constructed
to encode the geometrical information into GCSE. The learnt
dictionary can better describe the features with the similar
pattern, so that such dictionary can act as a surrogate for the
entire features. The framework of GCSE is summarized in
Algorithm (1). Steps 2 and 3 are to be discussed in details
in following sections.
Algorithm 1 Overview of GCSE.
Input: CT data, number of neighbors K, size of dictionary
k, initial dictionary D
0
2R
mk
.
1: Extract feature vectors X 2 R
mn
from the medical
data.
2: Construct a neighbor graph using the geometry distance.
3: Learn the dictionaryD

based on the objective function
encoding the geometrical structure.
4: Apply the conventional DR algorithm to D

to ﬁnd the
mapping P2R
dm
.
Output: Y =PX2R
dn
.
III. NEIGHBOR SELECTION USING GEOMETRY
DISTANCE
A neighbor graph is constructed to describe the locally
geometrical relationship of the feature vectors. As a measure,
Euclidean has shown good performance in the neighbor
selection. Nevertheless, Euclidean distance is limited as only
the pairwise distance to the target data is considered. It is
desired that the local relationships in the cluster are also
taken into account. The fundamental problem in neighbor
graph construction is how to deﬁne the distance in selecting
neighbors for each feature vector. In this section, a geometry
distance is computed to measure the geometrical similarity of
the features. Deﬁning this geometry distance as the neighbor
criterion, we can explore the geometry relation of features.
In the feature space R
m
with m dimensions, a parallelo-
tope is determined with column vectors X = [x
i
]
K
i=1
2
R
mK
as the sides, the volume of which can be computed
[13] by
Vol(X) = det
1
2
(X
T
X) = det
1
2
(G(X)) (1)
following the deﬁnition of Gramian [14]. We further illustrate
the distance from a sample to linear manifolds and judge
whether a set of vectors are linearly independent in the
space. The Euclidean distance fromx
K
to the linear manifold
spanned by X
[1;(K 1)]
=fx
i
g
K 1
i=1
can be represented [13]
with the volume of the manifold
h
K
[1;(K 1)]
=
s
Vol
2
(X
[1;K]
)
Vol
2
(X
[1;(K 1]
)
: (2)
Following the properties of block matrices, it can be
further obtained that
Vol
2
(X
[1;K]
)
= det([X
[1;(K 1)]
;x
K
]
T
[X
[1;(K 1)]
;x
K
])
= det(x
T
K
x
K
  (x
K
X
[1;(K 1)]
)
G
 1
(X
[1;(K 1)]
)(x
K
X
[1;(K 1)]
)
T
):
(3)
According to (1) and (2), we have the following result,
h
K
[1;(K 1)]
= det(x
T
K
x
K
  (x
K
X
[1;(K 1)]
)
G
 1
(X
[1;(K 1)]
)(x
K
X
[1;(K 1)]
)
T
)
= x
T
K
x
K
  (x
T
K
X
[1;(K 1)]
)
G
 1
(X
[1;(K 1)]
)(x
T
K
X
[1;(K 1)]
)
T
:
(4)
1291
However, sinceK >m,X
[1;(K 1)]
may be redundant, so
that the inverse matrix ofG(C
[1;(K 1)]
) doesn’t exist. In this
paper, a simple technique to solve this problem is employing
the Tikhonov regularization
h
K
[1;(K 1)]
= x
T
K
x
K
  (x
T
K
X
[1;(K 1)]
)
T
(G(X
[1;(K 1)]
) +I)
 1
x
T
K
X
[1;(K 1)]
(5)
with> 0 as the regulation parameter. The introduction of
also avoids the inverse of singular  whose eigenvalues are
near zero in computational implementation. Using distance
measurement in Eq. (5), the neighbors of each data point are
selected incrementally until the neighbor number is satisﬁed.
IV. DICTIONARY LEARNING
In this section, the dictionary is learnt by encoding the geo-
metric relation of feature vectors, and the stochastic gradient
descent method is employed to speed up the implementation
of dictionary learning.
A. Objective Function
Let D = [d
1
;d
2
d
k
]2R
mk
be the dictionary matrix,
and  = [
1
;
2

n
]2R
kn
denotes the sparse matrix.
Given a data setX = [x
1
;x
2
;x
n
]2R
mn
, the objective
function of original sparse representation can be deﬁned as
fD;g = arg min
fD;g
1
2
kX Dk
2
F
+kk
1
s:t:kd
i
k
2
 1; i = 1;k;
(6)
where is the regularization parameter. To prevent arbitrarily
large values ofD, the constraintkd
i
k
2
 1 is applied. Base
on this basic objective function, an extra item is integrated to
preserve the geometrical structure ofX. To achieve it, we can
construct a neighbor graph G with n vertices according to
the geometry distance in Section III. Each vertex represents
a feature vector of medical data, and it is associated with
a element in the weight matrix W of G. If x
j
belongs to
the neighbors of x
i
, w
i;j
= 1, otherwise, w
i;j
= 0. The
difference with the graph constructed by Euclidean distance
is that the weight matrix W is an asymmetric matrix.
To map the weighted graph G to the sparse coefﬁcients
, a reasonable criterion [15] is chosen by minimizing the
following function:
1
2
N
P
i=1
N
P
j=1
k
i
 
j
k
2
F
w
i;j
=
1
2
(
N
P
i=1
N
P
j=1

i

T
i
w
i;j
+
N
P
i=1
N
P
j=1

j

T
j
w
i;j
 2
N
P
i=1
N
P
j=1

T
i

j
w
i;j
)
=
1
2
(tr(R
T
) +tr(C
T
)) tr(W
T
)
= tr(L
T
);
(7)
where L =
1
2
(R +C) W is the Laplacian matrix. The
degrees of x
i
along the row and column are deﬁned as R
and C
R =diag(r
1
;r
2
;r
N
); r
i
=
P
N
j=1
w
i;j
C =diag(c
1
;c
2
;c
N
); c
j
=
P
N
i=1
w
i;j
:
(8)
By combining Eq. (6) and Eq. (7), we can obtain following
objective function:
D

= arg min
D
1
2
kX Dk
2
F
+kk
1
+
G
tr(L
T
)
s:t:kd
i
k
2
 1; i = 1;k;
(9)
where 
G
is the regularization parameter for the geometric
structure.
In this paper, stochastic gradient descent is employed to
handle the large amount of features of medical data. The
dictionary learning algorithm is summarized in Algorithm
2. The dictionary and coefﬁcients can be optimized alterna-
tively by minimizing over one while keeping the other one
ﬁxed [16]. To improve convergence rate further,  feature
vectors are selected randomly each iteration. Then the sparse
coefﬁcient 
t;i
of x
t;i
over the previous dictionary D
t 1
is
computed by minimizing Eq. (10). A and B are calculated
to update the dictionary using Algorithm 3. The details are
discussed in Section IV-C.
Algorithm 2 Dictionary Learning.
Input: X2R
mn
, ;
G
;
L
2R, initial dictionary D
0
2
R
mk
, batch size , number of iterations T .
1: A
0
 0, B
0
 0.
2: for t = 1 to T do
3: Select  feature vectorsfx
t;i
g

i=1
randomly.
4: Perform sparse coding using feature-sign search algo-
rithm for each vector x
t;i
.

t;i
= arg min
2R
k1
1
2
kx
t;i
 D
t 1
k
2
F
+kk
1
+
G
1
2
n
P
i=1
n
P
j=1
k
i
 
j
k
2
F
w
i;j
:
(10)
5: A
t
 A
t 1
+

P
i=1
(
t;i

T
t;i
)
6: B
t
 B
t 1
+

P
i=1
(x
t;i

T
t;i
):
7: ComputeD
t
based on block-coordinate descent, refer
to Section IV-C for details.
8: end for
Output: D

.
B. Sparse Coding
In this section, the sparse coding problem is solved by
ﬁxing the dictionary D. According to Algorithm 2, each
sparse coefﬁcient vector 
t;i
is updated individually, while
the rest are ﬁxed. In order to optimize 
t;i
individually, Eq.
(9) should be formulated in a vector form. The combination
of residual item and sparse regularizer can be denoted as
P
n
i=1
kx
i
 D
i
k
2
F
+
P
n
i=1
k
i
k
1
, where the subscript
t from x and  is omitted for simplicity. The Laplacian
1292
regularizer tr(L
T
) can be expanded as follows:
tr(L
T
) = tr
 
n
P
i;j=1
L
i;j

i

T
j
!
=
n
P
i;j=1
L
i;j

T
i

j
:
(11)
Thus, the problem Eq. (9) can be rewritten as


= arg min

1
2
n
P
i=1
kx
i
 D
i
k
2
F
+
n
P
i=1
k
i
k
1
+
G
n
P
i;j=1
L
i;j

T
i

j
:
(12)
Because only 
i
is updated one time, by ﬁxing other sparse
coefﬁcients, we can obtain the objective function for 
i
as
follows:


i
= arg min
i
1
2
kx
i
 D
i
k
2
F
+k
i
k
1
+
G
 
L
i;i

T
i

i
+ 2
T
i
n
P
j6=i;
L
i;j

j
!
:
(13)
In this paper, the above problem is solved using the
feature-sign search algorithm [16] as a unconstrained
quadratic optimization problem. It outperforms many exist-
ing algorithms, such as grafting [17] and LARS [18].
C. Dictionary Update
In this section, the dictionary update is performed using
block-coordinate descent by minimizing residual error in
Eq. (9). Each column of dictionary is updated sequentially
according to Eq. (15) under the constraint k d
i
k
2
 1.
Da
j
  b
j
is the gradient of f(D) =
1
2
k X  D k
2
F
with respect to d
j
, and  is the learning rate. As mentioned
in Section IV-A, the dictionary learning is implemented
based on stochastic gradient descent algorithm. Taking partial
differentiation with respect to D, we can obtain
@f
@D
= (x
i
 D
i
)
T
i
=D(
T
) x
i

T
i
=DA B;
(14)
where A = 
T
and B = x
i

T
i
. Since the previous
dictionaryD
t 1
is used in the proposed algorithm to provide
a prior knowledge for computing D
t
; number of iteration T
can be set to a small value. In our experiments, only one
iteration is enough for the convergence.
V. EXPERIMENTAL RESULTS AND DISCUSSIONS
In this section, three sets of experiments were performed to
evaluate the performance of GCSE on both synthetic and real
CT data sets. The GCSE ﬁrstly employed the synthetic data
to compare with the existing methods, including linear and
non-linear approaches. The convergence and the robustness
of related parameters were also studied. The second exper-
iment was to test the capability of handling a large data
set using a partial CT volume extracted from the golden
standard data set [19]. For the effect of volume rendering
using different transfer functions, there is no quantitative way
to measure that. Thus, we presented the rendered images that
Algorithm 3 Dictionary update.
Input: D = [d
1
;d
2
;d
k
]2 R
mk
, A = [a
1
;a
2
;a
k
2
R
kk
] =
P
t
i=1

i

t
i
, B = [b
1
;b
2
;b
k
2 R
mk
] =
P
t
i=1
x
i

t
i
, number of iteration T .
1: for i = 1 to T do
2: for j = 1 to k do
3: Update the jth column of the dictionary:
d
j
 d
j
 (Da
j
 b
j
)
d
j
 
1
max(kdjk
F
;1)
d
j
:
(15)
4: end for
5: end for
Output: D

(Updated dictionary).
successfully emphasize the region of interests to compare the
difference for visualization. Human tooth CT data was used
in the last test.
The speciﬁcations of data sets used in the experiments
are summarized in Table I. Toroidal Helix data set is used
directly as the input of dimensionality reduction methods.
For the real CT data sets, corresponding feature vectors
are both in 6 dimensions. The ﬁrst vector is related to the
intensity for each voxel. To discriminate the voxels next
to the boundaries. The following 4 ones are the gradient
information, including gradient magnitude and three com-
ponents in the gradient vector. The last one is the Hessian
measure, which can capture the edge information to deal with
complex conﬁgurations. The target dimension is set to 2 for
all experiments.
A. Comparison with Conventional Methods
In this experiment, Toroidal Helix data set was used to
compared GCSE with conventional methods, including PCA,
Isomap and LLE. As shown in Fig. 3a, all conventional
methods project the input data into a symmetric and ﬂower-
like shape, and preserve the structure and distribution of
the helix. The number of neighbors were 40 and 60 for
Isomap and LLE, respectively, while PCA does not need
any parameters for the dimensionality reduction.
Fig. 3b shows the results of GCSE on Toroidal Helix data
set. When the learnt dictionary D was ready, we applied
PCA, Isomap and LLE toD, respectively. In this experiment,
the size of dictionary was 50, the numbers of neighbors
for Isomap and LLE were 3 and 4. We can see that much
less neighbors are required to achieve the similar effect
compared with the conventional methods. It also preserves
the geometric information of the input.
To explain the less number of neighbors in GCSE, Fig.
4 illustrates the distribution of Toroidal Helix data set and
TABLE I: Speciﬁcations of synthetic and real CT data sets.
Data source Size of data set Size of features Dimtarget
Toroidal Helix 3800 3800 2
Pig’s head [19] 202120 68400 2
Human tooth 9481155 61180170 2
1293
PCA
Isomap, KK=40 LLE, KK=60
(a) Conventional methods
k=50, PCA k =50, Isomap, KK=3 k=50, LLE, KK=4
(b) GCSE
Fig. 3: Comparison between conventional methods and
GCSE.
dictionary used for dimensionality reduction. The dictionary
is better able to keep the topological structure with the less
atoms, so that the local structure can be presented using less
points in the dictionary, while more neighbors are needed to
express the same geometric information.
We also studied the convergence and robustness of GCSE
based on the stochastic gradient descent algorithm. Following
the convergence analysis of stochastic gradient descent in
[20], it is obvious that the dictionary can approximate to a
stationary point asymptotically. To verify this, we set differ-
ent sizes of dictionary and numbers of neighbors indicating
the geometric structure in the objective function. In the test,
the sizes of dictionary were f50; 100; 150; 200g, and the
values for the number of neighbors were f5; 10; 15; 20g.
GCSE is performed with the iteration number of 100, and the
batch size in each iteration is also set to 100. The residual
errors are shown in Fig. 5. In all cases, the average residual
error is 0:29.
B. Comparison with Sparse Embedding
This experiment was implemented to test the capability of
large data set handling using Nguyen’s SE and GCSE. We
tried to do the dimensionality reduction on gold standard data
set and human tooth CT. However, SE failed using both data
sets due to the memory limit. The same problem occurred
when we apply the conventional methods to these data sets.
GCSE can deal with this case, the evaluation of GCSE on
the large data set is presented in next section.
To compare the performance of GCSE and SE, a partial
CT volume extracted from gold standard data was used.
The feature vectors are in 6D, and the target dimension
is 2. In SE, the iteration number was set to 5, and a
?4 ?3 ?2 ?1 0 1 2 3 4
?4
?2
0
2
4
?2
?1
0
1
2
Fig. 4: Topologies of input data and dictionary in 3D. The
blue stars are input data, and the learnt dictionary is indicated
by the red stars.
polynomial kernel of degree was 4. For GCSE, the size of
target dictionary was set to 200, the iteration number was
100, and the size of batch processing in each iteration was
400. LLE was used to perform the dimensionality reduction
on the dictionary.
In order to facilitate the visualization of feature vectors
in the reduced dimension, the regions were assigned with
different colors according to the intensity information. Fig.
6 shows the results of SE and GCSE. Both of them can
discriminate the regions correctly. The difference is that
GCSE preserves the additional geometric information in the
original data, while SE just embedded the features into
different areas. For the computing time, GCSE takes around
3:74 s each iteration, and SE requires about 862:93 s for
every round. Although the number of iteration for SE is
less than GCSE’s, SE is time-consuming when the total
computing time is taken into account.
10
0
10
1
0
0.5
1
1.5
2
2.5
elapsed time(s)
residual error
 
 
k=50, K=5
k=50, K=10
k=50, K=15
k=50, K=20
(a) k = 50
10
0
10
1
0
0.5
1
1.5
2
2.5
elapsed time(s)
residual error
 
 
k=100, K=5
k=100, K=10
k=100, K=15
k=100, K=20
(b) k = 100
10
0
10
1
0
0.5
1
1.5
2
2.5
elapsed time(s)
residual error
 
 
k=150, K=5
k=150, K=10
k=150, K=15
k=150, K=20
(c) k = 150
10
1
10
2
0
0.5
1
1.5
2
2.5
elapsed time(s)
residual error
 
 
k=200, K=5
k=200, K=10
k=200, K=15
k=200, K=20
(d) k = 200
Fig. 5: Residual errors using different conﬁgurations on
Toroidal Helix data set.
1294
 
 
0
10
20
30
40
50
60
70
80
90
(a) SE.
 
 
0
10
20
30
40
50
60
70
80
90
(b) GCSE.
Fig. 6: Comparison between sparse embedding and proposed
method.
Fig. 7: Rendered images using different transfer functions.
C. Evaluation of Transfer Function in DVR
In this section, the rendering quality of boundaries in the
direct volume rendering is mainly evaluated using different
transfer functions on human tooth CT. Conventional methods
and SE can not handle this data set with the size of 6
1180170. One transfer function (TF-IG) is designed based
on the intensity and gradient information, the other one (TF-
GCSE) is obtained in the GCSE domain.
The rendering results are shown in Fig. 7. One of the
disadvantages for DVR using TF-IG is that some regions
close to the boundary can not be isolated exactly. As shown
in Fig. 7a, when the background/dentin boundary is labelled
with the desired color and opacity, the pulp/dentin and
background/enamel boundaries are also colored. Because
they belong to the same range deﬁned by the intensity and
gradient. In Fig. 7b, TF-GCSE provides a clearer view on the
boundaries. Because the Hessian measures are involved in the
resultant features obtained by GCSE. Materials involved in
the human tooth can also be identiﬁed based on TF-GCSE,
such as enamel, dentin and pulp.
VI. CONCLUSIONS
In this paper, a novel scheme for dimensionality reduction
is proposed by encoding the geometrical structure based on
a geometry distance. GCSE performs well by learning a
good dictionary as a problem of SR. To the best of our
knowledge, it is the ﬁrst time to introduce SR into the design
of transfer function in direct volume rendering. More impor-
tantly, GCSE enhances the capability to deal with the large
data set and enables fast implementation based on stochastic
gradient descent. Experimental results show the convergence
and robustness of GCSE on the synthetic data. We also
have tested GCSE on the CT data containing millions of
samples, and the features in the reduced dimension are able
to discriminate the materials and boundaries in the volume.
VII. ACKNOWLEDGEMENT
This research is partially supported by National Science
Foundation of China under Grant 61305114.
REFERENCES
[1] Jian Wang, Pascal Fallavollita, Lejing Wang, Matthias Kreiser, and
Nassir Navab. Augmented reality during angiography: integration
of a virtual mirror for improved 2d/3d visualization. In Mixed and
Augmented Reality (ISMAR), 2012 IEEE International Symposium on,
pages 257–264. IEEE, 2012.
[2] Kevin Cleary and Terry M Peters. Image-guided interventions: tech-
nology review and clinical applications. Annual review of biomedical
engineering, 12:119–42, August 2010.
[3] Gordon Kindlmann. Transfer functions in direct volume rendering:
Design, interface, interaction. CoursenotesofACMSIGGRAPH, 2002.
[4] LJP Van der Maaten, EO Postma, and HJ Van Den Herik. Dimension-
ality reduction: A comparative review. Journal of Machine Learning
Research, 10:1–41, 2009.
[5] Ian Jolliffe. Principal component analysis. Wiley Online Library,
2005.
[6] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global
geometric framework for nonlinear dimensionality reduction. Science,
290(5500):2319–2323, 2000.
[7] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality
reduction by locally linear embedding. Science, 290(5500):2323–2326,
2000.
[8] Shivani Agarwal, Aatif Awan, and Dan Roth. Learning to detect ob-
jects in images via a sparse, part-based representation. PatternAnalysis
and Machine Intelligence, IEEE Transactions on, 26(11):1475–1490,
2004.
[9] J-L Starck, Michael Elad, and David L Donoho. Image decomposition
via the combination of sparse representations and a variational ap-
proach. Image Processing, IEEE Transactions on, 14(10):1570–1582,
2005.
[10] Michael Elad. Sparse and redundant representations: from theory to
applications in signal and image processing. Springer, 2010.
[11] Hien V Nguyen, Vishal M Patel, Nasser M Nasrabadi, and Rama
Chellappa. Sparse embedding: a framework for sparsity promoting
dimensionality reduction. In Computer Vision–ECCV 2012, pages
414–427. Springer, 2012.
[12] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online
dictionary learning for sparse coding. In Proceedings of the 26th
Annual International Conference on Machine Learning, pages 689–
696. ACM, 2009.
[13] Carl Meyer. Matrix analysis and applied linear algebra book and
solutions manual, volume 2. Siam, 2000.
[14] Nils Barth. The gramian and k-volume in n-space: some classical
results in linear algebra. J Young Investig, 2, 1999.
[15] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral
techniques for embedding and clustering. In NIPS, volume 14, pages
585–591, 2001.
[16] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng. Efﬁcient
sparse coding algorithms. Advances in neural information processing
systems, 19:801, 2007.
[17] Simon Perkins and James Theiler. Online feature selection using
grafting. In In International Conference on Machine Learning, pages
592–599. ACM Press, 2003.
[18] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.
Least angle regression. Annals of Statistics, 32:407–499, 2004.
[19] SA Pawiro, P Markelj, F Pernuˇ s, and C Gendrin. Validation for
2D/3D registration I: A new gold standard data set. Medical Phys,
38(3):1481–1490, 2011.
[20] A. Shapiro and Y . Wardi. Convergence analysis of gradient descent
stochastic algorithms. Journal of Optimization Theory and Applica-
tions, 91(2):439–454, 1996.
1295
