Toward Image Based Visual Servoing for Aerial Grasping and Perching
Justin Thomas, Giuseppe Loianno, Koushil Sreenath, and Vijay Kumar
Abstract— This paper addresses the dynamics, control, plan-
ning, and visual servoing for micro aerial vehicles to perform
high-speed aerial grasping tasks. We draw inspiration from
agile, fast-moving birds, such as raptors, that detect, locate,
and execute high-speed swoop maneuvers to capture prey.
Since these grasping maneuvers are predominantly in the
sagittal plane, we consider the planar system and present
mathematical models and algorithms for motion planning
and control, required to incorporate similar capabilities in
quadrotors equipped with a monocular camera. In particular,
we develop a dynamical model directly in the image space,
show that this is a differentially-ﬂat system with the image
features serving as ﬂat outputs, outline a method for generating
trajectories directly in the image feature space, develop a
geometric visual controller that considers the second order
dynamics (in contrast to most visual servoing controllers that
assume ﬁrst order dynamics), and present validation of our
methods through both simulations and experiments.
I. INTRODUCTION
The dexterity and adaptability of living creatures far
exceeds what we see in modern robots but provides great
inspiration for avenues of research. For example, birds of
prey can not only cover great distances by ﬂight, but they are
also excellent hunters and are able to perch and walk. Some
raptors exhibit great speed and agility, and they use visual
target recognition and perception-action loops to swoop
down to grasp their prey [1]. For example, see Fig. 1 for
still images of a Red Kite swooping to capture a target.
Such capacities would be advantageous for robots in aerial
manipulation tasks. For example, placing sensors quickly,
perching to save energy, and dynamic grasping to acquire
and transport materials or other robots, are a few tasks that
increase the usefulness of aerial robots and require similar
capabilities to those of raptors. Aerial robots, however, do
not yet have a comparable rich set of capabilities, and they
are limited partially by low strength/weight actuators, heavy
materials, and batteries with low energy density and low
speciﬁc power. In addition, our poor understanding of the
perception-action loops required for agile ﬂight and manipu-
lation remains a limiting factor. For grasping or perching,
the robot must be able to detect the object of interest
and subsequently use visual feedback to control the robot’s
motion. To maintain agility, the robot must also accomplish
*We gratefully acknowledge the support of ARL grant W911NF-08-2-
0004 and ONR grants N00014-07-1-0829 and N00014-08-1-0696.
J. Thomas, G. Loianno, and V . Kumar are with the GRASP Lab,
University of Pennsylvania, Philadelphia, PA, USAfjut, loiannog,
kumarg@seas.upenn.edu
K. Sreenath is with the Depts. of Mechanical Engineering and
Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA
koushils@cmu.edu
Fig. 1. A Red Kite swoops down and uses visual feedback to approach,
grasp, and retrieve food on the ground [8].
this task with a minimal sensor payload and consideration
of the dynamics of the system [2], [3].
In scenarios like this, a monocular camera is an inexpen-
sive and versatile sensor of choice, especially when com-
bined with an Inertial Measurement Unit (IMU) [4], [5]. Such
applications requiring control feedback using a single camera
motivate either Position Based Visual Servoing (PBVS) or
Image Based Visual Servoing (IBVS) [6]. PBVS requires an
explicit estimation of the pose of the robot in the inertial
frame while IBVS acts directly using feedback from the
image coordinates. In particular, a single monocular camera
is sufﬁcient for visual servoing when there is some known
geometry or structure in the environment. Further, visual
servoing can even be effective in an unknown environment
while remaining insensitive to camera calibration [7].
Our goal is to ascribe to aerial vehicles the ability to ﬂy
above, grasp, or perch on a target using vision. The platform
considered is a quadrotor micro aerial vehicle (MA V) which
is similar to a helicopter, but has four rotors [9]. The
quadrotor platform is appealing because of its mechanical
simplicity, agility, and well understood dynamics [10].
Despite the fact that it is underactuated, it is possible
to design controllers that will guarantee convergence from
almost any point on SE(3), the Euclidean motion group.
In our group’s previous work, similar controllers have also
been derived for a quadrotor carrying a cable-suspended
payload [10]. However, both of these approaches require full
knowledge of the state. Therefore, our goal is to use similar
approaches with the dynamics of the system directly in the
image plane (rather than in the Cartesian space) to develop
an IBVS controller based on visual features of a cylinder.
We will also demonstrate a method to generate dynamically-
feasible trajectories in the image plane.
There are many excellent tutorials on visual servoing [11],
[6], [12], [13]. However, most approaches are limited to ﬁrst-
order or fully-actuated systems. For example, Taylor and
Ostrowski demonstrated robustness to camera calibration,
but only considered a ﬁrst-order system [7]. Cowan et al.
proved stability for second order systems, but assumed a fully
actuated system [14]. More recently, Hamel and Mahoney
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2113
Fig. 2. Top: A bald eagle snatches a ﬁsh from the water [18]. Bottom: A
quadrotor dynamically grasps at 3 m/s using a custom gripper [2].
leveraged a spherical camera model and utilized backstep-
ping to design non-linear controllers for a speciﬁc class of
underactuated second-order systems [15], [16]. As is typical
in backstepping, however, it is necessary to assume that the
inner control loops are signiﬁcantly faster than the outer
ones. Some preliminary efforts have been made to enable
autonomous landing, but require an estimate of velocity in
the inertial frame using an external motion capture system
[17]. Therefore, there is a lack of IBVS controllers which can
handle the dynamic motion required for aggressive grasping
and perching.
In this paper, we build upon existing IBVS literature,
generalizing from the typical ﬁrst-order fully actuated system
to a higher-order underactuated system, and we develop
IBVS controllers for dynamic vision-based grasping and
perching. In our previous work, we demonstrated a proof-
of-concept bio-inspired gripper which, when attached to a
quadrotor, enabled dynamic grasping of stationary objects as
displayed in Fig. 2 while moving at 3 m/s (or 9 body lengths
/ s) [2]. However, this work required a motion capture system
[19] and therefore restricted such aggressive maneuvers to a
structured environment.
The primary contribution of this paper is to enable
high-speed grasping maneuvers by developing a dynamical
model directly in the image space, showing that this is a
differentially-ﬂat system with the image features serving as
ﬂat outputs, developing a geometric visual controller that
considers the second order dynamics (in contrast to most
visual servoing controllers that assume ﬁrst order dynam-
ics), and presenting validation of our methods through both
simulations and experiments
1
.
The rest of the paper is structured as follows. In Section
II, a mapping from Cartesian space to the image space is
presented. Section III develops a dynamical model in the
image space, while Section IV establishes that the system
is differentially-ﬂat, with the visual features being the ﬂat
outputs, and presents a trajectory generation method. Section
V develops an IBVS controller for the full dynamical system
using geometric techniques. Finally, Sections VI and VII
present simulation and experimental results, respectively.
1
It must be noted that grasping maneuvers are predominanty in the sagittal
plane and thus our developed models and algorithms for motion planning
and control are based on a planar model (x z plane). However, since
the experimental system is 3D, we will use a Vicon-based motion capture
system to ensure stability for the yaw and they-axis dynamics. Thex z
dynamics will be stabilized thorugh our developed IBVS controller.
II. VISION
In this section, we present an overview of the vision
system, outline the camera model, and derive the geometric
constraints on the cylinder detection in the image plane.
A. Problem Formulation
The problem is formulated in the sagittal plane, which
is the dominant plane of actuation for aerial grasping ma-
neuvers in nature and enables high-speed dynamic grasping
for aerial robots [2]. This also allows us to simplify the
problem. When considering the sagittal plane, two image
features are sufﬁcient to establish a relationship between
the vision system and the pose of the robot. These features
could be any two known points; however, it is not always
possible to have such a structured environment in practice.
Conveniently, cylinders with known (or estimated) radii are
fairly common and provide sufﬁcient structure for visual
servoing and grasping.
We will use the following nomenclature. Let T be the
homogeneous transformation matrix from the camera frame
to the world frame, f
i
denote a focal length in the i
th
direction, c
i
be the center image pixel in the i
th
direction,
and  be an arbitrary scaling factor.
B. Camera Model
The camera is modeled using a standard pinhole perspec-
tive camera model so that a generic point in the world,

X Y Z 1

T
, is projected onto the image plane,

x
0
y
0
1

T
, according to [20] such that

2
4
x
0
y
0
1
3
5
=KP
0
T
 1
2
6
6
4
X
Y
Z
1
3
7
7
5
;
where
K =
2
4
f
x
0 c
x
0 f
y
c
y
0 0 1
3
5
; P
0
=

I
33
0
31

:
From here on, we will use the calibrated image coordinates,
2
4
x
y
1
3
5
=K
 1
2
4
x
0
y
0
1
3
5
;
which are equivalent to the transformation and projection of
points in the world to an image plane with unity focal length
and a centered coordinate system.
C. Geometry
Let the image features be the points whose rays are tangent
to the cylinder and lie in the vertical plane. In contrast to
typical visual servoing approaches, these points are now a
function of the position of the robot. Therefore, we cannot
use the standard image Jacobian as in [12], which assumes
the target points are stationary in the inertial frame.
We now formulate the mapping between the image fea-
tures and the robot pose. Letr
q
= (x
q
;z
q
)
T
2R
2
denote the
2114
? (x
t,1
,z
t,1
)
(x
t,2
,z
t,2
)
f
x
x
c
z
c
v
1,m
v
2,m
z
q
x
q
x
z
r
q
Fig. 3. We assume, without loss of generality, that the target is located at
the origin and the quadrotor is located at (xq , zq ). The focal length of the
camera,fx , deﬁnes the location of the image plane relative to the quadrotor
and the image coordinates are given byv
1
andv
2
. The optical ray tangent
to the target intersects the target at (xt , zt ). The coordinate system of the
camera is indicated by xc and zc .
position of the quadrotor in the inertial frame with the target
cylinder at the origin, as shown in Fig. 3. LetR
t
denote the
radius of the target cylinder, andr
t;i
2R
2
be a point on the
circumference that has a tangent passing through the focal
point. With the camera at the same position as the quadrotor,
we have two geometric constraints,
kr
t
k
2
=R
t
kr
q
k
2
2
=kr
q
 r
t
k
2
2
+R
2
t
:
Knowing the radius of the cylinder,R
t
, these equations have
two solutions which represent the two tangent points,
r
t;i
=
R
2
t
kr
q
k
2
0
@

x
q
z
q



 z
q
x
q

s
kr
q
k
2
R
2
t
  1
1
A
: (1)
Unfortunately, the features in the image plane are coupled
with the attitude, and if not compensated, would not allow
for the desired attitude-decoupled mapping between the state
of the robot and the image features. Therefore, we map
the calibrated image coordinates to coordinates on a virtual
level image plane similar to [21] by rotating the camera
coordinate system to a virtual frame where = 0. Then, the
problem can be formulated in the rotated coordinate system
where the virtual image plane is level. The virtual calibrated
coordinates of the features can then be computed using (1)
and

2
4
v
i
0
1
3
5
=P
0
T
 1
2
6
6
4
x
t;i
0
z
t;i
1
3
7
7
5
(2)
with the appropriate transformation T2SE(3). The virtual
coordinates, v = [v
1
; v
2
]
T
, in (2) provide two equations
which can be solved to determine the robot and camera
position as a function of the virtual image coordinates.
We also deﬁne the space S =fr
q
2R
2
j 2R
t
kr
q
k
B
r
;z
q
> 0g; such that the quadrotor’s position is bounded
below by 2R
t
and bounded above byB
r
, and the quadrotor is
always above the horizontal plane. Then, there existsV R
2
and a smooth global diffeomorphism   :S !V such that
v =
f
x
z
2
q
 R
2
t
2
4
x
q
z
q
+R
2
t
q
krqk
2
R
2
t
  1
x
q
z
q
 R
2
t
q
krqk
2
R
2
t
  1
3
5
   (r
q
); (3)
_ v =
d  (r
q
)
dt
=
@
@ _ r
q

d  (r
q
)
dt

_ r
q
J _ r
q
;
where J is the image Jacobian [22]. Note that J can be
expressed as a function of either the image coordinates or
the position of the robot by using (3) and the fact that
  is invertible. Having established a mapping between the
Cartesian coordinates and the image coordinates, we will
next develop a dynamic model of the quadrotor system
directly in the image coordinates.
III. DYNAMICS
The dynamics of this quadrotor system are well known in
literature. For simplicity, we restrict the robot to the vertical
(x z) plane as shown in Fig. 4 and we assume the gripper
is massless (See [23] for the complete 3-D dynamic model).
We deﬁne
r
q
=

x
q
z
q

; w
q
=

r
q


where r
q
is the position of the quadrotor and  is the pitch
angle. Then, the dynamics in the inertial frame take the form
D  w +C _ w +G =F (4)
where D2 R
33
is a diagonal inertial tensor because the
robot frame is aligned with the principal axes of the inertia.
In this case, centripetal and Coriolis terms, C2R
33
, are
zero. Gravity appears in G2R
31
, and F2R
31
is
F =

fRe
2
M

where R 2 SO(2), f 2 R is the total thrust, e
2
=

0 1

T
, and M is the pitch moment generated from
the difference of thrusts between the front and rear rotors
as depicted in Fig. 4. Since the system has three degrees
of freedom, given by w
q
, and only two control inputs that
appear in F, the system is underactuated.
r
q
x
z
? f
1
f
3
fRe
2
M
Fig. 4. The total thrust is f =
P
4
i=1
f
i
and is in the direction of Re
2
.
The moment,M, is the result of the thrust difference betweenf
3
andf
1
.
Now, _ r
q
and  r
q
can be expressed as functions of the image
coordinates using the inverse of the image Jacobian,J. Then,
the dynamics in (4) can be expressed in terms of the image
coordinates
D

J
 1
 v J
 1
_
JJ
 1
_ v



+C

J
 1
_ v
_


+G =F
which simpliﬁes to:
 v =
1
m
J [fRe
2
 G
A
] +
_
JJ
 1
_ v (5)
D
3

 =M (6)
2115
where G
A
denotes the upper left 2 2 block of G, D
3
is
thef3,3g element of D, and m is the mass of the robot.
Equation (5) presents the translational dynamics directly in
the image coordinates. Next, we will demonstrate that these
coordinates form a set of ﬂat outputs for the system, enabling
trajectory design directly in the image space.
IV. DIFFERENTIAL FLATNESS AND PLANNING
In this section, we formulate the trajectory planning prob-
lem in the image plane and compute trajectories that satisfy
the dynamic constraints (i.e. are dynamically feasible) in (5)
and (6). The central idea relies on the differential ﬂatness
(see [24] for deﬁnition) property of our system. We will
show that there is a set of so-called “ﬂat outputs” such that
there is a diffeomorphism from the vector of state variables
and inputs to the vector of ﬂat outputs and its derivatives. In
practice, this means that dynamically feasible trajectories can
be generated by considering sufﬁciently smooth (the order is
deﬁned by the map) trajectories in the space of ﬂat outputs. In
this paper, we show that the image coordinates,v2V R
2
,
serve as ﬂat outputs.
First, there exists a diffeomorphism between the image
coordinates and the position of the robot, namely   as deﬁned
in (3). From (5), we get:
fRe
2
=mJ
 1

 v 
_
JJ
 1
_ v

+G
A
;
and
f =kF
A
k;  = arctan

F
1
F
2

:
where F
A
=mJ
 1

 v 
_
JJ
 1
_ v

+G
A
. Further,
_
f =e
T
2
R
T
_
F
A
;
_
 =
1
f
e
T
1
R
T
_
F
A
:
Since f appears in the denominator, we require that f > 0
so that the thrust is always positive. From (6) and one more
derivative, we obtain:
M =D
3
1
f

e
T
1
R
T

F
A
  2
_
f
_


: (7)
Thus, all state variables and inputs can be written as func-
tions of the image coordinates and their derivatives. Since
we require the 4
th
derivative of the image coordinates (see
(7)), the planned trajectories in the image plane must be at
least C
4
continuous.
Following previous work [25], [2], since the input M is
an algebraic function of the fourth derivative (snap), it is
natural to plan smooth trajectories that minimize the snap of
the trajectory using the cost functional,
J
i
=
Z
t
f
t0


v
(4)
i
(t)



2
dt:
Choosing a polynomial basis allows the minimization to
be formulated as a Quadratic Program (QP) [25]. Further,
equality constraints can be enforced and can be determined
by desired robot positions (or velocities) using the   map
or by previously measured image coordinates. Finally, all
constraints on attitude and the ﬁeld of view must be written
as inequalities in the virtual image plane. Having obtained
a trajectory in the image space, we next develop a visual
controller that will track this trajectory.
V. CONTROL
A. Attitude Controller
First, let R
d
2 SO(2) denote the desired rotation matrix
deﬁned by a desired attitude, 
d
. Then, we deﬁne attitude
errors
e
R
=
1
2
 
R
T
d
R R
T
R
d

_
= sin( 
d
)
e


= 
 R
T
R
d


d
=
_
 
_

d
where
_
is the “vee” map deﬁned in [26]. These errors are
similar to [26] but simpliﬁed for the planar case. Also, we
deﬁne a conﬁguration error function as
	 (R;R
d
) =
1
2
tr

I R
T
d
R

:
The attitude controller is then given as below.
Proposition 1: [26, Prop. 1] (Exponential Stability of At-
titude Controlled Flight Mode) Consider the control moment
deﬁned as
M = K
R
e
R
 K


e


+D
3


d
;
whereK
R
andK


are positive scalars. Further, suppose the
initial conditions satisfy
	(R(0);R
d
(0))< 2;
ke


(0)k
2
<
2
D
3
k
R
(2  	(R(0);R
d
(0))):
Then, (e
R
;e


) = (0; 0) is exponentially stable for the
closed-loop system.
Proof: Follows from [26, Prop. 1]. Seehttp://www.
jtwebs.net/ICRA-2014/ for details.
B. Position Control
Let errors in the image plane be deﬁned by
e
v
=v v
d
:
Then, using (5), the image space error dynamics are
m e
v
=fJRe
2
 JG
A
+m
_
JJ
 1
_ v m v
d
:
The visual servoing controller in then given as below.
Proposition 2: (Exponential Stability of Visual Feature
Controlled Flight Mode) Consider the total thrust component
along the current body frame vertical axis deﬁned by
f =ARe
2
:
where A = G
A
+mJ
 1
[ K
p
e
v
 K
d
_ e
v
+  v
d
], and the
commanded attitude given by
R
c
e
2
=
A
kAk
:
Then, the zero equilibrium (e
v
; _ e
v
;e
R
;e


) = (0;0; 0; 0) is
locally exponentially stable.
Proof: See http://www.jtwebs.net/
ICRA-2014/ for details.
2116
VI. SIMULATION RESULTS
Using the trajectory generation method outlined in Sec-
tion IV, we can generate sample trajectories directly in the
image coordinates, representing a swooping maneuver. It is
reasonable to specify a limit on the attitude, which enables
the incorporation of linear visibility constraints, rather than
requiring non-linear visibility constraints when planning in
the Cartesian space. A sample trajectory is shown in Fig. 5
(top), where the boundary conditions and intermediate way-
point were computed using  , and with the derivatives in the
intermediate waypoint left unconstrained.
Next, using the generated desired trajectory in the image
plane, the controller from Section V is simulated on the
dynamic model given by (5)-(6). The simulation is started
with an initial image coordinate error of 0:10m, and the
resulting trajectory and error are plotted in Fig. 5.
0 1 2 3 4
 0:4
 0:2
0
0:2
0:4
Positions
v
1
v
2
v
1;d
v
2;d
0 1 2 3 4
 0:2
 0:1
0
0:1
0:2
time (s)
Errors
v
1
v
2
_ v
1
_ v
2
Fig. 5. A sample trajectory in simulation. The simulated image coordinates,
v
i
, and the desired coordinates, v
i;d
, are in the top graph where there is
an initial error of 0:1m in each coordinate. The feature errors and error
velocities are in the bottom graph.
VII. EXPERIMENTAL VALIDATION
A Hummingbird quadrotor equipped with a global shutter
Caspa
TM
VL camera and Computer on Module from Gumstix
[27] is used for experiments to ﬂy down and grasp a cylin-
der object. As mentioned earlier, grasping maneuvers are
predominantly in the sagittal plane and thus our developed
models and algorithms for motion planning and control are
planar. Since the experiments are on a 3D system, an external
Vicon-based motion capture system is used to stabilize
the yaw and lateral dynamics while our IBVS controller
stabilizes motion in the sagittal plane.
Visual detection and tracking of the cylinder object runs
onboard the robot, is based on blob tracking using Freeman
chain coding, and is obtained using the C++ Visp library
[28]. When the object is in the image and r
q
2 S, the
measured image points from the camera are mapped to the
virtual image plane using feedback from the IMU and the
transformation shown in Fig. 6, which is mathematically
equivalent to
v
i
= tan (arctan (v
i;m
) +)
where v
i;m
is the boundary of the cylinder in the actual
calibrated image. The points in the virtual plane are ﬁltered
Optical Axis 
Image Plane 
Virtual “level” 
Image Plane 
Optical Ray 
Camera 
Virtual 
Optical 
Axis 
? v
i
v
i,m
Fig. 6. The measured image feature points, v
i;m
, which are affected by
, are projected onto a virtual level image plane to decouple the motion
from the attitude of the robot and determine the coordinates v
i
.
Onboard 
Controller 
IBVS 
Vicon 
Camera 
Dynamics 
Lateral and 
Yaw Controller 
f,? c
Robot Ground Station 
Gumstix 
r
q
v
m
,? y, ˙ y, 
Fig. 7. A camera captures images of the cylinder, which are sent to the
Gumstix Overo Computer on Module (COM). The images are processed at
65 Hz using blob tracking; the boundaries of the cylinder are undistored,
calibrated, and sent back to a ground station along with the pitch as
measured from the IMU. From here, the ground station maps the points
to the virtual plane and computes desired control inputs using the IBVS
controller. In parallel, Vicon feedback is used to close the loop on the lateral
(y; _ y) and yaw ( ) dimensions of the robot. Then, the desired attitude is
sent to the onboard controller, which uses the IMU to control the attitude
at 1 kHz.
0 5 10 15
 0:5
0
0:5
time (s)
Image Coordinate
v
1
v
2
v
1;d
v
2;d
Fig. 8. Experimental results of the feature coordinates in the virtual plane
for a “swooping” trajectory. The feature coordinates are denoted byv
i
and
the desired trajectory is given by v
i;d
.
to improve the estimate of the image features and their
derivatives to compute J and
_
J. A block diagram of the
system is shown in Fig. 7.
The stability of the controller was demonstrated through
several experiments including hovering, vertical trajectories,
“swooping” trajectories, and hovering above a moving cylin-
der. Here we present a “swooping” trajectory, which includes
some components of the mentioned trajectories. See Fig. 8
for the planned and actual trajectories in the virtual image
plane, Fig. 9 for the corresponding estimated and actual
position in the inertial frame, and Fig. 10 for a sequence
of still images from a sample experiment. The reader can
observe the other trajectories in the attached video.
The aggression of our trajectories is limited beause of (i)
slow feedback due to limited onboard sensing and computa-
tion, (ii) a limited optical resolution, and (iii) a small optical
ﬁeld of view.
2117
0 5 10 15
0
0:5
1
time (s)
Position (m)
x
q;v
x
q
z
q;v
z
q
Fig. 9. Positions in the inertial frame for the experiment in Fig. 8. The
vision estimates of the position (using  ) are denoted by the “v” subscript.
The ground truth only has the “q” subscript.
Fig. 10. Still images from a sample “swooping” trajectory using the vision-
based controller developed in this paper. The background has been washed
out slightly to improve visibility.
VIII. CONCLUSION
This paper demonstrates a ﬁrst step towards autonomous
dynamic grasping and manipulation for micro aerial vehicles
in unstructured environments. In particular, we considered
a quadrotor system equipped with a monocular camera and
formulated the dynamics of the underactuated system directly
in the virtual image plane. The system was demonstrated
to be differential ﬂat, with the image coordinates being the
set of ﬂat outputs. We presented a trajectory generation
method which guarantees dynamic feasibility and enables
incorporating visual constraints as linear constraints. We
developed a non-linear vision-based controller for trajectory
tracking in the image space, presented a proof of stability,
and provided validation of the controller both in simulation
and in experimentation on a quadrotor. A next step would
be to generalize the control law to full three dimensions and
consider the yaw of the robot by using image moments to
detect the primary axis of the cylinder.
ACKNOWLEDGMENT
The authors would like to thank Kartik Mohta for his ex-
pertise and assistance with the vision hardware and software.
REFERENCES
[1] Norma Venable. Birds of Prey. West Virginia University Extension
Service, 1996.
[2] Justin Thomas, Joe Polin, Koushil Sreenath, and Vijay Kumar. Avian-
Inspired Grasping for Quadrotor Micro UA Vs. In International Design
andEngineeringTechnicalConferences&ComputersandInformation
in Engineering Conference (IDETC/CIE), Portland, 2013.
[3] Matko Orsag, Christopher Korpela, and Paul Oh. Modeling and
Control of MM-UA V: Mobile Manipulating Unmanned Aerial Vehicle.
Journal of Intelligent & Robotic Systems, 69(1-4):227–240, August
2012.
[4] Anastasios I Mourikis, Stergios I Roumeliotis, and Joel W Burdick.
SC-KF Mobile Robot Localization: A Stochastic Cloning Kalman
Filter for Processing Relative-State Measurements. IEEE Transactions
on Robotics, 23(4):717–730, August 2007.
[5] Stephan Weiss, Davide Scaramuzza, and Roland Siegwart. Monocular-
SLAM-based navigation for autonomous micro helicopters in GPS-
denied environments. Journal of Field Robotics, 28(6):854–874,
November 2011.
[6] S. Hutchinson, G.D. Hager, and P.I. Corke. A tutorial on visual servo
control. IEEE Transactions on Robotics and Automation, 12(5):651–
670, 1996.
[7] C.J. Taylor and J.P. Ostrowski. Robust vision-based pose control. In
Proceedings 2000 ICRA. Millennium Conference. IEEE International
Conference on Robotics and Automation. Symposia Proceedings (Cat.
No.00CH37065), volume 3, pages 2734–2740. IEEE, 2000.
[8] The Slow Mo Guys. ”Red Kites in Slow Motion”,
http://youtu.be/AYOx-iCMZhk.
[9] ”Ascending Technologies, GmbH,” http://www.asctec.de.
[10] Koushil Sreenath, Taeyoung Lee, and Vijay Kumar. Geometric Control
and Differential Flatness of a Quadrotor UA V with a Cable-Suspended
Load. In IEEE Conference on Decision and Control (CDC), pages
2269–2274, Florence, Italy, December 2013.
[11] B. Espiau, F. Chaumette, and P. Rives. A new approach to visual
servoing in robotics. IEEE Transactions on Robotics and Automation,
8(3):313–326, June 1992.
[12] Francois Chaumette and Seth Hutchinson. Visual servo control. I.
Basic approaches. IEEE Robotics & Automation Magazine, 13(4):82–
90, December 2006.
[13] F. Chaumette and S. Hutchinson. Visual servo control. II. Advanced
approaches [Tutorial]. IEEE Robotics & Automation Magazine,
14(1):109–118, March 2007.
[14] NJ Cowan, J.D. Weingarten, and D.E. Koditschek. Visual servoing via
navigation functions. IEEE Transactions on Robotics and Automation,
18(4):521–533, August 2002.
[15] Tarek Hamel and Robert Mahony. Visual Servoing of an Under-
Actuated Dynamic Rigid-Body System: An Image-Based Approach.
IEEE Transactions on Robotics & Automation, 18(2):187–198, 2002.
[16] Tarek Hamel and Robert Mahony. Image based visual servo control
for a class of aerial robotic systems. Automatica, 43(11):1975–1983,
November 2007.
[17] Daewon Lee, Tyler Ryan, and H. Jin. Kim. Autonomous landing of a
VTOL UA V on a moving platform using image-based visual servoing.
In 2012 IEEE International Conference on Robotics and Automation,
pages 971–976. IEEE, May 2012.
[18] Karen Bass, Brian Leith, Justin Anderson, Peter Bassett, Joe Stevens,
Hugh Pearson, and Jeff Turner. Nature’s Most Amazing Events.
[DVD]. BBC Worldwide Ltd. Programs, 2009.
[19] ”Vicon Motion Systems, Inc,” http://www.vicon.com.
[20] Yi Ma, Stefano Soatto, Jana Kosecka, and Shankar Sastry. An
Invitation to 3-D Vision: From Images to Geometric Models. Interdis-
ciplinary Applied Mathematics. Springer, New York, 2004.
[21] Hamed Jabbari, Giuseppe Oriolo, and Hossein Bolandi. Dynamic
IBVS control of an underactuated UA V. 2012 IEEE International
Conference on Robotics and Biomimetics (ROBIO), pages 1158–1163,
December 2012.
[22] Hong Zhang and J.P. Ostrowski. Visual Servoing with Dynamics:
Control of an Unmanned Blimp. In International Conference on
Robotics and Automation, volume 1, pages 618–623. IEEE, 1999.
[23] Koushil Sreenath, Nathan Michael, and Vijay Kumar. Trajectory
Generation and Control of a Quadrotor with a Cable-Suspended Load
A Differentially-Flat Hybrid System. In International Conference on
Robotics and Automation, 2013.
[24] R.M. Murray, M. Rathinam, and Willem Sluis. Differential ﬂatness
of mechanical control systems: A catalog of prototype systems. In
ASME International Congress and Exposition. Citeseer, 1995.
[25] Daniel Mellinger and Vijay Kumar. Minimum snap trajectory gen-
eration and control for quadrotors. In 2011 IEEE International
Conference on Robotics and Automation, pages 2520–2525. IEEE,
May 2011.
[26] Taeyoung Lee, Melvin Leok, and N.H. McClamroch. Control of
Complex Maneuvers for a Quadrotor UA V using Geometric Methods
on SE(3). Asian Journal of Control, 2011.
[27] ”Gumstix, Inc.,” http://www.gumstix.com.
[28] Andrew I. Comport, Eric Marchand, Muriel Pressigout, and Franc ¸ois
Chaumette. Real-time markerless tracking for augmented reality: the
virtual visual servoing framework. IEEE transactions on visualization
and computer graphics, 12(4):615–28.
2118
