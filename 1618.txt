On the Use of Homogeneous Transformations to Map
Human Hand Movements onto Robotic Hands
G. Salvietti
1
, M. Malvezzi
2
, G. Gioioso
1;2
, and D. Prattichizzo
1;2
Abstract— Replicating the human hand capabilities is a great
challenge in telemanipulation as well as in autonomous grasping
and manipulation. One of the main issues is the difference
between human and robotic hands in terms of kinematic
structure, which does not allow a direct correlation of the joints.
We recently proposed an object-based mapping algorithm
able to replicate on several robotic hand models the human
hand synergies. In such approach the virtual object shapes
were a-priori deﬁned (e.g. a sphere or an ellipsoid) and the
transformation was represented as the composition of a rigid
body motion and a scale variation. In this work, we introduce
a generalization of the object-based mapping that overcomes
the deﬁnition of a shape for the virtual object. We consider
only a set of reference points on the hands. We estimate a
homogeneous transformation matrix that represents how the
human hand motion changes its reference point positions. The
same transformation is then imposed to the reference points
on the robotic hand and the joints values obtained through
a kinematic inversion technique. The mapping approach is
suitable also for telemanipulation scenarios where the hand
joint motions are combined with a wrist displacement.
I. INTRODUCTION
Robotic hands present a high variability of kinematic
structures, actuation and control systems. They differ in the
number of ﬁngers, in the number of Degrees of Freedom
(DoFs) per ﬁnger, in the type of joints and actuators, etc.
[1]. In most of the telemanipulation scenarios, however, the
motion of such a heterogeneous set of devices is related to the
motion of a unique complex kinematic structure: the human
hand. This has led to the development of several mapping
strategies that strongly depend on the robotic hand structures.
Examples of these approaches are the ﬁngertip mapping [2],
the pose mapping [3] and the joint-to-joint mapping [4]. The
main drawbacks of these methods are mainly the lack of gen-
erality and the need of empirical or heuristic considerations
to deﬁne the correspondence between human and robotic
hands.
To overcome such limits, we presented a mapping deﬁned
in the task space and mediated by a virtual object. The
method was detailed in [5] considering a sphere as virtual
object and generalized in [6], considering an ellipsoid to
extend the possible transformations that can be imposed. The
The research leading to these results has received funding from the
European Union Seventh Framework Programme FP7/2007-2013 under
grant agreement 248587 of the project “THE - The Hand Embodied”, under
grant agreement 601165 of the project “WEARHAP - WEARable HAPtics
for humans and robots, and from the Italian Ministry for Research, Futuro
in Ricerca 2012 programme, under grant agreement RBFR12C6O8 of the
MODELACT project.
1
Department of Advanced Robotics, Istituto Italiano di Tecnologia, Via
Morego 30, 16163 Genoa, Italy. gionata.salvietti@iit.it
2
Universit` a degli Studi di Siena, Dipartimento di Ingegneria
dell’Informazione, Via Roma 56, 53100 Siena, Italy. fmalvezzi,
gioioso, prattichizzog@dii.unisi.it
Human Hand 
Homogeneous 
Transformation 
Robotic Hand 
Fig. 1: The mapping framework. The motion of the human
hand is captured by a set of reference points (p
h
i
) which are
used to compute a common homogeneous transformation.
The homogeneous transformation is also applied to a set of
reference points placed on the robotic hand (p
r
i
) and the new
conﬁguration computed using inverse kinematic techniques.
object-based mapping is obtained considering two virtual
objects, respectively on the human and on the robotic hand.
They are computed considering the minimum volume object
containing reference points suitably deﬁned, placed on the
respective hands. A conﬁguration variation of the human
hand induces a motion and a deformation of the virtual
object. We impose that the object deﬁned on the robotic hand
moves and deforms according to that deﬁned on the human
hand. The mapped motion of the robotic hand is then ob-
tained through pseudo-inversion techniques. This mapping
procedure, was adopted to map human hand synergies [7],
onto hands with very dissimilar kinematics [8]. However,
the deﬁnition of a shape for the virtual object implies that
the method obtains better results if the manipulated and the
virtual object are similar. Moreover, shearing deformation
cannot be reproduced and only in-hand manipulation has
been tested so far, since the few available parameters are
not sufﬁcient to describe both joints and wrist motions.
In this paper we introduce a new solution which overcomes
the deﬁnition of a virtual object shape and allows replicating
the motion of the human hand in a telemanipulation scenario
where also wrist motion is considered. The mapping is
based on the deﬁnition of a set of reference points, both on
the human and on the robotic hand: the reference points on
the human hand are necessary to evaluate the transformation
produced by the hand motion, the points on the robotic hands
are necessary to map such transformation on the robotic hand
(Fig. 1). A conﬁguration change on the human hand causes
a transformation of the reference point positions, which can
be generally represented by a six-dimensional displacement
and/or a non rigid deformation. In this paper, we assume
that this transformation can be represented as a linear trans-
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5352
formation, estimated from the displacement of the reference
points. The same linear transformation is then imposed to the
robotic hand reference points and the hand joint displacement
is consequently deﬁned by solving its inverse kinematics. A
linear transformation matrix can be, in general, decomposed
as the combination of different elementary transformations,
e.g., translation and rotations, scale variations and shear
deformations. Such decomposition is adopted in the proposed
mapping procedure to separately reproduce the contribution
in terms of internal forces [9], which are paramount for grasp
control, and in terms of the rigid body displacement imposed
by the hand on the manipulated object [10]. We simulated
a possible teleoperation scenario to test the applicability and
effectiveness of the proposed approach. In the teleoperation
task both hand and wrist motion are involved.
The paper is organized as it follows. Section II summarizes
the properties of homogeneous transformations and recalls
the deﬁnition of primitive transformations. Section III de-
scribes the procedure to map human hand movements to
robotic hands, based on homogeneous transformations, while
Section IV shows how the proposed procedure works with
some numerical tests, in which a robotic hand with a non
anthropomorphic structure is simulated. Finally Section V
provides some concluding considerations and describes fur-
ther developments of the work.
II. THE HOMOGENEOUS TRANSFORMATION PROPERTIES
The mapping procedure is based on the analysis of the
transformation of a set of reference points on the human
hand during hand motion. In the paper we assume that the
transformation is linear, so, indicating with p
i
and p
f
the
coordinates of a generic reference point in its initial and
ﬁnal conﬁguration, respectively, we have that
p
f
=Ap
i
+b;
where A is a 3 3 matrix representing the linear map and
b is a three-dimensional vector representing the translation
in the transformation. Introducing the augmented matrix and
vector notation, it is possible to represent both the translation
and the linear map using a single matrix multiplication, i.e.
^ p
f
=T ^ p
i
;
where ^ p
i;f
=
h
p
T
i;f
1
i
T
and
T =

A p
0 0 0 1

: (1)
.
Homogeneous 4 4 matrices are widely used in 3D
computer graphics system to represent solid bodies trans-
formations [11]. Homogeneous transformations are able to
represent all the transformations required to move an object
and visualize it: translation, rotation, scale, shear, and per-
spective. Any number of transformations can be multiplied to
form a composite matrix. Transformation matrices are widely
adopted also in continuum mechanics to describe material
displacements and strains, and methods to decompose from
the deformation gradient the contribution of rigid body mo-
tion and non rigid deformation are available in the literature,
see for instance [12], [13].
Rigid body motions are particular types of transformation
that preserve the distance between points and the angles
between vectors. They can be represented as the combination
of a rotation, deﬁned by the rotation matrixR2SO(3), and
a translation motion, deﬁned by the vector p2<
3
. SO(3)
(special orthogonal) is the set of all the 3 3 orthogonal
matrices with determinant equal to 1 [14]. The corresponding
homogeneous matrix can be expressed as shown in Fig. 2-a).
T is in this case a representation of a generic element of the
SE(3) group (special Euclidean).
Homogeneous matrices can be adopted also to describe
non rigid transformations: isotropic transformations, which
modiﬁes the object size by a scaling factor , without
moving it; non isotropic transformations, which modify the
object size by scaling factors [;;], in the x;y;z direc-
tions respectively, and shear transformations, that displaces
each point in ﬁxed direction, by an amount proportional
to its signed distance from a line that is parallel to that
direction. A generic non rigid transformation is qualitatively
represented in Fig. 2-b). In this study we do not consider
perspective transformations for the sake of simplicity. These
basic homogeneous transformations are usually referred to as
primitive transformations. Each of them can be represented
with a more meaningful and concise representation: a scalar
for the isotropic transformation, a vector for translation, 3D
scaling and shear, a quaternion for rotations. The recover of
the concise form from the primitive transformation matrix is
straightforward, but, on the other hand, once primitives have
been multiplied into a composite matrix, the recovery of the
primitive is not direct in general [15].
Different procedures to decompose a generic 4 4 matrix
into a series of primitive transformations are available in
the literature. In this paper we consider the extraction
of the rigid and non rigid motions contributions from a
generic linear transformation matrix. Consider, for instance,
the human hand/arm system moving along a trajectory while
the hand is changing the grasp forces exerted onto an object.
In this case a large rigid arm displacement is coupled with
a smaller non rigid deformation. The displacements of the
reference points on the human hand however contains both
the contributions. So that, in the mapping procedure, when
the arm and wrist is involved in the motion, we propose
to extract the rigid part of the motion from the complete
transformation matrix and to reproduce it with the robotic
arm, while the non rigid contribution to the reference point
conﬁguration variation is reproduced acting on the robotic
hand ﬁngers. This decomposition will be better explained in
the numerical experiment proposed in Sec. IV.
To consider rigid and non-rigid contribution, we need to
express the transformation matrix as follows
T =T
def
T
rb
; (2)
whereT
rb
=T
tr
T
rot
represents the rigid part of the displace-
ment, composed of a rotation and a translation, and T
def
takes into account the non rigid deformation, as shown in
Fig. 2-a) and b) respectively. The extraction of the translation
part of the rigid body motion from the starting matrix T is
straightforward considering eq. (1). The matrix A in eq. (1)
can be written, with the polar decomposition, as the product
5353
T
rb
=

R p
0 1

T
def
=
2
4
1 hxy hxz 0
hyx 1 hyz 0
hzx hzy 1 0
0 0 0 1
3
5
a) b)
Fig. 2: Examples of linear transformations applied to a cube
and corresponding homogeneous matrices: a) rigid body
motion, b) shear and scale transformation.
A =UR (3)
in whichR is orthogonal andU is an Hermitian semi-positive
matrix,R2SO(3) represents a rigid rotation, whileU takes
into account the non rigid deformation [16].
III. MAPPING PROCEDURE BASED ON HOMOGENEOUS
TRANSFORMATION
In this work, a model of the human hand is considered
as reference hand. The deﬁnition of the following reference
frames is necessary to identify hand motion. LetfN
w
g be
an inertial reference frame, adopted to describe hand/wrist
motion. LetfN
h
g indicate a frame on the human hand palm.
The conﬁguration offN
h
g reference frame with respect to
the inertial one depends on arm motion, and is described by
the homogeneous transformation matrix T
h
, which depends
on six parameters, namely the coordinates offN
h
g origin
and the relative orientation between the frames, described for
instance by Euler angles. Letp
w
2<
6
be a vector containing
fN
h
g position and orientation with respect tofN
w
g.
Various kinematic models of the human hand are available
in the literature, we chose a 20 DoFs model, in which each
ﬁnger has four DoFs [17]. Let us indicate with q
h
2<
nq
h
,
n
q
h
= 20, the generic conﬁguration of hand joints.
The number and position of the reference points on the
human hand,n
h
, can be arbitrarily set. Reference points can
be placed on the ﬁngertips, in the intermediate phalanges, in
correspondence of the joint axis, etc. The ﬁngertips represent
a natural choice for the reference points, since they are at
the end of the kinematic chains that deﬁne the ﬁngers, so
their conﬁgurations depend on all the joints [5].
Let us indicate withp
k
, withk = 1; ;n
h
the reference
points on the human hand. The vector p
h
k;c
2<
3
represents
the coordinates of the generic reference pointp
k
with respect
to fN
h
g when the hand assumes a conﬁguration C and
the joint values are q
h
c
. Let furthermore us indicate with
^ p
h
k;c
2 <
4
the corresponding augmented vectors, adopted
to represent afﬁne transformations, i.e. ^ p
h
k;c
= [p
h
k;c
T
1]
T
.
Finally, let us indicate with p
h
c
2<
3n
h
a vector containing
the coordinates of all the reference points in the generic
conﬁgurationC.
Letp
h
i
denote the initial position of the reference points on
the human hand. Their position is a function of hand initial
conﬁguration vector q
h
i
and the wrist initial conﬁguration
p
h
i;w
, and can be evaluated by the direct kinematic analysis
of the hand, i.e.
p
h
i
=f
k
(q
h
i
;p
h
i;w
): (4)
Let us then assume that, starting from this initial conﬁgura-
tion, the hand and the wrist are moved, let q
h
f
and p
h
f;w
be
the ﬁnal hand joint and wrist conﬁgurations. The reference
point positions on the human hand vary according to hand
and wrist kinematics, i.e. p
h
f
=f
k
(q
h
f
;p
h
f;w
) [18].
We assume that the conﬁguration variation of the refer-
ence points from p
h
i
to p
h
f
can be represented as a linear
transformation, i. e. for each point p
k
, with k = 1; ;n
h
,
the following linear relationship holds
^ p
h
k;f
=T ^ p
h
k;i
: (5)
Given the initial and ﬁnal reference point conﬁgurations ^ p
h
k;i
and ^ p
h
k;f
, we can evaluate the linear transformation T in
eq. (5) by solving the following linear system
^ p
h
k;f
=Mt; (6)
where t 2 <
12
contains the components of the linear
transformation T , i.e.
T =
2
6
4
t
1
t
2
t
3
t
4
t
5
t
6
t
7
t
8
t
9
t
10
t
11
t
12
0 0 0 1
3
7
5;
and the system matrix M2<
3n
h
12
is deﬁned as
M =
2
4
M
1

M
n
h
3
5
;
in which the generic matrix M
k
2<
312
is given by
M
k
=
2
4
^ p
h T
k;i
0
1;4
0
1;4
0
1;4
^ p
h T
k;i
0
1;4
0
1;4
0
1;4
^ p
h T
k;i
3
5
:
As already mentioned in Sec. II, T matrix can then be de-
composed as the product between a rigid body transformation
matrix T
rb
and a non rigid transformation T
def
.
The idea behind the proposed mapping procedure is then
to reproduce, on the reference points deﬁned on the robotic
hand, the same linear transformation computed on the human
hand. Note that the homogeneous transformation matrix T
obtained by solving the linear system in eq. (6) depends on
the human hand and wrist conﬁguration variation, and also
on the initial conﬁguration of reference points p
h
i
.
Let us indicate withfN
w;r
g an inertial reference frame,
adopted to describe robotic hand and arm motion, andfN
r
g
a reference frame on the robotic hand palm, let p
w;r
2<
6
be a vector describing the position and orientation of frame
fN
r
g with respect tofN
w;r
g, and letq
r
c
2<
nqr
indicate the
robotic hand joint vector. In general, since the robotic and
human hands have a different kinematic structure,n
qr
6=n
q
h
.
5354
A set of reference points are deﬁned also on the robotic
hand, indicated with p
r
s
, with s = 1; ;n
r
. In general, n
h
and n
r
are not related and n
h
6=n
r
.
In the initial reference conﬁguration the coordinates of
the reference points on the robotic hand are deﬁned by the
vectors p
r
s;i
, that can be collected in the vector p
r
i
2<
3nr
.
The ﬁnal conﬁguration of these points, according to the
above deﬁned linear transformation, can be evaluated as the
composition of two motions
^ p
r
s;f
=T
def
T
rb
^ p
r
s;i
=T
def
^ p
r
s;rb
: (7)
The reference point conﬁgurations after the rigid transfor-
mation can be collected in the vector p
r
rb
2 <
3nr
, while
p
r
f
2<
3nr
contains the ﬁnal reference point conﬁgurations.
The displacement vector p
r
due to the non rigid part of
the transformation is thus deﬁned as
p
r
=p
r
f
 p
r
rb
:
This displacement has to be reproduced by modifying the
robotic hand joint values, according to robotic hand inverse
kinematics. If the displacement p
r
is sufﬁciently small, the
linear approximation of the kinematics of the robot can be
considered. Consequently, the displacement that has to be
imposed to the robotic hand joints, can be evaluated as
q
r
=J
#
r
p
r
+N
Jr
 (8)
where J
r
is the robotic hand Jacobian matrix, the index
# denotes a generic pseudoinverse, N
Jr
is a basis of J
r
nullspace and  is a vector parametrizing the homogeneous
part of the inverse differential kinematics problem and man-
aging the presence of eventual redundant hand DoF [14].
Robotic hand joint variation q
r
is the displacement that
has to be imposed to the robotic hand joints in order to
obtain, on the robotic hand reference points, the same linear
transformation of the reference points on the human hand. If
the mapping is applied between two hand/arm systems, the
rigid body component of the motion, given by
p
r
rb
=p
r
rb
 p
r
i
can be exerted by the wrist/arm motion while the non-rigid
deformation is related to the hand joints.
The main steps of the mapping algorithm are summarized
in Fig. 3.
IV. NUMERICAL EXPERIMENT: TELEOPERATING A THREE
FINGERED ROBOTIC HANDS
This set of numerical simulations is aimed at mapping on a
robotic hand, a task in which the human hand is manipulating
a cubic object. The experiments were performed using and
adapting the functions available in SynGrasp, a Matlab
Toolbox for the simulation and the analysis of grasping
with several hand models [19]. In the assigned task, the
human wrist moves along a given trajectory and, at the same
time, the ﬁrst hand synergy is activated. Postural synergies
represent a way to simplify human hand structure, reducing
the number of DoFs necessary to deﬁne its posture [20].
The synergy idea has been brought to robotics, to reduce the
Human Hand 
Robot Hand 
Reference point 
 positions @ t 
Reference point 
 positions @ t+1 
Rigid body  
motion 
  q
r
=J
#
r
  p
r
+N
J
r
? Non-rigid  
deformation 
p
h
i
  ˆ p
r
rb
= T
a
rb
ˆ p
r
i
ˆ p
h
f
= T
a
ˆ p
h
i
Fig. 3: The mapping algorithm. The motion of the reference
points in the human hand between instant t and t + 1 is
represented with the homogeneous transformationT
a
, where
T
a
collects in a diagonal matrix the T matrix deﬁned for
each point. The rigid body transformation matrixT
a
rb
is used
to compute the rigid body contribution, while the non-rigid
transformation is obtained acting on robotic hand joints q
r
.
number of inputs necessary to actuate a robotic hand, thus
simplifying their mechanical and control structure [21], [7].
The synergy activation in this case produces both a variation
of the contact forces and a displacement of the object center,
which can be evaluated using the procedure discussed in [22].
These wrist and in–hand motions were mapped on a robotic
system represented by a three ﬁngered hand resembling the
Barrett Hand but with eight actuated joints (two ﬁngers with
three joints and one with two), and a six DoF arm. Both the
hands were grasping the same object, a cube with side 50mm.
The two hands started from a given grasp obtained through
the grasp planner available in SynGrasp, which considers
a procedure similar to that described in [23]. As reference
points for the mapping we assumed the contact points of the
hands with the object. In Fig. 4-a) human and robotic hand
conﬁgurations are sketched, and in Fig. 4-b) contact points
are shown. The grasp planner provided seven contact points
on the human hand and three contact points on the robotic
hand.
We considered a generic trajectory for the human wrist
represented by a cosine arc whose length was 470 mm in
the x direction and whose height in the z direction was 150
mm with respect to thefN
w
g reference frame. While the
human wrist is following this trajectory, hand ﬁnger joint
reference values are varied along the ﬁrst hand synergy. The
trajectory was sampled in a series of steps. For each step,
human hand and arm motion were mapped on the robotic
system with the proposed mapping approach.
Fig. 5-a) shows the human hand trajectory. The blue line
represents object center displacement. Fig. 5-b) shows the re-
sulting motion on the robotic hand. The red curve represents
robotic hand object center trajectory during the simulation.
As it can be seen, the robotic hand is able to follow the
human hand trajectory, even if it is the combination of a
5355
a) b)
Fig. 4: a) Human hand and three–ﬁngered robotic hand
grasping a cubic object: initial conﬁguration b) object center
(black dot), contact points on the human hand (blue dots)
and on the robotic hand (red dots).
generic six-dimensional wrist displacement with an in–hand
motion.
Fig. 6 shows, for the ﬁrst sampling step, the direction of
the object center displacement produced by the activation of
the hand synergies, i.e. without considering wrist motion, and
the corresponding variations of the contact forces, evaluated
according to [22]. While the object motion directions are
quite comparable, a simple comparison between contact
force variations is not possible, due to the different positions
and number of contact points between the human and robotic
hand.
Finally, Fig.7 shows the sensitivity of the proposed map-
ping procedure with respect to some operative parameters.
The upper diagram shows the effect of the applied synergies.
The ﬁnal reference values of the human hand joints were
evaluated as q
h
f
= q
h
i
+S; with  varying from 0:1
and 1, S representing the synergy matrix as deﬁned in
[21] and  = [1 0 0  ]
T
for the ﬁrst synergy,
 = [0 1 0  ]
T
for the second one, etc.. As it can be
seen, the sensitivity of  parameter on the trajectory error,
deﬁned as the distance between object centers at the end
of trajectory execution, is quite evident. The sensitivity is
furthermore different for the different synergies. This effect
is due to the different kinematics between human and robotic
hand. The three ﬁngered robotic hand, due to its kinematic
constraints, is not able to fully reproduce the object in–hand
motion produced by the human hand. The lower diagram
shows the sensitivity of the trajectory error on the size of
trajectory discretization step. In this case the sensitivity is
quite lower, i.e. the proposed mapping procedure is quite
robust with respect to the length of trajectory discretization
step.
V. CONCLUSIONS
The complex and different structures that characterize
robotic hands requires methods to unify their control. There
are applications, e.g. telemanipulation or learning by demon-
a)
0 50 100 150 200 250 300 350 400 450
?100
0
100
200
?100
?50
0
50
100
150
200
250
y
x
z
b)
Fig. 5: Mapping human hand motion on a robotic hand
during the execution of a trajectory combined with the
activation of hand synergies: a) human hand and object
trajectory; b) robotic hand trajectory, the blue line represents
human hand object center trajectory, the red one shows the
obtained robotic hand object center trajectory.
stration, in which a mapping between the human hand and
robotic hands is necessary. The development of a mapping
function between human and robotic hands, even with dis-
similar kinematics, is necessary to solve these issues. In this
paper we describe a mapping procedure based on the task
space, whose main requirement is the deﬁnition of a series
of reference points both on the human and on the robotic
hand. When the human hand changes its joint conﬁgurations,
the displacement of its reference points is used to deﬁne a
homogeneous matrix able to capture the hand motion. The
same homogeneous transformation matrix is adopted to eval-
uate the displacements of the robotic hand reference points,
and consequently, through inverse kinematics techniques, the
robotic hand joint variations.
The advantages of this type of mapping is that it does
not requires empirical and heuristic considerations, it is
general and can be applied to robotic hands with a kinematic
structure very different from the anthropomorphic one. The
mapping function is nonlinear, and depends on the initial
5356
Fig. 6: a) In–hand object center motion direction: human
hand (blue arrow) and robotic hand (red arrow). b) contact
force variations in the human hand contact points (blue
arrow) and in the robotic hand contact points (red arrows)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
synergy activation factor
trajectory error [mm]
 
 
0 50 100 150
0
2
4
6
8
trajectory sampling step [mm]
trajectory error [mm]
syn. 1
syn. 2
syn. 3
Fig. 7: Trajectory error sensitivities. Upper diagram: sensi-
tivity of the trajectory error on the coefﬁcient of synergy
activation, for the ﬁrst three synergies. Lower diagram:
sensitivity on the trajectory discretization step.
reference conﬁgurations of human and robotic hand.
This is a preliminary presentation of a homogeneous trans-
formation based mapping procedure. The numerical tests
presented in the paper show that its performance depends
on relative conﬁguration of the hands and this aspect needs
to be furthermore analyzed and compared with other map-
ping methods. However, since the method is based on the
transformation of a virtual set of points in the task space,
we expect that the dependency of mapping performance on
the relative conﬁgurations between the hands is substantially
due to the kinematic limits of the robotic hand and not on
the mapping features, like, for instance, in the joint–to–joint
mapping method. Other parameters have to be considered in
the analysis, for instance the role of the number and location
of the reference points on both the hands.
REFERENCES
[1] A. Bicchi, “Hands for dextrous manipulation and robust grasping:
a difﬁcult road towards simplicity,” IEEE Trans. on Robotics and
Automation, vol. 16, pp. 652–662, December 2000.
[2] A. Peer, S. Einenkel, and M. Buss, “Multi-ﬁngered telemanipulation
- mapping of a human hand to a three ﬁnger gripper,” in Robot and
Human Interactive Communication, 2008. RO-MAN 2008. The 17th
IEEE International Symposium on, pp. 465–470, August 2008.
[3] N. Y . Lii, Z. Chen, M. A. Roa, A. Maier, B. Pleintinger, and
C. Borst, “Toward a task space framework for gesture commanded
telemanipulation,” in RO-MAN, 2012 IEEE, pp. 925–932, IEEE, 2012.
[4] M. T. Ciocarlie and P. K. Allen, “Hand posture subspaces for dexterous
robotic grasping,” The International Journal of Robotics Research,
vol. 28, pp. 851–867, July 2009.
[5] G. Gioioso, G. Salvietti, M. Malvezzi, and D. Prattichizzo, “Mapping
synergies from human to robotic hands with dissimilar kinematics: an
approach in the object domain,” IEEE Trans. on Robotics, vol. 29,
no. 4, pp. 825–837, 2013.
[6] G. Gioioso, G. Salvietti, M. Malvezzi, and D. Prattichizzo, “An object-
based approach to map human hand synergies onto robotic hands with
dissimilar kinematics,” in Robotics: Science and Systems VIII, Sidney,
Australia: The MIT Press, July 2012.
[7] M. Gabiccini, A. Bicchi, D. Prattichizzo, and M. Malvezzi, “On the
role of hand synergies in the optimal choice of grasping forces,”
Autonomous Robots, pp. 1–18.
[8] G. Salvietti, G. Gioioso, M. Malvezzi, D. Prattichizzo, A. Serio,
E. Farnioli, M. Gabiccini, A. Bicchi, I. Sarakoglou, N. Tsagarakis,
and D. Caldwell, “Hands.dvi: A device-independent programming and
control framework for robotic hands,” in Gearing up and accelerating
cross-fertilization between academic and industrial robotics research
in Europe - Technology transfer experiments from the ECHORD
project, pp. 197–215, Springer Tracts in Advanced Robotics, Springer,
2014.
[9] A. Bicchi, “On the closure properties of robotic grasping,” The Int. J.
of Robotics Research, vol. 14, no. 4, pp. 319–334, 1995.
[10] G. Salvietti, L. Meli, G. Gioioso, M. Malvezzi, and D. Prattichizzo,
“Object-based bilateral telemanipulation between dissimilar kinematic
structures,” in Proc. IEEE/RSJ Int. Symp. Intelligent Robots and
Systems, (Tokyo, Japan), 2013.
[11] K. Shoemake and T. Duff, “Matrix animation and polar decomposi-
tion,” in Proceedings of the conference on Graphics interface, vol. 92,
pp. 258–264, Citeseer, 1992.
[12] D. Guan-Suo, “Determination of the rotation tensor in the polar
decomposition,” Journal of elasticity, vol. 50, no. 3, pp. 197–207,
1998.
[13] A. Hoger and D. E. Carlson, “Determination of the stretch and rotation
in the polar decomposition of the deformation gradient,” Quarterly of
applied mathematics, vol. 42, no. 1, pp. 113–117, 1984.
[14] R. Murray, Z. Li, and S. Sastry, A mathematical introduction to Robotic
Manipulation. 1994.
[15] R. Goldman, Recovering the Data from the Transformation Matrix,
vol. VII-2 of GEMS. Academic Press, 1991.
[16] N. J. Higham and R. S. Schreiber, “Fast polar decomposition of an ar-
bitrary matrix,” SIAM Journal on Scientiﬁc and Statistical Computing,
vol. 11, no. 4, pp. 648–655, 1990.
[17] S. Mulatto, A. Formaglio, M. Malvezzi, and D. Prattichizzo, “An-
imating a deformable hand avatar with postural synergies for haptic
grasping,” in Haptics: Generating and Perceiving Tangible Sensations.
Eurohaptics 2010, Lecture Notes in Computer Science, pp. 203–210,
Amsterdam, The Netherlands: Springer Verlag, 2010.
[18] D. Prattichizzo and J. Trinkle, “Grasping,” in Handbook on Robotics
(B. Siciliano and O. Kathib, eds.), pp. 671–700, Springer, 2008.
[19] M. Malvezzi, G. Gioioso, G. Salvietti, D. Prattichizzo, and A. Bicchi,
“Syngrasp: a matlab toolbox for grasp analysis of human and robotic
hands,” in Proc. IEEE Int. Conf. on Robotics and Automation, 2013.
[20] M. Santello, M. Flanders, and J. F. Soechting, “Postural hand synergies
for tool use,” The Journal of Neuroscience, vol. 18, pp. 10105–10115,
December 1998.
[21] D. Prattichizzo, M. Malvezzi, and A. Bicchi, “On motion and force
controllability of grasping hands with postural synergies,” in Robotics:
Science and Systems VI, pp. 49–56, Zaragoza, Spain: The MIT Press,
June 2011.
[22] D. Prattichizzo, M. Malvezzi, M. Gabiccini, and A. Bicchi, “On mo-
tion and force controllability of precision grasps with hands actuated
by soft synergies,” IEEE Transactions on Robotics, vol. in press, pp. 1–
17, 2013.
[23] A. T. Miller, S. Knoop, H. I. Christensen, and P. K. Allen, “Automatic
grasp planning using shape primitives,” in Robotics and Automa-
tion, 2003. Proceedings. ICRA’03. IEEE International Conference on,
vol. 2, pp. 1824–1829, IEEE, 2003.
5357
