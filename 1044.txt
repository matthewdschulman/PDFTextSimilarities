a
Probabilistic Stereo Egomotion Transform
Hugo Silva, Eduardo Silva
INESC TEC (formerly INESC Porto)
ISEP/IPP - School of Engineering, Polytechnic Institute of Porto
Email:hugo.m.silva,eduardo.silva@inescporto.pt
Alexandre Bernardino
Institute for Systems and Robotics
IST Lisbon, Portugal
Email: alex@isr.ist.utl.pt
Abstract—In this paper we propose a novel fully probabilistic
solution to the stereo egomotion estimation problem. We extend
the notion of probabilistic correspondence to the stereo case
which allow us to compute the whole 6D motion information in
a probabilistic way. We compare the developed approach against
other known state-of-the-art methods for stereo egomotion esti-
mation, and the obtained results compare favorably both for the
linear and angular velocities estimation.
I. INTRODUCTION
The use of mobile robots on modern world tasks is increas-
ing rapidly as well as their application scenarios. One of the
robots most complex tasks is navigation where typically GPS-
IMU sensor information is used. Some of these scenarios (e.g
urban areas, underwater GPS denied environments) are prone
to GPS-IMU failures, making it necessary to use other alterna-
tive or complementary sensors such as vision cameras for the
robot to perceive and navigate through out the environment.
When using visual sensors (cameras), robots must determine
motion measuring their displacement relative to static key
points in the environment, process which is usually denoted as
Visual Odometry (VO). The use of VO methods for obtaining
robot motion has been continuously subject of research by the
robotics and automotive industry over the past years. One way
of performing VO estimation is by determining instantaneous
camera displacement on consecutive frames, a process denoted
as visual egomotion estimation, and integrating over time
the obtained rotational and translational velocities. Monocular
egomotion estimation is subject to translation scale ambiguity,
i.e. in the absence of other sources of information, only
the translational velocity direction can be reliably measured.
Therefore, whenever possible, two cameras are used to have a
full velocity estimation, usually denoted as stereo egomotion
estimation e.g ( [1], [2], [3], [4]).
Most approaches to the stereo egomotion estimation prob-
lem rely on non-probabilistic correspondences methods. Com-
mon approaches try to detect, match and track key points
between images on adjacent time frames, and afterwards
use the largest subset of point correspondences that yield a
consistent motion. In probabilistic correspondence methods
matches are not fully committed during the initial phases of
the algorithm and multiple matching hypotheses are accounted
for. Our previous work in egomotion estimation (6DP) [1]
[2], has shown that the probabilistic correspondence methods
were a viable way to estimate egomotion with advantages in
precision over classical feature based methods. Nevertheless
6DP method was unable to estimate the translation scale factor
based only on probabilistic approaches, and required a mixed
approach to be able to recover all motion parameters.
In this paper, we developed a novel probabilistic stereo
egomotion method capable of computing 6-DOF motion pa-
rameters solely based on probabilistic correspondence ap-
proaches, and without the need to track or commit key point
matches between consecutive frames. The use of probabilistic
correspondence methods allows to maintain several match
hypotheses for each point, which is an advantage when there
are ambiguous matches (which is the rule in image feature
correspondences problems), because no commitment is made
before analyzing all image information. Another advantage is
that a full probabilistic distribution of motion provides a better
sensor fusion with other sensors, e.g. inertial.
Our proposed approach improves the work conducted in
[1], [2] and propose a fully probabilistic algorithm to perform
stereo egomotion estimation, which we denote as Probabilistic
Stereo Egomotion Transform (PSET). While in 6DP [1], a
mixed probabilistic and deterministic approach was used to
estimate rotation and translation parameters, PSET only em-
ploys probabilistic correspondences. The rotation estimation
is achieved the same way as in 6DP (with a 5D search over
the motion space based on the notion of epipolar constraint),
yet the translation scale factor is obtained by exploiting an
accumulator array voting scheme. The obtained results demon-
strate a clear performance improvement in the estimation of
the linear and angular velocities over current state-of-the-art
stereo egomotion estimation methods, when compared to iner-
tial measurement unit ground-truth information. Furthermore,
since real-time is a concern in today modern mobile robotics
applications the algorithm can be easily implemented using a
multi-core system.
This paper is organized as follows: In section II some related
work is presented. In section III, we discuss the monocular
approach to probabilistic egomotion estimation. Afterwards in
section IV we extend this approach to the stereo egomotion
estimation case. In section V results of the PSET Transform
compared with other state of the art stereo egomotion estima-
tion methods are presented. Finally, section VI contains the
conclusions with ﬁnal remarks and future work.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5461
II. RELATED WORK
One way to perform stereo VO is to estimate the instanta-
neous velocity (egomotion) of the vehicle or robot from a
sequence of images acquired using a stereo camera setup,
and integrate the velocity measurements in time. The main
advantage of a stereo camera conﬁguration setup is the ability
to easily recover the translation motion scale. Most of the
known stereo VO methods use the 3D position of observed
image key points that are computed by triangulating their
relative position between stereo images. Afterwards, camera
motion can be computed based on the alignment of the same
3D key points between adjacent time steps. The research work
on stereo VO estimation started with Olson et al [5], when
visual sensors became an alternative to the dead reckoning
estimation (e.g wheel odometry) that were unable to estimate
robot motion over long distances, and accumulated large errors
specially due to wheel slip in uneven terrain. However, the
main boost in the development of stereo VO methods was
given by Cheng et al [6] with the Mars Rover Project. The
proposed method estimated all 6-DOF motion parameters by
tracking image key points between left and right image pairs
in consecutive time frames, and determined the change of
position and attitude using maximum likelihood estimation.
Currently stereo VO methods are classiﬁed according to
the employed feature detection scheme or by the way mo-
tion estimation is performed [7]. In Alismail et al [8], a
study for evaluating Absolute Orientation (AO) methods and
Perspective-n-Point methods (PnP) for achieving robot pose
estimation using only stereo visual odometry is conducted.
Their work concluded that PnP methods are more accurate
than AO methods, AO methods consist on triangulating 3D key
points for every stereo pair. Then motion estimation is solved
using key point alignment algorithms such as the Procrustes
method [9], used in [1] or Iterative-Closest-Point (ICP) method
[10], used in Milella and Siegwart [11] for estimating motion
of an all-terrain rover. Nister et al [12], developed the ﬁrst
PnP algorithm (3D-2D camera pose estimation), computed in
real-time with an outlier rejection scheme, that minimized the
re-projection error. In what concerns image information, stereo
VO methods may use sparse or dense approaches. One of
the most relevant dense stereo VO applications was developed
by Howard [13] for ground vehicle applications. The method
makes no assumption of prior knowledge over camera motion,
and so it can handle very large image translations. However,
due to the absence of feature detectors invariant to rotation
and scaling, it only works on low-speed applications with
high frame-rate. Some authors like Ni et al [14], minimize
dependencies on feature matching and tracking algorithms
by simultaneously using an algorithm that computes feature
displacement in both cameras, together with a quadrifocal
setting within a RANSAC [15] framework.
More recently the application focus of stereo VO methods
has moved from planetary rover application to the development
of novel intelligent vehicles by the automotive industry, as the
one developed by Kitt et al [3]. Their method is available
as an open-source visual odometry library named LIBVISO.
Stereo egomotion estimation is based on image triples and the
online estimation of the trifocal tensor [16]. It uses rectiﬁed
stereo image sequences and produces an output 6D vector with
estimated linear and angular velocities. Comport et al [4] also
developed a stereo VO method based on the quadrifocal tensor.
By using tensor notation, the authors can compute motion us-
ing 2D-2D image pixels matches, thus yielding a more precise
motion estimation. Another way of developing stereo VO is to
combine with other absolute sensor information. Rehder et al
[17] developed a stereo visual odometry method that combined
visual data with GPS and IMU information. The proposed
method consistently fused stereo visual odometry information
with inertial measurements and sparse GPS information into
a single pose estimate in real-time. Kneip et al [18] also
proposed an alternative tightly coupled approach with vision
and IMU information. Their strategy for continuous robust
pose computation is based on the triangulation of frame to
frame point clouds when there is sufﬁcient disparity among
them.
In our previous work [1], [2] a probabilistic correspondence
method developed by [19] was used to compute the stereo ego-
motion estimation. The results obtained show an improvement
in the angular velocities estimation, but not in the linear veloc-
ities estimation where scale was obtained using deterministic
matches instead of probabilistic correspondences. In this paper,
we will extend the probabilistic correspondence approach to
the estimation of the linear velocities.
III. PROBABILISTIC EGOMOTION ESTIMATION
The seminal work of Domke et al [19] has introduced the
notion of probabilistic correspondence in the context of the
egomotion estimation problem. In this section we provide a
brief description of their method and introduce the notation
required for the remaining sections.
Given two images taken at different times, I
k
and I
k+1
, the
probabilistic correspondence between point s2R
2
in image I
k
and point q2R
2
in image I
k+1
, is deﬁned as a belief:
r
s
(q)= match(s;q)
where the function match() outputs a value between 0 and 1
expressing similarity in the appearance of the two points in
local neighborhoods. In [19] that function was implemented
by the correlation of a Gabor Filter bank response in the two
points. In our work we use the zero-mean normalized cross-
correlation function (ZNCC) to measure point similarity:
ZNCC
(s;q)
=
å
d2W
[I
k
(s+d) 
¯
I
k
][I
k+1
(q+d) 
¯
I
k+1
]
q
å
d2W
[I
k
(s+d) 
¯
I
k
]
2
q
å
d2W
[I
k+1
(q+d) 
¯
I
k+1
]
2
whereW denotes a 2D window centered at the origin whose
size deﬁnes the neighborhood of analysis around points s
and q. In practice we use a fast recursive implementation of
the ZNCC developed by Huang et al [20]. The probabilistic
correspondence is then computed as:
r
s
(q)=
ZNCC(s;q)+ 1
2
5462
so that its values range from 0 to 1.
Afterwards, motion hypotheses deﬁned by the incremen-
tal rotation matrix R and the translation velocity direction
ˆ t = t=ktk are checked by analyzing the probabilistic corre-
spondences r
s
(q) along the epipolar lines derived from R and
ˆ t [19]. A correspondenceq for points must satisfy the epipolar
constraint denoted by:
˜ s
T
E ˜ q= 0 (1)
where ˜ s and ˜ q are the homogeneous representations of s and
q, respectively. Matrix E is the so called Essential Matrix
[16], a 3 3 matrix of rank 2 and 5 degrees of freedom
that encodes the rigid camera motion. In order to obtain the
Essential Matrix (E) from the probabilistic correspondences,
[19] proposes the computation of a probability distribution
over the 5-dimensional space of essential matrices. Each
dimension of the space is discretized in 10 bins, thus leading
to 100000 hypotheses E
i
. For each point s the likelihood of
these hypotheses is evaluated by:
r(E
i
js)µ max
(˜ q)
T
E
i
˜ s=0
r
s
(q) (2)
Therefore for a single point s in image I
L
k
, the likelihood of
a motion hypothesis (E
i
) is proportional to the likelihood of
the best match obtained along the epipolar line generated by
the essential matrix. If one assumes statistical independence
between the measurements obtained at each point, the overall
likelihood of a motion hypothesis is proportional to the product
of the likelihoods for all points:
r(E
i
)µ
Õ
s
r(E
i
js) (3)
Finally, given the top ranked motion hypotheses, a Nelder-
Mead simplex method [21] is used to reﬁne the motion
estimate, and obtain the best motion, up to a scale factor, that
represents the image movement between the two frames.
IV. PROBABILISTIC STEREO EGOMOTION ESTIMATION
In this work we extend the notion of probabilistic correspon-
dence to the stereo case which allow us to compute the whole
6D motion information in a probabilistic way. In a stereo setup
we consider images I
L
k
, I
L
k+1
, I
R
k
and I
R
k+1
, where superscripts
L and R denote respectively the left and right images of the
stereo pair. Probabilistic matches of a point s in I
L
k
are now
computed not only for points q in I
L
k+1
but also for points r
in I
R
k
and p in I
R
k+1
:
r
s
(r)=
ZNCC(s;r)+ 1
2
r
s
(p)=
ZNCC(s;p)+ 1
2
For the sake of computational efﬁciency, analysis can be
limited to sub-regions of the images given prior knowledge
about the geometry of the stereo system or the motion given
by other sensors like IMU’s. In particular, for each point s,
coordinates r can be limited to a band around the epipolar
lines according to the stereo setup epipolar geometry.
Fig. 1: Stereo Egomotion Geometry
A. The Geometry of Stereo Egomotion
In this section we describe the geometry of the stereo
egomotion problem, i.e. will analyze how world points project
in the four images acquired from the stereo setup in two
consecutive instants of time according to its motion. This
analysis is required to derive the expressions to compute the
translational scale factor.
Let us consider the 44 rototranslations T
R
L
and M
k+1
k
that
describe, respectively, the rigid transformation between the left
and right cameras of the stereo setup, and the transformation
describing the motion of the left camera from time k to k+1:
T
R
L
=

R
R
L
t
R
L
0 1

M
k+1
k
=

R
k+1
k
t
k+1
k
0 1

where R


and t


denote the rotational and translational compo-
nents. We factorize the translational motiont
k+1
k
in its direction
ˆ t and amplitude a:
t
k+1
k
=a ˆ t
Given that rotational motion and translation direction are
computed by the method described in the previous section,
the computation of a is the objective to pursue.
Let us consider an arbitrary 3D point X = (X
x
;X
y
;X
z
)
T
expressed in the reference frame of the left camera at time
k. Considering normalized intrinsic parameters (unit focal
distance f = 1, zero central point c
x
= c
y
= 0, no skew),
the homogeneous coordinates of the projection of X in the
4 images is given by:
8
>
>
>
<
>
>
>
:
˜ s=X
˜ r=R
R
L
X+t
R
L
˜ q=R
k+1
k
X+a ˆ t
˜ p=R
R
L
R
k+1
k
X+aR
R
L
ˆ t+t
R
L
(4)
To illustrate the solution, let us consider the particular case
of parallel stereo. This will allow us to obtain the form of the
5463
solution with simple equations but does not compromise gen-
erality because the procedure to obtain the solution in the non
parallel case is analogous. In parallel stereo the cameras are
displaced laterally with no rotation. The rotation component is
the 3 3 identity (R
R
L
=I
33
) and the translation vector is an
offset (baselineb) along thex coordinate,t
R
L
=(b;0;0)
T
. In this
case, expanding the equations for s=(s
x
;s
y
)
T
and r=(r
x
;r
y
)
T
we obtain:
8
>
<
>
:
s
x
=
X
x
X
z
s
y
=r
y
=
X
y
X
z
r
x
=
(X
x
+b)
X
z
Introducing the disparity d as d=r
x
 s
x
we have d=
b
X
z
and
we can reconstruct the 3D coordinates of point X as a function
of the image coordinates r and s and the known baseline value
b:
X =(
s
x
b
d
s
y
b
d
b
d
)
T
Replacing this value now in (4) we obtain:
r=
"
(
sxb
d
+b)d
b
s
y
#
(5)
q=
2
6
4
r11s
x
b+r12s
y
b+r13b+at
x
d
r31s
x
b+r32s
y
b+r33b+at
z
d
r21s
x
b+r22s
y
b+r23b+at
y
d
r31s
x
b+r32s
y
b+r33b+at
z
d
3
7
5 (6)
p=
2
6
4
r11s
x
b+r12s
y
b+r13b+at
x
d+bd
r31s
x
b+r32s
y
b+r33b+at
z
d
r21s
x
b+r22s
y
b+r23b+at
y
d
r31s
x
b+r32s
y
b+r33b+at
z
d
3
7
5 (7)
We determine the translation scale factor a using (6) by:
q
x
=
A
z }| {
r11s
x
b+r12s
y
b+r13b+at
x
d
r31s
x
b+r32s
y
b+r33b
| {z }
C
+at
z
d
(8)
being a given by:
a =
A q
x
C
q
x
t
z
d t
x
d
(9)
The same procedure is applied to q
y
:
q
y
=
B
z }| {
r21s
x
b+r22s
y
b+r23b+at
y
d
r31s
x
b+r32s
y
b+r33b
| {z }
C
+at
z
d
(10)
being a given by:
a =
B q
y
C
q
y
t
z
d t
y
d
(11)
The translation scale factor a, can also be determined using
the same procedure applied to (7). Therefore, beinga an over-
determined parameter since there are four equations to one
unknown, we choose the a with the highest denominator to
minimize the effect of numerical errors. In case both denomi-
nators are low due to very low disparity or degenerate motions,
this particular point can not be used for the estimation.
B. Translational Scale Estimation
In the previous section we have seen that it is possible to
estimate the translational scale a from the observation of a
single static point s, if its point correspondences r, q and p
are known and there are no degeneracies. In practice, two
major problems arise: (i) it is hard to determine what are the
static points in the environment given that the cameras are
also moving; and (ii) it is very hard to obtain reliable matches
due to the noise and ambiguities present in natural images.
Therefore using a single point to perform this estimation is
doomed to failure. We must therefore use multiple points and
apply robust methodologies to discard outliers.
In our previous work [1], this was achieved by computing
the rigid transformation between point clouds obtained from
stereo reconstruction at times k and k+1 with a robust method
RANSAC [15]. Point correspondences were deterministically
assigned by searching for the best matches along epipolar
lines in space (from camera L to camera R) and time (from
time k to time k+ 1). In the current work, we extend the
probabilistic notion of correspondence to the stereo case.
Instead of deterministically committing to matches in space
and time, we create a probabilistic observation model for
possible matches:
P
match
(s;r;p;q)=r
s
(r)r
s
(q)r
s
(p)
where we assume statistical independence in the measurements
obtained in the pairwise probabilistic correspondence func-
tions r
s
(). Then, because each possible match (s;r;p;q) will
correspond to a value of a, we will create an accumulator
of a hypotheses, weighted by P
match
(s;r;p;q). Searching for
peaks in the accumulator will provide us the best (most agreed)
hypothesis for a given all the information in the images.
C. The Probabilistic Stereo Egomotion Transform
Here we detail how the method is implemented computa-
tionally. We assume E has been computed by the methods
described previously and the system calibration is known.
First a large set of points s
j
; j= 1J is selected. Selection
can be random, uniform or based on key points, e.g. Harris
corners [22] or Scale-Invariant features [23]. In our current
implementation SIFT features are used to detect salient points.
For each point s
j
, the epipolar lines E
calib
= ˜ s
T
j
S and
E
sq
= ˜ s
T
j
E are sampled at points r
l
and q
m
, in images I
R
k
and I
L
k+1
, respectively. Again sample point selection can be
random along the epipolar lines or based on match quality.
In our implementation we compute local maxima of match
quality over the epipolar lines. At this point we create tables
that associate to each triplet (s
j
;r
l
;q
m
) a disparity value d
jl
and a scale value a
jlm
, determined by either (9) or (11). Given
this information the value of p becomes uniquely determined
5464
by (7) and is stored as p
jlm
. The likelihood of this triplet is
then computed by:
l
jlm
=r
s
j
(r
l
)r
s
j
(q
m
)r
s
j
(p
jlm
)
Finally, all computed a
jlm
values and associated weights
l
jlm
are ﬁt using a robust (weighted) kernel density estimation
with a gaussian smoother [24]. The ﬁnal estimate of a is
computed as the value of the sample a that has the highest
smoothed probability density estimate.
D. Dealing with calibration errors
A common source of errors in a stereo setup is uncertainty
in the calibration parameters. Both intrinsic and extrinsic
parameter errors will deviate the epipolar lines from their
nominal values and inﬂuence the computed correspondence
probability values. To minimize these effects we modify the
correspondence probability function when evaluating sample
points such that a neighborhood of the point is analyzed and
not only the exact same coordinate:
r
0
s
(q)=max
q
0
2N (q)

r
s
(q
0
)exp
(q q
0
)
2
2s
2

whereN (q) denotes a neighborhood of the sample point q
which, in our experiments, is a 7 7 window.
E. EKF Linear and Angular velocities estimation
Having determined the translation scale factor(a), and the
results of the rotation (R) and translation (ˆ t), is possible to
compute the linear and angular velocities (V;W) between T
k
and T
k+1
. In order to achieve a more robust estimation and,
therefore, disregard erroneous instantaneous measurements, an
Extended Kalman ﬁlter, with a constant velocity model was
used to integrate the velocity estimates, for more detail see
[2].
V. RESULTS
A. Experimental Setup
In order to evaluate the PSET algorithm, we utilized one
of the sequences of the Karlsruhe dataset [3], and compared
performance against LIBVISO, and with our previous imple-
mentation of the 6DP [1]. We take the Inertial Measurement
Unit (RTK-GPS information) as ground-truth information.
1) Computational requirements: The code used to compute
the PSET transform was written in MATLAB as a proof
of concept, without using any kind of optimization. The
experiments conducted to compute the PSET transform were
performed using an Intel I5 Dual Core 3.2 GHz. The dataset
images have resolution of 1344 391, which consumes a
considerable amount of computational and memory resources
making unfeasible the computation of all image points using
standard CPU hardware. The PSET transform results were
obtained using only 1000 points to estimate the motion. It
computes at around 20 sec per image pair. Most of time is
consumed in the ﬁrst stage of the implementation, with the
dense probabilistic correspondences and the computation of
the motion up to a scale factor. Even so, the approach is
0 50 100 150 200 250 300
?2
?1
0
1
2
Angular Velocity Wx  
Frame Index
Wx degree/s
 
 
IMU
PSET?EKF
LIBVISO
0 50 100 150 200 250 300
?1
?0.5
0
0.5
Angular Velocity Wy  
Frame Index
Wy degree/s
0 50 100 150 200 250 300
?2
?1
0
1
2
Angular Velocity Wz 
Frame Index
Wz degree/s
Fig. 2: Results for the angular velocities estimation of 300
frames: ground truth (GPS-IMU information), ﬁltered PSET
measurements (PST-EKF) and 6D Visual Odometry Library
(LIBVISO). Even though all exhibit similar behaviors the
ﬁltered implementation PSET-EKF is the one which is closer
to GT (GPS-IMU) (see also table 1).
feasible and can be implemented in real-time for use on mobile
robotics applications. The main option is to develop a GPGPU
version of the PSET transform implementation since the
method copes with multiple hypothesis of correspondences,
as well as generated motion hypothesis, making it suitable to
be implemented into parallel hardware.
B. Stereo Egomotion results
In this section stereo egomotion estimation results are
presented. We compared the PSET estimation results with
other state-of-the-art estimation results namely 6DP [1] and
LIBVISO [3]. In Fig. 2, PSET and LIBVISO estimation of the
angular velocities are presented together with IMU ground-
truth information. The 6DP results are not presented in the
linear and angular velocities ﬁgures, due to the fact that, for
angular velocities PSET and 6DP use the same method of
computation and thus obtain identical results. One can observe
that PSET is closer performance to IMU than LIBVISO. This
is quantitatively assessed in Table 1, where the RMS error for
the LIBVISO method is about twice the error of the PSET/6DP
method.
In Fig. 3, one can observe the behavior of both methods in
the linear velocity estimation case. Both LIBVISO and PSET
present similar results for the linear velocity estimation case,
but PSET has about 50 % less overall RMS error, as can
be checked in Table 1. The results conﬁrm that estimating
the translation scale using probabilistic approaches produces
better results than using deterministic correspondences.
VI. CONCLUSIONS AND FUTURE WORKS
The PSET methodology described in this paper has proven
to be an accurate method of computing stereo egomotion.
The proposed approach is very interesting because no explicit
matching or feature tracking is necessary to compute the
vehicle motion. To the best of our knowledge this is the
5465
TABLE I: Comparison of the standard mean squared error between IMU and stereo egomotion estimation methods (LIBVISO,
6DP, and PSET ).
V
x
V
y
V
z
W
x
W
y
W
z
jjVjj jjWjj
LIBVISO 0.0674 0.7353 0.3186 0.0127 0.0059 0.0117 1.1213 0.0303
6DP 0.0884 0.0748 0.7789 0.0049 0.0021 0.0056 0.9421 0.0126
PSET 0.0700 0.0703 0.3686 0.0034 0.0019 0.0055 0.5089 0.0108
0 50 100 150 200 250 300
?0.1
0
0.1
0.2
0.3
Linear Velocity Vx 
Frame Index
Vx m/s
 
 
IMU
PSET?EKF
LIBVISO
0 50 100 150 200 250 300
?0.6
?0.5
?0.4
?0.3
?0.2
Linear Velocity Vy 
Frame Index
Vy m/s
0 50 100 150 200 250 300
4
4.5
5
5.5
Linear Velocity Vz 
Frame Index
Vz m/s
Fig. 3: Estimated linear velocities 300 frames. The PSET
transform exhibits a better performance than LIBVISO (see
table 1).
ﬁrst implementation of a full dense probabilistic method to
compute stereo egomotion. The results demonstrate that PSET
is more accurate then other SOA 3D egomotion estimation
methods, improving the overall accuracy in about 50 % over
LIBVISO. In future work the main task will be to develop
a GPGPU implementation of the PSET transform and test
it on difﬁcult and image structure repetitive scenarios where
common feature based approaches are more prone to failures.
A fused implementation with inertial measurements will also
be developed, in order to use the inertial information as motion
prior reducing the number of motion hypotheses needed to ﬁnd
the most probable motion.
ACKNOWLEDGMENTS
This work is co-ﬁnanced by Project ”NORTE-07-0124-
FEDER-000060” by the North Portugal Regional Operational
Programme (ON.2 O Novo Norte), under the National
Strategic Reference Framework (NSRF) through the European
Regional Development Fund (ERDF), by project SEAGULL
(SI IDT 34063), funded by the Portuguese government through
FEDER funds, and also by National Funds through the FCT
within project FCOMP-01-0124-FEDER-037281, and under
grant SFRH / BD / 47468 / 2008
REFERENCES
[1] H. Silva, A. Bernardino, and E. Silva, “Combining sparse and dense
methods for 6d visual odometry,” 13th IEEE International Conference
onAutonomousRobotSystemsandCompetitions,LisbonPortugal, April
2013.
[2] ——, “6d visual odometry with dense probabilistic egomotion estima-
tion,” 8th International Conference on Computer Vision Theory and
Applications, Barcelona Spain, February 2013.
[3] G. A. Kitt, B. and H. Lategahn, “Visual odometry based on stereo
image sequences with ransac-based outlier rejection scheme,” in IEEE
Intelligent Vehicles Symposium (IV). IEEE, 2010, pp. 486–492.
[4] A. Comport, E. Malis, and P. Rives, “Real-time quadrifocal visual
odometry,” The International Journal of Robotics Research, vol. 29, no.
2-3, pp. 245–266, Jan. 2010.
[5] C. Olson, L. Matthies, M. Schoppers, and M. Maimone, “Rover nav-
igation using stereo ego-motion,” Robotics and Autonomous Systems,
vol. 43, pp. 215–229, Feb. 2003.
[6] M. Maimone, L. Matthies, and Y . Cheng, “Visual Odometry on the Mars
Exploration Rovers,” inIEEEInternationalConferenceonSystems,Man
and Cybernetics. IEEE, 2005, pp. 903–910.
[7] F. F. Scaramuzza, D., “Visual odometry tutorial,” Robotics Automation
Magazine, IEEE, vol. 18, no. 4, pp. 80 –92, dec 2011.
[8] H. Alismail, B. Browning, and M. B. Dias, “Evaluating pose estimation
methods for stereo visual odometry on robots,” in In proceedings of
the 11th International Conference on Intelligent Autonomous Systems
(IAS-11), 2010.
[9] C. Goodall, “Procrustes Methods in the Statistical Analysis of Shape,”
Journal of the Royal Statistical Society. Series B (Methodological),
vol. 53, no. 2, pp. 285–339, 1991.
[10] S. Rusinkiewicz and M. Levoy, “Efﬁcient variants of the ICP algorithm,”
in Third International Conference on 3D Digital Imaging and Modeling
(3DIM), Jun. 2001.
[11] A. Milella and R. Siegwart, “Stereo-based ego-motion estimation using
pixel tracking and iterative closest point,” in in IEEE International
Conference on Computer Vision Systems, 2006, p. 21.
[12] D. Nist´ er, O. Naroditsky, and J. Bergen, “Visual odometry for ground
vehicle applications,” Journal of Field Robotics, vol. 23, no. 1, pp. 3–20,
2006.
[13] A. Howard, “Real-time stereo visual odometry for autonomous ground
vehicles,” in IEEE/RSJ International Conference on Intelligent Robots
and Systems, IROS 2008. Ieee, sep 2008, pp. 3946–3952.
[14] N. Kai and F. Dellaert, “Stereo tracking and three-point/one-point
algorithms - a robust approach,” in Visual Odometry, In Intl. Conf. on
Image Processing (ICIP, 2006, pp. 2777–2780.
[15] B. C. Fischler, M.A., “Random sample consensus a paradigm for model
ﬁtting with applications to image analysis and automated cartography,”
Communications of the ACM, vol. 24, no. 6, pp. 381–395, 1981.
[16] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision, 2nd ed. Cambridge University Press, ISBN: 0521540518, 2004.
[17] J. Rehder, K. Gupta, S. T. Nuske, and S. Singh, “Global pose estimation
with limited gps and long range visual odometry,” in IEEE Conference
on Robotics and Automation, May 2012.
[18] L. Kneip, M. Chli, and R. Siegwart, “Robust real-time visual odometry
with a single camera and an imu,” in Proc. of the British Machine Vision
Conference (BMVC), 2011.
[19] J. Domke and Y . Aloimonos, “A Probabilistic Notion of Correspondence
and the Epipolar Constraint,” in Third International Symposium on 3D
Data Processing, Visualization, and Transmission (3DPVT’06). IEEE,
Jun. 2006, pp. 41–48.
[20] J. Huang, T. Zhu, X. Pan, L. Qin, X. Peng, C. Xiong, and J. Fang, “A
high-efﬁciency digital image correlation method based on a fast recursive
scheme,” Measurement Science and Technology, vol. 21, no. 3, 2011.
[21] J. A. Nelder and R. Mead, “A simplex method for function minimiza-
tion,” The Computer Journal, vol. 7, no. 4, pp. 308–313, 1965.
[22] C. Harris and M. Stephens, “A combined corner and edge detection,” in
Proceedings of The Fourth Alvey Vision Conference, 1988, pp. 147–151.
[23] D. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision, vol. 60, no. 2, pp. 91–110,
2004.
[24] M. P. Wand and M. C. Jones, Kernel Smoothing (Chapman & Hall/CRC
Monographs on Statistics & Applied Probability), 1st ed. Chapman and
Hall/CRC, Dec. 1994.
5466
