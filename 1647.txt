A Vision Based Relative Navigation Framework for Formation Flight
Daniel B. Wilson, Ali H. G¨ oktoˇ gan and Salah Sukkarieh
Abstract— Unmanned aerial vehicle (UA V) formation ﬂight
can vastly increase operational range and persistence through
autonomous aerial refuelling or efﬁcient ﬂight on a wingman’s
wake vortices. Differencing individual UA V state estimates is not
sufﬁciently accurate for close formation operations and must
be augmented with vehicle-to-vehicle observations.
To this end, we propose a quaternion based unscented
Kalman ﬁlter to fuse information from each UA V sensor suite
with relative vision observations. The result is a vastly improved
relative state estimate that is resilient to brief vision dropouts
and degrades gracefully during extended dropouts. Simulated
formation ﬂight results validate the approach and provide a
numerical analysis of the algorithm performance. Ground based
experiments demonstrate the algorithm running in real-time on
a dual-UA V system. This represents a signiﬁcant step towards
an airborne implementation.
I. INTRODUCTION
Autonomous tight formation ﬂight of less than one
wingspan of separation [1] is a lucrative and worthy chal-
lenge because it enables a wide variety of applications, most
notably, reduced fuel consumption by utilisation of wake
vortices [1] and autonomous aerial refuelling [2]. These
unmanned aerial vehicle (UA V) applications can signiﬁcantly
increase the UA V’s operational range, endurance and persis-
tence without sacriﬁcing payload space for supplementary
fuel.
Successful, sustained tight formation ﬂight requires an
accurate and real-time relative state estimate. One method to
obtain this estimate is to subtract one vehicle’s GPS-based
state estimate from the other; the result will be known herein
as the raw relative estimate. This approach has merit during
high separation formation but the accuracy, particularly in
relative position, is in the order of meters which is not
sufﬁciently accurate for close proximity formation ﬂight. The
low accuracy is not only attributed to the individual sensor
accuracy, but also errors in measurement time synchronisa-
tion since absolute measurements are being differenced. This
problem also applies to more accurate DGPS systems. Highly
dynamic vehicles and sporadic communication dropouts fur-
ther amplify this effect.
To achieve the necessary accuracy, directly observed rela-
tive measurements must be used. Vision is a popular sensor
selection in the aerial domain due to its availability, compact
size and low weight. Vision techniques that have been
This work is supported by the Australian Research Council (ARC) Centre
of Excellence rogramme
D. B. Wilson, A. H. G¨ oktoˇ gan & S. Sukkarieh are with the Aus-
tralian Centre for Field Robotics, Faculty of Aerospace, Mechani-
cal and Mechatronic Engineering, The University of Sydney, NSW,
2006, Australia fd.wilson,a.goktogan,s.sukkariehg at
acfr.usyd.edu.au
applied in an aerial context include active visual contours
[3], silhouette based techniques [4] and feature extraction [2].
The downside to vision is the susceptibility to observation
dropouts as a result of occlusion, incorrect feature matching,
the target being outside the ﬁeld of view (FOV) and uncertain
lighting conditions. To negate these shortcomings and create
a resilient yet accurate relative state estimator, it is important
to incorporate constantly available, albeit absolute, informa-
tion from sensors such as inertial, magnetic, atmospheric and
GPS.
To this end, an unscented Kalman ﬁlter (UKF) [5] is pro-
posed to fuse vehicle-to-vehicle vision measurements with
information from GPS, inertial, magnetic and atmospheric
sensors, located on each UA V . A UKF has several advantages
over the traditional extended Kalman ﬁlter (EKF). It provides
at least second-order nonlinear approximation as opposed to
the ﬁrst-order EKF; often difﬁcult derivation of Jacobians is
not necessary; the ﬁlter is more robust to initial errors and
computation can occur in parallel. Resilience to initial error
is particularly important because of the large difference in
accuracy between the GPS and vision-based measurements.
A downside of the UKF is that a quaternion parametrisation
of the attitude results in a non-unit quaternion estimate when
the mean is computed. A brute force normalisation can be
made to work, but is undesirable. Instead, we use generalised
Rodrigues parameters (GRPs) to represent the attitude error.
In our scenario, a leader and follower UA V are ﬂying in
formation. Visual markers are mounted on the leader at each
wingtip, at the top of the tail ﬁn and on the right of the
tail plane as shown in Fig. 1. A forward facing camera is
mounted on the follower and provides relative measurements
to the leader’s visual markers.
The estimator was ﬁrst tested in a high ﬁdelity simu-
lated environment [6] where conditions are repeatable and
the ground truth is known. In this way, the algorithm
could be validated and analysed quantitatively over multi-
ple runs. These results exhibited compelling performance
improvements over both the raw relative estimate and the
benchmark vision-only pose estimation algorithm. Further,
improvements in the state of each individual vehicle were
observed. The algorithm was also implemented on a dual-
UA V system to further evaluate the estimator in real-time on
an embedded system. Results from these experiments show
good agreement with a vision-only relative pose algorithm
and have the added beneﬁt of being available when vision is
not where the estimate gracefully degrades to the raw relative
estimate. It should be noted that the vision information was
logged online but processed and fused ofﬂine.
The paper is organised as follows. Section II reviews
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4988
related research and orientates the contributions of the paper.
The estimation framework is described in Section III and
details the vision integration. Simulation and ground-based
results are presented and discussed in Section IV before
conclusions and suggestions for future work are offered in
Section V.
II. RELATED WORK
Vision-based navigation has gained traction recently, par-
ticularly in an airborne context, due to the onset of low cost
vision sensors and small, embedded processors capable of
real-time image processing. Previous work on multi-UA V
visual relative navigation can be categorised as vision-only,
loosely-coupled and tightly-coupled. Vision-only approaches
[7], [3], [8] estimate the relative pose using visual mea-
surements exclusively, typically by solving the PnP problem
[9], [10]. Although effective, a minimum of four points are
required for a unique solution and discrete switching to a
GPS-based estimate is required during visual outages. A
comparison to such an algorithm is provided in Section IV.
Loosely-coupled methods [11], [12] extend this approach by
fusing the vision-only relative pose with other onboard sen-
sors such as GPS and inertial from one or more aircraft. This
method avoids discrete switching when vision is unavailable,
however a minimum number of features is still required and
the observation covariance is difﬁcult to derive.
The algorithm being proposed in this paper can be cate-
gorised as tight-coupled, where the raw visual measurements
are fused directly. This adds resilience to visual outages,
avoids discrete mode switching and enables information
to be extracted from any number of features. Examples
of tightly-coupled estimation include work in [13] which
implemented a UKF to estimate the relative state, using
the follower state and visual azimuth, elevation and target
wingtip subtended angle. Fosbury and Crassidis [14] use
an EKF to fuse follower sensor measurements with target
marker visual measurements, however the target state was
assumed known. These works have neglected useful leader
and follower sensor information and have instead made
assumptions about the vehicle dynamics. A more robust
estimator that utilises all available information was proposed
in [2] where the indirect form of an EKF was used to
tightly fuse leader and follower sensor data with a marker-
based vision system. Our work extends this by utilising a
better performing unscented framework, incorporating mag-
netic and atmospheric measurements from both aircraft and
demonstrating the algorithm operating in real-time on a
payload constrained dual-UA V system.
III. MULTI-VEHICLE RELATIVE NAVIGATION
This section describes the complete multi-vehicle relative
navigation algorithm. First, a brief review of single UA V
INS/GPS estimation is provided for context, before being
extended to the dual-UA V relative navigation case. Then,
the integration of tightly-coupled visual measurements is
explained which includes the observation model, feature
Fig. 1: Leader-follower coordinate frames and the marker
based vision system.
correspondence determination and a method to quantify
visual observation validity.
A. UAV Navigation
An estimate of a vehicle’s state, x, is obtained by fusing
sensor measurements with complementary characteristics.
This is normally a recursive, two step process composed of
prediction and observation. Vehicle speciﬁc models can be
used in the prediction stage, however UA Vs often implement
standard inertial mechanisation equations [15]. The inputs to
the system, u, are the body frame three-axis acceleration
and angular rates, measured by an inertial measurement
unit (IMU). Measurements from GPS, magnetometers and
atmospheric sensors are used to correct the estimate in
the observation stage. In this work a UKF provides the
framework and each UA V implements its own instance.
x = [P V qa
B
!
B
]
T
(1)
u = [~ a
b
~ !
b
ib
]
T
(2)
The position, P = [XY Z]
T
and velocity, V =
[v
X
v
Y
v
Z
]
T
are expressed in the local tangential or naviga-
tion frame, deﬁned in north-east-down coordinates. The ori-
gin is the ground station position,P
0
in geodetic coordinates.
The quaternion, q = [q
0
q
1
q
2
q
3
]
T
, describes the attitude
of the vehicle and forms the rotation matrix, C
n
b
which
transforms from the body to navigation frame. The deﬁnition
of C
n
b
can be found in [15]. In subsequent sections, b will
be replaced with l or f to represent the leader and follower
body frames respectively. ~ a
b
and ~ !
b
ib
are the raw IMU
measurements and a
B
and !
B
are the IMU accelerometer
and gyro time varying bias’.
State propagation occurs using the inertial navigation
prediction model given in Eq. (3).
_ x =
2
6
6
6
6
4
_
P
_
V
_ q
_ !
B
_ a
B
3
7
7
7
7
5
=
2
6
6
6
6
4
V
C
n
b
~ a  (2!
n
ie
+!
n
en
)V +g
E
1
2
~


~ !
q
n
!
B
n
a
B
3
7
7
7
7
5
(3)
4989
g
E
is the Earth gravity vector in the navigation frame and
!
n
ie
is the rotation of Earth in the navigation frame, as deﬁned
in [15]. We assume that the navigation frame is ﬁxed and
thus !
n
en
is a zeros vector. a
B
and !
B
are modelled as a
random walk where n
a
B
and n
!
B
are zero-mean Gaussian
random variables. ~ a and ~ ! = [~ !
p
~ !
q
~ !
r
]
T
are the bias
and noise corrected IMU measurements. n
a
and n
!
are the
accelerometer and gyro measurement noise terms. The gyro
measurements are also corrected by the Earth’s rotation. A
full derivation of the INS mechanisation equations can be
found in [15].
State observation occurs sequentially to account for differ-
ences in sensor sample frequencies. h
gps
[x;k] is used when
GPS is available whereas h
no gps
[x;k] is implemented when
magnetometer and pressure observations are available.
h
gps
[x;k] =


P
gps

V
gps

=

P
 
+C
n
b
r
gps
+n
P gps
V
 
+C
n
b
~ !r
gps
+n
V gps

(4)
h
no gps
[x;k] =


h

 

=
"
h
0
 Z
 
+n
h
tan
 1

2(q0q
 
3
+q1q2)
1 2(q
2
2
+q
2
3
)

+n
 
#
(5)
Where r
gps
is the position of the GPS antenna relative to
the centre of gravity (CoG). n
P gps
, n
V gps
, n
h
and n
 
are the
measurement noise terms for the GPS position, GPS velocity,
pressure altitude and heading respectively.
Before the raw sensor data is used within the observation
models, some preprocessing is required. First, GPS geodetic
measurements are converted to the navigation frame using
the transformation in [15]. Then, heading,
~
 is calculated
from the observed magnetic vector,
~
H by ﬁrst de-rotating
through roll, and pitch, using Eq. (6) and then calculating
~
 using Eq. (7). A magnetic declination correction accounts
for the local difference in magnetic and true north.
~
M
x
=
~
H
x
cos +
~
H
y
sin sin +
~
H
z
cos sin
~
M
y
=
~
H
y
cos 
~
H
z
sin
(6)
~
 = atan2

 
~
M
y
;
~
M
x

+ 
dec
(7)
Altitude above mean sea level (MSL),
~
h, is now calculated
using the atmospheric pressure. First, the pressure at MSL,
p
MSL
, is estimated using Eq. (8) where p
0
is the initial
pressure andh
0
is the initial MSL height, as observed by the
GPS. Finally,
~
h is determined using Eq. (9) with L;T
0
;M
and R provided in Table I.
p
MSL
=p
0

1 
Lh
0
T
0

 
gM
RL
(8)
~
h =
T
0
L
 
1 

~ p
p
MSL

R;L
gM
!
(9)
Equations pertaining to the UKF prediction and update are
omitted for brevity, but can be found in [5], [13], [16]. The
downside to a quaternion attitude parametrisation in a UKF
Parameter Description Value
L temperature lapse rate 0.0065 K/m
T
0
MSL standard temperature 288.15 K
M molar mass of dry air 0.0289644 kg/mol
R universal gas constant 8.31447 J/(mol  K)
TABLE I: The International Standard Atmosphere constants.
is that a unit-norm cannot be guaranteed when computing the
mean. To overcome this, the method proposed in [16] was
implemented. Here, we replace q with a three-dimensional
GRP vector which represents attitude error and is initialised
to zero. This formulation has the added beneﬁt of reducing
the state dimensionality.
To extend this formulation to the dual-UA V case, the
individual vehicle state vectors are simply concatenated to
formx
ljf
. Equation (11) is then used to calculate the relative
position P
ljf
, relative velocity V
ljf
and relative quaternion
q
ljf
, where
 denotes quaternion multiplication. An alterna-
tive approach would be to estimate the relative state directly,
however simulated results have shown that this formulation
performs better. Further, simulated results in Section IV-
A show that when coupled with visual measurements, the
accuracy ofx
l
andx
f
is improved. The inertial bias states are
neglected for computational reasons; instead, each individual
UA V UKF estimates these onboard and corrects the sensor
measurements prior to being utilised in the relative UKF.
x
ljf
=

x
l
x
f

T
(10)
2
4
P
ljf
V
ljf
q
ljf
3
5
=
2
4
P
l
 P
f
V
l
 V
f
q
 1
f

q
l
3
5
(11)
State prediction occurs as per Eq. (3) and sensor correc-
tions occur sequentially using the models in Eqs. (4) and (5)
when new data is available.
B. Vision Integration
Relative pose estimation using vision sensors has been
well researched and many valid approaches exist. Our work
employs a feature based method where visual markers of a
known conﬁguration are mounted on a leader vehicle and
observed by a follower. Using the set of n correspondences
between the 3D marker positions,
l
j
and the 2D observations
~

j
, as well as the camera intrinsic parameters, the relative
pose can be calculated directly. This requires n  3 for
a solution and n  4 for a unique solution. A number of
algorithms are available to solve this PnP problem, including
POSIT [17] which is used as a benchmark in Section IV, the
Lu-Hager-Mjolsness algorithm [18] and an efﬁcient approach
called EPnP [19].
The downside to this vision only approach is that it fails
with incorrect point matching, occlusion or a target outside
the FOV . These brief or prolonged measurement dropouts
are highly undesirable, particularly during close proximity
4990
operations. Alternatively, one could fuse the pose estimate
from one of the aforementioned algorithms with the on-
board sensor data in a loosely-coupled arrangement, however
preliminary results with a ﬁxed measurement covariance
displayed inferior performance to the tightly-coupled equiv-
alent. One reason may be that the measurement covariance
is dynamic and a function of the relative pose, in addition to
the pixel noise. Deriving an expression for this is difﬁcult.
Instead, we propose a tightly-coupled approach which uses
n raw 2D marker observations,
~

j
= [u
j
v
j
]
T
;j = 1;:::;n.
In our case n = 4 and n  3 is required for observability
within the UKF [2], [14]. The expected observations


j
;j =
1;:::;m are calculated by ﬁrst transforming 
l
j
from the
leader’s body frame to the world frame, 
f
j
using Eq. (12).
In this case the world frame is the follower’s body frame.

f
j
=C
f
n
(C
n
l

l
j
+P
ljf
) (12)
Next, the vision sensor extrinsic parameters transform 
f
j
to the camera frame using Eq. (13). P
fjc
and C
c
f
are the
translation and rotation from the followers body frame to
the camera frame. C
c
f
includes both the camera mounting
orientation and the axes transformation.

c
j
=

C
c
f
P
fjc



f
j
1

(13)


j
is calculated usingK, the camera intrinsic matrix which
encapsulates the camera focal length, aspect ratio, principal
point and distortion. The ﬁnal vision measurement model is
provided in Eq. (15) and the correction occurs sequentially.



j
1

=K
2
4

c
xj
=
c
zj

c
yj
=
c
zj
1
3
5
(14)
h
vision
[x;k] =



1


2
:::


n

T
(15)
The vision-based observation model presented in this
section updates both the relative position and orientation by
assuming correct point matching. This is not always possible,
particularly when the target is far and the points are difﬁcult
to distinguish from one another. Rather than neglecting such
a measurement, a simpler observation model can be utilised
to extract P
ljf
information. Here, the vision observation
becomes the average or centroid of
~

j
;j = 1;:::;n as an
approximation for the target’s CoG and Eq. (12) is replaced
with Eq. (16) with m = 1. Although relative orientation
and range become unobservable, P
ljf
and V
ljf
accuracy is
improved.

f
j
=C
f
n
P
ljf
(16)
C. Point Matching
Beforey
vision
ljf
can be used, we need to match the observed
points
~

i
with the projected model points


j
, where the
mean of each set is
~


and



respectively. To do this,
unique marker characteristics could be used, which may
include colour, size, intensity and frequency. However, in
our application we have chosen to use homogeneous vi-
sual markers to simplify the MV task and instead use the
marker model to match the points. To do this, we use a
computationally efﬁcient, deterministic mutual nearest point
procedure [20]. Before this is implemented, we eliminate the
linear translation between the point sets by subtracting the
vector (



 
~


) from


j
. This eliminates errors in relative
azimuth, elevation as well as follower attitude and simpliﬁes
the matching process. The matrix  is then populated with
the pixel distances between
~

i
and


j
.
 =
2
6
4
d (


1
;
~

1
)  d (


1
;
~

n
)

.
.
. 
d (


m
;
~

1
)  d (


m
;
~

n
)
3
7
5 (17)
Whered(:; : ) is the linear pixel distance between points.

min
col
and 
min
row
are the minimum value of each column and
row of , respectively and 
index
col
contains the index of the
minimum value in each column.

min
col
=

min (d (


i
;
~

1
))  min (d (


i
;
~

m
))


min
row
=

min (d (


1
;
~

j
))  min (d (


n
;
~

j
))


index
col
=

index(
min
col1
)  index(
min
coln
)

(18)
For a point to be valid, it must satisfy Eq. (19), that is
to say a valid point in  must be the minimum of both
its column and row. A threshold of validity can also be
implemented to reject outliers and noise.

min
col
[i ] = 
min
col
[ 
index
col
[i ] ] (19)
D. Vision Rejection
Empirical results have shown that incorrect point matching
quickly leads to ﬁlter divergence, from which recovery is
difﬁcult. To protect against this, a method is proposed to
detect and reject incorrectly matched visual observations.
First, the observed relative orientation is calculated by
solving the PnP problem with the current point matching, us-
ing POSIT [17] for efﬁciency. The quaternion error,q
error
=
[q
0
%] between the predicted and observed quaternions is then
determined using Eq. (20) and the magnitude of rotation
error, 
error
is calculated using Eq. (21).
q
error
=q
 1
ljf

 q
posit
ljf
(20)

error
= 2 cos
 1
q
0
(21)
Observations where 
error
is above a threshold are re-
jected. By comparing the observed relative orientation with
the predicted relative orientation, we can reject observations
that suggest unlikely or impossible instantaneous changes in
orientation that disagree with the onboard inertial sensors.
This method exploits the fact that incorrect point matching
has a much greater effect on the relative orientation than
4991
0 50 100 150 200 250 300
?2
0
2
4
North error (m)
 
 
POSIT
Relative raw
Relative UKF
0 50 100 150 200 250 300
?4
?2
0
2
4
East error (m)
0 50 100 150 200 250 300
?0.4
?0.2
0
0.2
Time (s)
Down error (m)
(a) Relative position error
0 50 100 150 200 250 300
?0.5
0
0.5
North error (ms
?1
)
 
 
Relative raw
Relative UKF
0 50 100 150 200 250 300
?0.5
0
0.5
East error (ms
?1
)
0 50 100 150 200 250 300
?0.2
0
0.2
Time (s)
Down error (ms
?1
)
(b) Relative velocity error
0 50 100 150 200 250 300
?2
0
2
4
? error ( 
°
)
 
 
POSIT
Relative raw
Relative UKF
0 50 100 150 200 250 300
?2
0
2
? error ( 
°
)
0 50 100 150 200 250 300
?2
0
2
Time (s)
? error ( 
°
)
(c) Relative attitude error
Fig. 2: Raw relative, POSIT and relative UKF estimate accuracy at 10m separation. Vision dropouts are indicated by vertical
black lines.
relative position. Results from simulated ﬂights with random
point matching showed that a threshold of 20 times the mean

error
was conservative.
IV. IMPLEMENTATION
This section begins by presenting results from a high-
ﬁdelity multi-UA V 6DOF simulation for algorithm statistical
validation and performance evaluation. Simulation is useful
because it provides a repeatable environment where the
ground truth is known. Here, we see sizeable performance
increases in the vision based estimator when compared to
the raw estimate and even the vision only relative pose
estimation algorithm. Then, results from real-time, ground
based experiments are provided. These experiments were
designed to isolate the relative navigation problem, and
demonstrate the estimation framework functioning in real-
time on an embedded system.
A. Simulation Validation
In addition to facilitating algorithm validation and perfor-
mance testing, the Simulink based multi-UA V simulation was
designed to enable rapid algorithm design and deployment
and to facilitate collaboration between multiple parties.
Each UA V utilises 6DOF nonlinear ﬁxed wing equations
of motion to propagate the state at 100Hz, given the control
surface deﬂections, and external disturbances. The sensors
are modelled using environmental and atmospheric models
with white Gaussian noise, bias and cross coupling. The
GPS model also incorporates Gauss-Markov noise correla-
tion and transport delay. The actuators are modelled with
ﬁrst-order time lag, saturation limits and rate saturation
limits. Individual UA V state estimation is handled by a 16-
state quaternion UKF; a guidance module implements ’new
guidance’ [21] to follow a sequence of Dubins paths [22];
and cascaded, physics based PID controllers handle the low
level actuation. The guidance, navigation and control (GNC)
module operates at 100Hz and is conﬁgured to automatically
generate C code for use on the embedded system which is
detailed in the next section. This feature allows an error-
free and seamless transition from the high level graphical
Raw (1) POSIT (1) x
ljf
(1) Raw impr
P
ljf
(m)
North 2.069 ( 0.86) 0.353 ( 0.15) 0.205 ( 0.23) 90.1 %
East 1.901 ( 0.76) 1.017 ( 0.34) 0.427 ( 0.24) 77.5 %
Down 0.152 ( 0.11) 0.123 ( 0.03) 0.129 ( 0.07) 15.1 %
V
ljf
(m)
North 0.203 ( 0.09) - 0.216 ( 0.25) -6.54 %
East 0.227 ( 0.18) - 0.183 ( 0.18) 19.4 %
Down 0.068 ( 0.09) - 0.052 ( 0.05) 23.5 %
Q
ljf
(

)
 1.148 ( 0.81) 0.295 ( 0.01) 0.173 ( 0.24) 85.0 %
 1.095 ( 1.12) 0.660 ( 0.02) 0.208 ( 0.17) 81.0 %
 14.743 ( 8.90) 0.577 ( 0.01) 0.443 ( 0.85) 97.0 %
TABLE II: Raw relative, POSIT and relative UKF estimate
RMSE comparison from 100 simulations.
environment to actual implementation. Additional details of
this algorithm prototype and deployment workﬂow can be
found in [6].
The vision sensor was modelled with a resolution of
19201080 pixels and a FOV of 70

42

at 30 frames per
second. A wide FOV is desirable to allow all the visual
markers to be observable in a single frame during tight
formation ﬂight. The true pixel coordinates were calculated
using the true vehicle states and Eqs. (12) to (14). The
simulated measurements were then created by adding white
noise and placing the points in a random order. Points outside
the FOV were set to zero.
The simulated scenario consisted of a leader following a
700500m racetrack pattern with rounded corners of radius
100m. The aircraft had a commanded altitude of 100m
and commanded airspeed of 25m/s. A follower UA V was
commanded to maintain equivalent airspeed and altitude to
the leader at a position 10m directly behind the leader.
This was done by implementing the relative state estimator
using onboard and remote real-time sensor information as
well as a forward pointing camera. A simple pursuit based
guidance strategy kept the leader within the follower’s FOV
4992
sufﬁciently well, however the actual separation ﬂuctuated
between 10m-15m.
Results from 100 simulated missions are summarised
numerically in Table II. Here the accuracy of the vision-based
relative estimator is compared with the raw estimate and a
benchmark vision-only pose estimation algorithm known as
POSIT [17]. Although quaternions have been estimated to
avoid gimbal lock, attitude results are displayed as Euler
angles for convenience. The relative estimator shows signif-
icant performance improvements over the raw estimate, par-
ticularly in horizontal position and . This can be attributed
to the relative inaccuracy of the GPS and magnetometers.
As expected, the gains over POSIT are less but remain
notable which is likely due to a smoothing effect of the
vehicle inertial measurements. Improvements in the vertical
position accuracy are small because the atmospheric pressure
sensor provides accurate measurements. V
ljf
improvement
are negligible because the GPS velocity measurements are
already fairly accurate. A comparison of estimate error from
one simulated run is displayed in Figs 2a, 2b and 2c.
Here, we see the low precision of POSIT, particularly when
observing range. As an aside, a performance comparison
with an EKF implementation exhibited minor performance
improvement with a favourable initial estimate and larger
performance gains when the initial estimate was subject to
error. These ﬁndings are consistent with [16] and [23].
During the simulated ﬂight, occasional visual dropouts
occurred. These are shown in Figs. 2a and 2c by the vertical
lines which indicate the beginning and end of the dropout.
These vision failures can be attributed to either the markers
being outside the followers FOV or the algorithm outlined
in Section III-D rejecting the point matching. The duration
of these dropouts ranges from less than a second to ﬁve
seconds. During this time, POSIT completely failed, whereas
the relative UKF maintained its accuracy. To further test the
algorithm’s resilience, the ﬁlter was subject to a sustained
dropout. The resulting effect on P
ljf
accuracy is displayed
in Fig. 3 where vision is unavailable from 50 seconds
onwards. A graceful degradation to the raw relative estimate
is observed over 10-15 seconds for the horizontal position
components and a few seconds for the vertical component.
This difference in degradation time is a result of the higher
accuracy and more frequent atmospheric pressure sensor
updates, compared to the GPS. At this point, the relative
ﬁlter approximates the raw relative estimate.
Simulated results also show that the vision integration can
also improve individual UA V state estimates. Table III shows
consistent improvement in horizontal position accuracy of
around 30%. Although the accuracy appears to decrease in
vertical position, the change is within 1 bounds. Further
analysis showed that P
ljf
accuracy only shows marginal
degradation when less than four marker observations were
available. A single observation still provides useful informa-
tion. Conversely, Q
ljf
which quickly becomes unobservable
with less than three marker observations.
50 55 60 65 70
?4
?2
0
2
North (m)
 
 
True
Relative raw
Relative UKF
50 55 60 65 70
9
10
11
12
East (m)
50 55 60 65 70
?0.5
0
0.5
Time (s)
Down (m)
Fig. 3: Graceful degradation of the relative position when
vision data is unavailable from 50 seconds.
Leader x
l
Impr. Follower x
f
Impr.
P
ljf
(m)
North 1.388 1.022 26.4 % 1.455 1.002 31.2 %
East 1.319 0.917 30.5 % 1.252 0.918 26.7 %
Down 0.153 0.189 -23.2 % 0.077 0.134 -73.2 %
Q
ljf
(

)
 0.805 0.486 39.6 % 0.504 0.459 8.91 %
 0.978 0.726 25.8 % 0.682 0.694 -1.82 %
 13.834 2.577 81.4 % 5.641 2.512 55.5 %
TABLE III: Comparison of individual UA V state estimates
with the output of the relative estimator.
B. Ground-based Experiments
Ground based experiments were conducted on a dual-UA V
system to isolate the relative navigation aspect of the problem
and demonstrate the relative estimator operating in real-
time on an embedded system. This system consists of two
ﬁxed-wing UA Vs, an autopilot and formation ﬂight computer
(FFC) onboard each aircraft, LED markers on the leader, and
a camera on the follower. A overview of this system can be
found in Figs. 4a and 4b. The custom designed autopilot
utilises an ARM Cortex-M4 microcontroller, running the
ChibiOS/RT [24] hard real-time operating system. Making
use of the ChibiOS/RT hardware abstraction layer, each au-
topilot receives data from accelerometers, gyros, magnetome-
ters, GPS, barometer and a differential pressure sensors. This
data is utilised within the GNC module, as outlined in the
preceding section, to generate appropriate control deﬂections.
A SD card allows autopilot conﬁguration ﬁles to be read
and ﬂight-data to be logged during operation. A 900MHz
wireless link to the ground control station facilitates trans-
mission of telemetry and upload of real-time conﬁguration. A
custom ground station UI displays all ﬂight data, facilitates
data logging, allows onboard parameters to be conﬁgured
4993
(a) Leader UA V with LED markers visible, autopilot and the FFC (b) System architecture
Fig. 4: The dual-UA V system used in the ground experiments.
in-ﬂight and provides a Google Earth visualisation. Onboard
switching circuitry allows a ground based security pilot to
take control if necessary.
The FFC is built upon a 1GHz ARM Cortex-A8 mi-
crocontroller running Linux. The role of the FFC is to
estimate the relative state between the aircraft and to guide
the follower to rendezvous with the leader. This is done by
exchanging sensor data with the remote FFC via 900MHz
wireless link; receiving sensor data from the local autopilot
via serial communications and receiving visual information
from a camera module. This information is then fused
using the 20-state relative estimation framework outlined in
Section III before appropriate low level commands are sent
to the local autopilot. The relative navigation estimation and
control algorithms run at 100Hz and are again auto-generated
from the simulation environment. All communication, both
onboard and airborne, is handled by the open source Mavlink
protocol [25].
Since the core hardware and ﬁrmware is identical onboard
each UA V , either vehicle can act as the leader or follower and
its onboard estimate is simply the state of the remote aircraft,
relative to itself. The homogeneous nature of the aircraft also
means that adding a rearward facing camera to the leader is
trivial. It should be noted that the complete system has been
validated in autonomous ﬂight tests, albeit not in formation
and without online image processing.
The vision system consists of a camera mounted at the
front of the follower and four high power LED markers
positioned at the extremities of the leader as per Fig. 4a.
Timestamped 1080p vision data was logged in real-time
onboard the follower FFC. Marker positions were extracted
and fused ofﬂine using a circular Hough transform and HSV
thresholds. An intrinsic camera calibration was carried out
using the Matlab Camera Calibration Toolbox [26]. Work is
currently being undertaken to transition the marker extraction
work ﬂow to an embedded module, ready for an airborne
implementation.
When powered on, the aircraft automatically synchronised
the clocks onboard all four processors and then agreed on
the ground station position. Once complete, data exchange
was initiated and the relative estimator was run on each
FFC. Relative position and attitude estimates from one such
experiment are shown in Figs. 5a and 5b. Here, we can see
good agreement between the vision-only POSIT algorithm
and the output of the relative estimator. A slight bias can
be observed in the east and components which indicate a
slight error in the camera extrinsic calibration. Additionally,
we see that POSIT fails between 23-29 seconds because less
than four points are available. Here, the relative estimator is
able to utilise information from even a single visual marker
and only slowly degrades to the raw relative estimate when
no visual measurements are available.
V. CONCLUSION
This paper has presented a vision-based unscented ﬁlter for
UA V formation ﬂight. To enable quaternions to be used with
the unscented framework, the attitude error, parametrised by
GRPs was estimated rather than the quaternion directly. Sim-
ulation results validated the approach and exhibited a signiﬁ-
cant relative state accuracy over the raw relative estimate and
a vision-only pose estimation algorithm. Improvements in the
individual UA V state estimate were also observed. Ground
based experiments with a dual-UA V system demonstrated
the algorithm operating in real-time on an embedded system
and also showed resilience to short-term vision outages and
graceful degradation over extended periods. Future work will
4994
0 10 20 30 40 50 60
?6
?4
?2
0
2
4
North (m)
 
 
POSIT
Raw
UKF
0 10 20 30 40 50 60
0
2
4
East (m)
0 10 20 30 40 50 60
?2
?1
0
Down (m)
Time (s)
(a) Relative position
0 10 20 30 40 50 60
?20
0
20
40
Phi ( 
°
 )
 
 
POSIT
Raw
UKF
0 10 20 30 40 50 60
?20
0
20
40
Theta ( 
°
 )
0 10 20 30 40 50 60
?20
0
20
40
Psi ( 
°
 )
Time (s)
(b) Relative attitude
Fig. 5: Vision based relative estimation results from the dual-UA V system.
focus on transitioning the vision processing algorithm to an
embedded system and implementing appropriate formation
guidance to enable an airborne implementation.
REFERENCES
[1] R. J. Sattigeri, “Adaptive estimation and control with application to
vision-based autonomous formation ﬂight,” Ph.D. dissertation, 2007.
[2] W. R. Williamson, G. J. Glenn, V . T. Dang, J. L. Speyer, S. M.
Stecko, and J. M. Takacs, “Sensor fusion applied to autonomous aerial
refueling,”JournalofGuidance,Control,andDynamics, vol. 32, no. 1,
pp. 262–275, 2009.
[3] J. Doebbler, J. Valasek, M. J. Monda, and H. S., “Boom and receptacle
autonomous air refueling using a visual pressure snake optical sensor,”
in AIAA Atmospheric Flight Mechanics Conference and Exhibit.
American Institute of Aeronautics and Astronautics, 2006.
[4] S. M. Khansari-Zadeh and F. Saghaﬁ, “Vision-based navigation
in autonomous close proximity operations using neural networks,”
Aerospace and Electronic Systems, IEEE Transactions on, vol. 47,
no. 2, pp. 864–883, 2011.
[5] S. J. Julier, J. K. Uhlmann, and H. F. Durrant-Whyte, “A new approach
for ﬁltering nonlinear systems,” in American Control Conference,
Proceedings of the 1995, vol. 3, 2012, pp. 1628–1632 vol.3.
[6] D. B. Wilson, A. H. Goktogan, and S. Sukkarieh, “UA V rendezvous:
From concept to ﬂight test,” in Australasian Conference on Robotics
and Automation (ACRA), 2012.
[7] Z. Mahboubi, Z. Kolter, T. Wang, G. Bower, and A. Y . Ng, “Camera
based localization for autonomous uav formation ﬂight,” in Proceed-
ings of the AIAA@ Infotech Conference.
[8] J. Valasek, K. Gunnam, J. Kimmett, J. L. Junkins, D. Hughes, and
M. D. Tandale, “Vision-based sensor and navigation system for au-
tonomous air refueling,” Journal of Guidance, Control, and Dynamics,
vol. 28, no. 5, pp. 979–989, 2005.
[9] K. E. Wenzel, A. Masselli, and A. Zell, “Visual tracking and following
of a quadrocopter by another quadrocopter,” in Intelligent Robots and
Systems (IROS), 2012 IEEE/RSJ International Conference on, 2012,
pp. 4993–4998.
[10] R. Mati, L. Pollini, A. Lunghi, M. Innocenti, and G. Campa, “Vision-
based autonomous probe and drogue aerial refueling,” in Control
and Automation, 2006. MED’06. 14th Mediterranean Conference on.
IEEE, pp. 1–6.
[11] M. Mammarella, G. Campa, M. R. Napolitano, M. L. Fravolini, Y . Gu,
and M. G. Perhinschi, “Machine vision/gps integration using ekf for
the uav aerial refueling problem,”Systems,Man,andCybernetics,Part
C: Applications and Reviews, IEEE Transactions on, vol. 38, no. 6,
pp. 791–801, 2008.
[12] C. Giampiero, F. Mario Luca, F. Antonio, N. Marcello, S. Brad, and
P. Mario, Autonomous Aerial Refueling for UAVs Using a Combined
GPS-Machine Vision Guidance, ser. Guidance, Navigation, and Con-
trol and Co-located Conferences. American Institute of Aeronautics
and Astronautics, 2004, doi:10.2514/6.2004-5350.
[13] S. Oh and E. N. Johnson, “Relative motion estimation for vision-based
formation ﬂight using unscented kalman ﬁlter,” in AIAA Guidance,
Navigation and Control Conference and Exhibit. AIAA, 2007.
[14] A. Fosbury and J. Crassidis, “Relative navigation of air vehicles,”
Journal of Guidance, Control, and Dynamics, vol. 31, no. 4, pp. 824–
834, 2008.
[15] R. M. Rogers, Applied Mathematics in Integrated Navigation Systems,
3rd ed., ser. AIAA Education Series. AIAA, 2007.
[16] J. Crassidis and F. Markley, “Unscented ﬁltering for spacecraft attitude
estimation,” in AIAA Guidance, Navigation, and Control Conference.
AIAA.
[17] D. F. Dementhon and L. S. Davis, “Model-based object pose in 25
lines of code,” International Journal of Computer Vision, vol. 15, no.
1-2, pp. 123–141, 1995.
[18] C. P. Lu, G. D. Hager, and E. Mjolsness, “Fast and globally convergent
pose estimation from video images,” Pattern Analysis and Machine
Intelligence, IEEE Transactions on, vol. 22, no. 6, pp. 610–622, 2000.
[19] V . Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o(n)
solution to the pnp problem,” International Journal of Computer
Vision, vol. 81, no. 2, pp. 155–166, 2009.
[20] M. Mammarella, G. Campa, M. R. Napolitano, and M. L. Fravolini,
“Comparison of point matching algorithms for the uav aerial refueling
problem,” Machine Vision and Applications, vol. 21, no. 3, pp. 241–
251, 2010.
[21] S. Park, J. Deyst, and J. P. How, “A new nonlinear guidance logic for
trajectory tracking,” in Proceedings of the AIAA Guidance, Navigation
and Control Conference. Citeseer, 2004.
[22] L. E. Dubins, “On curves of minimal length with a constraint on
average curvature, and with prescribed initial and terminal positions
and tangents,” American Journal of Mathematics, vol. 79, no. 3, pp.
497–516, 1957.
[23] D. R. Lee and H. Pernicka, “Vision-based relative state estimation us-
ing the unscented kalman ﬁlter,” International Journal of Aeronautical
and Space Sciences, vol. 12, no. 1, pp. 24–36, 2011.
[24] G. Sirio, “Chibios/rt,” 2013. [Online]. Available: http://www.chibios.
org
[25] L. Meier, P. Tanskanen, F. Fraundorfer, and M. Pollefeys, “The
pixhawk open-source computer vision framework for mavs.” 2011.
[26] J. Bouguet, “Camera calibration toolbox for matlab,” 2009. [Online].
Available: http://www.vision.caltech.edu/bouguetj/calib doc/
4995
