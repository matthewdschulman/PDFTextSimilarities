Fast and Accurate PoseSLAM by Combining
Relative and Global State Spaces
Brian Peasley
1,2
and Stan BirchÞeld
1,2
1
Electrical and Computer Engineering Dept.
Clemson University, Clemson, SC 29634
{bpeasle,stb}@clemson.edu
2
Microsoft Robotics
Redmond, WA 98052
stanleyb@microsoft.com
AbstractÑ We revisit the question of state space in the
context of performing loop closure. Although a relative
state space has been previously discounted, we show that
such a state space is actually extremely powerful, able
to achieve recognizable results after just one iteration.
The power behind the technique (called POReSS) is the
coupling between parameters that causes the orientation
of one node to affect the position and orientation of other
nodes. At the same time, the approach is fast because, like
the more popular incremental state space, the Jacobian
never needs to be explicitly computed. Furthermore, we
show that while POReSS is able to quickly compute a
solution near the global optimum, it is not precise enough
to perform the Þne adjustments necessary to reach the
global minimum. As a result, we augment POReSS with
a fast variant of Gauss-Seidel (called Graph-Seidel) on a
global state space to allow the solution to settle closer to
the global minimum. We show that this combination of
POReSS and Graph-Seidel converges more quickly and
scales to very large graphs better than other techniques
while at the same time computing a competitive residual.
I. INTRODUCTION
PoseSLAM is the problem of simultaneous local-
ization and mapping (SLAM) using only constraints
between robot poses, in which only the robot poses (as
opposed to landmark positions) are estimated. To a large
extent, once the robot poses have been determined, a
map of the environment can be created by overlaying
the sensor data obtained at the poses. Assuming a graph-
based approach, the primary problem in PoseSLAM is
to optimize the graph in the presence of loop closure.
The choice of state space has an enormous impact
on the ability of an algorithm to solve for loop closure.
In their inßuential work on PoseSLAM, Olson et al.
[17] proposed the use of an incremental state space
(ISS) with a variant of stochastic gradient descent. The
advantage of this choice is that the Jacobian has a simple
formulation and therefore does not need to be explicitly
constructed. However, the coupling between the different
parameters is lost, so that a change in orientation for one
node does not directly affect the positions of the other
nodes. Curiously, in the same paper the idea of using a
relative state space (RSS) is dismissed with the argument
that the resulting Jacobian is highly nonlinear and non-
sparse, yielding a computationally expensive algorithm.
The Þrst two reasons are no doubt true, and as a result
no one (to our knowledge) has attempted to use an RSS
for loop closure.
In this paper we revisit the claim that using an RSS
is computationally expensive. In fact we arrive at a
surprising result, namely that the opposite conclusion is
true. By formulating the loop closure problem using an
RSS, we show that the same variation of stochastic gra-
dient descent Ñ which we call non-stochastic gradient
descent (NGD) for clarity Ñ is able to converge quickly,
typically producing meaningful results in just one iter-
ation. The key insight is that in an RSS the parameters
are coupled so that changing the orientation of one
node affects the global poses of all other downstream
nodes. Like the ISS, the RSS leads to a formulation
that is straightforward, leading to an implementation that
requires less than 100 lines of C++ code, with no linear
algebra required. We call this algorithm POReSS (Pose
Optimization by a Relative State Space).
While POReSS is able to achieve recognizable results
(meaning that the basic shape of the map is present) in
just one iteration, the coarse movements of the algorithm
prevent it from ever reaching the global minimum.
Therefore, we use POReSS as a starting point for a fast
variant of Gauss-Seidel, which we call Graph-Seidel,
operating on a global state space (GSS) to make Þne
adjustments to the poses, thus enabling it to settle into
a good solution. While Graph-Seidel requires many
iterations, each iteration is extremely fast. We demon-
strate that our combination of POReSS and Graph-Seidel
is able to achieve competitive results compared with
state-of-the-art, in less time and with fewer iterations.
Moreover, the approach scales well, able to operate on
graphs with tens of millions of nodes.
II. RELATED WORK
Our approach falls within the framework of graph-
based SLAM, which was pioneered by Lu and Milios
[14]. Duckett et al. [3] optimize the map via relaxation,
but this early work assumed knowledge of global ori-
entation, which makes the problem linear. Frese et al.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4268
[4] propose multi-level relaxation (MLR), a variant of
Gauss-Seidel, to Þnd the non-linear maximum likelihood
solution. Howard et al. [8] showed that the general
relaxation framework of Lu and Milios can be applied to
a broad range of problems, including not only SLAM but
also multi-robot SLAM and sensor network calibration.
To overcome the tendency of Gauss-Seidel to get
trapped in local minima, Olson et al. [17] proposed two
contributions: an alternative state space representation
(incremental state space) so that a single iteration up-
dates many poses, and a variant of stochastic gradient
descent that is robust to local minima and converges
more quickly than Gauss-Seidel. An extension of this
work to incremental optimization of pose graphs was
presented in [18]. Another extension is TORO (Tree-
based netwORk Optimizer) [7], which uses a tree-
based parameterization for describing the conÞguration
of nodes in the graph, as well as slerp functions for
handling 3D rotations [5].
Other researchers have investigated the problem of
nonlinear least squares minimization using sparse linear
algebra. Kummerle et al. [13] have developed g2o, a
ßexible open-source framework for 2D or 3D SLAM
and bundle adjustment. Square Root SAM (smoothing
and mapping) [2], and its incremental version iSAM
[10], formulate the problem as a factor graph. SLAM++
[19] is an efÞcient approach to nonlinear least squares.
Others use the preconditioned conjugate gradient (PCG)
[11], [15] or exploit the sparse structure of the linear
system [12]. Ranganathan et al. [20] show that loopy
belief propagation (LBP) is equivalent to Gauss-Seidel
relaxation but also recover the marginal covariances.
Relative bundle adjustment has been proposed [21] as
a way to avoid computing the solution in a single
Euclidean coordinate system.
III. GRAPH-BASED SLAM
The graph-based approach to SLAM attempts to Þnd
the maximum likelihood conÞguration given a set of
measurements. In this section we brießy review this
approach, loosely adopting the notation of [7]. The
graph is given by G = (P,E) consisting of a set
of vertices P = {p
i
}
n
i=0
representing robot poses,
and a set of edges E between pairs of robot poses.
Assuming a ground-based robot rolling on a horizontal
ßoor plane, theith pose is given by p
i
=

x
i
y
i
?
i

T
,
where

x
i
y
i

T
? R
2
, and ?
i
? SO(2). Typically
the poses are traversed in a sequential manner, so for
convenience we stack this sequence into the vector p =

p
T
1
ááá p
T
n

T
. These poses are in a global coordinate
system Þxed by convention at p
0
?

0 0 0

T
.
Let x =

x
T
1
ááá x
T
n

T
be a state vector that is
uniquely related to the sequence of poses through a
bijective function g such that x =g(p) and p =g
?1
(x).
In the simplest case x
i
? p
i
, so that the states are
equivalent to the global poses, but this is not required;
as we shall see, the choice of state space can have
signiÞcant impact upon the results.
Each edge (a,b)?E captures a constraint ?
ab
be-
tween poses p
a
and p
b
obtained by sensor measurements
or by some other means. For ease of presentation we
assume at most one edge between any two given poses,
but the extension to a multigraph is straightforward.
The uncertainty of the measurement is given by the
information matrix ½
ab
, which is the inverse of the
covariance matrix. If we let f
ab
(x) be the zero-noise
observation between poses p
a
and p
b
given the cur-
rent conÞguration x, then the discrepancy between the
predicted observation and the actual observation is the
residual:
r
ab
(x)??
ab
? f
ab
(x). (1)
Assuming a Gaussian observation model, the negative
log-likelihood of the observation is given by the squared
Mahalanobis distance
?
ab
(x)? r
T
ab
(x)½
ab
r
ab
(x), (2)
also known as the chi-squared error.
The goal of graph-based SLAM is to Þnd the conÞgu-
ration x that minimizes the energy ?(x)?
P
(a,b)?E
?
ab
.
This energy is a non-linear expression due to the orien-
tation parameters, thus requiring an iterative approach.
Let ÷ x be the current estimate for the state. The linearized
energy about this current estimate is given by
÷ ?(x)?
X
(a,b)?E
÷ r
T
ab
(x)½
ab
÷ r
ab
(x), (3)
where the linearized residual is given by the Þrst-order
Taylor expansion:
÷ r
ab
(x)? r
ab
(÷ x)?J
ab
(÷ x)(x?÷ x)
| {z }
?x
, (4)
where J
ab
(÷ x) is the Jacobian of the error e
ab
(x) ?
?r
ab
(x) evaluated at the current state.
Expanding the linear system, rearranging terms, dif-
ferentiating ¶÷ ?(x)/¶?x, and setting to zero yields
X
(a,b)?E
½
ab
(r
ab
(÷ x)?J
ab
(÷ x)?x) = 0. (5)
If we deÞne K as the matrix obtained by concatenat-
ing the ½
ab
horizontally, r(÷ x) as the vector obtained
by stacking r
ab
(÷ x) vertically, and J(÷ x) as the matrix
obtained by stacking J
ab
(÷ x) vertically, we obtain the
standard least squares system
Kr(÷ x) =KJ(÷ x)?x. (6)
4269
Multiplying both sides by (KJ)
T
yields the so-called
normal equations:
J
T
(÷ x)½J(÷ x)?x =J
T
(÷ x)½r(÷ x), (7)
where ½ =K
T
K.
For reference let us consider the dimensions of these
matrices. If we let m be the number of elements in the
state vector, then x
i
is an m?1 vector; typically for 2D
pose optimization we have m = 3 due to the translation
and orientation parameters. The vectors x, ÷ x, and ?x are
all mn? 1. Let m
?
be the number of elements in the
observation ?
ab
; typically m
?
=m. Then ?
ab
(x), f
ab
(x),
r
ab
(x), and÷ r
ab
(x) are allm
?
?1 vectors, ½
ab
ism
?
?m
?
,
and ?
ab
(x) and ÷ ?
ab
(x) are scalars. The Jacobian J
ab
(x)
is m
?
?mn. If we let n
?
=|E| be the number of edges
in the graph, then ½ is m
?
n
?
?m
?
n
?
, r(x) is m
?
n
?
?1,
and J(x) is m
?
n
?
?mn.
IV. STATE SPACES
As mentioned earlier, the choice of state space can
have a signiÞcant impact upon the results. In this section
we describe three different state spaces and outline their
strengths and weaknesses. In all cases, m
?
=m = 3.
A. Global state space (GSS)
The most natural choice for state space is the global
pose, that is, the pose of the robot in a global coordinate
system:
x
i
? p
i
=

x
i
y
i
?
i

T
. (8)
The use of a global state space (GSS) leads to a
simple formulation of the energy of the system and
subsequently a sparse Jacobian. However, since the GSS
representation directly solves for the global poses, each
node is only affected by the nodes to which it is directly
connected. This causes slow convergence since changes
will be propagated slowly and can easily be trapped in
a local minimum if the initial conditions are poor.
B. Incremental state space (ISS)
Olson et al. [17] propose using the incremental state
space, in which the state is the difference between
consecutive poses:
x
i
? p
i
?p
i?1
=
?
?
x
i
?x
i?1
y
i
?y
i?1
?
i
??
i?1
?
?
, i = 1,...,n, (9)
with x
0
?

0 0 0

T
.
With an incremental state space, the ith pose is given
by the sum of all states up to and including i:
p
i
=
i
X
k=0
x
k
. (10)
This state space allows changes to be propagated through
the system quickly because changing one state affects
the global pose of all nodes past it. However, the
coupling between the different parameters has been lost,
so that a change in orientation for one node does not
directly affect the positions of the other nodes.
C. Relative state space (RSS)
Another alternative is to use a relative state space:
x
i
?

x
?
i
y
?
i
?
?
i

T
, (11)
with x
0
?

0 0 0

T
. The parameters x
?
i
, y
?
i
, and ?
?
i
describe the relative Euclidean transformation between
the (i?1)th and ith poses, speciÞcally the ith pose in
the (i? 1)th coordinate frame. Assuming a righthand
coordinate system with positive angles describing coun-
terclockwise rotation, we have:
p
i
= p
i?1
+
?
?
cos?
i?1
?sin?
i?1
0
sin?
i?1
cos?
i?1
0
0 0 1
?
?
| {z }
R(?i?1)
?
?
x
?
i
y
?
i
?
?
i
?
?
|{z}
xi
(12)
=
i
X
k=1
R(?
k?1
)x
k
, (13)
where
?
b
=
b
X
k=1
?
?
k
(14)
is the global orientation, as mentioned earlier. If we
deÞne
a
p
b
as the relative pose between a and b, a<b,
that is, pose b in coordinate frame a, it is not difÞcult
to verify that
a
p
b
=R
T
(?
a
)(p
b
? p
a
) =
b
X
k=a+1
R(
a
?
k?1
)x
k
(15)
since
0
p
b
= p
b
and
0
?
b
=?
b
, where
a
?
b
=?
b
??
a
=
b
X
k=a+1
?
?
k
(16)
is the angle of frame b with respect to frame a. Note
that ?
a
=?
a
?
0
, so that R
T
(?
a
) =R(
a
?
0
).
V. APPROACH
In general, the solution to (7) is found by repeatedly
computing r(÷ x) and J(÷ x) for the current estimate, solv-
ing the equation for ?x, then adding ?x to the current
estimate to yield the estimate for the next iteration.
The process is repeated until the system converges (i.e.,
k?xk²?, where ? is a threshold).
The standard Gauss-Newton approach is to solve the
equation directly in each iteration, leading to
?x =M
?1
J
T
(÷ x)½r(÷ x) (17)
4270
where M = J
T
(÷ x)½J(÷ x) is a 3n?3n preconditioning
matrix. Instead, we propose a two-step approach that
Þrst uses a variation of stochastic gradient descent
in the relative state space (POReSS), followed by a
variant of Gauss-Seidel in the global state space (Graph-
Seidel). Although either of these is itself a standalone
solution, we show in the results that the two exhibit
complementary characteristics. The former is better at
quickly getting near the global minimum even with poor
initial conditions but can take many iterations to reach
convergence, while the latter is better at performing de-
tailed reÞnements of the estimate but requires a starting
point near the global minimum.
A. Non-stochastic gradient descent
Stochastic gradient descent (SGD) is a standard itera-
tive method for Þnding the minimum of a function. SGD
repeatedly updates the state based on a single constraint
between nodes a and b:
?x =?
ab
M
?1
J
T
ab
(÷ x)½
ab
r
ab
(÷ x), (18)
where?
ab
??/|b?a|, and? is a scalar learning rate that
follows an exponential decay. In contrast to traditional
SGD, in which the order of the constraints is chosen
randomly, we follow the approach of Olson et al. [17] in
which the order is deterministic. For clarity, we refer to
this approach as non-stochastic gradient descent (NGD).
Unlike OlsonÕs method, our approach selects constraints
in decreasing order of the number of nodes they affect.
Using a relative state space, the residual correspond-
ing to a constraint between nodes a and b is given by
r
ab
(x) =?
ab
?
a
p
b
. (19)
The Jacobian is obtained by differentiating the right side
of (15) with respect to the states:
J
ab
=

ááá 0
a
b
B
a+1
a
b
B
a+2
ááá
a
b
B
b
0 ááá

,
(20)
where
a
b
B
i
?
¶
¶x
i
a
p
b
=
?
?
cos
a
?
i?1
?sin
a
?
i?1
a
b
?
i
sin
a
?
i?1
cos
a
?
i?1
a
b
?
i
0 0 1
?
?
(21)

a
b
?
i
a
b
?
i

?
b
X
k=i

?x
?
k
sin
a
?
k?1
?y
?
k
cos
a
?
k?1
x
?
k
cos
a
?
k?1
?y
?
k
sin
a
?
k?1

(22)
if a + 1 ² i ² b, or
a
b
B
i
? 0 otherwise, where 0
is a vector of zeros. As in the incremental state space
approach of Olson et al. [17], we never need to compute
the Jacobian explicitly.
In the general case the Jacobian is neither sparse
nor linear. Plugging (20) into (18) yields a linear sys-
tem that is difÞcult to compute due to the M
?1
=
(J
T
(÷ x)½J(÷ x))
?1
term. Following Olson et al. [17],
instead of explicitly computing this matrix we instead
ignore all but the diagonal elements:
MÅ diag(J
T
(÷ x)½J(÷ x)) (23)
= diag
?
?
X
(a,b)?E
a
m
b
?
?
| {z }
m
, (24)
where diag(v)?
P
i
e
i
e
T
i
ve
T
i
creates a diagonal matrix
from a vector, e
i
is a vector of all zeros except a 1 in
the ith element, and
a
m
b
?

ááá 0
a
b
?
T
a+1
a
b
?
T
a+2
ááá
a
b
?
T
b
0 ááá

T
,
(25)
where
a
b
?
i
? diag(
a
b
B
T
i
½
ai
a
b
B
i
), (26)
and diag(A)?
P
i
e
T
i
Ae
i
e
i
extracts the diagonal of a
matrix.
When the constraint is between two consecutive
nodes, b =a+1, the Jacobian reduces to a very simple
form:
J
ab
=

ááá 0 I
{3?3}
0 ááá

, (27)
where I
{3?3}
is the 3? 3 identity matrix. Oftentimes
the vast majority of constraints in a pose optimization
problem are between consecutive nodes, in which case
this simple form yields a tremendous speedup. Note also
that as the preconditioned matrix is being constructed,
the computation is simpler in the case of consecutive
nodes:
a
m
b
?

ááá 0 diag(½
ab
) 0 ááá

T
. (28)
Plugging the simpliÞed Jacobian of (27) into (18),
approximating M by its diagonal elements, and taking
the Moore-Penrose pseudoinverse, we get
?x =?
ab
M
+

... 0 diag(½
ab
) 0 ...

T
r
ab
(÷ x)
(29)
Å?
ab

... 0 I
{3?3}
0 ...

T
r
ab
(÷ x) (30)
=?
ab
r
ab
(÷ x), (31)
where the second line is equal in the case of a di-
agonal ½
ab
. Thus it can be seen that in the case of
consecutive nodes, only one state needs to be modiÞed,
and the update is extremely simple. In the general
case, the complexity of computing J
T
ab
½
ab
r
ab
is pro-
portional to the number of nodes between a and b.
Pseudocode for POReSS can be seen in Algorithm 1,
where m?
P
a
m
b
?

m
1
ááá m
n

T
is the vector
such that M Å diag(m), r =

r
x
r
y
r
?

T
, ? =

?
x
?
y
?
?

T
, and mod
?
computes the modulo of
the last element of the vector while leaving the other
4271
Algorithm 1 POReSS
Input: relative states x
1:n
, ?
ab
and ½
ab
?(a,b)?E
Output: updated relative states x
1:n
? Precompute M
m? zeros(3n,1)
for (a,b) ?E do ?Note: a<b
m? m+
a
m
b
end for
? Minimize
while not converged do
for (a,b) ?E do ?Note: a<b
r? (?
ab
?
a
p
b
) mod
?
2¹
if b ==a+1 then
x
b
? x
b
+?r
else
r? ½
ab
r
for i?a+1 to b do
??R(
a
?
i?1
)
?
?
r
x
r
y
0
?
?
+r
?
?
?
a
b
?
i
a
b
?
i
1
?
?
x
i
? x
i
+
?
b?a
?
?
?
x
/m
3i?2
?
y
/m
3i?1
?
?
/m
3i
?
?
end for
end if
end for
decrease ?
end while
elements unchanged. To improve readability the pseu-
docode does not include all the optimizations used in
our implementation.
B. Graph-Seidel
While the use of non-stochastic gradient descent
(NGD) on a Relative State Space allows us to quickly
optimize a pose graph, it does a poor job of converging
to the correct solution. We address this in the second
phase of our optimization process. In this phase we
use an implementation of Gauss-Seidel that is optimized
for a graph, which we refer to as Graph-Seidel. Graph-
Seidel is better suited to Þnding exact solutions and can
perform well given adequate initial conditions. In our
Graph-Seidel optimization we do not use a relative state
space but instead use a global state space. This change is
made because the second phase provides a reÞnement to
the original optimization, and we want to prevent small
changes in one state from having large effects on the
entire system.
The residual is the same as before, but now we use
the left side of (15), which we combine with (19) to
yield
r
ab
(x) = ?
ab
?R
T
(?
a
)(p
b
? p
a
) (32)
To simplify the math we deÞne
r
?
ab
(x) ? R(?
a
)r
ab
(x) (33)
= p
a
? p
b
+R(?
a
)?
ab
(34)
½
?
ab
? R(?
a
)½
ab
R
T
(?
a
) (35)
and note that ?(x) does not change when substituting
r
?
ab
for r
ab
, and ½
?
ab
for ½
ab
.
Graph-Seidel differs from Gauss-Seidel by assuming
that R(?
a
) is constant when taking the derivative
¶?(x)
¶p
i
.
The key insight is that, if R(?
a
) is constant, then ?(x) is
convex in the states, and we do not need to linearize the
system at all. Instead we simply take derivatives to solve
directly for the states, then iterate by updating R(?
a
).
Employing this assumption and setting the derivative to
zero yields
X
(a,i)?E
in
i
½
?
ai
r
?
ai
(x) =
X
(i,b)?E
out
i
½
?
ib
r
?
ib
(x), (36)
where
E
in
i
? {(a,b) : (a,b)?E and b =i} (37)
E
out
i
? {(a,b) : (a,b)?E and a =i} (38)
are the set of edges into and out of, respectively, node i.
Since we are using a GSS, x = p and x
i
= p
i
for
i = 1,...,n. Rearranging terms yields
?
?
?
?
?
½
1
?½
12
ááá ?½
1n
?½
21
½
2
ááá ?½
2n
.
.
.
.
.
.
.
.
.
?½
n1
?½
n2
ááá ½
n
?
?
?
?
?
?
?
?
?
?
x
1
x
2
.
.
.
x
n
?
?
?
?
?
=
?
?
?
?
?
v
1
v
2
.
.
.
v
n
?
?
?
?
?
, (39)
where, assuming we do not have a multigraph,
½
ab
? ½
ba
?
?
?
?
?
?
½
?
ab
if ?
ab
exists
½
?
ba
if ?
ba
exists
0
{3?3}
otherwise
(40)
½
a
?
X
b
½
ab
(41)
v
i
?
X
(a,i)?E
in
i
R(?
a
)½
ai
?
ai
| {z }
edges in
?
X
(i,b)?E
out
i
R(?
i
)½
ib
?
ib
| {z }
edges out
.
(42)
This can be solved using Gauss-Seidel iterations of
the form:
x
(k+1)
i
= ½
?1
i
?
?
v
i
+
X
j<i
½
ij
x
(k+1)
j
+
X
j>i
½
ij
x
(k)
j
?
?
,
(43)
wherek is the iteration number; to improve convergence,
we use successive over-relaxation (SOR). Note that,
unlike Gauss-Seidel, Graph-Seidel does not actually
4272
Algorithm 2 Graph-Seidel
Input: global states x
1:n
, ?
ab
and ½
ab
?(a,b)?E
Output: updated global states x
1:n
while not converged do
? Compute v
for i? 1 to n do
v
i
?0
{3?1}
for (a,b)?E do
if a ==i then
v
i
? v
i
+½
ab
R(?
a
)?
ab
else if b ==i then
v
i
? v
i
?½
ab
R(?
a
)?
ab
end if
end for
end for
? Minimize
for i? 1 to n do
w?0
{3?1}
for (a,b)?E do
if a ==i then
w? w+½
ab
x
b
else if b ==i then
w? w+½
ab
x
a
end if
end for
x
i
? ½
?1
i
(v
i
+ w)
end for
end while
minimize a linear system, because it recomputes the
vector v each iteration. The algorithm is fast because
the matrix remains constant. As before, the pseudocode
in Algorithm 2 does not show any optimizations used in
our implementation.
VI. EXPERIMENTAL RESULTS
For evaluation the approach was applied to two
synthetic datasets and one real dataset, as well as to
a simple, single-loop graph of varying sizes (up to
40 million nodes and constraints). All timings were
performed on an Intel Core i7-3770 CPU at 3.4 GHz
with 16 GB RAM.
A. Synthetic datasets
Our approach of POReSS + Graph-Seidel was com-
pared with two state-of-the-art algorithms, TORO [6],
[5], [7] and g2o [13]. TORO is a tree-based implemen-
tation designed to increase the speed of the algorithm
of Olson et al. [17]. Except for parameter differences,
the solution found by TORO should be the same as that
found by Olson et al.; we used the former because it is
freely available online.
Ground Truth Corrupted Input
krk= 5212
POReSS TORO
1 iter. (0.007 sec.) 1 iter. (0.013 sec.)
krk= 2384 krk= 4522
POReSS+GS TORO
2+346 iter. (0.238 sec.) 200 iter. (2.639 sec.)
krk= 227 krk= 689
Fig. 1. The Manhattan World data set, containing 3500 nodes
and 5600 constraints (top); output of POReSS and TORO after
one iteration (middle); and Þnal output of POReSS+GS and TORO
(bottom). Not shown are the results for TORO+GS (krk= 356), g2o
(krk= 146), and g2o+GS (krk= 131).
We ran the algorithms against two synthetic datasets.
The Þrst is the Manhattan World of [13], shown in
Figure 1. POReSS achieves a recognizable result, cutting
the residual in half in only one iteration.
1
Subsequent
iterations of POReSS produce little change on this graph,
but following POReSS with several hundred Graph-
Seidel iterations reduces the residual by more than 95%
from its original value. In contrast, one iteration of
TORO barely changes the graph or residual, and even
after 10 times more computation than POReSS+GS,
TORO is only able to achieve a solution with much
higher residual. Although at a glance it may be difÞcult
to compare the solutions of POReSS+GS and TORO
from the Þgure, close visual inspection reveals that
indeed the former is more accurate in a number of
places, thus validating the residual as a measure of
accuracy. On this particular graph the best algorithm
1
For brevity we use residual for the norm of the residual. This is
equivalent to the chi-squared error since we use the identity matrix
for the information matrix all experiments.
4273
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2
0
1000
2000
3000
4000
5000
6000
Time (sec)
Residual
 
 
Graph?Seidel
POReSS (1 iteration) + GS
POReSS (2 iteration) + GS
POReSS (3 iteration) + GS
g2o
TORO
Fig. 2. The norm of the residual versus runtime for the different
algorithms, on the Manhattan World dataset. The plot of TORO, which
converges after more than 2 seconds, extends past what is shown here.
is g2o, which achieves a residual nearly half that of
POReSS+GS with approximately the same computation.
We do not show the output of g2o for lack of space.
Figure 2 plots the residual found by the algorithms
versus time. Graph-Seidel converges the fastest but set-
tles into a local minimum. POReSS makes signiÞcant
progress in one iteration, but subsequent iterations have
little effect. Graph-Seidel converges onto the same solu-
tion after approximately 0.06 seconds regardless of the
number of POReSS iterations (assuming at least 1). This
is the same amount of time that it takes g2o to converge.
Note that due to TOROÕs relatively long running time,
the scale of the plot does not reveal the fact that TORO
does indeed reduce the residual considerably. Note also
that each iteration of POReSS takes a noticeable amount
of time, but Graph-Seidel iterations are extremely fast.
The second dataset, obtained from [9], is a larger
graph with more interconnections, shown in Figure 3.
On this dataset POReSS alone was not as successful as
TORO at Þnding a solution, but POReSS+GS was able
to a signiÞcantly better solution in less time than TORO.
Note that the result shown is the Þnal result of both
algorithms after convergence. As with the Manhattan
dataset, the best algorithm is g2o, which achieves a
lower residual than either of the other approaches.
B. Real dataset
The third dataset consists of real data collected from
a vehicle driving in loops around a parking lot, from
[1], shown in Figure 4. On this dataset POReSS+GS
achieved the best results, even outperforming g2o.
C. Large single loop
As a Þnal experiment, we constructed graphs of
varying sizes. Each graph consisted of a single square
loop with one loop closing constraint between the Þrst
and last nodes, so that the number of nodes equaled
the number of edges (constraints). Noise was added to
the rotational component at the corners. The number of
nodes per side of the square was varied from 1000 to
Ground Truth Corrupted Input
krk= 309792
POReSS+GS TORO
3 + 773 iter. (3.33 sec.) 200 iter. (13.59 sec.)
krk= 1209 krk= 7058
Fig. 3. The dataset of [9] containing 10,000 nodes and 30,000
constraints (top). The Þnal result of POReSS+GS and TORO (bottom).
Not shown are the results for TORO+GS (krk= 3776), g2o (krk=
362), and g2o+GS (krk= 343).
4K 40K 400K 4M 40M
10
?4
10
?2
10
0
10
2
10
4
# Nodes and Constraints
time per iteration (seconds)
 
 
TORO
Olson
g2o
POReSS
Fig. 5. The runtime for one iteration of each algorithm to optimize
a single square loop with a certian number of nodes (equivalently
constraints).
10 million, so that the total number of nodes (or edges)
varied from 4000 to 40 million. Figure 5 shows the
runtime of one iteration of POReSS, g2o, TORO, and
our implementation of OlsonÕs algorithm. Regardless of
the size of the graph, POReSS required the least runtime.
On a graph of 40 million nodes, POReSS required
slightly more than two seconds per iteration.
VII. CONCLUSION
We have presented a two-step optimization process
for solving the PoseSLAM problem. The Þrst step
(POReSS) uses a relative state space and non-stochastic
gradient descent (NGD) using a simple formulation for
the Jacobian. The second step (Graph-Seidel) performs
further relaxation in a global state space by executing ex-
tremely fast iterations. The idea of a two-step process is
not new [16], but this particular combined approach con-
verges more quickly than previous approaches. More-
4274
Ground Truth Corrupted Input POReSS+GS TORO g2o
12 + 330 iter. (0.06 s) 100 iter. (0.37 s) 10 iter. (0.02 s)
krk= 56308 krk= 56 krk= 1190 krk= 408
Fig. 4. Dataset of vehicle driving around parking lot, from [1], containing 407 nodes and 1625 constraints, along with the results of the
algorithms. Not shown are the results for TORO+GS (krk= 796), and g2o+GS (krk= 163).
over, it is easy to implement, does not require linear
algebra, and produces competitive results compared with
state-of-the-art methods on several synthetic and real
datasets. Alternatively, Graph-Seidel can be used on its
own as a post-processing step to further improve the
results of other algorithms.
There is plenty of room for future work. One bottle-
neck to using a relative state space is the same as that
encountered using an incremental state space, namely
constraints that affect multiple states. Others have shown
that a tree representation of the graph allows for a
quicker update of the states. While POReSS does not
require this update, it does require a composition of
relative transformations between two nodes at either
endpoint of a constraint. As a result, running POReSS
on a tree representation of the graph should speed up this
composition. In addition, future work should focus on
including landmarks into the system as well as extending
the approach to 3D.
VIII. ACKNOWLEDGMENT
This research was partially supported by the U.S.
National Science Foundation under grant IIS-1017007.
REFERENCES
[1] J.-L. Blanco, F.-A. Moreno, and J. Gonzalez. A collection of
outdoor robotic datasets with centimeter-accuracy ground truth.
Autonomous Robots, 27(4):327Ð351, Nov. 2009.
[2] F. Dellaert and M. Kaess. Square root SAM: Simultaneous
location and mapping via square root information smoothing.
International Journal of Robotics Research, 25(12):1181Ð1204,
Dec. 2006.
[3] T. Duckett, S. Marsland, and J. Shapiro. Learning globally con-
sistent maps by relaxation. In Proceedings of the International
Conference on Robotics and Automation (ICRA), Apr. 2000.
[4] U. Frese, P. Larsson, and T. Duckett. A multilevel relaxation
algorithm for simultaneous localisation and mapping. IEEE
Transactions on Robotics, 21(2):196Ð207, Apr. 2005.
[5] G. Grisetti, S. Grzonka, C. Stachniss, P. Pfaff, and W. Burgard.
EfÞcient estimation of accurate maximum likelihood maps in
3D. In Proceedings of the IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pages 3472Ð3478, Oct.
2007.
[6] G. Grisetti, C. Stachniss, and W. Burgard. Nonlinear constraint
network optimization for efÞcient map learning. IEEE Transac-
tions on Intelligent Transportation Systems, 10(3):428Ð439, Sept.
2009.
[7] G. Grisetti, C. Stachniss, S. Grzonka, and W. Burgard. A tree
parameterization for efÞciently computing maximum likelihood
maps using gradient descent. In Proceedings of Robotics: Science
and Systems (RSS), June 2007.
[8] A. Howard, M. J. Matari« c, and G. Sukhatme. Relaxation on a
mesh: A formalism for generalized localization. In Proceedings
of the IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), Oct. 2001.
[9] M. Kaess, H. Johannsson, R. Roberts, V . Ila, J. Leonard, and
F. Dellaert. iSAM2: Incremental smoothing and mapping using
the bayes tree. International Journal of Robotics Research,
31:217Ð236, Feb. 2012.
[10] M. Kaess, A. Ranganathan, and F. Dellaert. iSAM: Incremental
smoothing and mapping. In IEEE Transactions on Robotics,
2008.
[11] K. Konolige. Large-scale map-making. In Proceedings of the
National Conference on ArtiÞcial Intelligence, July 2004.
[12] K. Konolige, G. Grisetti, R. Kummerle, W. Burgard,
B. Limketkai, and R. Vincent. Sparse pose adjustment for
2D mapping. In Proceedings of the IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2010.
[13] R. KŸmmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Bur-
gard. g2o: A general framework for graph optimization. In
Proceedings of the International Conference on Robotics and
Automation (ICRA), pages 3607Ð3613, May 2011.
[14] F. Lu and E. Milios. Globally consistent range scan alignment for
environment mapping. Autonomous Robots, 4:333Ð349, 1997.
[15] M. Montemerlo and S. Thrun. Large-scale robotic 3-D mapping
of urban structures. In Proceedings of the International Sympo-
sium on Experimental Robotics (ISER), June 2004.
[16] E. Olson. Robust and EfÞcient Robotic Mapping. PhD thesis,
Massachusetts Institute of Technology, June 2008.
[17] E. Olson, J. Leonard, and S. Teller. Fast iterative optimization
of pose graphs with poor initial estimates. In Proceedings of the
International Conference on Robotics and Automation (ICRA),
pages 2262Ð2269, May 2006.
[18] E. Olson, J. Leonard, and S. Teller. Spatially-adaptive learning
rates for online incremental SLAM. In Robotics: Science and
Systems, June 2007.
[19] L. Polok, V . Ila, M. Solony, P. Smrz, and P. Zemcik. Incremen-
tal block Cholesky factorization for nonlinear least squares in
robotics. In Proceedings of Robotics: Science and Systems, June
2013.
[20] A. Ranganathan, M. Kaess, and F. Dellaert. Loopy SAM. In
Proceedings of the International Joint Conference on ArtiÞcial
Intelligence, Jan. 2007.
[21] G. Sibley, C. Mei, I. Reid, and P. Newman. Adaptive relative
bundle adjustment. In Proceedings of Robotics: Science and
Systems, Aug. 2009.
4275
