ConservativeEdgeSparsiﬁcationforGraphSLAMNodeRemoval
Nicholas Carlevaris-Bianco and Ryan M. Eustice
Abstract—This paper reports on optimization-based methods
for producing a sparse, conservative approximation of the dense
potentials induced by node marginalization in simultaneous
localization and mapping (SLAM) factor graphs. The proposed
methods start with a sparse, but overconﬁdent, Chow-Liu tree
approximation of the marginalization potential and then use
optimization-based methods to adjust the approximation so that
it is conservative subject to minimizing the Kullback-Leibler
divergence (KLD) from the true marginalization potential. Re-
sults are presented over multiple real-world SLAM graphs and
show that the proposed methods enforce a conservative approxi-
mation, while achieving low KLD from the true marginalization
potential.
I. INTRODUCTION
Graph-based simultaneous localization and mapping
(SLAM) [1–7] has been used to successfully solve many
challenging SLAM problems in robotics. In graph SLAM,
the problem of ﬁnding the optimal conﬁguration of historic
robot poses (and optionally the location of landmarks), is
associated with a Markov random ﬁeld or factor graph. In
the factor graph representation, variables are represented
by nodes, and measurements between nodes by factors.
Under the assumption of Gaussian measurement noise the
graph represents a least squares optimization problem.
The computational complexity of this problem is dictated
by the density of connectivity within the graph, and by
the number of nodes and factors it contains. Therefore, the
computational complexity of the graph’s optimization problem
can be reduced by removing nodes (variable marginalization)
and by removing edges from the Markov random ﬁeld
(sparsiﬁcation).
Methods that directly sparsify the graph connectivity in an
information ﬁltering framework include [8–10]. In Thrun et al.
[8], weak links between nodes are removed to enforce spar-
sity. Unfortunately, this removal method causes the resulting
estimate to be overconﬁdent (i.e., inconsistent) [11]. In Walter
et al. [9], odometry links are removed in order to enforce
sparsity in feature-based SLAM.
Methods that remove nodes from the graph include [12–
16]. True node marginalization induces dense connectivity
between nodes in the elimination clique. In Folkesson and
Christensen [12], and in the dense-exact version of generic
linear constraints (GLCs) presented by Carlevaris-Bianco and
*ThisworkwassupportedinpartbytheNationalScienceFoundationunder
award IIS-0746455, the Ofﬁce of Naval Research under award N00014-12-
1-0092, and Ford Motor Company via the Ford-UM Alliance under award
N015392.
N. Carlevaris-Bianco is with the Department of Electrical Engineering &
Computer Science, University of Michigan, Ann Arbor, MI 48109, USA
carlevar@umich.edu.
R. Eustice is with the Department of Naval Architecture & Ma-
rine Engineering, University of Michigan, Ann Arbor, MI 48109, USA
eustice@umich.edu.
(a) Original Graph (b) Node Marginalization
(c) Chow-Liu Tree Approx. (d) Conservative Approx.
Fig. 1: Method overview. Starting with the original factor graph
(a), the red node is marginalized. This induces a densely connected
factor over the marginalization clique (b). The true uncertainty
ellipses (dashed blue lines) are not affected by marginalization. The
dense marginalization potential is then approximated using a sparse
Chow-Liu tree (c). The uncertainty ellipses after the Chow-Liu tree
approximation (red lines) are overconﬁdent (note the yellow regions
that are no longer probabilistically plausible). The sparse Chow-Liu
treeapproximationisadjustedsothatitisconservative(d),modifying
the uncertainty ellipses (green lines).
Eustice [14], a linearized factor exactly reproducing the ef-
fect of marginalization at the given linearization point is
introduced over the elimination clique. Unfortunately, when
removing many nodes, the dense connectivity induced by
true marginalization quickly compounds, causing a loss of
sparsity and greatly increasing the computational complexity
of the graph optimization problem. This quickly outweighs the
computationalbeneﬁtsofnoderemoval,makingthesemethods
impractical for many applications.
Kretzschmar and Stachniss [13] propose using a Chow-
Liu tree (CLT) [17] approximation, calculated over the con-
ditional distribution of the elimination clique, to guide sparse
measurement composition in producing new non-linear factors
over the elimination clique. This heuristic approximates true
marginalization while maintaining sparsity. However, because
measurement composition is used to compute the new factors,
the true CLT approximation is not computed. Additionally,
information may be double counted during measurement com-
position, producing an inconsistent estimate [14].
The sparse approximate version of GLC proposed in [14]
addresses these issues using linear factors to accurately imple-
ment the CLT approximation without double counting mea-
surement information. This method was successfully applied
to control the computational complexity in long-term multi-
session SLAM [15].
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 854
x
0
z
01
z
23
x
1
x
3
x
4
x
2
z
12
z
13
z
0 z
34
(a) Original Graph
x
0
z
t
x
3
x
2
(b) Target Info.
x
0
x
3
x
2
x
0
x
3
x
2
≈
(c) Chow-Liu Tree
x
0
x
3
x
2
w
x
0
x
3
x
2
w
(d) Root Shift
x
0
x
3
x
4
x
2
z
34 z
0
glc
z
03
glc
z
02
glc
(e) Final Graph
Fig. 2: Sample factor graph where node x1 is to be removed (a). Here Xm = [x0,x1,x2,x3]. The factors Zm = [z0,z01,z12,z23,z13]
(highlighted in red in (a)) are those included in calculating the target information over the marginalization clique Xt = [x0,x2,x3] (b).
The original distribution associated with the target information, p(x0,x2,x3|Zm), is approximated using the Chow-Liu maximum mutual
information spanning tree as p(x0|Zm)p(x2|x0,Zm)p(x3|x0,Zm) (c). The pairwise potentials are reparameterized with respect to x0 to
avoid linearization in the world-frame (d). New GLC factors are computed and inserted into the graph replacing Zm (highlighted in green
in (e)). Note that node removal only affects the nodes and factors within the Markov blanket of x1 (dashed line). The methods proposed
in this paper modify the CLT step (c) in order to ensure that the approximation is conservative.
The CLT produces an approximation with the lowest
Kullback-Leibler divergence (KLD) among all tree structures.
Unfortunately, achieving minimum KLD often requires that
the CLT approximation be slightly overconﬁdent with re-
spect to the true distribution, as illustrated in Fig. 1(c). In
most SLAM applications, conservative estimates are strongly
preferred to overconﬁdent estimates. During map building,
overconﬁdence can adversely affect data association, causing
the system to miss valid loop closures. Additionally, when
using the resulting map, overconﬁdent estimates can lead to
unsafe path planning and obstacle avoidance [18]. Here, as in
[10] and [16], we deﬁne a conservative estimate as one where
the covariance of the sparsiﬁed distribution is greater than or
equal to that of the true distribution, i.e.,
˜
?≥ ?.
Recently, Vial et al. [10] and Huang et al. [16] have
proposed methods that explicitly ensure conservative approx-
imations during graph sparsiﬁcation. In Vial et al. [10], an
optimization-based method is proposed that, given a desired
sparsity pattern, minimizes the KLD between the sparsiﬁed
distribution and the true distribution while ensuring that the
sparsiﬁed distribution is conservative. This method performs
favorably in comparison with [8] and [9], however, as the
authors of [10] acknowledge, the computational cost of the
optimization grows quickly with the size of the matrix being
sparsiﬁed. To avoid this they propose a problem reduction
that allows their method to be applied to a subset of the
graph’s variables. The problem reduction still involves the
expensive inversion of the block of the information matrix
associated with the entire graph beyond the subproblem’s
Markov blanket, which will also be intractable for large
graphs.
The method proposed by Huang et al. [16] performs node
marginalization by inducing a densely connected linear factor
as in [12] and dense-exact GLC [14]. To perform edge
sparsiﬁcation, the authors formulate an optimization problem
that seeks to minimize the KLD of the approximation while
requiring a conservative estimate and encouraging sparsity
through L
1
regularization. This optimization problem is then
appliedtothelinearizedinformationmatrixassociatedwiththe
entire graph, which limits its applicability to relatively small
problems, and prevents relinearization after sparsiﬁcation. Us-
ingL
1
regularization to promote sparsity is appealing because
itdoesnotrequirethesparsitypatterntobespeciﬁed—instead,
it automatically removes the least important edges. However,
because the sparsity pattern produced is arbitrary, it is unclear
how the resulting information matrix might be decomposed
into a sparse set of factors, which is important if one wishes
to exploit existing graph SLAM solvers such as iSAM [7, 19].
In this paper we explore optimization-based methods for
conservative sparsiﬁcation of the dense cliques induced by
node marginalization. The proposed methods are integrated
within the GLC framework proposed in [14] and are designed
to maintain the advantages of sparse-approximate GLC. Our
proposed methods address some of the aforementioned short-
comings of the methods proposed in [10] and [16].
• Like [10] and [16] our proposed methods ensure that the
sparse approximation remains conservative while provid-
ing a low KLD from the true distribution.
• Our methods produce a new set of factors using only
the current factors as input, and do not require the full
linearized information matrix as input as in [10] and [16].
• The computational complexity of our method is depen-
dent only upon the size of the elimination clique, and
not on the size of the graph beyond the clique. We
do not require a large matrix inversion to formulate the
subproblem as in [10], nor do we operate over the entire
graph as in [16].
The remainder of this paper is outlined as follows: In
SectionIIwebrieﬂyreviewtheGLCnoderemovalframework
within which our proposed methods are implemented. The
proposed conservative sparsiﬁcation techniques are described
in Section III. In Section IV the proposed methods are eval-
uated over a variety of real-world SLAM datasets. Finally, a
discussion and concluding remarks are provided in Section V
and Section VI.
II. GENERIC LINEAR CONSTRAINT NODE REMOVAL
The conservative sparsiﬁcation methods proposed in this
paper are developed within the GLC node removal framework.
Therefore, weﬁrst providean overviewof GLC noderemoval.
For a full discussion and derivation we refer the reader to
[14]. Here we focus on the CLT-based sparse-approximate
version of GLC, as the goal of the proposed methods are
to produce a sparse approximation with low KLD, with the
855
additional constraint that the approximation is conservative.
Sparse-approximate GLC node removal, illustrated in Fig. 2,
is performed as follows:
1) The potential induced by marginalization over the elim-
ination clique is computed.
2) The potential is approximated using a Chow-Liu tree.
3) The variables in the CLT potentials are reparameterized
as relative transforms.
4) The CLT potentials are implemented as linear factors,
referred to as generic linear constraints, replacing the
original factors over the elimination clique in the graph.
The methods proposed in this paper modify the CLT step
(step (2), Fig. 2(c)) in order to ensure that the approximation
is conservative.
III. METHOD
TheﬁrststepintheGLCnoderemovalprocessistoidentify
the potential induced in the graph by marginalization. This
potential is characterized by its information matrix, which we
refer to as the target information, ?
t
. Letting X
m
? X be
the subset of nodes including the node to be removed and the
nodes in its Markov blanket, and lettingZ
m
?Z be the subset
of measurement factors that only depend on the nodes inX
m
,
we consider the distribution p(X
m
|Z
m
) ? N
?1
 
?
m
,?
m

.
From ?
m
we can then compute the desired dense target infor-
mation, ?
t
, by marginalizing out the elimination node using
the standard Schur-complement form [5]. Because marginal-
ization only affects the elimination clique, a factor, or set of
factors, which induces the potential characterized by ?
t
, will
induce the same potential as true node marginalization at the
given linearization point.
It is important to note that the constraints in Z
m
may be
purely relative and/or low-rank (e.g., bearing or range-only)
and,therefore,maynotfullyconstrainp(X
m
|Z
m
).Asaresult,
?
t
, and distributions derived from ?
t
, may be low rank.
As in Vial et al. [10] and Huang et al. [16], we wish
to minimize the KLD of the sparse approximation while
producing a consistent estimate. When the distribution means
are equal (i.e.,?
t
= ?
t
?
t
and ˜ ?
t
=
˜
?
t
?
t
), the KLD between
the marginalization-induced factor characterized by ?
t
and its
approximation characterized by
˜
?
t
is given by
D
KL

N
?1
 
?
t
,?
t

kN
?1
 
˜ ?
t
,
˜
?
t


=
1
2

tr(
˜
?
t
?
?1
t
)+ln
|?
t
|
|
˜
?
t
|
?dim(?
t
)

.
(1)
Noting that ?
t
and the state dimension are constant, the KLD
optimization objective with respect to
˜
?
t
can be written as
f
KL
(
˜
?
t
) = tr(
˜
?
t
?
?1
t
)?ln|
˜
?
t
|. (2)
However, as previously mentioned, ?
t
will, in general,
be low rank, making the KLD ill deﬁned. In [10], a full
rank subproblem is deﬁned, but its implementation requires
inverting the information matrix associated with the rest of
the graph beyond the subproblem’s Markov blanket. In [16],
optimization is performed over the full information matrix,
which will always be full rank for well-posed SLAM graphs.
We know that ?
t
will be a real, symmetric, positive semi-
deﬁnite matrix due to the nature of its construction. In general
then, it has an eigen-decomposition given by
?
t
=

u
1
··· u
q

?
?
?
?
1
0 0
0
.
.
. 0
0 0 ?
q
?
?
?
?
?
?
u
?
1
.
.
.
u
?
q
?
?
? = UDU
?
,
(3)
where U is a p? q orthogonal matrix, D is a q? q matrix,
p is the dimension of ?
t
, and q = rank(?
t
). Noting that the
KLD is invariant under parameter transformations, we rewrite
the KLD objective as
f
KL
(
˜
?
t
) = tr(U
?
˜
?
t
UD
?1
)?ln|U
?
˜
?
t
U|. (4)
When ?
t
is full rank
f
KL
(
˜
?
t
) = tr(
˜
?
t
UD
?1
U
?
)?ln|U
?
||
˜
?
t
||U|
= tr(
˜
?
t
?
?1
t
)?ln|
˜
?
t
|,
which is exactly equivalent to (2). When ?
t
is low rank (4)
computestheKLDoverthesubspacewhere?
t
iswelldeﬁned.
This allows us to work with the low-rank target information
and limit the extent of the optimization problem to the elimi-
nation clique. Intuitively, this parameter transformation can be
thought of as using the pseudo-inverse [20] of ?
t
to compute
the KLD. However, it is important that the transformation
be applied to
˜
?
t
so that, during optimization, ln|U
?
˜
?
t
U| is
evaluated instead of ln|
˜
?
t
|, which will be undeﬁned because
the optimal
˜
?
t
will also be low-rank.
A. Chow-Liu Tree Approximation
The original version of sparse-approximate GLC approx-
imated the marginalization-induced factor using a Chow-Liu
tree. The CLT produces the minimum KLD among all trees by
computing the pairwise mutual information between all nodes,
and building the maximum mutual information spanning tree.
The CLT can be expressed as
N
?1
 
?
t
,?
t

≈N
?1
 
˜ ?
t
,
˜
?
CLT

=
Y
i
p(x
i
|x
p(i)
), (5)
where x
p(i)
is the parent of x
i
, and for the root of the CLT
p(x
0
|x
p(0)
) = p(x
0
). The information added to the graph by
the CLT approximation can then be written as
˜
?
CLT
=
X
i
?
i
, (6)
where each ?
i
is the information associated with one of the
unary or binary factors in the tree, padded with zeros so that
the appropriate dimensions are achieved.
The methods proposed in this paper all start with the CLT
approximation and then use optimization methods to “adjust”
the approximation to ensure that it is conservative. Intuitively,
this can be thought of as numerically growing the uncertainty
of the CLT so that it is conservative, while minimizing the
additional KLD from the true distribution. Each method will
produce, by construction, an approximation with the same
sparsity pattern as the CLT.
856
TABLE I: Experimental Datasets
Dataset Node Types Factor Types # Nodes # Factors
Intel Lab 3-DOF pose 3-DOF odometry, 3-DOF laser scan-matching 910 4,454
Killian Court 3-DOF pose 3-DOF odometry, 3-DOF laser scan-matching 1,941 2,191
Victoria Park 3-DOF pose, 2-DOF Landmark 3-DOF odometry, 2-DOF landmark observation 7,120 10,609
Duderstadt Center 6-DOF pose 6-DOF odometry, 6-DOF laser scan-matching 552 1,774
EECS Building 6-DOF pose 6-DOF odometry, 6-DOF laser scan-matching 611 2,134
USS Saratoga 6-DOF pose 6-DOF odometry, 5-DOF monocular-vision, 1-DOF depth 1,513 5,433 %
(a) Intel Lab (b) Killian Court (c) Victoria Park
(d) Duderstadt Center (e) EECS Building
(f) USS Saratoga
Fig. 3: Graphs used in evaluation. Blue links represent full-state (3-
DOF or 6-DOF) relative-pose constraints from odometry and laser
scan-matching. Red links represent 5-DOF relative-pose constraints
modulo-scale from monocular vision. Cyan links represent landmark
observation factors.
B. Covariance Intersection
As discussed in §I, the CLT approximation is often over-
conﬁdent in practice. This is due to the fact that the tree
structure is not, in general, capable of capturing the full
correlation structure of the original distribution. The covari-
ance intersection algorithm, proposed in [21], can be used to
consistently merge measurements with unknown correlation
and can be used to weight the CLT factors so that their
sum is conservative. Clearly, we should be able to do better
than covariance intersection because the true correlation in the
original distribution, ?
t
, is known. Covariance intersection,
however, does provide an easy-to-compute lower bound on
the approximation performance to which we can compare
additional methods. Additionally, it provides a strictly-feasible
starting point for more complex optimization problems. The
approximate target information produced by covariance inter-
section is deﬁned as a convex combination of the CLT factors,
˜
?
CI
(w) =
X
i
w
i
?
i
, (7)
where each w
i
scales the information added by each factor.
The optimal weights can then be found by solving the convex
semideﬁnite program,
minimize
w
f
KL
(
˜
?
CI
(w))
subject to
X
i
w
i
= 1.
(8)
C. Weighted Factors
Because the true distribution is known, we can relax co-
variance intersection’s requirement that weights sum to one.
Instead we constrain the weights to be between zero and one,
and add the conservative constraint proposed in [10] and [16].
We refer to this formulation as “weighted factors,” and its
approximate target information is deﬁned as
˜
?
WF
(w) =
X
i
w
i
?
i
. (9)
The optimal weights can then be found by solving
minimize
w
f
KL
(
˜
?
WF
(w))
subject to 0≤ w
i
≤ 1, ?i
?
t
≥
˜
?
WF
(w),
(10)
which is again a convex semideﬁnite program.
D. Weighted Eigenvalues
Instead of weighting each factor by a single value, ﬁner
grained control can be achieved by weighting each factor
along its principal axes independently. We refer to this for-
mulation as “weighted eigenvalues.” Each factor has an eigen-
decomposition given by
?
i
=

u
i
1
··· u
i
qi

?
?
?
?
i
1
0 0
0
.
.
. 0
0 0 ?
i
qi
?
?
?
?
?
?
?
u
i
1
?
.
.
.
u
i
qi
?
?
?
?
?
. (11)
Using the eigen-decomposition of each factor we can write
the approximate target information as
˜
?
WEV
(w) =
X
i
qi
X
j=1
w
i
j
?
i
j
u
i
j
u
i
j
?
=
X
k
w
k
?
k
u
k
u
?
k
,
(12)
with the optimal weights found by solving
minimize
w
f
KL
(
˜
?
WEV
(w))
subject to 0≤ w
k
≤ 1, ?k
?
t
≥
˜
?
WEV
(w).
(13)
857
Duderstadt
?20 ?10 0 10 20 30 40 50
?70
?60
?50
?40
?30
?20
?10
0
x [m]
y [m]
(a) Covariance Intersection
?20 ?10 0 10 20 30 40 50
?70
?60
?50
?40
?30
?20
?10
0
x [m]
y [m]
(b) Weighted Factors
?20 ?10 0 10 20 30 40 50
?70
?60
?50
?40
?30
?20
?10
0
x [m]
y [m]
(c) Weighted Eigenvalues
?20 ?10 0 10 20 30 40 50
?70
?60
?50
?40
?30
?20
?10
0
x [m]
y [m]
(d) Chow-Liu Tree
Intel
?30 ?20 ?10 0 10
?10
?5
0
5
10
15
20
25
x [m]
y [m]
(e) Covariance Intersection
?30 ?20 ?10 0 10
?10
?5
0
5
10
15
20
25
x [m]
y [m]
(f) Weighted Factors
?30 ?20 ?10 0 10
?10
?5
0
5
10
15
20
25
x [m]
y [m]
(g) Weighted Eigenvalues
?30 ?20 ?10 0 10
?10
?5
0
5
10
15
20
25
x [m]
y [m]
(h) Chow-Liu Tree
Fig. 4: Sample 3-? uncertainty ellipses for the Duderstadt graph with 50.0% node removal (top row) and the Intel graph with 33.3% node
removal (bottom row). True marginal ellipses are shown in cyan, while the marginal ellipses from the approximate distribution are shown
in red. For the Duderstadt graph both weighted factors (b) and weighted eigenvalues (c) produce distributions very similar to the CLT (d)
while remaining conservative. For the Intel graph both weighted factors (f) and weighted eigenvalues (g) produce similar distributions that
are noticeably more conservative than the CLT (h).
Notethatthenumberofoptimizationvariableshasincreasedin
comparisontothecovarianceintersectionandweightedfactors
formulations. As will be demonstrated in §IV, this results in
a signiﬁcant increase in the computational cost.
E. Implementation Considerations
Each of the proposed semideﬁnite programs are convex
and can be efﬁciently solved using interior point methods
[22–24]. Interior point methods require that a strictly-feasible
starting point be found before optimization, i.e., an initial
approximation, where 0 < w
i
< 1, ?i and ?
t
>
˜
?
t
(w).
Covariance intersection with uniform weights provides an
easy-to-compute strictly-feasible starting point, and is used in
all experiments.
For low rank target information, the conservative constraint,
?
t
?
˜
?
t
(w), is semideﬁnite and will have at least one zero
eigenvalue, and therefore, no strictly-feasible starting point
exists. Additionally, it prevents the evaluation of the optimiza-
tion problem’s gradient and Hessian. Instead, the conservative
constraint is implemented as
?
t
+?I≥
˜
?
t
(w),
so that a strictly-feasible starting point exists and the gradient
and Hessian can be evaluated. Our experimental evaluation
indicates that the actual value of ? has very little effect on the
results. All experiments are performed with ? = 0.1, though
values 10
?5
≤ ?≤ 1 produced nearly equivalent results.
IV. EXPERIMENTAL RESULTS
Toevaluatetheproposedmethods,wetesttheirperformance
on a variety of SLAM graphs (summarized in Fig. 3 and
Table I), including:
• Two standard 3-degree of freedom (DOF) pose-graphs,
Intel Lab and Killian Court.
• The Victoria Park 3-DOF graph with both poses and
landmarks.
• Two 6-DOF pose-graphs built using data from a Segway
ground robot equipped with a Velodyne HDL-32E laser
scannerastheprimarysensingmodality,DuderstadtCen-
ter and EECS Building.
• A 6-DOF graph produced by a Hovering Autonomous
Underwater Vehicle (HAUV) performing monocular
SLAM for autonomous ship hull inspection [25],
USS Saratoga.
The proposed algorithms were implemented using iSAM
[7, 19, 26] as the underlying optimization engine. For each
graph, the original full graph is ﬁrst optimized using iSAM.
Then the different node removal algorithms are each used to
remove a set of nodes evenly spaced throughout the trajectory.
Finally, the graphs are again optimized in iSAM.
For each experiment the true marginal distribution is re-
covered by obtaining the linearized information matrix from
the full graph about the optimization point and performing
Schur complement marginalization. This provides a ground-
truthdistributionthatwecandirectlycompareourconservative
858
TABLE II: Experimental KLD Results
Covariance Intersection Weighted Factors Weighted Eigenvalues Chow-Liu Tree
Dataset % Removed KLD CLT Ratio KLD CLT Ratio KLD CLT Ratio KLD
Intel Lab 33.3% 45,954.10 175.85? 4,191.73 16.04? 3,439.63 13.16? 261.33
Killian Court 66.7% 984.30 20.58? 194.85 4.07? 186.40 3.90? 47.82
Victoria Park 75.0% 3,062.49 15.85? 835.38 4.32? 599.00 3.10? 193.24
Duderstadt Center 50.0% 14,552.30 3,038.06? 60.82 12.70? 33.54 7.00? 4.79
EECS Building 25.0% 1,671.48 357.15? 32.31 6.90? 18.49 3.95? 4.68
USS Saratoga 33.3% 11,290.80 13,441.43? 12.13 14.44? 4.63 5.51? 0.84
distribution against. In order to provide a benchmark, the CLT
approximation as proposed in [14] is also evaluated.
The results for each method, in terms of KLD, are shown in
Table II.The“CLTratio”columnsprovideadirectcomparison
with the CLT, which is not guaranteed to be conservative, but
serves as a baseline as it is the minimum KLD distribution
among all spanning trees. As one would expect, covariance in-
tersection produces a very high KLD because it is excessively
conservative. The weighted factors formulation improves the
KLD signiﬁcantly with respect to covariance intersection,
while the weighted eigenvectors formulation improves the
KLD further still.
For the Duderstadt, EECS, and Saratoga graphs the sub-
jective difference in the quality of the estimates is very
small, with weighted factors, weighted eigenvalues, and the
CLT producing visually indistinguishable results, see Fig. 4
top row. For the Intel dataset, and to a lesser extent the
Killian and Victoria datasets, there appears to be more room
for improvement, with a noticeable difference between the
weighted eigenvalues result and the CLT for the Intel graph,
see Fig. 4 bottom row.
Toevaluatethe“conservativeness”oftheproposedmethods,
we plot the minimum eigenvalue of the covariance-form
consistencyconstraintforeachnodemarginal,i.e.min?(
˜
?
ii
?
?
ii
), depicted in Fig. 5. Values below zero indicate overcon-
ﬁdence, with only the CLT producing overconﬁdent results.
Covarianceintersection,weightedfactors,andweightedeigen-
valuesallproduceconservativeestimates,witheachproducing
a slightly tighter estimate (closer to zero) than the previous.
Finally, we consider the computational cost of the proposed
methods. A plot showing the node removal time as a function
of the number of variables in the elimination clique is shown
in Fig. 6. The average node removal times for covariance
intersection, weighted factors, weighted eigenvalues and the
CLT were 7, 32, 448, and 5 milliseconds, respectively. Even
though covariance intersection and weighted factors were both
solving optimization problems over the same number of vari-
ables, weighted factors is more expensive. This is due to the
fact that the equally-weighted covariance intersection solution,
used as the initial point for all optimizations, was often
very close to the optimal covariance intersection solution and
therefore,covarianceintersectionconvergedquickly.Weighted
eigenvalues solves a larger optimization problem and therefore
is substantially slower. Still, the weighted eigenvalues formu-
lation will often have signiﬁcantly fewer variables than [10]
(which optimizes all non-zero entries in the upper triangle of
the information matrix) and [16] (which optimizes every entry
100 200 300 400 500 600
0
0.02
0.04
0.06
0.08
0.1
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(a) Intel 33.3% Removed
100 200 300 400 500 600
0
0.01
0.02
0.03
0.04
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(b) Killian 66.7% Removed
500 1000 1500
0
5
10
15
20
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(c) Victoria Park 75.0% Removed
50 100 150 200 250
?2
?1
0
1
2
x 10
?3
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(d) Duderstadt 50.0% Removed
100 200 300 400
?8
?6
?4
?2
0
x 10
?3
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(e) EECS 25.0% Removed
200 400 600 800 1000
0
0.5
1
1.5
2
2.5
3
x 10
?6
Pose Number
Min EV of Consistency Constraint
 
 
CI
WF
WEV
CLT
(f) Saratoga 33.3% Removed
Fig. 5: Minimum eigenvalue of the consistency constraint for each
pose marginal (i.e., min?(
˜
?ii ? ?ii)). Covariance intersection,
weighted factors, and weighted eigenvalues all produce conservative
estimates (values greater than zero), with each producing a slightly
tighter estimate (closer to zero) than the previous.
in the upper triangle of the information matrix) for a given
information matrix size.
We note that the computational cost of the proposed meth-
ods increases quickly with the size of the node removal
cliques. However, as experimentally shown in [15], sparse ap-
proximate node removal maintains small cliques in real-world
SLAM graphs even when removing a very high percentage
of nodes. This, in turn, results in essentially constant node
removal time regardless of the the number of nodes removed
and size of the graph beyond the elimination clique.
V. DISCUSSION
The proposed algorithms are presented in the context of
sparsifying the dense cliques produced by node marginaliza-
tion.However,thesetechniquescouldbeappliedtoportionsof
the graph to perform sparsiﬁcation, without removing nodes.
All of the presented methods start with the Chow-Liu tree
859
10 20 30 40 50 60 70 80 90
10
?4
10
?3
10
?2
10
?1
10
0
10
1
10
2
10
3
Number of Clique Variables
Node Removal Time (sec.)
 
 
CI
WF
WEV
CLT
Fig. 6: Node removal processing time as a function of the number
of variables in the elimination clique. Average node removal times
(solid lines) for covariance intersection, weighted factors, weighted
eigenvalues and the CLT were 7, 32, 448, and 5 milliseconds,
respectively.
as their basis. We believe that this is a reasonable starting
point as it is the minimum KLD spanning tree. However,
there is no guarantee that, after using the optimization based
methods to ensure a conservative estimate, the CLT’s sparsity
pattern remains optimal. Furthermore, other non-tree sparsity
patterns may be of interest. This is one strongly appealing
aspect of the L
1
regularization method proposed in [16] in
that it automatically selects the sparsity pattern.
Finally, there is a trade off between the complexity of the
optimization problem and the accuracy of the approximation
it is able to achieve. Based upon our experimental results, we
feel that the weighted factors formulation provides the best
trade off between KLD and computation time, as weighted
eigenvalues only performs marginally better while being sub-
stantially more computationally expensive.
VI. CONCLUSIONS
This paper explored several optimization-based methods
that can be used to produce a sparse, conservative approxima-
tion of potentials induced by node marginalization in SLAM
graphs. Starting with the sparse, but overconﬁdent, CLT ap-
proximation of the marginalization potential, these methods
adjust the CLT approximation so that it is conservative, while
minimizing the KLD from the true marginalization potential.
Usingresultsfrommultiplereal-worldSLAMgraphs,wehave
shown that the algorithm enforces a conservative approxima-
tion, while achieving low KLD.
REFERENCES
[1] F. Lu and E. Milios, “Globally consistent range scan alignment for
environment mapping,” Autonomous Robots, vol. 4, pp. 333–349, Apr.
1997.
[2] F. Dellaert and M. Kaess, “Square root SAM: Simultaneous localization
and mapping via square root information smoothing,” Int. J. Robot. Res.,
vol. 25, no. 12, pp. 1181–1203, 2006.
[3] S. Thrun and M. Montemerlo, “The graph SLAM algorithm with
applications to large-scale mapping of urban structures,” Int. J. Robot.
Res., vol. 25, no. 5-6, pp. 403–429, 2006.
[4] E. Olson, J. Leonard, and S. Teller, “Fast iterative alignment of pose
graphs with poor initial estimates,” in Proc. IEEE Int. Conf. Robot. and
Automation, Orlando, FL, USA, May 2006, pp. 2262–2269.
[5] R. M. Eustice, H. Singh, and J. J. Leonard, “Exactly sparse delayed-
state ﬁlters for view-based SLAM,” IEEE Trans. Robot., vol. 22, no. 6,
pp. 1100–1114, Dec. 2006.
[6] K. Konolige and M. Agrawal, “FrameSLAM: From bundle adjustment
to real-time visual mapping,” IEEE Trans. Robot., vol. 24, no. 5, pp.
1066–1077, 2008.
[7] M.Kaess,A.Ranganathan,andF.Dellaert,“iSAM:Incrementalsmooth-
ing and mapping,” IEEE Trans. Robot., vol. 24, no. 6, pp. 1365–1378,
Dec. 2008.
[8] S. Thrun, Y. Liu, D. Koller, A. Ng, Z. Ghahramani, and H. Durrant-
Whyte, “Simultaneous localization and mapping with sparse extended
information ﬁlters,” Int. J. Robot. Res., vol. 23, no. 7/8, pp. 693–716,
Jul./Aug. 2004.
[9] M.R.Walter,R.M.Eustice,andJ.J.Leonard,“Exactlysparseextended
information ﬁlters for feature-based SLAM,” Int. J. Robot. Res., vol. 26,
no. 4, pp. 335–359, Apr. 2007.
[10] J. Vial, H. Durrant-Whyte, and T. Bailey, “Conservative sparsiﬁcation
for efﬁcient and consistent approximate estimation,” in Proc. IEEE/RSJ
Int. Conf. Intell. Robots and Syst., San Francisco, CA, USA, Sep. 2011,
pp. 886–893.
[11] R. Eustice, M. Walter, and J. Leonard, “Sparse extended information
ﬁlters: insights into sparsiﬁcation,” in Proc. IEEE/RSJ Int. Conf. Intell.
Robots and Syst., Edmonton, Alberta, Canada, Aug. 2005, pp. 3281–
3288.
[12] J. Folkesson and H. Christensen, “Graphical SLAM—a self-correcting
map,” in Proc. IEEE Int. Conf. Robot. and Automation, New Orleans,
LA, April 2004, pp. 383–390.
[13] H. Kretzschmar and C. Stachniss, “Information-theoretic compression
of pose graphs for laser-based SLAM,” Int. J. Robot. Res., vol. 31, pp.
1219–1230, 2012.
[14] N. Carlevaris-Bianco and R. M. Eustice, “Generic factor-based node
marginalization and edge sparsiﬁcation for pose-graph SLAM,” in Proc.
IEEEInt.Conf.Robot.andAutomation,Karlsruhe,Germany,May2013,
pp. 5728–5735.
[15] ——, “Long-term simultaneous localization and mapping with generic
linear constraint node removal,” in Proc. IEEE/RSJ Int. Conf. Intell.
Robots and Syst., Nov. 2013, pp. 1034–1041.
[16] G. Huang, M. Kaess, and J. J. Leonard, “Consistent sparsiﬁcation
for graph optimization,” in Proc. European Conf. Mobile Robotics,
Barcelona, Spain, Sep. 2013.
[17] C. Chow and C. N. Liu, “Approximating discrete probability distribu-
tions with dependence trees,” IEEE Trans. on Info. Theory, vol. 14, pp.
462–467, 1968.
[18] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. Cambridge,
MA: MIT Press, Sep. 2005.
[19] M. Kaess, H. Johannsson, D. Rosen, N. Carlevaris-Bianco,
and J. Leonard, “Open source implementation of iSAM,”
http://people.csail.mit.edu/kaess/isam, 2010.
[20] C. R. Rao and S. K. Mitra, Generalized Inverse of Matrices and its
Applications. John Wiley & Sons, 1971.
[21] S. J. Julier and J. K. Uhlmann, “A non-divergent estimation algorithm
in the presence of unknown correlations,” in Proc. Amer. Control Conf.,
Albuquerque, NM, USA, Jun. 1997, pp. 2369–2373.
[22] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge
University Press, 2004.
[23] L. Vandenberghe and S. Boyd, “Semideﬁnite programming,” SIAM
Review, vol. 38, no. 1, pp. 49–95, 1996.
[24] L. Vandenberghe, S. Boyd, and S.-P. Wu, “Determinant maximization
with linear matrix inequality constraints,” SIAM J. on Matrix Analysis
and Applicat., vol. 19, no. 2, pp. 499–533, 1998.
[25] F.S.Hover,R.M.Eustice,A.Kim,B.Englot,H.Johannsson,M.Kaess,
and J. J. Leonard, “Advanced perception, navigation and planning for
autonomous in-water ship hull inspection,” Int. J. Robot. Res., vol. 31,
no. 12, pp. 1445–1464, Oct. 2012.
[26] M. Kaess and F. Dellaert, “Covariance recovery from a square root
information matrix for data association,” Robot. and Autonomous Syst.,
vol. 57, pp. 1198–1210, Dec. 2009.
860
