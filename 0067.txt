Attention-driven Object Detection and Segmentation of Cluttered Table
Scenes using 2.5D Symmetry
Ekaterina Potapova, Karthik M. Varadarajan, Andreas Richtsfeld, Michael Zillich and Markus Vincze
Automation and Control Institute
Vienna University of Technology
1040 Vienna, Austria
fpotapova,varadarajan,ari,zillich,vinczeg@acin.tuwien.ac.at
AbstractÑ The task of searching and grasping objects in
cluttered scenes, typical of robotic applications in domestic
environments requires fast object detection and segmentation.
Attentional mechanisms provide a means to detect and priori-
tize processing of objects of interest. In this work, we combine
a saliency operator based on symmetry with a segmentation
method based on clustering locally planar surface patches,
both operating on 2.5D point clouds (RGB-D images) as input
data to yield a novel approach to table-top scene segmentation.
Evaluation on indoor table-top scenes containing man-made
objects clustered in piles and dumped in a box show that our
approach to selection of attention points signiÞcantly improves
performance of state-of-the-art attention-based segmentation
methods.
I. INTRODUCTION
Segmentation of objects from a static scene is a crucial
step in many robotic tasks. Different approaches have been
proposed to tackle the object segmentation problem, which
can be broadly classiÞed into two groups: discriminative and
agglomerative. Discriminative segmentation algorithms tend
to classify the whole scene at once and assign a label to every
pixel [1], [2], [3]. Agglomerative segmentation algorithms
grow regions from a seed point to segment the foreground
object. Active segmentation or attention-driven segmentation
are agglomerative methods that segment images starting from
a Þxation point or region [4], [5].
Segmentation in cluttered scenes is a critical module in
robotics, with the need to Þnd task relevant objects quickly
amongst a possibly large number of distractors.
In this paper, we present a novel method for attention-
driven segmentation for cluttered table scenes. The contri-
bution of this paper is two-fold: Þrst, we employ a novel
object detection and selection algorithm based on attention
points from 2.5D symmetry saliency maps Þrst presented in
[6]. Secondly, we introduce a segmentation procedure based
on clustering of planar surface patches using color similarity
and a notion of compactness. We evaluate our approach on
two databases consisting of different types of table scenes
ranging from simple to complex scenarios. We show that the
*The research leading to these results has received funding from the
Austrian Science Fund (FWF) under grant agreement No. TRP 139-N23
InSitu and from the European CommunityÕs Seventh Framework Programme
FP7/2007-2013 under grant agreements No. 600623, STRANDS and No.
610532, SQUIRREL.
(a) Mishra et al. [4] (b) Proposed approach
Fig. 1: An example of active segmentation for a cluttered
table scene. Fixation points are shown in black with num-
bering reßecting the order of attention shift. As can be
seen, the approach of Mishra et al. (a) does not segment
all attended objects properly, while the proposed approach
(b) successfully deals with the scene complexity.
proposed approach works better than existing approaches for
attention-driven segmentation (Fig. 1).
The paper is organized as follows: In Section II, we
review related work. Section III and IV describe the proposed
algorithm in detail. The evaluation in Section V shows the
beneÞts of our algorithm. Section VI concludes the paper
with a discussion about future directions of research.
II. RELATED WORK
The focus of this paper is segmentation of indoor table
scenes typical of robotic task environments. Therefore, we
primarily concentrate on the work developed for RGB-D
data. A number of discriminative segmentation algorithms
use depth information to boost segmentation performance
for complex indoor scenarios. Such algorithms include those
proposed in [7], [8], [9], [10], [11], [12].
The concept of active segmentation or attention-driven
segmentation was Þrst presented by Aloimonos et al. [13].
It was argued that the human visual system investigates and
observes the scene by a set of Þxations that are followed by
segmentation. Attention-driven segmentation usually has two
stages. During the Þrst stage, a selection mechanism detects
candidate object locations. During the second stage, the
detected objects are segmented. The attention-driven segmen-
tation approaches in [5], [14], [4] propose different solutions
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4946
for the two stages. Given that the selection mechanism is
directed by a speciÞc search task [15] in the case of robotic
applications, attention-driven segmentation Þnds greater use
than discriminative segmentation approaches.
In [14], Mishra et al. proposed a framework for active
segmentation, where an object is segmented using object
boundaries, given a Þxation point on the object. Though
originally in the paper no strategy for detection of Þxation
points was proposed, it was discussed that visual attention
mechanisms [16], [17], [18] can be used for such a selection.
Later, [4] extended the active segmentation approach to use
the concept of Òsimple objects and border ownershipÓ, which
is deÞned using depth, color and/or motion information about
the scene. A new strategy for the calculation of Þxation
points was also proposed. Kootstra et al. [5] proposed
an attention-driven graph-cut segmentation. Objects are lo-
calized with Þxation points extracted from 2D symmetry
saliency maps. An energy minimization function is applied
to depth and color along with a support plane constraint for
segmentation. It is worth mentioning that both the above
segmentation algorithms [5], [4] were developed speciÞcally
for table top scenes and require information about the support
plane. However, both approaches fail to segment objects if
the scene consists of multiple occluded and cluttered objects,
having several colors and textures. These types of scenes are
common in domestic robotic tasks and need to be resolved
correctly to enable manipulation of objects.
As mentioned earlier, the problem of Þxation points selec-
tion stays open with numerous solutions. Selecting Þxations
as attention points of the saliency map is one of the widely
used approaches [5], [6]. Potapova et al. [6] proposed the use
of 2.5D symmetry based saliency maps to extract Þxation
points for segmentation. It was shown that 2.5D symmetry-
based saliency maps capture the properties of the scene better
than 2D based saliency maps.
In this paper, we adopt the idea of Potapova et al. [6]
for object detection and extend it with a novel attention
point selection algorithm. Furthermore, a novel segmentation
algorithm is introduced, using the Þxation point to enable
clustering of planar surface patches, similar to [8], using
color similarity and the notion of compactness.
III. OBJECT DETECTION
In this section, we describe the detection of good object
candidate locations in a cluttered scene. Einhauser et al. [19]
showed that objects attract human attention better than early
vision saliency features. Symmetry is one of the character-
istics of many natural as well as human-made objects and
at the same time a powerful attentional cue [20]. Therefore,
we based our object detection strategy on the calculation of
a 2.5D reßective symmetry-based saliency map.
A. Saliency Map from 2.5D Symmetries
We follow the algorithm in [6] to generate the reßective
symmetry based saliency map, starting from a 2.5D point
cloud, i.e. a rectangular array of depth values. Each point p
in the point cloud P is indexed by image coordinates (i;j),
has color (r;g;b), and is characterized by a set of values
(x;y;z; n), where (x;y;z) are spatial coordinates, and n
is the estimated surface normal at that point. Normals are
calculated by locally Þtting planes to the 50 nearest points.
Following ideas by Minovic et al. [21] and Sun et al. [22],
Potapova et al. [6] proposed to estimate the local amount of
symmetry s(p) at point p on the neighborhood N(p). Mi-
novic et al. [21] showed that planes of reßective symmetries
are perpendicular to the directions of the objectÕs principal
axes. The principle axes of a 3D model can be detected
from the Extended Gaussian Image (EGI) created from point
normals as was proposed by Sun et al. [22].
In our scenario the EGI for pointp is created from normals
of the points in the neighborhood N(p), which is a 10 10
pixel window around p. The principal axesfu
1
;u
2
;u
3
g of
the local surface in the neighborhood N(p) are estimated
using Principal Component Analysis (PCA) on the EGI.
The corresponding reßective symmetry planesf
1
;
2
;
3
g
are planes going through the point p perpendicular to the
respective principal axes.
For a given reßective plane 
i
(i = 1; 2; 3) the neighbor-
hood N (p) is divided into two neighborhoods N
i1
(p) and
N
i2
(p), so that8p
0
2N (p):
p
0
2
(
(N
i1
(p)) if d (p
0
;
i
)> 0
(N
i2
(p)) if d (p
0
;
i
)< 0
(1)
where d (p
0
;
i
) is the signed Euclidean distance from point
p
0
to the plane 
i
.
For each neighborhood N
ij
(j = 1; 2) the mean point p
ij
and the mean normal n
ij
are calculated.
The amount of local reßective symmetry for the point p
over the reßective plane 
i
is then given by:
s (p) = max
i=1;2;3
f

i
(N(p);p)g (2)


i
(N(p);p) =e
 4zi
e
 4di
!
1
!
2
(3)
4d
i
represents the absolute difference between distances
from mean points p
i1
and p
i2
to the reßective plane 
i
:
4d
i
=jjd (p
i1
;
i
)j jd (p
i2
;
i
)jj (4)
where d
 
p
ij
;
i

is the distance from mean point p
ij
to the
plane
i
.4d
i
reßects the fact that we are searching for points
where the parts of the neighborhood left and right of the
symmetry plane are positioned symmetrically.
4z
i
in eq. 3 represents the absolute difference between
z-coordinates (depth values) of mean pointsp
i1
andp
i2
and
therefore favors symmetries facing the view point
4z
i
=


z
p
i1
 z
p
i2


(5)
!
1
in eq. 3 measures the complanarity between the linel
i
connecting p
i1
and p
i2
and the plane spanned between two
mean normals n
i1
and n
i2
:
!
1
=








n
i1
n
i2
jjn
i1
n
i2
jj
l
i








(6)
4947
(a) Saliency Map (b) Attention points (c) Surface patches (d) Segmentation result
Fig. 2: Object detection process: (a) shows saliency map calculated from 2.5D symmetries (shown in green superimposed
on the original image); (b) shows attention points from symmetry (red) and skeletal line segments (green) (please note
that for visualization purposes both attention points and skeletons were dilated); (c) shows planar surface patches and (d)
shows segmentation result with respective attention points. Please note that only Þrst three attention points and respective
segmentations are shown.
l
i
=
p
i1
 p
i2
jjp
i1
 p
i1
jj
(7)
!
2
in eq. 3 measures the similarity between mean normal
directions based on the symmetry operator from Reisfeld et
al. [23] and is calculated as follows:
!
2
= (1  cos (
1
+
2
)) (1  cos (
1
 
2
)) (8)
where
j
is the angle between mean normaln
ij
andl
i
. This
term is largest in regions, where the normals are oriented
completely opposite to each other and smallest in regions,
where normals have the same orientation (e. g. ßat surfaces).
We take the product of all four terms in eq. 3, since we
are searching for local symmetries with neighborhoods that
produce high values for all four quantities.
B. Multi-Scale Symmetry-Based Saliency Map
The above symmetry is deÞned locally over a neighbor-
hood around a given point. To capture symmetries at different
scales we calculate saliency maps on a Gaussian pyramid of
depth images and then sum them using across scale addition
[17]:
S =
Ln
M
l=L1
s
l
(9)
where L
1
and L
n
are the Þnest and the coarsest scales of
the pyramid. In our experiments we used four levels of the
pyramid. Finally, saliency map S is normalized to the range
[0; 1]. Figure 2a shows an example of saliency map from
2.5D symmetries.
C. Attention Points from Symmetry
From the multi-scale symmetry-based saliency map S we
extract salient regions as 8-neighbor connected components
of pixels with saliency value bigger than zerofC
k
g. The
average saliency S
k
of each salient region C
k
is computed
as
S
k
=
1
n
k
X
p2C
k
S (p) (10)
where n
k
is the number of pixels in the region, S (p) is the
saliency value of the point p.
Small errors in normal calculation due to sensor noise
accordingly introduce noise in the saliency map. To this end
we remove salient regions C
k
where S
k
< 
sal
. In our
experiments we set 
sal
to 10% of the maximum saliency
value. We plan to use a better noise model for the sensor to
eliminate the need for this threshold.
The skeleton T
k
is extracted from the connected compo-
nentC
k
. Symmetry attention pointsff
k
g are extracted from
the skeleton T
k
as junction points, if they exist, or as mid-
points for simple skeletal line segments. Figure 2b shows
examples of attention pointsff
k
g and skeletons T
k
.
IV. OBJECT SEGMENTATION
Given attention points we want to segment the scene
incrementally. We Þrst cluster points into planar patches
based on their normals similar to Richtsfeld et al. [8]. We
then cluster these patches beginning from the attention points
by connecting similar patches as long as a given object-ness
measure is valid.
A. Clustering Normals
Neighboring points are clustered to uniform patches with-
out discontinuities using point normals. Normal clustering
starts at the point with lowest curvature and greedily as-
signs neighboring points as long as they Þt to the initial
plane model. The algorithm iteratively creates planar surface
patches until all points belong to some plane or are identiÞed
as noise. After normal clustering we obtain a set of planar
surface patchesf
t
g (Figure 2c).
B. Clustering Patches
Patchesf
t
g are now greedily clustered into object hy-
potheses
k
. Object hypotheses are initialized using the sym-
metry attention pointsff
k
g, which are sorted in decreasing
order of S
k
. Given a symmetry point f
k
, all patches 
t
bordering this point (with a 5 pixel radius) form an initial
cluster. Patches are then greedily added to the cluster subject
to a color and compactness constraint. Once a cluster cannot
be extended further, the next cluster is initiated from the next
attention point.
4948
0.002 0.004 0.006 0.008 0.01
Compactness threshold
0.7
0.8
0.9
1
F?score
 
 
First
Best
All
TOSD Willow
Fig. 3: F -score for proposed segmentation algorithm using
different thresholds for compactness 
com
for TOSD and
Willow datasets. As can be seen from the plot, the optimal
segmentation is achieved when threshold 
com
= 0:005.
1) Color Similarity: A new patch 
0
t
is considered to
be a part of the object only if its color model is similar
to the already existing model for the object. The color
similarity CS
in
between a new patch 
0
t
and an object 
k
is computed as the Chi-square distance between their HSV
color histograms.
CS
in
(
k
;
0
t
) =
2
(H(
k
);H(
0
t
)) (11)
We also calculate the similarity CS
out
between the patch

0
t
and not-object, i. e. a mask 
0
k
surrounding the object,
which is deÞned as the part of the image outside an enlarged
mask 
k
of the object. This enlarged mask is created from
the 
k
by doubling its area.
CS
out
(
0
k
;
0
t
) =
2
(H(
0
k
);H(
0
t
)) (12)
The color constraint is fulÞlled if CS
out
<CS
in
.
2) Object Compactness: A new patch patch 
0
t
is only
added to an object, if its addition does not violate the
compactness measure  of the object. Compactness  is
calculated as the mean of the shortest distances of the object
points to the visible surfaces of the objectÕs 3D convex hull.
Let a setfp
ki
g be object points, V
k
be the corresponding
objectÕs convex hull, and v
j
be a set of faces facing the
viewpoint. Compactness measure  is then calculated as:
 =
1
n
k
X
p
ki
d
min
(p
ki
;V
k
) (13)
where n
k
is the number of object points and d
min
(p
ki
;V
k
)
is the shortest distance from the point to any visible face
d
min
(p
ki
;V
k
) = min
j
d(p
ki
;v
j
) (14)
The compactness constraint for a patch 
t
is fulÞlled if
compactness measure of the object plus patch 
t
is smaller
than the given threshold  < 
com
. As shown in the
evaluation section the optimal value for the compactness
threshold 
com
= 0:005. Examples of segmented objects
using these constraints are shown in Figure 2d and Figure 4.
V. RESULTS AND EVALUATION
We evaluated our segmentation algorithm on two pub-
licly available databases: the Table Object Scene Database
(TOSD)
1
and the Willow Garage Table Objects Database
(Willow)
2
.
Other databases as Caltech256, Pascal VOC, LabelMe,
BerkeleyÕs B3DO, NYUÕs Depth Dataset, UWÕs RGB-D
Object Dataset do not cater to our speciÞc task of cluttered
table scene segmentation.
TOSD database is targeted towards segmentation evalua-
tion and consists of scenes with varied object conÞguration
complexities. It is composed of images with complex and
cluttered scenes, as well as scenes where only several boxes
or other simple objects are presented, as shown in Fig. 4.
The TOSD database consists of 111 scenes for training and
131 scenes for testing.
The Willow Garage database was originally presented as
a benchmark for object recognition for the ÒWillow Garage:
Solutions in Perception ChallengeÓ. While the database was
created for the task of object recognition, it still serves
as a good benchmark for the performance evaluation of
segmentation algorithms. The Willow database consists of
175 images taken from the challenge Þnal test set.
Labeling for both databases is in the form of precise seg-
mentation mask contours as opposed to bounding boxes. This
makes the evaluation more precise and allows to evaluate
how algorithms perform in terms of under-segmentation and
over-segmentation.
Evaluation was carried out by varying two speciÞc aspects
Ð namely, object detection (i. e. placing attention points
on objects) and object segmentation. We do not attempt to
directly measure the quality of object detection, but instead
present the effect of choices in the object detection method-
ology on our object segmentation approach. In addition, the
evaluation was performed to compare the performance of
our approach against several state-of-the-art segmentation
approaches.
A. Object Detection Strategies
In our work, we applied several strategies for object
detection in order to estimate their inßuence on the object
segmentation. As described earlier, the primary object de-
tection strategy used in our pipeline involves the generation
of saliency maps from 2.5D symmetries. In this strategy
(TJ3D), attention points are selected as points of T-Junctions
(or mid-points for simple lines) in symmetry lines extracted
from the 2.5D symmetry-based saliency maps (Figure 2b).
The second strategy (WTA3D) employed, extracts attention
points using Winner-Take-All (WTA) [24] from the 2.5D
symmetry-based saliency maps. To see how the use of 2.5D
information improves the quality of a detection strategy, we
also include attention points using Winner-Take-All from 2D
symmetry-based saliency maps [20] (WTA2D).
1
https://repo.acin.tuwien.ac.at/tmp/permanent/
TOSD.zip
2
http://vault.willowgarage.com/wgdata1/vol1/
solutions_in_perception/Willow_Final_Test_Set/
4949
M09+TJ3D M09+WTA3D M11 G10 Proposed Algorithm
Fig. 4: Visual comparison of different segmentation algorithms. Row 1 shows segmentation results for Willow database
for different segmentation algorithms and rows 2-7 show segmentation results for TOSD database. Segmentation masks and
attention points are shown in different colors with respective numbering reßecting the order of attention shift. Our results are
shown in the last column. As can be seen, all algorithms except the proposed approach have difÞculties handling cluttered
table scenes.
B. Object Segmentation
To evaluate the attention-based aspect of the algorithm
we performed comparison against an attention-driven active
segmentation algorithm proposed by Mishra et al. [14]
(M09), as well as its extension which uses depth information
as described in [4] (M11).
Though it is clearly not fair to compare our approach
to algorithms that use only color information, it is still
interesting to see how the performance differs. Interactive
segmentation algorithms [25], [26], [27] require user input
4950
TOSD Willow
Segmentation All Best First All Best First
Mean Std Mean Std Mean Std Mean Std Mean Std Mean Std
M09+TJ3D 0.59 0.07 0.60 0.06 0.59 0.07 0.76 0.05 0.78 0.04 0.76 0.05
M09+WTA2D 0.54 0.07 0.65 0.05 0.52 0.06 0.68 0.07 0.85 0.02 0.72 0.05
M09+WTA3D 0.57 0.07 0.62 0.06 0.58 0.06 0.74 0.05 0.80 0.04 0.75 0.05
M11 0.57 0.08 0.66 0.06 0.62 0.07 0.82 0.06 0.88 0.03 0.86 0.04
G10 0.47 0.08 0.50 0.08 0.47 0.08 0.66 0.08 0.71 0.06 0.68 0.07
Proposed Algorithm 0.80 0.05 0.81 0.04 0.80 0.04 0.964 0.010 0.974 0.010 0.970 0.011
TABLE I: F -score for different segmentation algorithms evaluated on TOSD and Willow datasets.
(e. g. bounding box as in [25]). In scenarios where it is not
possible for a user to provide input, the user behavior can be
simulated by a computational model of the visual attention
system [28], [29]. Therefore, we selected a state-of-the-art
interactive segmentation algorithm presented by Gulshan et
al. [26] (G10) to compare with our algorithm. The algo-
rithm proposed in [26] requires strokes of foreground and
background as input. Foreground strokes in our evaluation
were simulated as twice dilated skeleton lines from saliency
maps. Background strokes were simulated as rectangles near
the image border 20% smaller than the size of the original
image.
The output segmentation masks are compared to the
ground truth labeling in terms of the F -measure deÞned as
2PR=(P +R). We calculated precision P as the fraction
of the segmentation mask overlapping with the ground truth
and recall R as the fraction of the ground truth overlapping
with segmentation mask.
Segmentation algorithm M09 was evaluated using object
detection strategies TJ, WTA2D and WTA3D, mentioned
earlier. Segmentation algorithm M11 was evaluated with its
own object detection strategy, because this strategy is an
intrinsic part of the algorithm. Segmentation algorithm G10
was evaluated using symmetry lines as foreground strokes.
Evaluation results are presented in Table I.
Note that the attention mechanism cannot rule out that
several attention points come to lie on the same object. In
this case, each attention point leads to a possibly different
segmentation for an object. Therefore, we calculated three
F -scores: the label Þrst in Table I refers to the segmentation
from the Þrst attention point, best refers to the best segmen-
tation w.r.t. ground truth, and all refers to the average score
over all segmentations for an object. If Þrst is lower than best
this means that the attention points are not optimal. Ideally
the Þrst attention point leads to the best segmentation. If all is
signiÞcantly lower than best this means that the segmentation
algorithm depends a lot on the position of the attention point.
AllF -scores in Table I are averaged over all objects and all
scenes.
The proposed segmentation algorithm depends on the
value of the threshold
com
. To Þnd the optimal value of the
threshold, we evaluatedF -scores against threshold values for
both databases. As can be seen from Figure 3, the optimal
value is 0.005, balancing between over-segmentation (smaller
values) and under-segmentation (larger values). Note that the
scenes in the Willow Garage database are simpler (isolated
standing objects), so that a further increase in
com
does not
lead to under-segmentation and performance stays constant.
The highest value of F -score obtained was 0.81 for TOSD
at this optimal value of 
com
. As can be seen from Table
I, the primary object detection strategy (TJ3D) proposed
in this paper results in improved performance for all types
of segmentation algorithms compared to other detection
strategies. Evaluation results also show that our combined
approach of detection and segmentation performs better on
both databases than state-of-art segmentation algorithms.
Results for G10 show that color-only segmentation cannot
handle complicated table scenarios without good user in-
put. Figure 4 shows visual segmentation outputs for some
segmentation strategies. Row 1 shows an example when
different attention points can give different segmentation
results for the same object. It can be seen that the good
results achieved by the proposed approach also correspond to
visually pleasing segmentations compared to other attention-
driven approaches.
VI. CONCLUSION AND FUTURE WORK
In this paper we proposed a novel attention-driven algo-
rithm for cluttered table scene segmentation. We combined
a novel object detection strategy using a saliency operator
based on 2.5D symmetry with attention point estimation
based on symmetry lines and T-Junction points. This was fur-
ther combined with a segmentation approach based on greedy
clustering of planar surface patches using the notion of
compactness and color similarity. Our approach shows good
results on typical cluttered table scenes containing human
made objects with an F -score of 81%. We have shown that
our selection of attention points improves performance of
attention based segmentation methods and that our combined
attention and segmentation approach improves over state-
of-the-art attention-driven segmentation approaches. Future
work will lie in the area of attention-driven segmentation of
more complex scenes directed by task speciÞcations.
REFERENCES
[1] P. F. Felzenszwalb and D. P. Huttenlocher, ÒEfÞcient Graph-Based Im-
age Segmentation,Ó International Journal of Computer Vision (IJCV),
vol. 59, no. 2, pp. 167Ð181, 2004.
[2] J. Shi and J. Malik, ÒNormalized Cuts and Image Segmentation,Ó IEEE
Transactions on Pattern Analysis and Machine Intelligence (PAMI),
vol. 22, no. 8, pp. 888Ð905, 2000.
[3] P. Arbelaez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and J. Malik,
ÒSemantic Segmentation using Regions and Parts,Ó in IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), 2012, pp.
3378Ð3385.
4951
[4] A. K. Mishra and Y . Aloimonos, ÒVisual Segmentation of Simple
Objects for Robots,Ó in Robotics: Science and Systems (RSS), 2011.
[5] G. Kootstra, N. Bergstr¬ om, and D. Kragic, ÒFast and Automatic
Detection and Segmentation of Unknown Objects,Ó in IEEE-RAS
International Conference on Humanoid Robots (Humanoids), 2010,
pp. 442Ð447.
[6] E. Potapova, M. Zillich, and M. Vincze, ÒLocal 3D Symmetry for
Visual Saliency in 2.5D Point Clouds,Ó in Asian Conference on
Computer Vision (ACCV), 2013, pp. 434Ð445.
[7] A.
¬
Uckermann, R. Haschke, and H. Ritter, ÒReal-Time 3D Seg-
mentation of Cluttered Scenes for Robot Grasping,Ó in IEEE-RAS
International Conference on Humanoid Robots (Humanoids), 2012,
pp. 1734Ð1740.
[8] A. Richtsfeld, T. M¬ orwald, J. Prankl, M. Zillich, and M. Vincze,
ÒSegmentation of Unknown Objects in Indoor Environments,Ó in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 2012, pp. 4791 Ð 4796.
[9] N. Campbell, G. V ogiatzis, C. Hern« andez, and R. Cipolla, ÒAutomatic
3D Object Segmentation in Multiple Views using V olumetric Graph-
cuts,Ó in British Machine Vision Conference (BMVC), 2007, pp. 530Ð
539.
[10] J. Wan, T. Xia, S. Tang, and J. Li, ÒRobust Range Image Segmentation
Based on Coplanarity of Superpixels,Ó in International Conference on
Pattern Recognition (ICPR), 2012, pp. 3618Ð3621.
[11] D. Sedlacek and J. Zara, ÒGraph-Cut Based Point Cloud Segmenta-
tion for Polygonal Reconstruction,Ó in International Conference on
Computer Vision Systems (ICVS), 2009, pp. 218Ð227.
[12] J. Strom, A. Richardson, and E. Olson, ÒGraph-Based Segmentation
for Colored 3D Laser Point Clouds,Ó in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2010, pp. 2131Ð
2136.
[13] J. Aloimonos, I. Weiss, and A. Bandyopadhyay, ÒActive Vision,Ó
International Journal of Computer Vision (IJCV), vol. 1, no. 4, pp.
333Ð356, 1988.
[14] A. Mishra, Y . Aloimonos, and C. L. Fah, ÒActive Segmentation with
Fixation,Ó in IEEE International Conference on Computer Vision
(ICCV), 2009, pp. 468Ð475.
[15] S. Frintrop, P. Jensfelt, and H. I. Christensen, ÒAttentional Landmark
Selection for Visual SLAM,Ó in IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2006, pp. 2582Ð2587.
[16] D. Walther and C. Koch, ÒModeling Attention to Salient Proto-
Objects,Ó Neural Networks, vol. 19, no. 9, pp. 1395Ð1407, 2006.
[17] L. Itti, C. Koch, and E. Niebur, ÒA Model of Saliency-Based Visual
Attention for Rapid Scene Analysis,Ó IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI), vol. 20, no. 11, pp. 1254Ð
1259, 1998.
[18] N. D. B. Bruce and J. K. Tsotsos, ÒSaliency, Attention, and Visual
Search: An Information Theoretic Approach,Ó Journal of Vision, vol. 9,
no. 3, pp. 1Ð24, 2009.
[19] W. Einhauser, M. Spain, and P. Perona, ÒObjects Predict Fixations
Better than Early Saliency,Ó Journal of Vision, vol. 8, no. 14, pp. 1Ð
26, 2008.
[20] G. Kootstra, A. Nederveen, and B. d. Boer, ÒPaying Attention to
Symmetry,Ó in British Machine Vision Conference (BMVC), 2008, pp.
1115Ð1125.
[21] P. Minovic, S. Ishikawa, and K. Kato, ÒSymmetry IdentiÞcation of
a 3D Object Represented by Octree,Ó IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI), vol. 15, pp. 507Ð514, 1993.
[22] C. Sun and J. Sherrah, Ò3D Symmetry Detection Using The Extended
Gaussian Image,Ó IEEE Transactions on Pattern Analysis and Machine
Intelligence (PAMI), vol. 19, pp. 164Ð168, 1997.
[23] D. Reisfeld, H. Wolfson, and Y . Yeshurun, ÒContext Free Attentional
Operators: the Generalized Symmetry Transform,Ó International Jour-
nal of Computer Vision (IJCV), vol. 14, pp. 119Ð130, 1995.
[24] D. K. Lee, L. Itti, C. Koch, and J. Braun, ÒAttention Activates Winner-
Take-All Competition Among Visual Filters,Ó Nature neuroscience,
vol. 2, no. 4, pp. 375Ð381, 1999.
[25] C. Rother, V . Kolmogorov, and A. Blake, ÒGrabCut: Interactive Fore-
ground Extraction Using Iterated Graph Cuts,Ó in ACM SIGGRAPH
2004, 2004, pp. 309Ð314.
[26] V . Gulshan, C. Rother, A. Criminisi, A. Blake, and A. Zisser-
man, ÒGeodesic Star Convexity for Interactive Image Segmentation,Ó
in IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2010, pp. 3129Ð3136.
[27] Y . Boykov and M.-P. Jolly, ÒInteractive Graph Cuts for Optimal
Boundary and Region Segmentation of Objects in N-D Images,Ó in
IEEE International Conference on Computer Vision (ICCV), 2001,
pp. 105Ð112.
[28] Y . Niu, Y . Geng, X. Li, and F. Liu, ÒLeveraging Stereopsis for
Saliency Analysis,Ó in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2012, pp. 454Ð461.
[29] A. Borji and L. Itti, ÒState-of-the-Art in Visual Attention Modeling,Ó
IEEE Transactions on Pattern Analysis and Machine Intelligence
(PAMI), vol. 35, no. 1, pp. 185Ð207, 2013.
4952
