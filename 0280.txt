 
 
? 
Abstract—This paper presents an odometer architecture 
which combines a monocular camera and an inertial measure-
ment unit (IMU). The trifocal tensor geometry relationship 
between three images is used as camera measurement infor-
mation, which makes the proposed method without estimating 
the 3D position of feature point. In other words, the proposed 
method does not have to reconstruct environment. Meanwhile, 
the camera pose corresponding to each of the three images are 
refined in filter to form a multi-state constraint Kalman filter 
(MSCKF). Consequently, this paper proposes a sliding window 
odometry which has a balance between computational cost and 
accuracy. Compared with traditional visual odometry or sim-
ultaneous localization and mapping (SLAM) method, the pro-
posed method not only meets the requirement of odometer in the 
ego-motion estimation, but also suit for real-time application. 
This paper further proposes a random sample consensus 
(RANSAC) algorithm which is based on three views geometry. 
The RANSAC algorithm can effectively reject feature points 
which are mismatch or located on independently moving objects, 
thus it make the overall algorithm capable of operating in dy-
namic environment. Experiments are conducted to show the 
effectiveness of the proposed method in real environment. 
I. INTRODUCTION 
The recent advancement of MEMS technology makes the 
inertial measurement unit (IMU) small in size, low cost and 
power efficient. Thus, inertial navigation system (INS) based 
on an IMU have been widely used to estimate the trajectory of 
vehicle such as smart automobiles, micro air vehicles, robots, 
etc. However, due to error accumulation problem, it is very 
hard to get reliable result by only using the IMU. In order to 
tackle this problem, some INSs rely on GPS signal to peri-
odically correct the IMU. But in some GPS-denied environ-
ments (e.g., indoors, underground, in tunnel, in space, etc.), 
this method does not work. Besides, the quality of GPS signal 
is affected by surrounding environment. 
The camera is another choice to correct the IMU. Cameras 
are low cost, small size and can provide rich information 
about surrounding environment. By tracking feature points 
between several images, the motion of camera can be esti-
mated [1]. The camera and the IMU are complementary 
sensors [2]. The IMU has lower uncertainty of measurement 
at fast motion and the camera can track feature points pre-
cisely at slow motion. Furthermore, the IMU can provide real 
scale which usually cannot be obtained by only using single 
camera. Based on above reasons, a monocular camera and an 
 
Jwu-Sheng Hu is with the Institute of Electrical Control Engineering, 
National Chiao-Tung University, Hsinchu, Taiwan, ROC. and Mechanical 
and Systems Research Laboratories, Industrial Technology Research 
Institute (E-mail: jshu@cn.nctu.edu.tw  ). 
Ming-Yuan Chen was with the Institute of Electrical Control Engineering, 
National Chiao-Tung University, Hsinchu, Taiwan, ROC. (E-mail: 
mychen.sid@gmail.com ). 
 
IMU are used for sensor fusion to estimate the ego-motion in 
this paper. Sensor fusion between the camera and the IMU is 
a popular research topic. It can provide many applications 
such as real scale estimation [3] [4], image de-blurring, IMU 
assisted feature tracking, ground plane detection, ego-motion 
estimation [5]-[9], etc. During the last two years, this research 
topic was trying to solve the initial condition problems [10]. 
The main purpose of this paper is the ego-motion estimation. 
Therefore, we propose an odometer architecture which com-
bines a monocular camera and an IMU. The camera geometry 
constraints between three images are used as camera meas-
urement information, which makes the proposed method 
without estimating the 3D position of feature point. Mean-
while, the camera pose corresponding to each of the three 
images are refined in filter to form a multi-state constraint 
Kalman filter (MSCKF). Consequently, the proposed method 
is a sliding window odometry which has a balance between 
computational cost and accuracy. In order to effectively reject 
feature points which are mismatch or fall on independently 
moving objects, the random sample consensus (RANSAC) 
algorithm based on three views geometry is used to choose 
inliers. 
This paper is organized as follows. Related work is de-
scribed in Section II. In Section III we explain our approach 
for visual assisted IMU odometer in detail. Experiments are 
described in Section IV. Finally, Section V concludes this 
paper. 
II. RELATED WORK 
In general, there are two ways that the ego-motion can be 
estimated in map, which are visual odometry and visual 
simultaneous localization and mapping (SLAM) [11]. Visual 
odometry is a technique that estimates the ego-motion by 
using single or multiple cameras. The term visual odometry 
was first proposed by Nister et al. [12]. The reason for its 
name is similar with wheel odometry that both are estimating 
the motion trajectory by using the measurement information 
of sensors. Visual SLAM can be treated as corresponding 
research of visual odometry. In visual SLAM, Davison et al. 
first proposed one method of visual SLAM which only use 
single camera, and they named it monoSLAM [13]. In order 
to solve the depth estimation problem caused by parametri-
zation of feature points in monoSLAM, Civera et al. proposed 
inverse depth parametrization to describe feature points [14]. 
After that, Civera et al. further proposed 1-point RANSAC 
method to reject unsuitable feature points [15]. The main 
difference between visual SLAM and visual odometry is that 
the goal of visual SLAM is keeping tracking the map of the 
environment, while visual odometry aims at estimating the 
motion trajectory incrementally. Considering our application 
which is the motion trajectory estimation, this paper focuses 
A Sliding-Window Visual-IMU Odometer Based on Tri-focal Tensor 
Geometry 
Jwu-Sheng Hu and Ming-Yuan Chen 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3963
 
 
on visual odometry. 
The most intuitive way to fuse IMU and monoSLAM is 
using the IMU motion model to replace the constant velocity 
motion model assumed in monoSLAM [5]. This method 
belongs to the tightly-coupled sensor fusion architecture. 
However, it is a SLAM-type method. The 3D position of 
feature point is estimated in the filter state vector that causes 
the following two problems: (1) the 3D position of feature 
point is assumed as Gaussian distribution and (2) as the 
number of feature points increase, the size of the filter state 
vector becomes enormous, resulting in more computational 
effort. In order to solve the above problems and focus on 
estimating the ego-motion, Mourikis et al. proposed a method 
that does not require estimating the 3D position of feature 
point in the filter state vector [6]. They use several past 
camera poses as the filter state vector. In measurement update 
step, the camera poses in the filter state vector are used to 
estimate the 3D position of feature point by a least square 
solution and then the 3D position of feature point is 
re-projected back to each camera pose which is used as the 
measurement model. Based on this, they proposed a MSCKF. 
The MSCKF is a sliding window odometry which has a 
balance between computational cost and accuracy.  
In original MSCKF, it still requires a least square solution 
to estimate the 3D position of feature point. However, from 
the viewpoint of visual odometry, estimating the 3D point of 
feature point is not necessary, because it does not focus on 
reconstructing the map. In order to avoid estimating the 3D 
point of feature point, we use the camera geometry con-
straints (epipolar geometry and trifocal tensor) between three 
images as camera measurement information. In the literature, 
method that used trifocal tensor as the measurement model of 
Kalman filter can be traced back to the monocular visual 
odometry proposed by Ying-Kin et al. in 2006 [16]. Rather 
than only using epipolar geometry as measurement model [7], 
using trifocal tensor additionally can provide the consistent 
scale. After that, methods that use the camera geometry con-
straints as measurement information in different types of 
sensors were proposed (e.g., stereo camera [17], stereo cam-
era with IMU [8], monocular camera with IMU [9], etc.). In 
this paper, we use the filter state design methodology of 
MSCKF and the camera geometry constraints between three 
images to propose a sliding window odometry which use a 
monocular camera and an IMU. In the following, we describe 
the algorithm in detail. 
III. DESCRIPTION OF THE METHODOLOGY 
The goal of the proposed method is to estimate the pose of 
the IMU in the global frame. Fig. 1 shows the geometric 
relationships between the IMU frame {I}, the camera frame 
{C} and the global frame {G}. The rotation matrix and posi-
tion pair
( , )
II
CC
Rp
 denotes the transformation of the camera 
frame with respect to the IMU frame. 
( , )
GG
II
Rp
 pair repre-
sents the transformation of the IMU frame in global frame. 
G
Li
p
 is the position of i-th feature point in the global frame. 
 
Fig. 1. IMU-Camera coordinates transition map 
The IMU measures tri-axis acceleration and tri-axis angu-
lar velocity in real metric units. The measurements are given 
by the following equations: 
? ?? ?
   ,    
T G G G I
m I a a m g g
R ? ? ? ? ? ? ? a q a g b n ? ?bn
  (1) 
where 
? ?
G
I
R q
 is the direction cosine matrix (DCM) corre-
sponding to quaternion 
G
I
q
, 
G
a denotes the linear accelera-
tion of the IMU with respect to the global frame, 
G
g
 is the 
gravitational acceleration in the global frame, 
I
? is angular 
velocity of the IMU in the IMU frame, 
a
n
 and 
g
n
 are mod-
eled as white Gaussian noise, 
a
b
 and 
g
b
 are the biases of the 
accelerometer and gyroscope, respectively. 
The state used to describe the IMU usually comprises its 
position, orientation, velocity and the biases of the accel-
erometer and the gyroscope. Let 
IMU
x
 be the IMU state: 
T
G T G T G T T T
IMU I I I a g
?? ?
??
x p q v b b
      (2) 
The vector 
IMU
x
 is also called the IMU true state. In this 
paper, the biases 
a
b
 and 
g
b
 are modeled as Gaussian random 
walk process driven by 
ba
n
 and 
bg
n
, respectively. The IMU 
true-state kinematics describing the time evolution of the 
IMU state is given by the following equations [18]: 
1
  ,  0   ,  
2
                     ,      
T
G G G G I T G G
I I I I I
a ba g bg
?? ? ? ? ?
??
??
p v q q ? va
b n b n
  (3) 
where ? denotes quaternion multiplication. In order to 
minimize the dimension of the filter state vector and achieve 
the purpose of linearization, divide the IMU true state into 
nominal and error state: 
1
ˆ
ˆ ˆ , 1 , 
2
ˆ ˆ
                           ,      
T
G G G G G G T G G G
I I I I I I I I I
a a a g g g
?
??
? ? ? ? ? ?
??
??
? ? ? ?
p p p q q ? v v v
b b b b b b
(4) 
where 
? ?
ˆ ˆ ˆ
ˆ ˆ , , , ,
G G G
I I I a g
p q v b b
 and 
? ?
, , , ,
G G G
I I I a g
? p ? v b b
are 
the IMU nominal state and the IMU error state, respectively. 
Since the mean of the noise is assumed as zero, the IMU 
nominal-state kinematics can be obtained by taking expecta-
tion of the IMU true-state kinematics (3): 
?? I
? ? C
C
x
C
y
C
z
I
x
I
z
I
y
? ? G
G
z
G
x
 
G
y
 
,
II
CC
Rp
,
GG
II
Rp
G
Li
p
3964
 
 
? ?
3 1 3 1
1
ˆ ˆ ˆ ˆ
ˆ ˆ ˆ ˆ  ,    ,  
2
ˆ ˆ
                             ,        
G G G G G G G
I I I I I I
ag
R
??
? ? ? ? ?
??
p v q q ? v q a g
b 0 b 0
   (5) 
with 
ˆ
ˆ
ma
?? a a b
 and 
ˆ
ˆ
mg
?? ? ? b
. By using the IMU true-state 
kinematics (3) and the IMU nominal-state kinematics (5), the 
IMU error-state kinematics can be obtained: 
? ? ? ? ? ?
ˆ  ,      
ˆ ˆ ˆ
ˆ
                           ,       
G G G G
I I I I g g
G G G G G
I I I I a I a
a ba g bg
R R R
??
?
? ?? ? ? ? ??
??
?? ? ? ? ??
??
??
pv ? ? ? bn
v q a ? q b q n
b n b n
   (6) 
A. Structure of the filter state vector 
The filter state vector comprises the IMU state and a his-
tory of last two poses of the camera. The filter state is also 
divided into nominal and error state. The filter nominal state 
is given by: 
1 1 2 2
ˆ ˆ
ˆ ˆ ˆ ˆ
k
T
T G T G T G T G T
k IMU I I I I
??
?
??
x x p q p q
      (7) 
where pair 
? ?
11
ˆ
ˆ ,
GG
II
pq
 denotes the nominal-state pose of the 
IMU corresponding to the last but one pose of the camera, 
while pair 
? ?
22
ˆ
ˆ ,
GG
II
pq
 is corresponding to the last pose of 
the camera. The filter error state is given by: 
1 1 2 2 k
T
T G T G T G T G T
k IMU I I I I
?? ?? ?
??
x x p ? p ?
  (8) 
Since past poses in filter prediction step has no dynamic, 
assume its process model is zero: 
11
22
ˆ
ˆ 0  ,  0
ˆ
ˆ 0  ,  0
GG
II
GG
II
??
??
pq
pq
  and  
11
22
0  ,  0
0  ,  0
GG
II
GG
II
?
?
??
??
p ?
p ?
   (9) 
B. Filter propagation 
In filter prediction step, the nominal state use nominal-state 
kinematic (5)(9) with 4-th order Runge Kutta to predict. The 
prediction of error state is given by 
k c k IMU
??
c
x F x G n
. Dig-
itize 
c
F
 to obtain 
d
F
 by using Taylor series: 
? ?
22
27
1
exp
2!
d c c c
t t t ? ? ? ? ?? ? ? F F I F F
   (10) 
Analysis of the 
d
F
 matrix shows that its some elements have 
repetitive and sparse structure [4]. Therefore, without any 
approximation, it can be written as: 
3 12 13 14 15 3 12
3 3 22 3 3 3 3 25 3 12
3 3 32 3 34 35 3 12
3 3 3 3 3 3 3 3 3 3 12
3 3 3 3 3 3 3 3 3 3 12
12 3 12 3 12 3 12 3 12 3 12
d
?
? ? ? ?
??
? ? ? ? ?
? ? ? ? ?
?????
??
??
??
??
?
??
??
??
??
??
??
I ? ? ? ? 0
0 ? 00 ? 0
0 ? I ? ? 0
F
0 0 0 I 0 0
0 0 0 0 I 0
0 0 0 0 0 I
   (11) 
Let 
c
Q
 be the noise covariance matrix: 
? ?
2 2 2 2
3 3 3 3
, , , 
T
c IMU IMU g a ba bg
diag? ? ? ? ? ? ? ? ? ? Q n n I I I I
(12) 
where 
2
g
?
, 
2
a
?
, 
2
ba
?
 and 
2
bg
?
 is the variance of noises 
g
n
,
a
n
, 
ba
n
 and 
bg
n
, respectively. Digitize 
c
Q
 to obtain 
d
Q
: 
? ? ? ?
T
T
d d c c c d
t
d ? ? ?
?
?
?
Q F GQG F
    (13) 
With 
d
F
 and 
d
Q
, the prediction equation for error state co-
variance matrix 
kk
P
 is given by 
1 1 1
T
d d d k k k k ? ? ?
?? P F P F Q
. 
C. Measurement update 
The measurement model employed for updating the filter 
state estimate is given by the epipolar geometry and the tri-
focal tensor. As shown in Fig. 2, the epipolar geometry de-
scribes the geometry relationship between two images of the 
same static scene [1]. 
 
Fig. 2. the epipolar geometry between two views 
The algebraic representation of epipolar geometry can be 
derived by the fundamental matrix F . Let t and R be the 
position and rotation matrix of the frame 
2
C
 with respect to 
the frame 
1
C
. Then, the fundamental matrix is defined as 
1 TT??
?? ??
??
F K R t K
, where K is the camera intrinsic pa-
rameter matrix which can be obtained after camera calibra-
tion. Since the image point 
2
' m
 lies on the epipolar line 
21 m
? l Fm
, the epipolar geometry constraint is given by: 
21
0
T
? m Fm
         (14) 
The trifocal tensor encapsulates the geometry relationships 
between the three different viewpoints and is independent of 
the scene structure [1]. Fig. 3 shows the point-line-point 
correspondence between three views which can be used to 
transfer the image point by the trifocal tensor. 
 
Fig. 3 the point-line-point correspondence between three views 
Let 
1
?? ?
??
P I 0
, 
24
?? ?
??
P Aa
 and 
34
?? ?
??
P B b
 be the projec-
tion matrices of the camera at three different viewpoints 
1
C , 
  
 
1
C 2
C
1
m 2
m
M
m1
l
m2
l
1
I 2
I
'
2
m
' M
Epipolar plane 
 
L
1
C
 
 
2
C
3
C
M
1
m
2
l
2
m
3
m
3965
 
 
2
C
 and 
3
C
, where A and B are the 33 ? matrices, 
i
a
 and 
i
b
 denote the i-th column of the matrices 
2
P
 and 
3
P
, respec-
tively. According to the projection matrices and a line lies on 
3D space, the trifocal tensor can be derived: 
44
TT
i i i
?? T ba a b
        (15) 
Next, we use the trifocal tensor to transfer the point 
1
m
 in 
first frame and a line passing through the point 
2
m
 in second 
frame into the third frame which is called the point-line-point 
correspondence: 
3 1 2
T
ii
i
m
??
?
??
??
?
m T l
       (16) 
The line 
2
l
 was recommended to choose as the line perpen-
dicular to the epipolar line in [1]. 
 The measurement model which comprises the epipolar 
geometry and the trifocal tensor is given by the following 
equations. Since we focus on consecutive camera pose, two 
epipolar constraints in three images are used. By assuming 
the correspondence of i-th feature point in three images is 
? ?
1 2 3
,, m m m
, the measurement value 
i
z
  is given by: 
? ? ? ?
2 1,2 12 1
1 2 3 3 2,3 23 2
12
, , ,
TT
TT
ik
i
T
ii
i
h
??
??
? ??
??
??
?? ? ? ? ??
??
??
??
??
??
??
?? ??
?
m R t m
z x m m m m R t m
K m T l
   (17) 
with 
? ?
? ? ? ? ? ?
? ? ? ?
? ? ? ?
11
33
1 1 1
1 1 2 2 3 3
, 1 , 1
2
ˆ ,  ,    ,    ,  
 ,   ,  1,2
    ,         ,  1,2
     , 
j j j j j
j j j j j
k k k
TT
G G G G G
j j C C j j C C C
G G I G G G I
C I C C I I C
G G I G G G I
C I C C I I C
e
g
j
R R j
RR
l
??
? ? ?
??
? ? ? ?
? ? ? ?
? ? ? ?
? ? ?
?
x x x m K m m K m m K m
R R R t R p p
R q R p p q p
R q R p p q p
l ? ? ? ?
? ?
2 1 2 2 2 1 12 12 1 1 2 3
2 2 2
, , ,  , ,
    , ,1
TT
T
e u e v e e e e
T
uv
l m l m l l l l
mm
? ? ? ? ? ??
??
?
R t m
m
(18) 
where 
k
x
 is true state which is obtained by the nominal and 
error state according to (4). Since the measurement model is 
nonlinear, we use a Sigma-Point approach  to update the filter 
state estimate. After measurement update, use error state 
kk
x
 
to correct nominal state and then obtain ˆ
kk
x
. In order to keep 
only three poses in the filter state vector, replace old state by 
current state and revise error covariance: 
7 7 7 9 7 7 7 7 6 6 6 9 6 6 6 6
9 7 9 9 9 7 9 7 9 6 9 9 9 6 9 6
7 7 7 9 7 7 7 7 6 6 6 9 6 6 6 6
7 7 7 9 7 7 7 7 6 6 6 9 6 6 6 6
,=
ˆ ˆ  ,     ,  
ne
T
k n k k e k e e k k k k
? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?
?
? ? ? ?
? ? ? ?
? ? ? ?
? ? ?
I 0 0 0 I 0 0 0
0 I 0 0 0 I 0 0
TT
0 0 0 I 0 0 0 I
I 0 0 0 I 0 0 0
x T x x T x P T P T
(19) 
D. RANSAC 
In order to reject feature points which are mismatch or 
located on independently moving objects, we use RANSAC 
algorithm to select inliers. In Kalman filter, the procedure of 
RANSAC algorithm in this paper is similar to 1-point 
RANSAC EKF [15]. However, since the proposed method 
does not estimate the 3D position of feature point, it cannot 
use Euclidean distance to decide inlier by re-project the 3D 
position of feature point. Therefore, the proposed method 
uses the trifocal tensor to decide inlier: 
? ?
? ?
1 2 3
1 2 3 3 1 2
,,
, ,   
Inliers
T
ii
i
threshold
??
?? ??
? ? ?
??
??
?? ??
??
?
m m m
m m m m K m T l
(20) 
Since the trifocal tensor is derived under static assumption, 
the criterion (20) can detect the feature points located on 
independently moving objects which makes the overall algo-
rithm capable of operating in dynamic environment. 
E. Overall algorithm 
The proposed visual assisted IMU odometer is summarized 
in Algorithm 1. It has the following characteristics: (1) 
tightly-coupled sensor fusion approach, (2) structure-less 
visual inertial odometry and (3) perform optimization over a 
sliding window of filter states. 
Algorithm 1: Visual Assisted IMU Odometer 
1 Initialize 
00
ˆ x
, 
00
P
 and 
27 1 00 ?
? x0
 
2 for 
1, k?
 do 
{ Time update } 
3     Compute 
d
F
 and 
d
Q
 by (11) and (13) 
4     %%% Propagate error state and error covariance %%% 
5     
27 1 1 kk ? ?
? x0
, 
1 1 1
T
d d d k k k k ? ? ?
?? P F P F Q
 
6     Use 4-th order Runge Kutta method to predict 
1
ˆ
kk?
x
 
{ Measurement update } 
7     if  New image then 
8         Match feature points in last three images to get 
? ?
1 2 3
,,
i
m m m
 
9         Use RANSAC to find inliers 
10         %%% Generate sigma points and predict measurement %%% 
11         
? ?
? ? 27 111
l
k k k k
l
L ?
???
? ? ? X 0 P
  
12         
? ?
? ?
? ? 1 2 3 11
ˆ , , , ,
ll
i k k k k i
hg
??
? Z x X m m m
, 
2
0
ˆ
L
ll
i s i
l
W
?
?
?
zZ
 
13         %%% update error state and error covariance%%% 
14         
? ?? ?
2
0
ˆ ˆ
ii
T L
ll
z z i i i i
l?
? ? ? ?
?
P Z z Z z R
 
15         
? ?? ?
2
27 1 1
0
ˆ
i
T L
l l l
xz c i i kk
l
W
? ?
?
? ? ?
?
P X 0 Z z
 
16         
1
i i i
k xz z z
?
? K P P
 
17         
? ?
1
ˆ
k i i k k k k?
? ? ? x x K z z
, 
1 ii
T
k z z k k k k k?
?? P P K P K
 
18         Use 
kk
x
 to correct nominal state estimate and then obtain ˆ
kk
x
 
19         %%% Replace old state and revise error covariance %%% 
20         
ˆ ˆ ,  ,  
T
k n k k e k e e k k k k
? ? ? x T x x T x P TP T
 
21     end if 
22 end for 
3966
 
 
IV. EXPERIMENTAL RESULTS 
The proposed method is evaluated by using a publicly 
available real-world dataset [20]. The Matlab code of the 
proposed method can be downloaded from the internet [24]. 
In KITTI dataset, the sensor used for data recording consist of 
two grayscale and two color video cameras (Point Grey Flea2, 
10 Hz, 1392?512 pixel resolution, 90
o
?35
o
 opening angle), a 
laser scanner and a GPS/IMU INS (OXTS RT 3003, 100 Hz). 
The proposed method only uses the measurements of a single 
grayscale camera and the IMU (acceleration and angular 
velocity) to estimate the ego-motion. 
Geiger et al. [20] provided two versions of data which are 
raw and synchronized. After the manual synchronization, the 
IMU sampling rate is 10Hz. Since the synchronization be-
tween the camera and the IMU is important, we use the 
synchronized data to verify the proposed method. In the ex-
periments, the extraction and matching of feature points are 
performed using the SIFT algorithm [21]. In order to maintain 
a certain amount of computational cost, we use the “bucket-
ing” concept [17] to choose a subset of feature points. The 
initial velocity of the IMU and the initial direction of gravity 
were obtained from GPS/IMU INS. We use Euclidean dis-
tance and rotation angle to define position and orientation 
error and use a table to show overall RMSE and end point 
error. The proposed method is compared with the following 
methods: (1) GPS/IMU INS and use it as ground truth, (2) 
pure IMU navigation which is obtained by integrating accel-
eration and angular velocity and (3) monocular and stereo 
visual odometry proposed by Geiger et al. [22]. In the fol-
lowing, the experiments are conducted in three cases. 
A.   Case 1 
The trajectory is about 540 m, takes 78 sec. The average 
speed is about 25 km/h. Fig. 4 shows the motion trajectory 
estimation results. It can be found that the result of the pure 
IMU navigation is not reliable with the IMU error accumu-
lation. As shown in TABLE 1, the proposed method outper-
forms the other methods in the overall RMSE and end point 
error. It is worth noting that the angle from the pure IMU 
navigation is more close to the ground truth than the position. 
The main reason is that the angle is obtained by using single 
integration, while the position is obtained by using double 
integral. Furthermore, the orientation error would propagate 
to the position error in the gravity compensation step. The 
result of the monocular visual odometry is better than the pure 
IMU navigation. In general, the real scale cannot be obtained 
by only using single camera. In this monocular visual odom-
etry, the real scale is derived by assuming that the camera is 
moving at a known and fixed height on the ground. 
B.  Case 2 
The trajectory is about 2160 m, takes 94.5 sec. The average 
speed is about 82 km/h. The data in case 2 was acquired on 
the highway. Thus, the average speed in case 2 is much faster 
than case 1. The motion trajectory estimation results are 
shown in Fig. 5 and TABLE 2. It can be found that the result 
of the pure IMU navigation in case 2 is more reliable than in 
case 1. The main reason is that the IMU has lower uncertainty 
of measurement at fast motion which makes the better inte-
gration result. 
C.  Case 3 
The trajectory is about 3577 m, takes 440 sec. The average 
speed is about 29 km/h. The path in this case is longer than in 
case 1 or case 2. Fig. 6 and TABLE 3 show the motion tra-
jectory estimation results. After the three cases, we conclude 
that the motion trajectory estimated by the sensor fusion of 
two different sensors is more reliable. 
 
Fig. 4. the motion trajectory estimation results in KITTI dataset case1 
TABLE 1. the overall RMSE and the end point error results in KITTI case 1 
Algorithm Overall  
position 
RMSE (m) 
Overall  
orientation 
RMSE (deg) 
End point 
position 
error (m) 
End point 
orientation 
error (deg) 
Proposed 
method 
4.0018  1.1628 6.4478 1.0586 
Pure IMU  
navigation 
2748 11.1340 6009 9.4521 
Monocular VO  33.9685 7.8149 67.5990 11.3223 
Stereo VO 15.3520 4.0740 26.7258 6.3967 
 
Fig. 5. the motion trajectory estimation results in KITTI case2 
TABLE 2. the overall RMSE and the end point error results in KITTI case 2 
Algorithm Overall  
position 
RMSE (m) 
Overall  
orientation 
RMSE (deg) 
End point 
position 
error (m) 
End point 
orientation 
error (deg) 
Proposed 
method 
34.2638  2.3190 28.3338 2.4629 
Pure IMU  
navigation 
64.8753 4.0528 147.6343 10.2659 
Monocular VO  596.3744 96.4882 704.6405 164.5494 
Stereo VO 215.7575 19.0431 300.1239 27.5330 
8.39 8.391 8.392 8.393 8.394 8.395
48.9825
48.983
48.9835
48.984
48.9845
48.985
48.9855
48.986
Longitude
Latitude
 
 
Ground truth
Proposed method
Pure IMU navigation
Monocular VO(Geiger et al. 2011)
Stereo VO(Geiger et al. 2011)
8.465 8.47 8.475 8.48 8.485 8.49
49.004
49.006
49.008
49.01
49.012
49.014
49.016
49.018
49.02
49.022
Longitude
Latitude
 
 
Ground truth
Proposed method
Pure IMU navigation
Monocular VO(Geiger et al. 2011)
Stereo VO(Geiger et al. 2011)
3967
 
 
 
Fig. 6. the motion trajectory estimation results in KITTI case3 
TABLE 3. the overall RMSE and the end point error results in KITTI case 3 
Algorithm Overall  
position 
RMSE (m) 
Overall  
orientation 
RMSE (deg) 
End point 
position 
error (m) 
End point 
orientation 
error (deg) 
Proposed 16.9207  0.8480 13.4773 0.8194 
Pure IMU  
navigation 
6742 4.7723 14731 10.0392 
Monocular VO  211.2474 14.8340 304.5535 23.7477 
Stereo VO  73.4203 10.6717 118.9049 17.7305 
V. CONCLUSIONS 
This paper presents an odometer architecture which 
combines a monocular camera and an IMU. The trifocal 
tensor geometry relationship between three images is used as 
camera measurement information, which makes the proposed 
method without estimating the 3D position of feature point. 
Meanwhile, the camera pose corresponding to each of the 
three images are refined in filter to form a MSCKF. The 
proposed method has the following characteristics: (1) 
tightly-coupled sensor fusion approach, (2) structure-less 
visual inertial odometry and (3) perform optimization over a 
sliding window of filter states. This paper further proposes a 
RANSAC algorithm which is based on three views geometry 
to select inliers. The experiments are conducted to show the 
effectiveness of the proposed method by using a publicly 
available real-world dataset. The results show the error of the 
IMU can be effectively constrained by the proposed method 
and the estimated ego-motion is close to the actual path. 
ACKNOWLEDGMENT 
This work was supported in part by the National Science 
Council, Taiwan, under grant # NSC 101-2221-E-009-002. 
REFERENCES 
[1] R. Hartley and A. Zisserman, Multiple View Geometry in computer 
vision, 2nd ed. Cambridge University Press, 2008. 
[2] Corke, J. Lobo, and J. Dias, "An Introduction to Inertial and Visual 
Sensing," Intl. Journal of Robotics Research, vol. 26, no. 6, pp. 
519-535, Jun. 2007. 
[3] H. Jwu-Sheng, T. Chin-Yuan, C. Ming-Yuan and S. Kuan-Chun, 
"IMU-Assisted Monocular Visual Odometry Including the Human 
Walking Model for Wearable Applications," in Proc. of the IEEE Intl. 
Conf. on Robotics and Automation (ICRA), Karlsruhe, Germany, May 
6-10, 2013. 
[4] S. Weiss and R. Siegwart, "Real-Time Metric State Estimation for 
Modular Vision-Inertial Systems," in Proc. of the IEEE Intl. Conf. on 
Robotics and Automation (ICRA), Shanghai, China, May 9-13, 2011, 
pp. 4531-4537. 
[5] P . Pinies, T. Lupton, S. Sukkarieh, and J. D. Tardos, "Inertial Aiding of 
Inverse Depth SLAM using a Monocular Camera," in Proc. of the IEEE 
Intl. Conf. on Robotics and Automation (ICRA), Roma, Italy, Apr. 
10-14, 2007, pp. 2797-2802. 
[6] A. I. Mourikis and S. I. Roumeliotis, "A Multi-State Constraint Kalman 
Filter for Vision-aided Inertial Navigation," in Proc. of the IEEE Intl. 
Conf. on Robotics and Automation (ICRA), Roma, Italy, Apr. 10-14, 
2007, pp. 3565-3572. 
[7] J. O. Nilsson, D. Zachariah, M. Jansson, and P. Handel, "Realtime 
implementation of visual-aided inertial navigation using epipolar con-
straints," in Proc. of the IEEE/ION Position Location and Navigation 
Symposium (PLANS), Myrtle Beach, SC, USA, Apr. 23-26, 2012, pp. 
711-718. 
[8] E. Asadi and C. L. Bottasso, "Tightly-coupled vision-aided inertial 
navigation via trifocal constraints," in Proc. of the IEEE Intl. Conf. on 
Robotics and Biomimetics (ROBIO), Shenzhen, China, Dec. 12-14, 
2012, pp. 85-90. 
[9] V . Indelman, P . Gurfil, E. Rivlin, and H. Rotstein, "Real-Time Vi-
sion-Aided Localization and Navigation Based on Three-View Geom-
etry," IEEE Trans. on Aerospace and Electronic Systems, vol. 48, no. 3, 
pp. 2239-2259, Jul. 2012. 
[10] A. Martinelli, "Vision and IMU Data Fusion: Closed-Form Solutions 
for Attitude, Speed, Absolute Scale, and Bias Determination," IEEE 
Trans. on Robotics, vol. 28, no. 1, pp. 44-60, Feb. 2012. 
[11] D. Scaramuzza and F. Fraundorfer, "Visual Odometry [Tutorial]," IEEE 
Robotics Automation Magazine, vol. 18, no. 4, pp. 80-92, Dec. 2011 
[12] D. Nister, O. Naroditsky, and J. Bergen, "Visual Odometry," in Proc. of 
the IEEE Computer Society Conf. on Computer Vision and Pattern 
Recognition (CVPR), vol. 1, Washington, DC, USA, Jun. 27-Jul. 2, 
2004, pp. 652-659. 
[13] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, "MonoSLAM: 
Real-Time Single Camera SLAM," IEEE Trans. on Pattern Analysis 
and Machine Intelligence, vol. 29, no. 6, pp. 1052-1067, Jun. 2007. 
[14] J. Civera, A. J. Davison, and J. Montiel, "Inverse Depth Parametriza-
tion for Monocular SLAM," IEEE Trans. on Robotics, vol. 24, no.5, pp. 
932-945, Oct. 2008. 
[15] J. Civera, O. G. Grasa, A. J. Davison, and J. M. M. Montiel, "1-Point 
RANSAC for EKF Filtering. Application to Real-Time Structure from 
Motion and Visual Odometry," Journal of Field Robotics, vol. 27, no. 
5, pp. 609-631, Sep. 2010. 
[16] Y. Ying-Kin, W. Kin Hong, M. M. Y. Chang, and O. Siu Hang, "Re-
cursive Camera-Motion Estimation With the Trifocal Tensor," IEEE 
Trans. on Systems Man and Cybernetics Part B Cybernetics, vol. 36, 
no. 5, pp. 1081-1090, Oct. 2006. 
[17] B. Kitt, A. Geiger, and H. Lategahn, " Visual Odometry based on 
Stereo Image Sequences with RANSAC-based Outlier Rejection 
Scheme," in Proc. of the IEEE Intelligent Vehicles Symposium (IV), La 
Jolla, CA, USA, Jun. 21-24, 2010, pp. 486-492. 
[18] J. Sola. (2012, Nov. 6). Quaternion kinematics for the error-state KF 
[Online]. Available: 
http://www.joansola.eu/JoanSola/objectes/notes/kinematics.pdf 
[19] S. Julier and J. Uhlmann  "A new extension of the Kalman filter to 
nonlinear systems",  Proc. SPIE 3068, Orlando, FL, USA, April 21, 
1997,  pp.182 -193. 
[20] A. Geiger, P . Lenz, and R. Urtasun, "Are we ready for Autonomous 
Driving? The KITTI Vision Benchmark Suite," in Proc. of the IEEE 
Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), 
Providence, RI, USA, Jun. 16-21, 2012, pp. 3354-3361. 
[21] D. G. Lowe, "Distinctive Image Features from Scale-Invariant Key-
points," Int. Journal of Computer Vision, vol. 60, no. 2, pp. 91-110, 
Nov. 2004. 
[22] A. Geiger, J. Ziegler, and C. Stiller, " StereoScan: Dense 3d Recon-
struction in Real-time," in Proc. of the IEEE Intelligent V ehicles Sym-
posium (IV), Baden-Baden, Germany, June 5-9, 2011, pp. 963-968. 
[23] J. Y. Bouguet. (2010, Jul. 9). Camera Calibration Toolbox for Matlab 
[Online].Available:  http://www.vision.caltech.edu/bouguetj/calib_doc/ 
[24] http://www.mathworks.com/matlabcentral/fileexchange/43218-visuali
nertial-odometry 
8.392 8.394 8.396 8.398 8.4 8.402 8.404 8.406
48.98
48.981
48.982
48.983
48.984
48.985
48.986
48.987
48.988
48.989
Longitude
Latitude
 
 
Ground truth
Proposed method
Pure IMU navigation
Monocular VO(Geiger et al. 2011)
Stereo VO(Geiger et al. 2011)
3968
