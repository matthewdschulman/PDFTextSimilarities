Perceptive Feedback for Natural Language Control of Robotic
Operations
Yunyi Jia, Ning Xi, Joyce Y. Chai, Yu Cheng, Rui Fang and Lanbo She
Abstract—A new planning and control scheme for natural
language control of robotic operations using the perceptive
feedback is presented. Different from the traditional open-loop
natural language control, the scheme incorporates the high-
level planning and low-level control of the robotic systems and
makes the high-level planning become a closed-loop process
such that it is able to handle some unexpected events in the
robotics system and the environment. The experimental results
on a natural language controlled mobile manipulator clearly
demonstrate the advantages of the proposed method.
I. INTRODUCTION
Recent years have seen a growing interest in technology
on natural language control of robotic operations. The appli-
cations of such systems range from service and entertainment
robots to industrial robots [1-3].
Natural language carries high-level discrete symbolic in-
formation and robot control is a low-level continuous pro-
cess. Controlling robotic operations by natural language re-
quires a mechanism to connect these two different represen-
tations. Despite recent progress, research on natural language
control of robotic systems still faces many challenges. For
example, most previous work has focused on grounding
linguistic terms to the robot’s perception and action [4-7] so
that a natural language command can be translated to formal
representations accessible by the robot (e.g., sensorimotor
skills of the robot). However, an important issue which
has not been well addressed is that, even given a perfect
translation, unexpected events could happen that will prevent
the execution of the natural language command. For example,
suppose a human says “pick up the red bottle” and the robot
understands perfectly what actions are involved (i.e., coming
up with a task schedule given the current situation) and which
bottle needs to be picked up. However, the situation may
change after the human’s command, for example, the robotic
arm could be blocked or the object may be moved. The robot
would fail in executing the command. Thus it is important to
have a scheme that will allow the natural language controlled
robotic system to intelligently handle unexpected events in
both the robotic system and the environment.
This research work is partially supported under U.S. Army Research
Ofﬁce Contract No. W911NF-11-D-0001, and U.S. Army Research Ofﬁce
Grant No. W911NF-09-1-0321 and W911NF-10-1-0358, and National Sci-
ence Foundation Award No. CNS-1320561 and IIS-1208390.
Yunyi Jia, Ning Xi and Yu Cheng are with the Department of
Electrical and Computer Engineering, Michigan State University, East
Lansing, MI 48824 USA (E-mails: jiayunyi@msu.edu, xin@egr.msu.edu,
chengyu9@msu.edu).
Joyce Y. Chai, Rui Fang and Lanbo She are with the Department of
Computer Science and Engineering, Michigan State University, East Lans-
ing, MI 48824 USA (E-mails: jchai@cse.msu.edu, fangrui@cse.msu.edu,
shelanbo@cse.msu.edu).
Researches on natural language control of robotic opera-
tions have been conducted for many years. In the human-
robot interaction research, many studies have been focused
on converting natural language commands to some types of
logic representations and then mapping them to predeﬁned
primitives of robot actions [8-10]. Some studies have specif-
ically looked into temporal logic [11] and spatial references
[12] for natural language control. Natural language tasks
are interpreted as these structured representations and many
approaches have been proposed to generate the robot motions
based on these representations and some abstracted plant
models [13-16]. Recently, some studies have also used the
probability-based methods to directly convert natural lan-
guage commands to robot motions through off-line training
or online learning of the robotic systems [17-20].
The approaches above have been mainly focused on gen-
erating the robot motions to achieve the tasks described by
natural language. The high-level planning including the task
scheduling and action planning and the low-level control
of the robotic system are implemented as separate and
sequential processes. This formalism usually needs to assume
that the control system can absolutely accomplish the actions
and there are no exceptions in both the robotic system and the
environment. If the assumption is violated, errors will occur
to completely cease the control systems [16][21]. This is
because the high-level planning is an open-loop process and
it cannot anticipate and also does not consider the unexpected
events. This introduces problems and difﬁculties in handling
unexpected events in the robotic system and the environment,
such as unexpected obstacles and environment changes.
To address this issue, this paper proposes a planning
and control scheme using the perceptive feedback to han-
dle unexpected events after receiving the natural language
commands. The perceptive feedback is deﬁned as the feed-
back representing the perception of the robotic system and
the environment. Since the paper is focused on designing
the planning and control or the robotic system for natural
language control, we assume that the conversion from the
natural language commands to the logic representations
can always be correctly achieved. Then, a planning and
control framework is designed to implement the discrete
task scheduling, continuous action planning, and continuous
control of the robotic system using the perceptive feedback
and make the high-level planning become a closed-loop
process. As a result, the system is able to handle some
unexpected events happened in the robotic system and the
environment.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6673
II. NATURAL LANGUAGE CONTROL OF ROBOTIC
OPERATIONS
Robotic System
Continuous Controller
Sensory Information
Discrete 
Task Scheduler
Continuous 
Action Planner
Speech
Recognition
Natural 
Language
Processing
speech
natural 
language
task 
specifications
control
feedback
Action plan
Motion plan
Human 
user
Fig. 1. Natural language control process
The schema of natural language control of robotic opera-
tions is shown in Fig. 1. The human user gives commands to
the robotic system through speech. The speech is recognized
as natural language in text by Voice Recognition. A natural
language processing is applied to transform the natural
language to formal representations which denote the task
speciﬁcations. The Discrete Task Scheduler generates the
desired discrete behaviors described by actions to realize
or satisfy the task speciﬁcations. The Continuous Action
Planner generates the desired continuous behaviors described
by trajectories for the robotic system to achieve the desired
actions. The Continuous Controller then controls the robotic
system to track the desired trajectories based on feedback of
sensory information.
For natural language processing in this paper, a
partial parser based on combinatory categorial grammar
(CCG) [22] is applied to extract semantic information
from human utterances [2]. We have deﬁned a set of
basic CCG lexicon rules, which covers key lexicons
in our domain, such as actions, object colors, shapes,
spatial relations and so forth. Given a human utterance,
our CCG parser repeatedly searches for the longest
sequence covered by the grammar until the end of
the utterance. For example, given the utterance “pick
up the small brown block”, our parser will generate a
semantic representation as follows:(Action:PICK-UP;
Argument:x(size(x,small),color(x,brown),
object-type(x, block))). Given this representation,
the argument x is further grounded to a physical object
in the environment based on its semantic constraints.
Graph-matching approaches have been developed for this
purpose of referential grounding [5][6]. Once the references
are grounded, a ﬁnal state based on the action PICK-UP
is created. This ﬁnal state is passed to the task scheduler
to come up with a sequence of robotic operations. The
translation between actions to ﬁnal states are currently
governed by a set of template rules.
The high-level planning including the task scheduling and
action planning and the low-level control work together to
realize the natural language tasks. These processes, however,
are carried out separately and the task scheduling and ac-
tion planning are purely open-loop processes. If unexpected
events in the robotic system or environment occur, such
formalism of control will encounter problems to handle them.
To solve this problem, similar to feedback control, feed-
back information can be sent to the high-level task schedul-
ing and action planning. In general, the feedback includes the
static and dynamic information of the robotic system such as
robot position, moving direction, etc., and the environment
such as object position, color, shape, weight, etc. It is like
the human’s perception and thus named perceptive feedback.
It can be obtained from the sensory information including
both robot sensors and environmental sensors.
III. PERCEPTIVE PLANNING AND CONTROL FOR
NATURAL LANGUAGE CONTROLLED ROBOTIC SYSTEMS
A. Perceptive Abstracted Model of the Robotic System
Based on the perceptive feedback, the entire system can be
abstracted by a ﬁnite-state automaton using some predeﬁned
rules. In order to obtain a simple model for possible online
implementations, only the states and actions (transitions)
necessary for the tasks are considered to construct the
automaton. The primitive actions are modelled as automata
and then connected through the mutual states to construct
the ﬁnal automaton. Each state contains not only a state
symbol representing the discrete state but also some variables
representing the continuous states such as the positions.
Since the model can can be updated based on the perceptive
feedback, it is named perceptive system model and described
by a 5-tuple automaton
A=(Q(P), ?,?,q
0
(p
0,?
),Q
m
(p
m,?
)) (1)
where
• Q(P) is the set of states determined by the perceptive
feedback (Q is the set of state symbols and P is the
set of variables). A single state i is represented by
q
i
(p
i,?
) and p
i,?
contains the continuous variables such
as positions to represent the initial or ﬁnal states of all
actions related to the state i.
•

is the set of actions.
• ? : Q(P)?

? Q(P) is the state transition of actions.
• q
0
(p
0,?
) ? Q(p) is the initial state determined by the
perceptive feedback.
• Q
m
(p
m,?
) ? Q(P) is the set of marked states repre-
senting the completion of a task, which is determined
by the task speciﬁcations and perceptive feedback.
Example 1: Consider that there are three objects and the
task is to control a robot to pick up one object. Fig. 2 shows
the model to describe the states and transitions of interest for
the task. The states are represented by d
1
d
2
(p
d1d2,?
). The
6674
ﬁrst digit d
1
?{0,1,2,3} means that the end-effector is at
the position of object d
1
if d
1
?{1,2,3} and at the initial
position if d
1
=0. The second digit d
2
?{0,1} means that
the gripper is open if d
2
=0 and closed if d
2
=1. The
actions ? are represented by symbols, where m
ij
means to
move the end-effector from position i to position j and o and
c means to open and close the gripper respectively. p
d1d2,?
represents the positions of the end-effector or the gripper
ﬁngers for actions m
ij
, o or c related to the state d
1
d
2
.
30
(      ) 30,
p
?
31
(      ) 31,
p
?
20
(      ) 20,
p
?
21
(      ) 21,
p
?
10
(      ) 10,
p
?
11
(      ) 11,
p
?
01
(      ) 01,
p
?
00
(      )
00,
p
?
01
m
10
m
12
m
21
m
23
m
32
m
30
m
03
m
02
m
20
m
13
m
31
m
o
c
o
c
o c
o c
Fig. 2. Example of perceptive system model
B. Perceptive Planning of the Robotic System
The perceptive planner generation is to generate the action
and motion plans for the robotic system to realize the given
task. It contains three sequential steps. The ﬁrst step is to
generate the discrete solution, i.e., desired action behaviors,
which is like the traditional task scheduling. The second
step is to generate the continuous solution for the discrete
solution, i.e., desired trajectories, which is like the traditional
action planning. The third step is to construct a plan to
incorporate the discrete and continuous solutions.
In the ﬁrst step, the task described by natural language
can be converted to some formal representations, such as
temporal logic formula and automata based on the predeﬁned
rules and policies. For a general formulation, the synthesis
algorithms in discrete event systems can be adopted to
generate the desired action behaviors to realize the task such
as the supervisory controller synthesis algorithm [23][24],
and linear temporal logic synthesis algorithm [21]. For some
speciﬁc formulation in natural language control, such as
pick-up task in the example 1, the task can be represented by
a single marked state. The synthesis is thus to ﬁnd a solution
from the initial state to the marked state in the perceptive
system model.
In the second step, based on the variables p
i,/delta
in the
initial and ﬁnal states of each action, existing online motion
planning algorithms can be adopted to generate the desired
trajectory for the action. The algorithms include minimum-
time based motion planning, minimum-energy based motion
planning, etc.
In traditional planning methods, the two steps above have
ﬁnished the high-level planning. The desired discrete action
behaviors and the desired continuous trajectories are then
parameterized by time t. They are executed by the low-level
controller according to the time evolvement. The high-level
planning and low-level control are connected in an open-loop
manner, which may cause problems when unexpected events
happen. For example, if the robot is stopped, the desired
action behaviors and the desired trajectories, however, will
still keep evolving since time never stops.
To handle this issue, the third step is to model both
action behaviors and motion trajectories using a non-time
reference instead of time t. The non-time reference is named
perceptive reference and represented by s. It is a variable
which can carry the information of the system outputs. We
have used this reference to model the continuous trajectories
of robotic systems for coordination and teleoperation in our
previous works [25][26]. It is extended to model both discrete
actions and continuous trajectories in this paper. Different
from the method of using multiple references in [27], a
single perceptive reference is used for both discrete actions
and continuous trajectories. In each action, the perceptive
reference s is a piece of continuous variable related to the
trajectory, such as travelled distance along the trajectory. The
trajectory is then parameterized by s in each action. For
multiple actions, the overall perceptive reference is then a
piece-wise continuous variable. For the entire plan, similar
to the function of time t, the evolvement of s within a piece
drives the robot to move along the continuous trajectory and
the jump of s between two pieces drives the robot to transit
from one action to another action. The perceptive hybrid plan
can then be represented by a 6-tuple automaton
A
PP
=

Q(P), ?,?(s),Y
d
?
(s),q
0
(p
0
),Q
m
(p
m
)

(2)
where Q(P) is the set of states,

is the set of events
(actions), ?(s): Q(P)?

? Q(P) is the state transition
function parameterized by s, Y
d
?
(s) is the desired continuous
trajectory parameterized by s for the action ?, q
0
(p
0,?
) ?
Q(p) is the initial state, and Q
m
(p
m,?
) ? Q(P) is the set of
marked states.
Example 2: Take the model in Example 1 as an example.
Suppose the initial state is ”01” and the given task is ”Pick
up the object 2”. The accomplishment of the given task
can be represented by a marked state ”21”. The synthesis
solution is then a sequence of actions to drive the state
from ”01” to ”21”. Deﬁne the perceptive reference s as
the distance traveled by the end-effector or the distance
between the gripper ﬁngers for different desired actions.
The desired trajectories for these actions are planned and
then parameterized by s. After this, the perceptive planner is
ﬁnally obtained and shown in Fig. 3. For each transition ? of
an action,s has a speciﬁc deﬁnition and a valid range[s
0
?
,s
f
?
],
where s
0
?
denotes the start of the action and s
f
?
denotes
6675
the ﬁnish of the action. The perceptive reference s is a
piece-wise continuous variable. Its values in each piece drive
the desired continuous trajectories and its ”jumps” between
adjacent pieces drive the desired discrete action behaviors.
20 21 01 00
(), ( )
d
o
os Y s
0
,
f
oo
s ss ?? ?
??
02
02
(), ( )
d
m
ms Y s
02 02
0
,
f
mm
s ss ?? ?
??
(), ( )
d
cc
ms Y s
0
,
f
cc
s ss ?? ?
??
Fig. 3. Perceptive plan example
The solution of high-level planning is represented by a
perceptive planner which is parameterized by the perceptive
reference that is determined by the perceptive feedback. This
creates a mechanism such that the system inputs generated
from the high-level planning is determined by the system
outputs. Thus, the high-level planning becomes a closed-loop
process and is able to handle unexpected events. The way
of determining the perceptive reference by the perceptive
feedback will be introduced in the following subsection.
C. Perceptive Control of the Robotic System
If there are no unexpected events in the environment,
the ﬁrst step of the perceptive control is to determine the
optimal perceptive reference s
?
from the real-time perceptive
feedback. For a single robot system, to achieve the best
trajectory tracking during the execution of an action, a tem-
porary perceptive reference can be computed by minimizing
the tracking errors of the robot, which is described by
s
temp
= arg min
s?S?


Y
d
?
(s)?Y



(3)
where S
?
=[s
0
?
,s
f
?
] is the range of perceptive reference
during the transition of the action and Y is the real-time
output of the robot position. This equation can be solved by
orthogonal projection from the current position to the desired
trajectory.
When the system lies in the execution of an action, it
is driven by continuous behaviors and s
temp
is in [s
0
?
,s
f
?
).
Then, the desired continuous behavior should continue to be
driven by s
temp
. When the system just ﬁnishes an action and
is about to start the next action, s
temp
is equal to s
f
?
and it
will be driven by discrete behaviors. Therefore, the optimal
perceptive reference can be designed by
s
?
=

s
temp
,ifs
temp
?
	
s
0
?
,s
f
?


s
0
next(?)
,ifs
temp
= s
f
?
(4)
Once the optimal perceptive reference is determined, the
inputs for the underlying continuous controller can be ob-
tained by plugging s
?
into the perceptive plan.
When unexpected events in the environment, such as
moving objects and adding or removing objects, it implies
that the previous perceptive system model cannot represent
the current system any more and therefore the previously
generated system plan is no longer suitable. Then, the
perceptive feedback needs to be used to update the perceptive
system model and the system plan needs to be regenerated
based on the updated system model. Therefore, the general
control process is illustrated in Fig. 4 and described as
following.
Step 1: Convert the natural language to a formal repre-
sentation to describe the task speciﬁcations, e.g., a marked
state;
Step 2: Abstract or update the perceptive system model
from the perceptive feedback and generate a perceptive plan
to achieve the task speciﬁcations;
Step 3: Run the high-frequency perceptive control loop
1) Acquire perceptive feedback from robot and environ-
mental sensors;
2) If unexpected events happen to the environment, return
the perceptive feedback to Step 2; Otherwise, continue
to 3);
3) Generate s
?
using the perceptive feedback and use it
to determine the instantaneous input Y
d
?
(s
?
) for the
continuous controller;
4) Execute continuous controller for achieving Y
d
?
(s
?
);
5) go to 1).
Remarks:
1) If unexpected events happen in the environment, such
as moving, adding and removing objects, the per-
ceptive feedback will be used to update the system
model and then a new perceptive hybrid plan can
be generated to achieve original task. If unexpected
events happen in the robotic system, e.g., the robot is
unexpectedly stopped, the optimal perceptive reference
s
?
stops evolving as well. Both the desired discrete
action behavior and continuous motion behavior are
then suspended. Once the unexpected event is removed,
the robot starts to move and thens
?
continues evolving.
This will automatically recover the desired behaviors
with no re-setting or re-planning.
2) The formalism can be extended to natural language
control of multiple concurrent robotic systems. The
only concern is to design a perceptive reference which
can carry perceptive feedback from all concurrent
robotic systems. Some ideas of designing the per-
ceptive reference for such systems can be found in
[26][28].
IV. EXPERIMENTAL RESULTS FOR NATURAL LANGUAGE
CONTROL OF ROBOTIC OPERATIONS
The experimental setup is shown in Fig. 5. A human user
controlled a mobile manipulator to operate three blocks (a big
red block, a big brown block, and a small brown block) using
speech. The speech was recognized as natural language text
using the Dragon Speech Recognition software and converted
to a formal language ”action+object” using the semantic
processing. It was then mapped to a formal representation,
i.e., a marked state, based on predeﬁned mapping rules. The
perceptive feedback contains the positions of the end-effector
and gripper ﬁngers and the positions, color and shape of the
objects which are obtained from a calibrated vision system.
6676
Perceptive 
Plan 
Generation
Perceptive 
Sys. Model
Task 
Specifications
Robotic 
System
Perceptive
Planner
Continuous 
Controller
Robot and Env.
Sensors
Perceptive Reference 
Generation
) (s Y
d
s perceptive reference
percep tiv e feed back
control feedback
perceptive feedback
Natural 
Language
Natural 
Language 
Processing
Env. 
unexpected 
events?
Yes
No
Fig. 4. Natural language control using the perceptive feedback
In Fig. 6 shows the result of handling unexpected events in
the robotic system. The human user asked the robot to pick
up the big red block by saying ”Pick up the red block”. While
the robot was moving to the red block, at 7 s and 12 s, it was
blocked by a human’s hand. Then, the perceptive reference
stopped evolving, which suspended all following desired
discrete action behaviors and desired continuous trajectories.
When the hand was removed at 11 s and 14 s, the perceptive
reference started to evolve again and automatically drove
the robot to accomplish the following action behaviors and
trajectories. The task was then ﬁnally ﬁnished.
Fig. 7 and 8 show the results of handling unexpected
events in the environment. The human user asked the robot
to pick up the big brown block and big red block by saying
”Take that big brown block” and ”Grasp that red block”
respectively. While the robot was moving to them, the big
brown block was moved to other positions twice at 11 s and
22 s in Fig. 3 and the red block was swapped with the big
brown block at 4 s in Fig. 9. The perceptive feedback was
then sent back to the update the perceptive system model
and new plans were also generated to achieve the original
tasks.
Although the designed tasks are simple, these results can
clearly demonstrate the advantages of the designed mehod
for handling unexpected events in the robotic system and
the environment.
Fig. 5. Experimental setup
V. CONCLUSIONS
A new natural language control scheme for robotic op-
erations using the perceptive feedback has been proposed.
The high-level planning including discrete task scheduling
and continuous action planning and the low-level continuous
control are presented. The high-level planning solution is
driven by a perceptive reference which is determined by
the perceptive feedback. This makes the high-level planning
become closed-loop such that the robot can accomplish
the high-level natural language tasks in the presence of
unexpected events in both the robotic system and the en-
vironment. Although the examples in the paper are simple,
the proposed scheme provides a framework for designing
complicated tasks for natural language controlled robotic
systems in handling more unexpected events in our future
work. In addition, using the perceptive feedback to assist
the natural language processing for converting the natural
language commands to the logic representations will also be
our future work.
REFERENCES
[1] C. Breazeal, A. Brooks, J. Gray, G. Hoffman, C. Kidd, H. Lee,
J. Lieberman, A. Lockerd and D. Mulanda, ”Humanoid robots as
cooperative partners for people,” Journal of Humanoid Robots, 1,
2004.
[2] J.Y. Chai, L. She, R. Fang, S. Ottarson, C. Littley, C. Liu and K.
Hanson, ”Collaborative effort towards common ground in situated
human robot dialogue,” 9th ACM/IEEE International Conference on
Human-Robot Interaction, Bielefeld, Germany, 2014.
[3] H.I. Christensen, G.M. Kruijff and J. Wyatt, Cognitive Systems,
Springer, 2010.
[4] R. Fang, C. Liu and J.Y. Chai, ”Integrating word acquisition and
refer- ential grounding towards physical world interaction,” 14th ACM
International Conference on Multimodal Interaction, ICMI ’12, 2012,
pp. 109-116.
[5] C. Liu, R. Fang and J.Y. Chai, ”Towards mediating shared percep-
tual basis in situated dialogue,” Proceedings of the SIGDIAL 2012
Conference, 2012, pp. 140-149,.
[6] C. Liu, R. Fang, L. She and J.Y. Chai, ”Modeling collaborative refer-
ring for situated referential grounding,” Proceedings of the SIGDIAL
2013 Conference, Metz, France, 2013, pp. 78-86.
[7] D. Roy, ”Grounding words in perception and action: computational
insights,” TRENDS in Cognitive Sciences, vol. 9, no. 8, pp. 389-396,
2005.
6677
0 s 5 s
7 s 11 s 12 s 14 s
“Pick up that red block”
15 s 17 s
Fig. 6. Results of natural language control: the robot is blocked
0 s 11 s 13 s 22 s 24 s
“Take the big brown block”
30 s 34 s 37 s
Fig. 7. Results of natural language control: the object is moved
0 s 3 s 4 s 6 s 12 s
“Grasp that red block”
14 s 19 s
Fig. 8. Results of natural language control: the object is swapped
[8] S. Lauria, T. Kyriacou, G. Bugmann, J. Bos, and E. Klein, ”Converting
natural language route instructions into robot-executable procedures,”
in Proceedings of the 2002 IEEEInternational Workshop on Robot and
Human Interactive Communication, 2002, pp. 223-228.
[9] A. J. Martignoni III and W. D. Smart, ”Programming robots using
high-level task descriptions,” in Proceedings of the AAAI Workshop
on Supervisory Control of Learning and Adaptive Systems, 2004, pp.
49-54.
[10] M. Nicolescu and M. J. Mataric, ”Learning and interacting in human-
robot domains,” IEEE Transactions on Systems, Man, and Cybernetics,
Part B: special issue on Socially Intelligent Agents-The Human in the
Loop, vol. 31, no. 5, pp. 419-430, 2001.
[11] M. Kloetzer, C. Belta, ”Temporal Logic Planning and Control of
Robotic Swarms by Hierarchical Abstractions,” IEEE Transactions on
Robotics, vol. 23, no. 2, pp. 320-330, 2007.
[12] C. Liu, J. Walker, and J. Y. Chai, ”Ambiguities in Spatial Language
Understanding in Situated Human Robot,” AAAI 2010 Fall Sympo-
sium on Dialogue with Robots, 2010.
[13] M. Skubic, D. Perzanowski, S. Blisard, A. Schultz, W. Adams, M.
Bugajska, D. Brock, ”Spatial language for human-robot dialogs,” IEEE
Transactions on Systems, Man, and Cybernetics, Part C: Applications
and Reviews, vol. 34, no. 2, pp. 154-167, 2004.
[14] S. Konrad and B. H. C. Cheng, ”Facilitating the construction of
speciﬁcation pattern-based properties,” in Proceedings of the IEEE
International Requirements Engineering Conference, 2005, pp. 329-
338.
[15] E. A. Topp, H. Huttenrauch, H. I. Christensen, and K. S. Eklundh,
”Bringing together human and robotics environmental representations-
a pilot study,” in Proceedings of the IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems, 2006, pp. 4946-4952.
[16] H. Kress-Gazit, G. E. Fainekos and G. J. Pappas, ”Temporal Logic-
based Reactive Mission and Motion Planning,” IEEE Transactions on
Robotics, vol. 25, no. 6, pp. 1370-1381, 2009.
[17] W. Takano and Y. Nakamura, ”Statistically integrated semiotics that
enables mutual inference between linguistic and behavioral symbols
for humanoid robots,” in Proceedings of the IEEE International Con-
ference on Robotics and Automation, 2009, pp. 646-652.
[18] W. Takano and Y. Nakamura, ”Bigram-based natural language model
and statistical motion symbol model for scalable language of humanoid
robots ,” in Proceedings of the IEEE International Conference on
Robotics and Automation, pp. 1231-1237, 2012.
[19] M. Ralph, M. A. Moussa, ”Toward a Natural Language Interface
for Transferring Grasping Skills to Robots,” IEEE Transactions on
Robotics, vol. 24, no. 2, pp. 468-475, 2008.
[20] X. He, T. Ogura, A. Satou, O. Hasegawa, ”Developmental Word
Acquisition and Grammar Learning by Humanoid Robots Through
a Self-Organizing Incremental Neural Network,” IEEE Transactions
on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 37, no.
5, pp. 1357-1372, 2007.
[21] H. Kress-Gazit, G. E. Fainekos and G. J. Pappas, ”Translating struc-
tured English to robotic controllers,” Advanced robotics, no. 22, pp.
1343- 1359, 2008.
[22] M. Steedman and J. Baldridge, ”Combinatory categorial grammar,”
Non-Transformational Syntax, Oxford: Blackwell, pp. 181-224, 2011.
[23] P. J. Ramadge and W. M. Wonham, ”The control of discrete event
systems,” Proceedings of the IEEE, vol. 77, no. 1, pp. 81-98, 1989.
[24] J. Goryca, R.C. Hill, ”Formal synthesis of supervisory control software
for multiple robot systems,” in Proceedings of American Control
Conference, 2013, pp.125-131.
[25] N. Xi, T.J. Tarn and A.K. Bejczy, ”Intelligent Planning and Control for
Multirobot coordination: An Event-Based Approach,” IEEE Transac-
tion On Robotics and Automation, vol. 12, no. 3, pp. 439-452, 1996.
[26] Y. Jia, N. Xi and J. Buether, ”Design of single-operator-multi-robot
teleoperation systems with random communication delay,” in Proceed-
ings of the IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2011, pp. 171-176.
[27] Y. Sun, N. Xi and Y. Wang, ”Modeling and analysis of perceptive robot
controller based on hybrid automata,” IEEE International Conference
on Robotics and Automation, 2004, pp. 2924-2929.
[28] Y. Jia and N. Xi, ”Coordinated Formation Control for Multi-Robot
Systems with Communication Constraints,” IEEE/ASME International
Conference on Advanced Intelligent Mechatronics, 2011, pp. 158-163.
6678
