Extracting Common Sense Knowledge from Text for Robot Planning
Peter Kaiser
1
Mike Lewis
2
Ronald P. A. Petrick
2
Tamim Asfour
1
Mark Steedman
2
Abstract— Autonomous robots often require domain knowl-
edge to act intelligently in their environment. This is particu-
larly true for robots that use automated planning techniques,
which require symbolic representations of the operating en-
vironment and the robot’s capabilities. However, the task of
specifying domain knowledge by hand is tedious and prone to
error. As a result, we aim to automate the process of acquiring
general common sense knowledge of objects, relations, and
actions, by extracting such information from large amounts of
natural language text, written by humans for human readers.
We present two methods for knowledge acquisition, requiring
only limited human input, which focus on the inference of
spatial relations from text. Although our approach is applicable
to a range of domains and information, we only consider one
type of knowledge here, namely object locations in a kitchen
environment. As a proof of concept, we test our approach using
an automated planner and show how the addition of common
sense knowledge can improve the quality of the generated plans.
I. INTRODUCTION AND RELATED WORK
Autonomous robots that use automated planning to make
decisions about how to act in the world require symbolic
representations of the robot’s environment and the actions
the robot is able to perform. Such models can be aided by the
presence of common sense knowledge, which may help guide
the planner to build higher quality plans, compared with the
absence of such information. In particular, knowledge about
default locations of objects (the juice is in the refrigerator)
or the most suitable tool for an action (knives are used for
cutting) could help the planner make decisions on which
actions are more appropriate in a given context.
For example, if a robot needs a certain object for a task, it
can typically employ one of two strategies in the absence of
prior domain knowledge: the robot can ask a human for the
location of the object, or the robot can search the domain
in an attempt to locate the object itself. Both techniques
are potentially time consuming and prevent the immediate
deployment of autonomous robots in unknown environments.
By contrast, the techniques proposed in this paper allow the
robot to consider likely locations for an object, informed by
common sense knowledge. This potentially improves plan
quality, by avoiding exhaustive search, and does not require
the aid of a human either to inform the robot directly or to
encode the necessary domain knowledge a priori.
While it is not possible to automatically generate all
the domain knowledge that could possibly be required,
we propose two methods for learning useful elements of
1
Institute for Anthropomatics and Robotics, Karlsruhe Institute of Tech-
nology, Karlsruhe, Germanyfpeter.kaiser, asfourg@kit.edu
2
School of Informatics, University of Edinburgh, Edinburgh,
Scotland, United Kingdom fmike.lewis, rpetrick,
steedmang@inf.ed.ac.uk
Fig. 1: The humanoid robots ARMAR-IIIa (left) and
ARMAR-IIIb working in a kitchen environment ([5], [6]).
domain knowledge based on information gathered from
natural language texts. These methods will provide the set
of object and action types for the domain, as well as certain
relations between entities of these types, of the kind that
are commonly used in planning. As an evaluation, we build
a domain for a robot working in a kitchen environment
(see Fig. 1) and infer spatial relations between objects in
this domain. We then show how the induced knowledge can
be used by an automated planning system. (The generated
symbols will not be grounded in the robot’s internal model;
however, approaches to establish these links given names of
objects or actions are available (e.g., [1], [2], [3] and [4]).)
The extraction of spatial relations from natural language
has been studied in the context of understanding commands
and directions given to robots in natural language (e.g.,
[7], [8], [9]). In contrast to approaches based on annotated
corpora of command executions or route instructions, or the
use of knowledge bases like Open Mind Common Sense
[10] explicitly created for artiﬁcial intelligence applications,
we extract relevant relations from large amounts of text
written by humans for humans. The text mining techniques
used in [11], [12], [13] to extract action-tool relations to
disambiguate visual interpretations of kitchen actions are
related. In [14], spatial relations are inferred based on search
engine queries and common sense databases.
In the following, we describe a process for learning
domain ontologies (Section II) and for extracting relations
(Section III). The last two sections evaluate both methods
(Section IV) and describe how the resulting knowledge can
be used in an automated planning system (Section V).
II. AUTOMATIC DOMAIN ONTOLOGY LEARNING
In this section, we propose a method for automatically
learning a domain ontologyD—a set of symbols that refer to
a robot’s environment or capabilities—with very little human
input. The method can be conﬁgured to learn a domain
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 European Union 3749
of objects or actions. With robotic planning in mind, it is
crucial that in either of these cases, the contained symbols
are not too abstract. In terms of a kitchen environment,
interesting objects might be saucepan, refrigerator or apple,
while abstract terms like minute or temperature that do not
directly refer to objects are avoided. Similarly, we focus on
actions that are directly applied to objects like knead, open
or screw, and ignore more abstract actions like have or think.
Automatic domain ontology learning is based on a
domain-deﬁning corpusC
D
, which contains texts concerning
the environment that the domain should model. For example,
a compilation of recipes is a good domain-deﬁning corpus
for a kitchen environment. Note that these texts have been
written by humans for human readers, and no efforts are
taken to make them more suitable for C
D
. However, C
D
needs to be part-of-speech (POS) tagged and possibly parsed
if compound nouns are to be appropriately recognized.
1
The domain-deﬁning corpus C
D
is used to retrieve an
initial vocabularyV which is then ﬁltered for abstract sym-
bols. Depending on the type of symbol that this vocabulary
is meant to model, only nouns or verbs are included in
V. In the ﬁrst step, C
D
is analyzed for word frequency
and the k most frequent words are extracted (see Alg. 1).
Only words with a part-of-speech-tag (POS-tag) equal to
p2fnoun; verbg are considered. The resulting vocabulary
V is then ﬁltered according to the score (w;p) which
expresses the concreteness of a word w.
Algorithm 1: learnDomainOntology(C
D
;p;k; 
min
)
1 V mostFrequentWords(C
D
;p;k)
2 D fw2D : (w;p) 
min
g
3 returnD
Fig. 2 gives an overview of the domain ontology learning
process. Additionally, it shows details about the relation
extraction procedure that will be discussed below, and the
interoperability between the two methods. In the following
section, we discuss the concreteness score  in detail.
A. The Concreteness 
Having a measure of concreteness is necessary for ﬁl-
tering symbols that are too abstract to play a role in our
target domain. In particular, the score (w;p) resembles the
concreteness of a word w with POS-tag p using the lexical
database WordNet ([16], [17]). For nouns, WordNet features
an ontology that differs between physical and abstract en-
tities. However, a word can have different meanings, some
of which could be abstract and others not. WordNet solves
this issue by working on word-senses rather than on words.
For a wordw with a sense
2
s from the setS(w) of possible
senses of w, we can compute a Boolean indicator c
w;s
that
1
We use the Stanford Parser [15] to do this.
2
WordNet numbers the different word-senses, so S(w)N.
Fig. 2: The process of domain ontology learning (left) and
relation extraction (right). The ontology resulting from the
ﬁrst method can be used as input to relation extraction.
tells us if s is a physical meaning of w:
c
w;s
=
(
1; if s is a physical meaning of w
0; otherwise:
(1)
WordNet also features a frequency measure f
w;s
that indi-
cates how often a wordw was encountered in the sense ofs,
based on a reference corpus. As we are not doing semantic
parsing onC
D
, we do not know which of the possible senses
of w is true. However, we can compute a weighted average
of the concreteness of the different meanings ofw, weighing
each word-sense with its likeliness:
(w) =
P
s2S(w)
f
w;s
c
w;s
P
s2S(w)
f
w;s
: (2)
As a byproduct,  can only have nonzero values for words
that are contained in WordNet, which ﬁlters out misspelled
words or parsing errors.
As there is no suitable differentiation in WordNet’s on-
tology for verbs, we can not apply the exact same approach
here. However, WordNet features a rough clustering of verbs
that we use to deﬁne the ﬁlter. We set c
w;s
to 1, if the
verb w with sense s is in one of the following categories:
verb.change, verb.contact, verb.creation or verb.motion.
III. RELATION EXTRACTION
The second technique we propose for information acquisi-
tion deals with relations between symbols, deﬁned using syn-
tactic patterns. Such patterns capture the syntactic contexts
that describe the relevant relations as well as the relations’
arguments. For example, the pattern
 
(#object; noun); (in; prep); (#location; noun)

(3)
describes a prepositional relation between two nouns using
the preposition in. The pattern also deﬁnes two classes,
3
#object and #location, which stand for the two arguments
of the relation. Given the above syntactic pattern, two types
of questions are relevant in this work:
3
In examples we use a hash to indicate a classname.
3750
Fig. 3: A dependency path for the fragment milk in refrig-
erator contains the words, their respective POS-tag and the
syntactic relation between them.
4
 Class inference: Is a symbolw more likely an object or
a location?
 Relation inference: What is the most likely location for
a symbol w?
The acquisition of relational information is interesting for
endowing a robot with initial knowledge of its environment.
Our main application for this method is the extraction of
spatial relations in a kitchen setting, such as the location
of common objects. However, the proposed method is not
constrained to objects and locations, and we will show
different use cases in the evaluation in Section IV-C.
The relation extraction process works in two phases:
 In the crawling phase, the text sources are searched for
predeﬁned syntactic patterns. Words falling into classes
deﬁned in the patterns are counted. The counts are
compiled to a set of distributions.
 In the query phase, information can be queried from the
distributions computed in the crawling phase. Different
kinds of queries are possible.
The foundation for relation extraction is the domain-
independent corpusC
I
. In contrast to the domain-deﬁning
corpusC
D
,C
I
contains unrestricted text. Because it is rare
for common sense information to be explicitly expressed,
the size of C
I
is crucial. We assume that the domain-
independent corpus is dependency parsed, i.e., consists of
syntactic dependency paths of the kind shown in Fig. 3. A
further discussion ofC
I
is given in Section IV-C.
In the following sections, we give a formal deﬁnition of a
syntactic pattern and explain the two phases in further detail.
Fig. 2 gives an overview of relation extraction.
A. Syntactic Patterns
The goal of the crawling phase is to search large amounts
of texts for syntactic patterns predeﬁned by the user. These
patterns are designed to specify a relation between classes of
words. For example, pattern (3) describes a spatial relation
between the two classes #object and #location. The fragment
milk in refrigerator would match the pattern and would result
in the assignments #object=milk and #location=refrigerator.
Formally, a syntactic pattern is deﬁned as a sequence of
tuples containing a symbol s
i
and a POS-tag p
i
:
 =
 
(s
1
;p
1
); ; (s
k
;p
k
)

: (4)
When matching the pattern to a sequence of words, the tuples
will match exactly one word of the sequence. The condition
for a match depends on the symbol s
i
:
4
NN - noun, IN - preposition
dobj - direct object, prep - preposition, pobj - prepositional object
 Ifs
i
is a word, thei-th tuple matches to this exact word
with POS-tag p
i
.
 Ifs
i
is a classname, thei-th tuple matches to all words
fromD having the POS-tag p
i
.
We will use the predicates isclass(s
i
) and isword(s
i
) to differ
between the two possible meanings of the symbol s
i
.
The search for matches happens on word-sequences. Such
sequences can represent sentences or, as is the case for
our corpusC
I
, dependency paths. A word-sequence contains
words w
i
together with their respective POS-tag t
i
:
 =
 
(w
1
;t
1
); ; (w
n
;t
n
)

; (5)
Alg. 2 decides if an element (s;p) from a syntactic pattern
matches an element (w;t) from a word-sequence. If the
symbol s is a class, it only checks if the word w is part
of the domain ontologyD
p
that contains the valid words
with POS-tag p. If s is a word, it must equal w. In both
cases, the POS-tags p and t have to match.
Algorithm 2: match((s;p); (w;t);D)
1 if isclass(s) then
2 return p =t^w2D
p
3 end
4 else if isword(s) then
5 return p =t^s =w
6 end
Alg. 3 describes the matching process for a complete
syntactic pattern (using Alg. 2). If a match is found, the class
conﬁguration is returned as a set of class assignments, i.e.,
class-word pairs. For example, using pattern (3), the fragment
milk in refrigerator results in the class conﬁguration:
K =

(#object; milk); (#location; refrigerator)
	
: (6)
Algorithm 3: conﬁguration(; ;D)
1 for i = 1; ;jj jj + 1 do
2 I f0; ;jj  1g
3 if match(
j+1
; 
i+j
;D)8j2I then
4 returnf(s
j+1
;w
i+j
) :j2I; isclass(s
j+1
)g
5 end
6 end
7 return;
B. The Crawling Phase
In the crawling phase,C
I
is searched for pattern matches
using Alg. 2 and Alg. 3. Two different distributions are then
computed based on the resulting class conﬁgurations:
 The Relation Distribution D
R
counts the occurrences
of class conﬁgurations (e.g., (6)). D
R
is suitable for
answering the question: How likely is a class conﬁgu-
ration for the relation induced by pattern ?
3751
 The Class Distribution D
C
counts the occurrences of
individual class assignments. It is suitable for answering
the question: How likely is a class for a given word?
Alg. 4 shows how D
R
and D
C
are computed given a set of
dependency pathsS and a syntactic pattern .
Algorithm 4: computeDistribution(S; ;D)
1 D
R
 Empty Relation Distribution
2 D
C
 Empty Class Distribution
3 foreach  = ((w
1
;t
1
); ; (w
n
;t
n
))2S do
4 K conﬁguration(; ;D)
5 ifK6=; then
6 D
R
[K] D
R
[K] + 1
7 foreach (c;w)2K do
8 D
C
[(c;w)] D
C
[(c;w)] + 1
9 end
10 end
11 end
12 return (D
R
;D
C
)
C. The Query Phase
The query phase uses the distributions D
R
and D
C
to
compute pseudo-probabilities for class assignments.
A class query (c;w) approximates the probability of a
word w falling into a class c. If   =fc
1
; ;c
l
g is the set
of deﬁned classes, the class query can be formulated as:
(c;w) =
D
C
[(c;w)]
P
x2 
D
C
[(x;w)]
: (7)
A relation query (Q;c

) approximates the prob-
ability of a relation with class-assignments Q =
f(c
1
;w
1
); ; (c
l
;w
l
)g, normalizing over the possible val-
ues of the class c

. With Q

=f(c;w)2 Q : c6= c

g, the
relation query can be formulated as:
(Q;c

) =
D
R
[Q]
P
v2D
D
R
[Q

[f(c

;v)g]
: (8)
In the evaluation, we will consider both types of queries.
IV. EVALUATION
To evaluate the proposed methods of domain learning
and relation extraction, we ﬁrst show that it is possible
to use a specialized corpus to generate a domain ontology
of entity types that matches people’s expectations for the
kitchen environment. We then use another, more general text
corpus, to infer spatial relations and action-tool relations for
those entities. These components are independent: we show
in Section V that hand speciﬁcation of the domain entities by
a human expert can aid the automated extraction processes.
A. Prerequisites
To learn a domain ontology using the proposed method,
the two text corporaC
D
andC
I
must ﬁrst be deﬁned.
1) The domain-deﬁning CorpusC
D
: This corpus is used
to generate an initial vocabulary by analysing word frequen-
cies.C
D
should therefore be reasonably large but, more im-
portantly, should contain descriptions of common objects and
actions from the desired domain. For a kitchen environment,
we chose to buildC
D
from a set of about 11,000 recipes,
5
with a total size of 19.5 MB.
2) The domain-independent Corpus C
I
: The domain-
independent corpus is used to sort entities into different
classes according to the results of syntactic pattern matches.
C
I
does not need to be a different corpus thanC
D
, but it
is difﬁcult to extract reliable information on rare relations
from small corpora. This is especially true for common sense
knowledge that isn’t often explicitly expressed. Hence,C
I
should be extensive. As it is often difﬁcult to gather large
amounts of text about a speciﬁc topic, it is useful to separate
C
I
fromC
D
, and use a large standard corpus forC
I
.
We use the Google Books Ngrams Corpus [18], in the
following referred to as the Google Corpus, which contains a
representation of 3.5 million English books containing about
345 billion words in total. The corpus is already parsed,
tagged and frequency counted. The Google Corpus does not
work on sentences, but on syntactic ngrams (Fig. 3), which
are n content-word long subpaths of the dependency paths.
We use the corpus in its arcs form which contains syntactic
ngrams with two content-words (n = 2) plus possible non-
content-words like prepositions or conjunctions. However,
the proposed methods can also be used in combination with
corpora containing longer syntactic ngrams.
B. Domain Ontology Learning
Using the corpora mentioned above, we can run the
method for automatic domain ontology learning. Generating
a domain ontology for nouns using parameter values of
k = 300; 
min
= 0:35 results in an ontology of 198 words of
which the 80 most frequent ones are listed in Table I. The 20
most frequent nouns that were part of the initial vocabulary,
but did not pass the concreteness ﬁlter are listed in Table II.
Analogously, the 80 most frequent actions from a do-
main ontology learnt from verbs using the parameters k =
300; 
min
= 0:2 are depicted in Table III. The full domain
ontology contains 206 verbs. The 20 most frequent verbs that
did not pass the concreteness ﬁlter are listed in Table IV.
Results show that for objects, as well as actions, the
generated domain ontologies are reasonable, but contain
obvious mistakes. For example, the concrete noun cream
was rejected while abstract nouns like top and bottom were
included. The reason for this is the diversity of possible word
senses present in WordNet that can mislead the ﬁlter .
To evaluate the strength of the domain learning method,
we asked four people
6
to manually extract kitchen-related
objects and actions from the sets of the 300 most frequent
nouns and verbs fromC
D
. Fig. 4 shows the F
1
-scores of
the automatically learnt domain ontologies for objects and
5
From http://www.ehow.com.
6
Native speakers of English, not involved in the research.
3752
TABLE I: Automatic domain ontology learning (objects)
k = 300; 
min
= 0:35
1 wine 21 milk 41 container 61 salad
2 water 22 bottle 42 home 62 tea
3 meat 23 fruit 43 bag 63 grill
4 bowl 24 pot 44 garlic 64 center
5 sugar 25 dough 45 skillet 65 soup
6 mixture 26 glass 46 hand 66 alcohol
7 pan 27 side 47 lid 67 coffee
8 oil 28 pepper 48 onion 68 beer
9 top 29 meal 49 skin 69 sheet
10 oven 30 ﬂour 50 saucepan 70 world
11 salt 31 ﬁsh 51 egg 71 diet
12 dish 32 refrigerator 52 beef 72 freezer
13 cheese 33 drink 53 layer 73 blender
14 cup 34 chocolate 74 piece 89 batter
15 butter 35 turkey 55 liquid 75 pasta
16 chicken 36 bottom 56 spoon 76 pork
17 juice 37 cake 57 surface 77 addition
18 bread 38 place 58 restaurant 78 dinner
19 rice 39 ice 59 fat 79 vodka
20 sauce 40 knife 60 plate 80 powder
TABLE II: Not part of the object domain ontology
k = 300; 
min
= 0:35
1 time 6 taste 11 temperature 16 type
2 ﬂavor 7 way 12 day 17 tbsp
3 heat 8 variety 13 process 18 color
4 recipe 9 cream 14 boil 19 hour
5 food 10 amount 15 cooking 20 half
actions, using different values for 
min
, compared to the
domains created by the human participants. The results show
that enabling the concreteness ﬁlter (
min
> 0) signiﬁcantly
increases the quality of the resulting domain for nouns as
well as for verbs. The results also show that values of roughly

min
> 0:5 result in a too restrictive ﬁlter. While in the case
of nouns, the restrictive ﬁlter still produces a better domain
than if no ﬁlter is applied, this is not true for verbs: the qual-
ity of a verb-domain drops dramatically the more restrictive
the ﬁlter gets. The reason for the difference between the two
plots is that verbs often have a variety of possible meanings.
By contrast, nouns usually have a predominant interpretation,
at least in terms of the differentiation between physical and
abstract meanings. This is also reﬂected in the fact that the
participants found it signiﬁcantly harder to create a domain
of actions than to create a domain of nouns.
C. Inference
We now evaluate the relation and class inference mecha-
nisms described in Section III. To illustrate the capabilities of
these methods, we generated the results using the manually
created domain ontology as a gold standard. We additionally
show how false positives can affect the process by using the
automatically learnt domain ontology. The parameter 
min
can be determined in practice by generating and evaluating
an ontology for a subset of the initial vocabulary using plots
similar to Fig. 4. Different syntactic patterns can be used to
conduct different kinds of inference. The following sections
show examples of possible queries.
1) Location Inference: A good use of knowledge acqui-
sition is the exploration of spatial relations between objects
TABLE III: Automatic domain ontology learning (actions)
k = 300; 
min
= 0:2
1 add 21 cool 41 come 61 stick
2 make 22 ﬁll 42 press 62 beat
3 place 23 leave 43 freeze 63 clean
4 remove 24 go 44 garnish 64 begin
5 cook 25 bring 45 pick 65 burn
6 pour 26 hold 46 open 66 spread
7 stir 27 reduce 47 slice 67 replace
8 do 28 follow 48 become 68 whisk
9 put 29 heat 49 refrigerate 69 boil
10 take 30 pan 50 soak 70 produce
11 get 31 sprinkle 51 dip 71 preheat
12 turn 32 dry 52 form 72 squeeze
13 set 33 start 53 shake 73 chill
14 cut 34 melt 74 cause 89 top
15 cover 35 sit 55 pull 75 peel
16 mix 36 chop 56 break 76 ﬁt
17 combine 37 drain 57 wash 77 move
18 create 38 rinse 58 simmer 78 coat
19 prepare 39 blend 59 lay 79 increase
20 bake 40 roll 60 transfer 80 seal
TABLE IV: Not part of the action domain ontology
k = 300; 
min
= 0:2
1 be 6 keep 11 eat 16 check
2 use 7 let 12 choose 17 enjoy
3 have 8 allow 13 need 18 give
4 serve 9 try 14 help 19 see
5 show 10 ﬁnd 15 buy 20 want
and locations using prepositional contexts. For instance,
pattern (3) matches fragments where two nouns, #object
and #location, are linked by the preposition in. This pattern
can be used in combination with the above object ontology
(Table I) to infer spatial relations in a kitchen environment.
Table V shows the most likely locations for the ten most
frequent objects from the automatically learnt domain ontol-
ogy.
7
Note that for generating the results we used pattern (3)
combined with three similar patterns using the prepositions
on, at and from. Table V presents two sets of locations for
each object: the upper, highlighted rows refer to the manually
created domain ontology and the lower, non-highlighted rows
refer to the automatically learnt domain ontology in Table I.
Results from the automatically learnt domain ontologies
are more noisy, and distractive terms like side or bottom
haven’t been ﬁltered out. (We can also tune domain genera-
tion to work in a more restrictive way, e.g., by using theF
0:5
measure instead of F
1
to emphasize precision over recall.)
The results demonstrate that the system is able to infer
typical locations for objects. However, two problems con-
strain its performance. First, the automatically learnt domain
ontology does not contain typical locations like cupboard
or drawer, because these words do not frequently appear
in the initial vocabulary. Second, the system is not able to
differ between container objects like pot or pan, and actual
locations like refrigerator or oven (i.e., objects that have a
ﬁxed position in the kitchen). Improving the domain entity
speciﬁcation by using more diverse but relevant domain spe-
ciﬁc corpora is the subject of ongoing research. In Section V
7
We consider top and oven not to be objects.
3753
0.0 0.2 0.4 0.6 0.8 1.0
Concreteness Filter Threshold ? min
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
F1 -Score (Objects)
F1 -Score (Actions)
Fig. 4: Automatically learnt domain ontologies are evaluated
using different values of 
min
, by comparing them to domain
ontologies created manually by human participants.
TABLE V: Results for location inference
Highlighted rows: Manually created domain ontology. Non-highlighted
rows: Automatically learnt domain ontology.
refr. - refrigerator, scp. - saucepan, swc. - sandwich, kit. - kitchen
object ﬁrst second third fourth
wine
glass / 0.20 bottle / 0.20 table / 0.15 cup / 0.14
glass / 0.13 bottle / 0.13 table / 0.10 cup / 0.09
water
surface / 0.09 bottle / 0.07 water / 0.07 glass / 0.07
bottom / 0.06 side / 0.06 surface / 0.05 bottle / 0.04
meat
table / 0.08 pan / 0.06 swc. / 0.05 pot / 0.05
diet / 0.11 table / 0.05 pan / 0.04 swc. / 0.04
bowl
table / 0.51 refr. / 0.06 stem / 0.05 kit. / 0.05
table / 0.27 hand / 0.20 top / 0.06 side / 0.05
sugar
water / 0.15 bowl / 0.15 scp. / 0.10 milk / 0.08
water / 0.13 bowl / 0.13 scp. / 0.08 milk / 0.06
mixture
pan / 0.10 bowl / 0.08 water / 0.08 dish / 0.08
top / 0.10 pan / 0.07 bowl / 0.05 water / 0.05
pan
oven / 0.40 stove / 0.19 rack / 0.18 pan / 0.02
oven / 0.29 stove / 0.14 rack / 0.13 hand / 0.07
oil
skillet / 0.22 pan / 0.19 scp. / 0.08 board / 0.07
skillet / 0.19 pan / 0.16 scp. / 0.07 board / 0.06
salt
water / 0.40 bowl / 0.17 scp. / 0.05 food / 0.04
water / 0.33 bowl / 0.14 diet / 0.06 scp. / 0.04
dish
table / 0.30 oven / 0.24 menu / 0.11 pan / 0.03
table / 0.20 oven / 0.16 hand / 0.12 top / 0.04
we show the effect of more helpful entity speciﬁcation.
2) Tool Inference: A similar approach that also includes
the action domain ontology uses the preposition with to infer
relations between actions and tools. The following syntactic
pattern matches a verb #action and a noun #tool from the
respective domain ontology, linked together by with:
 
(#action; verb); (with; prep); (#tool; noun)

: (9)
Table VI shows the three most probable tools for different
actions from the kitchen domain. The results are shown for
both the manually created domain ontology (upper rows,
highlighted) and the automatically learnt one (lower rows).
3) Class Inference: Another possible result the system can
compute is the probability that a word falls into a certain
class of syntactic pattern. For example, given the above
pattern (3), the system can approximate the probability that a
word names an object or a location (Table VII). These results
TABLE VI: Results for tool inference
Highlighted rows: Manually created domain ontology. Non-highlighted
rows: Automatically learnt domain ontology.
object ﬁrst second third
cut
knife / 0.80 fork / 0.01 machine / 0.01
knife / 0.68 hand / 0.04 world / 0.03
ﬂip
spatula / 0.89 spoon / 0.06 fork / 0.03
spatula / 0.65 hand / 0.24 spoon / 0.05
mash
fork / 0.58 spoon / 0.16 butter / 0.09
fork / 0.59 spoon / 0.16 butter / 0.09
stir
spoon / 0.50 fork / 0.20 spatula / 0.08
spoon / 0.48 fork / 0.19 spatula / 0.08
can be used to improve the location inference results, e.g.,
by dropping words that seem unlikely to name a location.
TABLE VII: Results for class inference
Manually created domain ontology (left), automatically learnt
domain ontology (right)
symbol object location object location
wine 0.80 0.20 0.85 0.15
water 0.48 0.52 0.56 0.44
meat 0.77 0.23 0.81 0.19
bowl 0.11 0.89 0.16 0.84
sugar 0.93 0.07 0.95 0.05
mixture 0.72 0.28 0.77 0.23
pan 0.17 0.83 0.18 0.82
oil 0.82 0.18 0.83 0.17
oven 0.12 0.88 0.11 0.89
salt 0.95 0.05 0.96 0.04
4) Computation Time: In this work, the domain ontology
learning and distribution computation steps are considered to
be run ofﬂine. However we note that the computation time for
these steps depends heavily on the sizes and representations
of C
D
and C
I
. Processing the Google Corpus
8
requires
especially high computational power. On the other hand, the
inference step consists of simple lookups in precomputed
distributions, and can therefore be done online.
V. PLANNING WITH COMMON SENSE KNOWLEDGE
In this section we show how the domain knowledge
induced by the processes described above can be used with
an automated planning system to improve the quality of
generated plans. We have chosen to use the PKS (Planning
with Knowledge and Sensing [19], [20]) planner for this
task, since PKS has previously been deployed in robot
environments like the one in Fig. 1 [21]. However, one of
the strengths of the above approach is that it is not planner
(or domain) dependent, and the method we outline for PKS
can be adapted to a range of different planners and domains.
As an example scenario, we will focus on the use of
spatial relations in a small kitchen domain. The domain
contains the entities cereal, counter, cup, cupboard, juice,
plate, refrigerator and stove. Table VIII shows the results of
the location inference method. We will ﬁrst postprocess this
data for planning by considering the entity juice.
A. Postprocessing
Given the initial domain of objects, and using pattern (3),
we can approximate the probability of an object o being
8
The Google Arcs Corpus contains 38G of compressed text.
3754
TABLE VIII: Location inference for a small domain
Omitted values are zero.
object counter
cup
cupboard
dishwasher
juice
plate
refrigerator
stove
cereal 1.00
cup 0.25 0.33 0.13 0.01 0.18 0.02 0.08
juice 0.02 0.43 0.21 0.08 0.26
plate 0.14 0.10 0.07 0.58 0.06 0.06
spatially related to a location l by issuing a relation query:
P (loc =ljobj =o) =


(obj;o); (loc;l)
	
;loc

: (10)
To put these results into a suitable form for planning, we in-
troduce the predicate at and output the computed likelihoods
for pairs of objects. The resulting relations that are extracted,
and their likelihoods, are shown in the top half of Table IX.
The postprocessor must now reﬁne the results, possibly
making use of additional information about the structure
of the planning domain and the types of objects that are
available. Reﬁnement can be done in three possible ways:
1) Symbol Mapping: A word that describes an object in
natural language may not necessarily match the symbol
name for that object in the planning domain. This is
currently corrected by an appropriate mapping process
that uses a dictionary of likely synonyms. E.g., the
word refrigerator may be mapped to fridge.
2) Type Filtering: Many planners have the concept of
object types, which enables us to ﬁlter relations that
have entity arguments of the incorrect type. Assuming
the planning domain provides us with a type location
that is required for the second argument of at, the
postprocessor can then remove the extracted relations
at(juice, cup), at(juice, juice), and at(juice, plate), since
the entities cup, juice, and plate are not locations.
3) Instantiation: The symbols extracted by our processes
will often refer to classes of objects, rather than the
speciﬁc object identiﬁers used by the planner. Making
use of type information in the planning domain, the
postprocessor can instantiate objects of the appropriate
types from the extracted relational information. For
instance, the class juice might be instantiated into
two objects, applejuice and orangejuice. These objects
can subsequently be substituted in any relations that
contains the appropriate class type.
The ﬁnal set of postprocessed relations from our example
is shown in the bottom half of Table IX. We note that the ne-
cessity and possibility of applying these postprocessing steps
depends on the nature of the planning domain. Furthermore,
the information that is needed to perform the postprocessing,
i.e., the symbol mapping table or the type information, needs
to be manually encoded in the planning domain.
Given the postprocessed set of relations, the ﬁnal step
is to decide how this information will be included in the
planning domain. For planners that work with probabilistic
representations, the relation/likelihood information could be
TABLE IX: Extracted and postprocessed relations
Extracted
Relations Likelihood
at(juice, cup) 0.43
at(juice, refrigerator) 0.27
at(juice, juice) 0.21
at(juice, plate) 0.08
at(juice, counter) 0.02
Postprocessed
Relations Likelihood
at(applejuice,fridge) 0.27
at(applejuice,counter) 0.02
at(orangejuice,fridge) 0.27
at(orangejuice,counter) 0.02
directly encoded. For planners like PKS that do not deal with
probabilities, there are two main possibilities:
1) The most probable location for each object could be
encoded as a single fact in the planner’s knowledge,
i.e., at(applejuice, fridge) and at(orangejuice, fridge).
2) Some or all of the most probable locations could be
encoded as a disjunction of possible alternatives, i.e.,
at(applejuice, fridge)j at(applejuice, counter), and
at(orangejuice, fridge)j at(orangejuice, counter).
Depending on the domain, either form may be appropriate.
B. Plan Generation
Consider the task of ﬁnding the apple juice container in
the kitchen. In the absence of precise information as to the
object’s location, but knowing there are various places in the
kitchen where objects could be located (e.g., counter, cup-
board, fridge, stove), a planner could potentially build a plan
for a robot to exhaustively check all locations: move-robot-to-
counter, check-for-apple-juice, if not present move-robot-to-
cupboard, check-for-apple-juice, if not present move-robot-
to-fridge, etc., until all locations have been checked. If the
robot does not have information-gathering capabilities to
check for the apple juice in a particular location, the planner
may not be able to generate such a plan at all.
With the availability of more certain information about
the location of the apple juice, the planner can potentially
eliminate some parts of the plan (e.g., by ignoring certain lo-
cations), or at least prioritise certain likely locations over oth-
ers, resulting in higher quality plans. For instance, in the case
that the planner had the knowledge at(applejuice, fridge),
resulting from the above relation extraction process, then
the planner could build the simple plan move-robot-to-fridge,
under the assumption that the extracted information was true.
Similarly, if the planner had the disjunctive informa-
tion at(applejuice, fridge)j at(applejuice, counter) then the
planner could build the plan: move-robot-to-fridge, check-
for-apple-juice, if not present move-robot-to-counter. Again,
this plan improves on the exhaustive search plan by only
considering the most likely locations for the apple juice,
resulting from the extracted relational information.
One inherent danger when dealing with common sense
knowledge is that the plans that are built from such infor-
mation alone may ultimately fail to achieve their goals in
3755
the real world. For instance, even though relation extraction
provides us with likely locations for the apple juice, there
is no guarantee that this is the way the robot’s world is
actually conﬁgured. (E.g., another robot may have left the
apple juice on the stove.) However, such information does
give us a starting point for building plans, in the absence of
more certain information, and can also aid plan execution
monitoring to guide replanning activities in the case of plan
failure. (E.g., if a plan built using common sense knowledge
fails to locate the apple juice, fall back to the exhaustive
search plan for the locations that haven’t been checked.)
Finally, we note that the use of common sense knowledge
may improve the efﬁciency of plan generation, since in
general more speciﬁc information helps constrain the plan
generation process. However, plan generation time is both
domain and planner dependent, and it is difﬁcult to quantify
any improvements without experimentation. (E.g., planning
time went from 0.003s to 0.001s in our small examples.)
VI. CONCLUSION AND FUTURE WORK
We have presented two techniques for reducing the amount
of prior, hardcoded knowledge that is necessary for building
a robotic planning domain. Using the methods described
here, a domain ontology of object and action types can be
deﬁned automatically, over which user-deﬁned relations can
be inferred automatically from sources of natural language
text. The resulting representation of common sense domain
knowledge has been tested using an automated planning
system, improving the quality of the generated plans.
As future work, we are exploring a number of improve-
ments to our techniques. First, more specialized corporaC
I
,
longer syntactic patterns, or databases of common sense
knowledge might help in overcoming the sparsity of com-
mon sense information in text sources. Second, the location
inference does not perform any checks for plausibility. While
the class inference will help in ﬁltering results that are not
locations at all, additional methods are needed to differentiate
between locations for temporary storage and locations for
long-term storage. Another interesting improvement would
be the generalization of inferred relations to still missing
knowledge. For example one could conclude by analyzing
text sources that bowl and dish are conceptually similar and
therefore apply relations inferred for bowls also to dishes.
Finally, we are investigating the application of our methods
to robot domains other than the kitchen environment.
ACKNOWLEDGMENT
The research leading to these results received funding from
the European Union’s 7th Framework Programme FP7/2007-
2013, under grant agreement N
o
270273 (Xperience).
REFERENCES
[1] M. Ternoth, U. Klank, D. Pangercic, and M. Beetz, “Web-enabled
robots,” Robotics & Automation Magazine, vol. 18, no. 2, pp. 58–68,
2011.
[2] M. Waibel, M. Beetz, J. Civera, R. D’Andrea, J. Elfring, D. Galvez-
Lopez, K. Haussermann, R. Janssen, J. Montiel, A. Perzylo,
B. Schiessle, M. Tenorth, O. Zweigle, and R. van de Molengraft,
“Roboearth,” Robotics Automation Magazine, IEEE, vol. 18, no. 2,
pp. 69–82, 2011.
[3] K. Welke, P. Kaiser, A. Kozlov, N. Adermann, T. Asfour, M. Lewis,
and M. Steedman, “Grounded spatial symbols for task planning based
on experience,” in 13th International Conference on Humanoid Robots
(Humanoids). IEEE/RAS, 2013.
[4] A. Kasper, R. Becher, P. Steinhaus, and R. Dillmann, “Developing and
analyzing intuitive modes for interactive object modeling,” in ICMI
’07: Proceedings of the 9th international conference on Multimodal
interfaces. New York, NY , USA: ACM, 2007, pp. 74–81.
[5] T. Asfour, K. Regenstein, P. Azad, J. Schr¨ oder, N. Vahrenkamp,
and R. Dillmann, “ARMAR-III: An integrated humanoid platform
for sensory-motor control,” in IEEE International Conference on
Humanoid Robots (Humanoids), 2006, pp. 169–175.
[6] T. Asfour, P. Azad, N. Vahrenkamp, K. Regenstein, A. Bierbaum,
K. Welke, J. Schr¨ oder, and R. Dillmann, “Toward humanoid manip-
ulation in human-centred environments,” Robotics and Autonomous
Systems, vol. 56, no. 1, pp. 54–65, 2008.
[7] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller,
and N. Roy, “Understanding natural language commands for robotic
navigation and mobile manipulation,” in Proceedings of the 25th
National Conference on Artiﬁcial Intelligence. AAAI, 2011, pp.
1507–1514.
[8] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward understanding
natural language directions,” in Proceedings of the 5th International
Conference on Human-Robot Interaction (HRI). IEEE, 2010, pp.
259–266.
[9] D. Chen and R. Mooney, “Learning to interpret natural language
navigation instructions from observations,” in Proceedings of the 25th
AAAI Conference on Artiﬁcial Intelligence (AAAI-2011), 2011, pp.
859–865.
[10] P. Singh, T. Lin, E. T. Mueller, G. Lim, T. Perkins, and W. L. Zhu,
“Open mind common sense: Knowledge acquisition from the general
public,” in On the Move to Meaningful Internet Systems 2002: CoopIS,
DOA, and ODBASE. Springer, 2002, pp. 1223–1237.
[11] C. Teo, Y . Yang, H. Daum´ e III, C. Ferm¨ uller, and Y . Aloimonos, “A
corpus-guided framework for robotic visual perception,” in Workshop
on Language-Action Tools for Cognitive Artiﬁcial Agents, held at the
25th National Conference on Artiﬁcial Intelligence. San Francisco:
AAAI, 2011, pp. 36–42.
[12] ——, “Toward a Watson that sees: Language-guided action recogni-
tion for robots,” in IEEE International Conference on Robotics and
Automation. St. Paul, MN: IEEE, 2012, pp. 374–381.
[13] M. Tamosiunaite, I. Markelic, T. Kulvicius, and F. W¨ org¨ otter, “Gen-
eralizing objects by analyzing language,” in 11th International Con-
ference on Humanoid Robots (Humanoids). IEEE/RAS, 2011, pp.
557–563.
[14] K. Zhou, M. Zillich, H. Zender, and M. Vincze, “Web mining driven
object locality knowledge acquisition for efﬁcient robot behavior,”
in 2012 International Conference on Intelligent Robots and Systems
(IROS). IEEE/RSJ, 2012, pp. 3962–3969.
[15] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” Pro-
ceedings of the 41st Annual Meeting on Association for Computational
Linguistics ACL 03, vol. 1, pp. 423–430, 2003.
[16] G. A. Miller, “WordNet: a lexical database for English,” Communica-
tions of the ACM, vol. 38, pp. 39–41, 1995.
[17] C. Fellbaum, WordNet: An Electronic Lexical Database. Cambridge,
MA: MIT Press, 1998.
[18] Y . Goldberg and J. Orwant, “A dataset of syntactic-ngrams over time
from a very large corpus of english books,” in Second Joint Conference
on Lexical and Computational Semantics, 2013, pp. 241–247.
[19] R. P. A. Petrick and F. Bacchus, “A knowledge-based approach to
planning with incomplete information and sensing,” in International
Conference on Artiﬁcial Intelligence Planning and Scheduling (AIPS-
2002), 2002, pp. 212–221.
[20] ——, “Extending the knowledge-based approach to planning with
incomplete information and sensing,” in International Conference on
Automated Planning and Scheduling (ICAPS 2004), 2004, pp. 2–11.
[21] R. Petrick, N. Adermann, T. Asfour, M. Steedman, and R. Dillmann,
“Connecting knowledge-level planning and task execution on a hu-
manoid robot using Object-Action Complexes,” in Proceedings of the
International Conference on Cognitive Systems (CogSys 2010), 2010.
3756
