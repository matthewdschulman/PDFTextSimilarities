Learning Depth-Sensitive Conditional Random Fields for Semantic
Segmentation of RGB-D Images
Andreas C. M¨ uller and Sven Behnke
Abstract—We present a structured learning approach to
semantic annotation of RGB-D images. Our method learns to
reason about spatial relations of objects and fuses low-level
class predictions to a consistent interpretation of a scene. Our
modelincorporatescolor,depthand3Dscenefeatures,onwhich
an energy function is learned to directly optimize object class
prediction using the loss-based maximum-margin principle of
structural support vector machines. We evaluate our approach
on the NYU V2 dataset of indoor scenes, a challenging dataset
covering a wide variety of scene layouts and object classes. We
hard-code much less information about the scene layout into
our model then previous approaches, and instead learn object
relations directly from the data. We ﬁnd that our conditional
random ﬁeld approach improves upon previous work, setting
a new state-of-the-art for the dataset.
I. INTRODUCTION
For robots to perform varied tasks in unstructured envi-
ronments, understanding their surroundings is essential. We
formulate the problem of semantic annotation of maps as
a dense labeling of RGB-D images into semantic classes.
Dense labeling of measured surfaces allows for a detailed
reasoning about the scene.
In this work, we propose the use of random forests
combined with conditional random ﬁelds (CRF) to perform
robust estimation of structure classes in RGB-D images. The
CRF is learned using a structural support vector machine,
allowing it to integrate the noisy categorization produced by
a pixel-based random forest to a consistent interpretation of
the scene.
We thereby extend the success of learned CRF models for
semantic segmentation in RGB images to the domain of 3D
scenes. Our emphasis lies on exploiting the additional depth
and 3D information in all processing steps, while relying on
learning to create a model that is adjusted to the properties
of the sensor input and environment.
Our approach starts with a random forest, providing a
noisy local estimate of semantic classes based on color
and depth information. These estimates are grouped together
using a superpixel approach, for which we extend previous
superpixel algorithms from the RGB to the RGB-D domain.
We then build a geometric model of the scene, based on
the neighborhood graph of superpixels. We use this graph
not only to capture spatial relations in the 2D plane of the
image, but also to model object distances and surface angles
in3D,usingapointcloudgeneratedfromtheRGB-Dimage.
The process is illustrated in Figure 1.
All authors are with the Autonomous Intelligent Systems Group, Univer-
sity of Bonn, Germany. Email: amueller@ais.uni-bonn.de
CRF Prediction
RGB Image
Depth Image
Superpixel 
3D Point Cloud
Random Forest 
Unary Features
Pairwise Features
Fig. 1. Overview of the proposed semantic segmentation method.
We assess the accuracy of our model on the challeng-
ing NYU Segmentation V2 dataset [1], where our model
outperforms previous approaches. Our analysis shows that
while our random forest model already has competitive
performance, the superpixel-based grouping and in particular
the loss-based learning are integral ingredients of the success
of our method.
II. RELATED WORK
The task of dense semantic annotation of 3D maps has
seen an increased interest in recent years. Early work in-
cludes the approach of N¨ uchter and Hertzberg [2], who com-
bine 6D SLAM, surface annotation, and object recognition
to build semantically annotated maps. The approach was
demonstrated on a mobile robot in an indoor environment.
More recently, Sengupta et al. [3] introduced a dataset
of semantically annotated street-scenes in a closed track,
captured as pairs of stereo images. They approach the task
by jointly reasoning about 3D layout and semantics of
the scenes and produce a dense labeling on image level.
Sengupta et al. [4] extended their approach to produce
a volumetric reconstruction of the scene, together with a
dense semantic labeling of the volumetric representation.
This image segmentation method builds on the hierarchical
CRF approach of Ladicky et al. [5], which is similar in spirit
toourapproach,butusesPottspotentialstogetherwithcross-
validation to set potentials.
Recent work on indoor semantic annotation of maps
mostly focused on RGB-D images, which are now easy to
obtain using structured light sensors. St¨ uckler et al. [6], for
example, used a Random Forest to obtain a dense semantic
labeling of images and integrated predictions over multiple
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6232
views in 3D. They evaluated their approach on table-top and
simple indoors scenes. Silberman and Fergus [7] introduced
the NYU Depth Dataset V1 dataset, which consisted of a
large variety of densely annotated indoor scenes, captured
as RGB-D images. Their work also introduced a baseline
method for semantic segmentation of RGB-D image, which
is based on a CRF over superpixels, with unary potentials
given by interest point descriptors. While pairwise potentials
for the CRF were carefully designed for the dataset, po-
tentials were either directly set by hand or estimated using
empirical frequencies. This is in contrast to our work, which
applies structured prediction techniques to learn potentials
thatoptimizepredictiveperformance.Renetal.[8]evaluated
the design of features for semantic labeling of RGB-D data,
and used a hierarchical segmentation to provide context.
While they also deﬁned a CRF on superpixels, their model
is again not learned, but a weighted Potts model, using only
a probability of boundary map, and not taking spatial layout
into account at all. Silberman et al. [1] extended the NYU
Depth Dataset V1 to the NYU Depth Dataset V2 that we
are using in this work. Their focus is on inferring support
relationsinindoorscenes,suchasobjectsrestingontablesor
shelfs,whichinturnrestontheﬂoor.Theirapproachisbased
on robust estimation of 3D plane hypotheses, which are then
jointlyoptimizedwithsupportrelationsandstructureclasses.
Silberman et al. [1] use a complex pipeline, employing
signiﬁcant domain knowledge. In this work, on the other
hand, we try to learn all relevant domain speciﬁc features
directly from the data, which allows us to out-perform the
work of Silberman et al. [1] with respect to structure class
segmentation.
Couprie et al. [9] approach the task of semantic segmen-
tation of structure classes in RGB-D using the paradigm of
convolutional neural networks, extending previous work of
Farabetetal.[10]andSchulzandBehnke[11].Similartoour
approach, Couprie et al. [9] combine the output of a pixel-
based, low-level learning algorithm with an independent
unsupervisedsegmentationstep.Incontrasttotheirwork,we
improve our results by not only averaging predictions within
superpixels, but also explicitly learning interactions between
neighboring superpixels, favoring a consistent interpretation
of the whole image.
St¨ uckler et al. [12] extend the approach of St¨ uckler et al.
[6] to a real-time system for online learning and prediction
of semantic classes. They use a GPU implementation of
random forests, and integrate 3D scene information in an
online fashion. They evaluate their approach on the dataset
of Silberman et al. [1] with good results. We use the
implementation of random forest provided by St¨ uckler et
al. [12], but instead of integrating predictions over time, we
focus on exploiting the structure within a single frame.
While many of the works mentioned in this chapter make
use of a CRF approach, we are not aware of any prior work
on semantic annotation of 3D maps that fully learns their
potentials.
Fig. 2. Visualization of the height computed using the method described
in Section III-B. Input images are shown on the left (depth not shown), the
computed height is depicted on the right. The top row exempliﬁes a typical
scene, while the bottom row shows a scene without horizontal surfaces,
where our method fails.
III. LEARNING DEPTH-SENSITVE CONDITIONAL
RANDOM FIELDS
We take a CRF approach, whose nodes represent a la-
beling of superpixels. We use an energy consisting of ﬁrst
and second order factors (also called unary and pairwise
potentials), with learned potential functions. Let us denote
the representation of an input image by x and a labeling
of superpixels into semantic classes by y. Then the general
form of the energy is
g(x,y)=
X
v?V
?
v
(x,y
v
)+
X
(v,w)?E
?
v,w
(x,y
v
,y
w
). (1)
HereV enumerates the superpixels, andE ?V ?V is a set
of edges, encoding adjacence between superpixels.
We learn the unary and pairwise energy functions ?
v
and
?
v,w
from the training data using a structural support vector
machine (SSVM) [13]. The concept of SSVMs allows for
a principled, maximum-margin based, loss-sensitive training
of CRFs. Learning the potential yields much more complex
interactions than the simple Potts potentials that are often
used in the literature.
In general, structural SSVMs learn the parameters ? of a
predictor of the form
f(x)=argmax
y?Y
?
T
?(x,y). (2)
We choose ? in Equation (1) to be linear in the learnable
parameters and the data-depended features, resulting in a
formequivalenttoEquation(2).Ourfeaturesaredescribedin
detail below. We use the1-slack formulation of the structural
SVM[13]andsolvethemaximizationinEquation(2)usinga
combination of fusion moves [14] and the AD
3
algorithm of
Martins et al. [15]. In contrast to graph-cut inference, fusion
moves and AD
3
can work with arbitrary potential functions,
and allow precise learning using the SSVM approach.
6233
A. Low Level Segmentation
We take a super-pixel based approach to semantic seg-
mentation. Our superpixel generation is based on the SLIC
algorithm [16]. We extend the standard SLIC algorithm,
which works on the Lab space, to also include depth in-
formation. The resulting algorithm is a localizedk-Means in
Lab-D-XY space. Our implementation is publicly available
through the scikit-image library
1
. Similar to Silberman et
al. [1], we found little visual improvement over the RGB
segmentation when using additional depth information. On
the other hand, estimation of per-superpixel features based
onthe3Dpointcloudwasmorerobustwhenincludingdepth
information into the superpixel procedure. The resulting
superpixels are compact in the 2D image. As the density
of the corresponding point cloud is dependent on depth, we
did not succeed in creating superpixels that are compact in
3D while maintaining a meaningful minimum size.
B. Unary Image Features
Our method builds on the probability output of a random
forest, trained for pixel-wise classiﬁcation of the struc-
ture classes. We use the GPU implementation provided by
St¨ uckler et al. [12]
2
. The input for training are the full RGB-
D images, transformed to Lab color space. Each tree in the
forest uses training pixels only from a subset of training
images. For each training image, an equal number of pixels
for each occurring class is sampled. Split features are given
by difference of regions on color or depth channels. Region
size and offsets are normalized using the depth at the target
pixel. We accumulate the probabilistic output for all pixels
within a superpixel, and use the resulting distribution as a
feature for the unary node potentials in our CRF model.
We augment these prediction with another feature, based on
the height of a superpixel in 3D. This is a very informative
feature, in particular to determine the ﬂoor. To compute the
height of a (super) pixel, we ﬁrst ﬁnd the “up” direction.
We use a very simple approach that we found effective: we
cluster normal directions of all pixels into 10 clusters using
k-means, and use the one that is most parallel to the Y
direction,whichroughlycorrespondstoheightinthedataset.
We then project the 3D point cloud given by the depth along
this direction, and normalize the result between 0 and 1.
Thisprocedureworksrobustlygiventhereissomehorizontal
surface in the image, such as the ground or a table. A few
scenes contain only walls and furniture, and the approach
fails for these. Figure 2 illustrates a typical case and one
of the much rarer failure cases. While we could use a more
elaborate scheme, such as the one from Silberman et al. [1],
we suspect that the feature is of little use in scenes without
horizontal surfaces.
C. Pairwise Depth-Sensitive Features
There are ﬁve different features used to build pairwise
potentials in our model:
1
http://scikit-image.org
2
https://github.com/deeplearningais/curfil
Fig. 3. Visualization of one of the pairwise features, the similarity between
superpixel normals. The image shows the zoom-in of a bedroom scene,
together with the superpixel over-segmentation. Lines connect adjacent su-
perpixels, and line-strength gives the magnitude of the orientation similarity.
• Constant. A constant feature allows to model general
neighborhood relations.
• Color Contrast. We employ a non-linear color contrast,
as is common in the computer vision literature, between the
superpixel mean colors c
i
and c
j
: exp
 
??kc
i
?c
j
k
2

.
• Vertical Alignment. We model the directed angle between
superpixel centers in the 2D image plane. This allows the
model to learn that “structure” is above “ﬂoor”, but not the
other way around.
• Depth Difference. We include the signed depth difference
between superpixels, which allows the model to detect depth
discontinuities that are not represented in the 2D neighbor-
hood graph of the superpixels.
• Normal Orientations. Differences in normal vector orien-
tation are a strong clue on whether two superpixels belong
to the same surface, and therefore the same structural class.
We compute the 3D orientation of normals using the method
of Holz et al. [17], as implemented in the point cloud
library (pcl)
3
. All normals within a superpixel are then
averaged, to get a single orientation for each superpixel.
The feature is computed as the difference of
π
4
and the
(undirected) angle between the normals belonging to two
adjacent superpixels. An example is shown in Figure 3. The
change in normal orientation highlights that pillow and wall
aredistinctobjects,eventhoughthereisnostrongdistinction
in color or depth.
IV. EXPERIMENTS
We evaluate our approach on the public NYU depth V2
segmentation dataset of indoor scenes. The dataset comes
with a detailed annotation of 1449 RGB-D images belonging
to a wide variety of indoor scenes, categorized into 26 scene
classes. The annotation contains four semantic structural
classes: structure, ﬂoor, furniture and prop. There is an
additional “void” class, which is used for object boundaries
and hard-to-annotate regions. We follow the literature in
excluding these pixels completely from the evaluation. We
optimize our model for average class accuracy (the mean of
3
http://pointclouds.org
6234
TABLE I
QUANTITATIVE COMPARISON OF THE PROPOSED METHOD WITH THE LITERATURE.
ground structure furniture props class average pixel average
RF 90.8 81.6 67.9 19.9 65.0 68.3
RF + SP 92.5 83.3 73.8 13.9 65.7 70.1
RF + SP + SVM 94.4 79.1 64.2 44.0 70.4 70.3
RF + SP + CRF 94.9 78.9 71.1 42.7 71.9 72.3
Silberman et al. [1] 68 59 70 42 59.6 58.6
Couprie et al. [9] 87.3 86.1 45.3 35.5 63.5 64.5
St¨ uckler et al. [12]
†
95.6 83.0 75.1 14.2 67.0 70.9
The best value in each column is printed in bold
†
. The upper part of the table shows contributions by different parts of our pipeline. RF stands
for random forest prediction, RF + SP for aggregated random forests prediction within superpixels, RF + SP + SVM for an SVM trained on the unary
potentials, and RF + SP + CRF is our proposed pipeline. We optimized our approach for class average accuracy.
†
Note that the work of St¨ uckler et al. [12] is not directly comparable, as they integrated information over multiple frames, and did not measure accuracy
for pixels without valid depth measurement.
the diagonalof the confusionmatrix),putting more emphasis
on the harder classes of props and furniture, which have
smaller area than structure and ﬂoor. The dataset is split
into 795 images for training and 654 images for testing. Our
approach is implemented using our PYSTRUCT library
4
. We
use SCIKIT-LEARN [18] fork-means clusteringand the SVM
baseline.
All hyper-parameters were adjusted using 5-fold cross-
validation on the training set. The hyper parameters of the
random forests were found using the hyperopt frame-
work [19]. For the CRF model, the only hyper-parameters
are related to the superpixel segmentation, and the single
hyper-parameterC of the structuralSVM formulation. These
were adjusted using grid search. We found 500 superpixels
perimagetoworkbest,whichallowforamaximumpossible
performanceof 95%averageclassaccuracy onthe validation
set. Training of the random forests took about 15minutes
on a NVIDIA GeForce GTX Titan. Training the structural
SVM took 45minutes on a Xeon X5650 CPU. Prediction
using only the random forest takes 33ms on average, while
segmentation and prediction using the structural SVM ap-
proximately take an additional 500ms.
A. Results
Table I compares different components of our approach
with the literature. Please note that we ﬁrst designed our
ﬁnal model, using only the validation data. We now report
accuracies of simpler models for reference, but these results
were not used for model selection. To separate the inﬂuence
of loss-based training and the spatial reasoning of the CRF,
we also train a usual support vector machine (SVM) on the
unary potentials for comparison.
Therandomforestprediction,asreportedinSt¨ uckler et al.
[12] is already quite competitive. Grouping into superpixels
slightly improves performance, by removing high-frequency
noise and snapping to object boundaries. Somewhat sur-
prisingly, using a standard unstructured SVM with rescaled
loss already advances the mean accuracy above the previous
state-of-the art. We attribute this mostly to the ability of the
SVM to exploit correlation between classes and uncertainty
within the superpixels. Additionally, the SVM has access
4
http://pystruct.github.io
Fig. 4. Visualization of some of the learned potentials. The left potential
is on the feature encoding whether one superpixel is above the other in
the image. The right potential is applied to the relative depth between
superpixels. See section IV-A for details.
to the “height” feature, that was not included in the random
forest. This performance is still improved upon, both in class
average and pixel average performance by the learned CRF
approach, yielding the best published result so far for both
measures. The increase over the standard SVM is 1.5% for
class average accuracy and 2.0% for pixel average accuracy.
A visualization of the impact of each processing step
can be found in Figure 5, which shows prediction results
on the test set. The four prediction methods correspond to
the rows of Table I. The difference between the SVM and
CRF approaches are clearly visible, with the CRF producing
results that are very close to the ground truth in several
complexscenes.Wefoundthatourapproachimprovesresults
most for scenes with a clear geometric structure, which is
not surprising. We see that evidence from the random forest
is often very noisy, and biased away from the “props” class.
While the unstructured SVM can correct somewhat for the
class imbalance, it has no way to make larger areas consis-
tent, which the CRF can. On the other hand, performance of
the CRF deteriorates slightly on very crowded scenes with a
mixtureofsmallfurnitureandpropobjects,ascanbeseenin
thetworight-mostimages.Inthesescenes,depthinformation
is often noisy, and it is hard to make geometric statements
on the superpixel level. As the input from the random forest
is also often of low quality for crowded scenes, the CRF has
little chance to recover.
Figure 4 visualizes two of the learned potential functions.
Higher values correspond to favored conﬁgurations. We did
not force potentials to be symmetric or anti-symmetric,
which makes interpretation of the ﬁgures a bit harder, but
increases performance. Edges are constructed to go from top
6235
left to bottom right. Potentials below the diagonal are those
for an edge going from a label given by the column to the
one given by the row, while the ones above the diagonal
are for the opposite direction. For vertical alignment, one
would therfore expect to ﬁnd anti-symmetric potentials.
However the left-right direction seems to also contain useful
information, breaking the symmetry. One can see that the
vertical alignment potential expresses that the ﬂoor is much
morelikelytobebelowotherclasses.Italsoencodesthefact
that props rest on furniture, but not the other way around.
The potential of the depth feature encodes, for example, that
thegroundisusuallybehindtheotherclasses,whilefurniture
is in front of structures, such as the wall.
V. SUMMARY AND DISCUSSION
We introduce a CRF formulation for semantic segmen-
tation of structure classes in RGB-D images. We base our
model on the output of an efﬁcient GPU implementation
of random forest, and model spatial neighborhood using a
superpixel-based approach. We combine color, depth and 3D
orientation features into an energy function that is learned
using the SSVM approach. By explicitly modeling 3D re-
lations in a fully learned framework, we improve the state-
of-the-art on the NYU V2 dataset for semantic annotation
of structure classes. While our approach allows modeling
of spatial relations, these are limited to local interactions.
In future work, these interactions could be extended to
larger areas using latent variable models or higher order
potentials [5]. Another possible line of future work is to
combineour approach witha more task-speciﬁcone, directly
including support plane assumptions into the model, as done
by Silberman et al. [1]. Finally, we could also combine our
single-frame approach with the approach of St¨ uckler et al.
[12], which fuses individual views in 3D to exploit temporal
coherence. While this work did not explicitly address real
time application, the random forest implementation that we
build upon allows for real-time processing [12]. The SLIC
superpixel algorithm can also be implemented on GPU
in real-time, as was demonstrated by Ren and Reid [20],
and similarly the normal features we use also have real-
time capabilities [17]. Finally, fusion move inference for
our model is very efﬁcient for our model, opening up the
possibility to implement our approach entirely in real time.
REFERENCES
[1] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “In-
doorSegmentationandSupportInferencefromRGBD
Images,”inEuropeanConferenceonComputerVision,
2012.
[2] A.N¨ uchterandJ.Hertzberg,“Towardssemanticmaps
formobilerobots,”RoboticsandAutonomousSystems,
2008.
[3] S. Sengupta, P. Sturgess, P. H. Torr, et al., “Automatic
dense visual semantic mapping from street-level im-
agery,” in Intelligent Robots and Systems, 2012.
[4] S. Sengupta, E. Greveson, A. Shahrokni, and P. H.
Torr, “Urban 3d semantic modelling using stereo
vision,” in International Conference on Robotics and
Automation, 2013.
[5] L. Ladicky, C. Russell, P. Kohli, and P. Torr, “Asso-
ciative hierarchical CRFs for object class image seg-
mentation,” in International Conference on Computer
Vision, 2009.
[6] J. St¨ uckler, N. Biresev, and S. Behnke, “Semantic
mapping using object-class segmentation of RGB-D
images,” in Intelligent Robots and Systems, 2012.
[7] N. Silberman and R. Fergus, “Indoor scene segmen-
tation using a structured light sensor,” in Computer
Vision Workshops (ICCV Workshops), 2011.
[8] X.Ren,L.Bo,andD.Fox,“RGB-(D)SceneLabeling:
Features and Algorithms,” in Computer Vision and
Pattern Recognition, 2012.
[9] C.Couprie,C.Farabet,L.Najman,andY.LeCun,“In-
door semantic segmentation using depth information,”
in International Conference on Learning Representa-
tions, 2013.
[10] C. Farabet, C. Couprie, L. Najman, and Y. LeCun,
“Learning hierarchical features for scene labeling,”
Pattern Analysis and Machine Intelligence, 2013.
[11] H. Schulz and S. Behnke, “Learning object-class
segmentation with convolutional neural networks,” in
11th European Symposium on Artiﬁcial Neural Net-
works (ESANN), vol. 3, 2012.
[12] J. St¨ uckler, B. Waldvogel, H. Schulz, and S. Behnke,
“Dense Real-Time Mapping of Object-Class Seman-
tics from RGB-D Video,” Journal of Real-Time Image
Processing, 2014.
[13] T.Joachims,T.Finley,andC.-N.J.Yu,“Cutting-plane
training of structural SVMs,” Machine Learning, vol.
77, no. 1, 2009.
[14] V. Lempitsky, C. Rother, S. Roth, and A. Blake,
“Fusionmovesformarkovrandomﬁeldoptimization,”
Pattern Analysis and Machine Intelligence, 2010.
[15] A. F. Martins, M. A. Figueiredo, P. M. Aguiar, N. A.
Smith, and E. P. Xing, “An augmented lagrangian ap-
proach to constrained map inference,” in International
Conference on Machine Learning, 2011.
[16] R. Achanta, A. Shaji,K.Smith, A. Lucchi,P. Fua,and
S. S¨ usstrunk, “SLIC Superpixels Compared to State-
of-the-Art Superpixel Methods,” Pattern Analysis and
Machine Intelligence, 2012.
[17] D. Holz, S. Holzer, R. B. Rusu, and S. Behnke, “Real-
Time Plane Segmentation using RGB-D Cameras,” in
RoboCup International Symposium, 2011.
[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
et al., “Scikit-learn: machine learning in python,”
Journal of Machine Learning Research, vol. 12, 2011.
[19] J. Bergstra, R. Bardenet, Y. Bengio, B. K´ egl, et
al., “Algorithms for hyper-parameter optimization,” in
Neural Information Processing Systems, 2011.
[20] C. Y. Ren and I. Reid, “gSLIC: a real-time implemen-
tation of SLIC superpixel segmentation,” University of
Oxford, Technical Report, 2011.
6236
Input Image
Random Forest Prediction
Superpixel Voting
Support Vector Machine on Superpixels
Conditional Random Field on Superpixels
Ground Truth
Ground Structure Furniture Props Void
Fig. 5. Qualitative evaluation of the CRF. The ﬁrst three images illustrate errors in the original prediction that can be corrected, while the second two
images illustrate failure modes. Pixels marked as void are excluded from the evaluation. See the section IV-A for details.
6237
