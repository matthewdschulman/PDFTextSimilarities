Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images
Alexander Hermans, Georgios Floros and Bastian Leibe
Abstract— Dense semantic segmentation of 3D point clouds
is a challenging task. Many approaches deal with 2D semantic
segmentation and can obtain impressive results. With the
availability of cheap RGB-D sensors the ﬁeld of indoor semantic
segmentation has seen a lot of progress. Still it remains unclear
how to deal with 3D semantic segmentation in the best way.
We propose a novel 2D-3D label transfer based on Bayesian
updates and dense pairwise 3D Conditional Random Fields.
This approach allows us to use 2D semantic segmentations to
create a consistent 3D semantic reconstruction of indoor scenes.
To this end, we also propose a fast 2D semantic segmentation
approach based on Randomized Decision Forests. Furthermore,
we show that it is not needed to obtain a semantic segmentation
for every frame in a sequence in order to create accurate
semantic 3D reconstructions. We evaluate our approach on both
NYU Depth datasets and show that we can obtain a signiﬁcant
speed-up compared to other methods.
I. INTRODUCTION
3D scene understanding is required in many robotics
applications. For scenarios such as autonomous navigation
or general object interaction, knowledge of the surrounding
scene is vital. While semantic segmentation can be an impor-
tant cue for scene understanding [1], [2], [3], computational
efﬁciency has not been a focus of research so far.
Motivated by mobile robotics scenarios, we want to create
a semantically annotated 3D reconstruction of a surrounding
scene, where every 3D point is assigned a semantic label.
Furthermore, we want to enforce spatial and temporal consis-
tency in such reconstructions. However, there is no clear-cut
method for the transfer of 2D labels into a globally consistent
semantic 3D reconstruction yet.
In this paper we address the task of dense semantic 3D
scene understanding of indoor scenes. We build a point cloud
reconstruction of the scene and assign a semantic label to
each 3D point. As an initial step, we perform an efﬁcient
2D semantic segmentation of the RGB-D frames. Our main
contribution is a novel way to transfer the 2D image labels
into a 3D reconstruction based on Bayesian updates [4]
and dense pairwise Conditional Random Fields (CRFs) [5],
which allows us to enforce temporal and spatial constraints.
Semantic segmentation of 2D images recorded from in-
door scenes has been shown to be an especially challenging
task [6]. This is caused by the large variability of both object
and scene types, varying illumination and unconstrained
scene layouts. Current methods achieve good results, but they
typically need more than one minute per image [7], [8], [9].
As a ﬁrst step towards a more efﬁcient method, we propose
a semantic segmentation approach based on Randomized
Decision Forests (RDFs). Due to their parallel nature, RDFs
All authors are with Computer Vision Group, RWTH Aachen University.
Email: fhermans,floros,leibeg@vision.rwth-aachen.de
Randomized  
Decision Forest 
3D Scene 
Reconstruction 
Dense Conditional 
Random Field 
Fig. 1: A high level overview of our approach. We classify RGB-D images
using a Randomized Decision Forest and reﬁne the result using a dense
CRF. Based on a sequence we create a 3D point cloud reconstruction of a
scene. Using our novel 2D-3D label transfer approach we can assign a class
label to each 3D point, giving us a dense semantic reconstruction.
are well suited for efﬁcient semantic segmentation. It has
previously been shown that they can obtain impressive results
[10]. However, the more expressive variants tend to suffer
from expensive feature computations [9]. We propose a set
of features for RDFs which keeps them efﬁcient, without
loosing too much accuracy. When compared to fast semantic
segmentation methods our approach obtains state-of-the-art
results for both speed and classiﬁcation accuracy.
Considering that in most typical scenarios the movement
between two consecutive frames is small, the question arises
whether it is necessary to segment every frame in a sequence.
An important insight we gain is that semantic segmentation
does indeed not need to be performed for every frame. We
analyze how our semantic segmentation approach, coupled
with the 2D-3D label transfer, behaves when processing only
a subset of the frames. Based on this, we show that it is not
critical that each part is real-time capable, but that it still
sufﬁces if certain components of our approach run at far
lower frame rates.
This paper is structured as follows. The next section gives
an overview of related work. Section III describes the general
idea of our complete approach. Sections IV, V and VI then
describe the components of our approach in more detail.
Finally, Section VII discusses our experimental results.
II. RELATED WORK
Several 3D semantic segmentation approaches exist for
both outdoor [11], [12], [13] and indoor [3], [14], [2], [8]
scenes. Floros et al. [11] create a stereo 3D reconstruction
and enforce temporal consistency through an additional
potential in a CRF ranging over several frames in a se-
quence. Triebel et al. [12] label scene reconstructions in
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2631
Output 
2D Semantic  
Segmentation 
3D Refinement 
3D Reconstruction 
RGB-D Input 
Accu Accu Accu Accu Accu Accu Accu Accu 
State State State 3D Dense CRF 3D Dense CRF 3D Dense CRF 
Display Display Display 
Fig. 2: Overview of our method. Between the input ( ), where each box represents an input frame, and output ( ) our approach is split into three
processes. The 3D Reconstruction ( ) fuses new frames into the point cloud reconstruction. The 2D Semantic Segmentation ( ) classiﬁes the RGB-D
image and accumulates the results for each 3D point. Finally, the 3D Reﬁnement ( ) fuses this accumulated information with the previous state of a point.
It can clearly be seen that the different processes can be executed at different rates. The visualized frequencies do not correspond to actual values.
an unsupervised way. They focus on the actual learning of
different classes based on online clustering and incremental
belief updates. Hu et al. [13] create a labeled 3D scene
reconstruction. They aim to speed up the fusion of streamed
laser scan data for outdoor scenes, based on an efﬁcient
representation of the point cloud. In contrast to this latter
approach, we also use the valuable information obtained from
color images. N¨ uchter et al. [3] create 3D maps from laser
scan data and reason about planes they ﬁnd in the scene.
They use heuristics to decide if a plane belongs to a wall,
ﬂoor, ceiling or a door and furthermore detect some objects
in the scene based on additional classiﬁers. This approach
is limited to certain classes, while our approach can learn
a larger number of classes using the RDF classiﬁers. Most
similar to our approach is that of St¨ uckler et al. [14]. They
obtain a 3D voxel reconstruction of a scene from an RGB-
D SLAM approach and fuse the labels of different RGB-
D frames using Bayesian updates. However, where we aim
to densely label a 3D scene, their goal is to improve the
classiﬁcation of a set of objects in a scene. Anand et al. [2]
stitch together a small set of RGB-D frames to an indoor
point cloud reconstruction. They segment the point cloud
into patches and use both visual and geometrical cues for
labeling the scene. They propose a relaxed version of their
initially slow inference method, but it is unclear how long
the feature computation takes. Valentin et al. [8] create a 3D
mesh for the scene and obtain label hypotheses for every
face based on appearance and geometric properties using a
boosted classiﬁer. A CRF is deﬁned over the 3D mesh to
get a globally consistent segmentation. They only perform
inference on a full scene once, instead of building up the
semantic reconstruction incrementally.
For 2D image semantic segmentation, common pipelines
use the Textonboost framework [15], [16], which is opti-
mized using a CRF [16], [5]. While these approaches obtain
impressive results for outdoor scenes, different approaches
are needed for the more complex indoor scenes. Silberman
and Fergus [6] use a neural network classiﬁer based on SIFT
features extracted from both the color and depth images and
optimize the result using a grid CRF. Ren et al. [7] use
a complex approach based on kernel descriptors and linear
support vector machines evaluated on different levels of a
segmentation tree. For the second NYU Depth dataset, Sil-
berman et al. [1] focus on structural labels and support rela-
tionships. These are jointly inferred, formulated as an integer
programming problem. Couprie et al. [17] use a multiscale
Convolutional Neural Network (CNN) to infer both semantic
and structural classes. To label video sequences, they enforce
a simple temporal consistency based on temporally smoothed
superpixels in consecutive frames. Gupta et al. [9] obtain
state-of-the-art performance by creating superpixels based on
depth and appearance contours and classifying these using
either SVMs or RDFs. Apart from the CNN approach [17],
all of the above 2D semantic segmentation methods need
about a minute to obtain semantic segmentation results for a
single image. The CNN approach needs approximately 0.7
seconds per image.
III. APPROACH
We aim at creating a 3D point cloud reconstructions with
dense and consistent label information. We want to create
this reconstruction online while moving through an unknown
scene. Our guiding questions is: how can the necessary
transfer from image-level segmentation to 3D point labeling
be achieved and how can this process be designed with
components that have vastly different run-times?
Our approach is decoupled into three separately running
processes (see Fig. 2). The 3D reconstruction process takes
a new image from a sequence of RGB-D images and
adds it to the currently existing 3D reconstruction. The 2D
semantic segmentation process creates a soft classiﬁcation
for each pixel that corresponds to a 3D point in the cloud
and accumulates this information for the points. The 3D
reﬁnement process, which is the core of our approach, takes
the newly accumulated information for each point and fuses
it into the point’s current state, which is represented by a
class distribution. To obtain a globally optimal class labeling
for the 3D reconstruction, we distribute the information
over neighboring points depending on the distance, color
similarity, and normal orientation. Fig. 2 also shows that the
input data arrives at a higher rate than the other processes are
executed. This is justiﬁed by the observation that the camera
position tends to be very similar from frame to frame, so
it sufﬁces to run both the reconstruction and the semantic
segmentation process only for a subset of the input frames.
This also means we can accumulate information from several
frames before we update a 3D point’s state in the reﬁnement
process. Consequently, we need to store a 3D point’s current
state and its currently accumulated information.
2632
The next sections will explain each process in detail.
As the 3D reﬁnement does not depend on the type of 2D
semantic segmentation and 3D reconstruction, we will start
with the 3D reﬁnement.
IV. 3D REFINEMENT
The 3D reﬁnement process consists of two steps. The ﬁrst
step takes newly accumulated classiﬁcation data and fuses it
with the current state. We model the ﬁrst step as a Bayesian
update [4] that takes the current belief for a 3D point and
updates it with the new predictions. The second step enforces
spatial consistency which we achieve by applying a dense
pairwise CRF over the 3D point cloud. A new point’s state
is initialized with a uniform class distribution.
A. Fusing New Classiﬁcations
Our goal is to obtain a class distribution for each 3D point.
At time t, we denote the class distribution of a single point
as c
t
and all the pixel measurements that contribute to this
point are denoted as x
t
0
=fx
0
;x
1
;:::x
t
g, meaning we are
interested inp(c
t
jx
t
0
). Applying Bayes’ rule to this gives us:
p(c
t
jx
t
0
) =
1
Z
t
p(x
t
jx
t 1
0
;c
t
)p(c
t
jx
t 1
0
); (1)
where Z
t
= p(x
t
jx
t 1
0
). Here we can use a ﬁrst or-
der Markov assumption and assume that p(x
t
jx
t 1
0
;c
t
) =
p(x
t
jc
t
) because the measurement x
t
is conditionally in-
dependent of previous measurements given the class c
t
.
Furthermore, we assume a smoothness of the posterior and
thus use p(c
t
jx
t 1
0
)p(c
t 1
jx
t 1
0
). This gives us:
p(c
t
jx
t
0
) =
1
Z
t
p(x
t
jc
t
)p(c
t 1
jx
t 1
0
): (2)
Based on this, we can take the 3D point’s current state
p(c
t 1
jx
t 1
0
) and update it with the accumulated new pre-
dictions from the 2D semantic segmentation. We consider
classiﬁers that yield a prediction of the posterior b p(c
t
jx
t
).
Again using Bayes’ rule, we reformulate Eq. 2 to:
p(c
t
jx
t
0
) =
1
Z
t
p(c
t
jx
t
)p(x
t
)
p(c
t
)
p(c
t 1
jx
t 1
0
): (3)
While we cannot estimate the actual prior probabilityp(x
t
),
we can simply fuse it with the normalization factor Z
t
as it
does not depend onc
t
, leading to the ﬁnal update equation:
p(c
t
jx
t
0
) 
1
Z
0
t
b p(c
t
jx
t
)p(c
t 1
jx
t 1
0
)
p(c
t
)
; (4)
withZ
0
t
=p(x
t
0
jx
t 1
0
)p(x
t
)
 1
. This can now be computed,
as both the old state, the class prior, and the new prediction
are known and we do not need to compute Z
t
explicitly,
but can instead normalize the probability distribution. We
assume that the class prior p(c
t
) does not change over time
and thus instead use a ﬁxed predetermined valued p(c).
B. Enforcing Spatial Consistency
After the new data has been fused into the state, we want to
both distribute this information over neighboring points and
also obtain information from neighbors. Inﬂuence between
neighbors should be proportional to their distance and their
visual and geometrical similarity. For this purpose, we use a
dense CRF. Let C =fC
1
;C
2
;:::;C
N
g be a set of random
variables corresponding to the 3D pointsi2f1;:::Ng. Each
random variable C
i
takes a label fromL =fl
1
;l
2
;:::;l
k
g
when considering k different classes. Based on a CRF, the
probability distribution for a possible labelingc2L
N
given
a point cloud X is deﬁned by:
P (C =cjX) =
1
Z(X)
exp ( E(cjX)); (5)
where E(cjX) is the Gibbs energy deﬁned over the CRF
graph and Z(X) is the partition function. The dense CRF
graph G = (V;E) contains vertices v
i
corresponding to
the random variables C
i
, with edges between each pair of
vertices (E =VV). The Gibbs energy is deﬁned over the
unary and pairwise cliques in the graph given by:
E(cjX) =
X
i2V
 
u
(c
i
jX) +
X
(i;j)2E
 
p
(c
i
;c
j
jX): (6)
As the unary potential 
u
(c
i
jX), we use the negative loga-
rithm of the current state of point i. The pairwise potential
 
p
(c
i
;c
j
jX) is given by a linear combination of Gaussian
kernels:
 
u
(c
i
jX) =  log(p(c
i
jX)) (7)
 
p
(c
i
;c
j
jX) = (c
i
;c
j
)
K
X
m=1
w
(m)
k
(m)
(f
i
;f
j
); (8)
where(x
i
;x
j
) is a simple Potts model,f
i
andf
j
are feature
vectors for the pointsi andj, andk
(m)
are Gaussian kernels,
given by:
k
(m)
(f
i
;f
j
) = exp

1
2
(f
i
 f
j
)
>

(m)
(f
i
 f
j
)

: (9)
These are characterized by the symmetric positive-deﬁnite
precision matrix 
(m)
, which deﬁnes the shape of the kernel.
Each of which has a weight w
(m)
. When limiting the
pairwise potential to a combination of Gaussian kernels,
the efﬁcient mean-ﬁeld approximation inference approach
introduced by Kr¨ ahenb¨ uhl and Koltun [5] can be applied.
The probability distributionP (C) is approximated by a dis-
tributionQ(C) that minimizes the KL-divergenceD(QjjP ),
such that Q is a product over its marginals Q(X) =
Q
i
Q
i
(C
i
). This distribution is approximated by an iterative
approach with linear complexity in the number of points.
After a number of iterations, the globally optimal labeling
can be obtained for each point by setting the label C
i
=
arg max
l
Q
i
(l).
In our case we use two Gaussian kernels for the pairwise
potential. The ﬁrst is an appearance kernel,
w
(1)
exp
 
 
jp
i
 p
j
j
2
2
2

 
jl
i
 l
j
j
2
2
2

!
; (10)
where p are the 3D positions of the points and l are color
vectors in LAB colorspace. The parameters

and

specify
the range in which points with similar coordinates and colors
affect each other, respectively. This kernel is used to model
longer range connections between points with a similar
appearance. The second Gaussian kernel is a smoothness
kernel,
w
(2)
exp

 
jp
i
 p
j
j
2
2
2

 
jn
i
 n
j
j
2
2
2


; (11)
2633
p
p
x1;y1;c1
p
x2;y2;c2
d
(a) (b)
Fig. 3: Visualization of different features. (a) A pixel value comparison,
used by several features. (b) Patches relative to a pixel. The feature value
is the average of the patch.
where n are the 3D normals of the points. This kernel
operates on a smaller range, speciﬁed by

, thus enforcing a
local, appearance-agnostic smoothness amongst points whose
normal vectors are similar. These parameters can be obtained
using piece-wise learning.
These models tend to converge very fast (10 iterations)
in which case the probability distribution of the possible
classes collapses to a single class. As we want to use the
probability distribution in further Bayesian updates, we only
execute a few iterations of the dense CRF. However, since
the distributions can already be very focused on a single
class after one iteration, we do not set the new distribution
as the new state directly. Instead, we use the distribution as
a prediction in another Bayesian update step as deﬁned in
Eq. 4. This gives us the ﬁnal class distribution after the 3D
reﬁnement step.
V. 2D SEMANTIC SEGMENTATION
We use a RDF to create 2D semantic segmentations. As
each pixel is evaluated separately by a RDF, the results can
be rather noisy. To obtain a spatially smoother result, we
apply a dense pairwise CRF in 2D on the RDF output.
A. Randomized Decision Forest
We use a typical RDF framework, containing a number
of binary trees. A pixel traverses the tree and the path is
decided based on a feature and a threshold stored in every
node. Once the pixel reaches a leaf node, it is assigned the
corresponding class distribution. The ﬁnal result is obtained
by averaging the distributions of the different leaf nodes.
1) Features: The RDFs draw their strength from the
features used to make splitting decisions. More expressive
features can lead to higher classiﬁcation accuracies [9]. But
often these features are based on computationally expensive
superpixel and segment computations. Only using simple
features, such as pixel comparisons, can already yield im-
pressive results [10]. However, the classiﬁcation accuracies
have so far not reached levels comparable to state-of-the-art
classiﬁers. We have evaluated a large set of computationally
feasible features to ﬁnd a good balance between speed and
accuracy. Among others, we have tried pixel and patch
comparisons in the image or depth map, normal vectors,
and more complex geometrical features. This section will
describe the seven features in our ﬁnal selection.
Following [10], we use a simple color feature. It returns
the value of a color channelc
1
, from a pixelp
x1;y1;c1
, relative
to the current pixel p within a dd patch surrounding p (as
shown in Fig. 3a). Our second feature takes two relative
pixels and compares their depth values by subtraction [18].
Furthermore, the relative distances are normalized by the
depth of the current pixel, which makes the feature more
robust to camera translations. The third feature is the same
as the second, with the difference that now again color values
are compared instead of depth values. Both color features are
evaluated in the LAB colorspace. The two most expressive
features are the height of a point in 3D and the relative depth
in the scene. The relative depth is deﬁned as the depth of a
pixel, normalized by the furthest depth in the corresponding
image column [6]. The last two features we use are based
on patches in the image, relative to a pixel (see Fig. 3b). We
calculate oriented gradients over either the L color channel
or the depth map and perform soft binning similar to [19].
Then we extract average values of patches within these bins.
Empirically we have found this feature set to work best for
our purposes of indoor semantic segmentation.
2) Training: Our training procedure is similar to that of
[10]. Initially, we sample our training samples I uniformly
from the training images. Each of the trees is trained on a
subset of these training samples I
0
I. A set of samples I
0
is assigned to the root node of a tree and the set is recursively
split up and distributed to the child nodes. For each node,
500 splitting features and for each of these 13 thresholds are
considered. Unlike [10], we use the normalized information
gain [20] to select the features with the best score. In our
experiments this gave slightly better classiﬁcation accuracies
than the unnormalized information gain. Splitting is stopped
once either the maximum tree depth is reached, the leaf nodes
are pure, or they contain less than a minimum number of
samples n
min
.
The used training data contains a large bias towards
certain classes, which is reﬂected by a bias towards these
classes in the resulting segmentations. To overcome this
problem, we sample a set of training pixels with an equal
class distribution for each tree. This set is passed through
the tree to learn new class distributions in the leaf nodes.
Although this slightly decreases the global accuracy of the
RDF, it gives a signiﬁcant improvement of the class-average
accuracy. Furthermore, we can assume an equal class prior in
the Bayesian update steps as all used probabilities are based
on results of RDFs trained with this scheme.
B. 2D Dense Pairwise CRF
To obtain smoother 2D semantic segmentation results, we
apply a Dense Pairwise CRF as described in section IV-B.
The difference being that the unary potential is now directly
obtained from the RDF and that the pairwise potentials
are different. We again use a smoothness kernel and an
appearance kernel, but the feature vectors differ. For the
smoothness kernel we only use the 2D pixel locations,
which simply enforces smoothness in the image and for the
appearance kernel we replace the 3D point location by the
2D pixel location. As we only evaluate the RDF for pixels
with a valid depth, all other pixels are initialized using a
class equal unary potential.
2634
VI. 3D SCENE RECONSTRUCTION
Our 3D reconstruction approach is based on our previous
work for stereo depth reconstruction [11]. We adjusted it
to handle RGB-D frames, but the idea remains similar.
We use Visual Odometry (VO) information to merge the
point clouds of each frame to a global reconstruction of
the scene. For the VO estimation we use the fovis library
[21], which ﬁnds 2D feature matches across images and
computes 3D transforms based on the correspondences. The
VO information is computed for every frame in a sequence.
This step is fairly inexpensive and takes25ms for each
frame on a single core.
Given the camera position and orientation, we project
points into global 3D space and associate each point with a
zero-velocity Kalman ﬁlter, which tracks the point’s spatial
uncertainty. We initialized the Kalman ﬁlter using forward
error propagation, which uses the image level uncertainties
and propagates them into 3D space, based on the projection
matrix. The forward error propagation for the Kinect sensor
is explained in [22]. To ﬁnd correspondences between new
points and existing points, we project every existing point
onto the camera plane, giving us a hypothetical depth value at
a certain pixel location. If the difference between the actually
measured depth value and the hypothetical depth is below
a threshold we fuse the new point with the existing point.
We then use the new point as a measurement to update the
Kalman ﬁlter, giving us an updated spatial uncertainty and
3D position. If no matching point exists in the point cloud,
we create a new point. In order to obtain clean point cloud
reconstructions we only create a new point if the spatial
uncertainty is below a threshold, if it has a valid normal
vector, and if this normal vector is not close to perpendicular
to the vector between the camera center and the 3D point.
We track the pixel-point correspondences to transfer the 2D
class probabilities to the 3D points once available. Note that
this procedure can easily be replaced by a SLAM approach
with loop closure (such as [23]) for improved results and is
not the focus of our approach.
VII. EXPERIMENTAL RESULTS
A. Datasets
We evaluate our 2D semantic segmentation approach on
the two NYU Depth datasets [6], [1]. Both contain images
from indoor scenes, recorded with a Kinect sensor. The ﬁrst
dataset contains 2284 labeled RGB-D images, for which
missing depth values in the depth maps have been ﬁlled.
We evaluate our approach on the 12 core classes provided in
[6]. The second dataset contains 1449 labeled images with
the same preprocessing. While it contains fewer annotated
images, there is a larger variety of scenes and objects. We
use the default training and test set splits. Furthermore, both
datasets contain a large amount of raw images, which we
use for the reconstructions. The test images from the ﬁrst
NYU Depth dataset are taken from 29 scenes. We created
reconstructions for all of these scenes. For four scenes the
visual odometry failed to produce correct trajectories, caused
by missing frames in the sequences. For the 3D labeling
3 5 7 9
65
66
67
68
69
70
71
72
73
74
75
Accuracy (%)
3D Refinement rate
 
 
5 Iterations
4 Iterations
3 Iterations
2 Iterations
1 Iterations
Baseline
2D Segmentation
65
65.5
66
66.5
67
Process rate
(1,1,5)
(2,2,10)
(3,3,15)
(4,4,20)
(5,5,25)
(6,6,30)
Accuracy (%)
 
 
Full apprach
Baseline
2D Segmentation
Fig. 4: Quantitative reconstruction results. (left) The colored lines each
represent results for different numbers of 3D dense CRF iterations. The
different measurements represent after how many new frames the 3D
reﬁnement is executed. Dashed lines represent the global accuracy and
full lines class-average accuracy. (right) Results for different rates of
the processes. The ﬁrst number represents the execution rate of the 3D
Reconstruction, the second that of the 2D Semantic segmentation and the
third that of the 3D Reﬁnement, using 1 3D dense CRF iteration.
experiments we excluded these scenes and evaluated on the
remaining 26 scenes, containing bathrooms, bedrooms, a
bookstore, kitchens, living rooms and ofﬁces. In total we
used over 25k images, with 652 ground truth annotations.
Unless speciﬁed differently, results are based on the ﬁrst
dataset, as our main evaluation is done on this dataset. While
the second dataset has a higher annotation quality, fewer
images are labeled per scene, rendering the evaluation of
3D semantic reconstructions less expressive.
B. 3D Scene Experiments
We evaluate our approach in a causal way, meaning after
fusing a new image into the 3D reconstruction we track
the 2D-3D correspondences and for each pixel, we take
the most likely label of the corresponding 3D point. This
gives us a label for every pixel with a valid depth value.
We then evaluate based on the 2D annotations. We run the
3D reconstruction and 2D semantic segmentation for each
frame in a sequence and accumulate the predictions for each
3D point. For the 2D semantic segmentation we use the RDF
described in Section V-A and reﬁne the segmentation using a
2D dense CRF which runs for 3 iterations. The 3D reﬁnement
is only executed everyn
th
frame. To limit the computational
cost of the reﬁnement step, we only update points that
accumulated new predictions in the past 10 frames.
As a baseline for our approach we take our single-image
semantic segmentation, which evaluates a RDF and then
applies a dense pairwise CRF in 2D using 12 iterations. Here
we also only consider pixels with a valid depth to obtain a
fair comparison (Fig. 4, shown in black). We also show the
results when only evaluating the 2D semantic segmentation
for every frame and simply accumulating the output in the
3D points (Fig. 4, shown in magenta).
The left graph in Fig. 4 shows both class-average (contin-
uous lines) and global accuracy (dashed lines) for different
values of n and for different numbers of 3D dense CRF
iterations. As can be seen, our approach outperforms the
baseline in every tested conﬁguration. The improvement
mainly depends on the number of 3D dense CRF iterations.
The best results are obtained using 5 iterations and running
the 3D reﬁnement every 5
th
frame. This gives a class-average
2635
Fig. 5: Qualitative results of our 3D reconstructions. (top row)) Semantic reconstructions (bottom row) Corresponding RGB reconstructions. For these
reconstructions a semantic segmentation was created for every frame and the 3D reﬁnement was done every ﬁve frames, using three 3D dense CRF
iterations. All scenes are from the ﬁrst NYU Depth dataset. Label colors are listed in Table II. Different viewpoints are shown in the supplementary video.
TABLE I: Timing results. The upper part of the table lists the time needed
to execute different components of our approach. Times are averaged over
the different scenes and a range is listed if the times differ due to the
different point cloud sizes. The lower part shows different frames rate of
different conﬁgurations. All timings are based on a Intel i7 3.4GHz CPU.
Component Consumed time (ms)
General Preprocessing 110
Gradient calculation 70
Visual Odometry 25
Point cloud fusion 70 - 120
RDF evaluation 200
2D CRF 3 or 12 Iter. 360 960
3D CRF 1, 2 or 3 Iter. 400 - 1800 500 - 2200 600 - 2500
3D CRF 4 or 5 Iter. 600 - 3000 650 - 3700
Setup Frame rate (Hz)
Single-image baseline 0.75
(1,1,5) 1 3D CRF Iter. 0.8 - 1.0
(3,3,15) 1 3D CRF Iter. 2.2 - 2.5
(6,6,30) 1 3D CRF Iter. 3.9 - 4.6
accuracy of 67:24%, which improves the baseline of 65:33%
by almost 2%. Qualitative results for three scenes using this
conﬁguration can be seen in Fig. 5. While the numerical
improvement seems rather small, it should be noted that the
results were obtained purely based on spatial and temporal
consistency.
The results are relatively stable regarding the number of
executed reﬁnement steps. In fact, for a higher number of 3D
dense CRF iterations, it is better to execute the 3D reﬁnement
less often. We believe this is due to the fact that the class
distributions will collapse if the CRF is applied too often,
which in turn does not work well with the Bayesian updates.
C. Runtime Analysis
Table I shows the runtime of several components in our
approach. The upper part gives a detailed time for different
steps in our pipeline. The general preprocessing includes
color conversions, normal, and basic feature computations.
Together with the gradient calculation, the RDF evaluation,
and a 2D CRF this makes up the basic 2D segmentation.
The 3D reconstruction consists of the VO and the point
cloud fusion. The point cloud fusion tends to vary based
on the number of points in a scene, as we loop over all
points to ﬁnd corresponding pixels. The 3D reﬁnement step
consists of a 3D dense CRF and two Bayesian updates. The
latter do not contribute to the actual runtime signiﬁcantly.
The lower part of Table I show the frame rates for the
different conﬁgurations. The single-image baseline, which
uses 12 dense CRF iterations, runs at0.75Hz. Our full 3D
reconstruction approach, running the 3D reﬁnement every
5
th
frame with 1 3D CRF iteration, runs slightly faster while
obtaining better results and creating a reconstruction of the
scene.
Motivated by the fact that we do not need to run the 3D
reﬁnement every frame to improve results, we investigated
how the results are affected if we only run the 3D recon-
struction and 2D semantic segmentation for a subset of the
frames as well. Starting out from the conﬁguration where the
3D reﬁnement is executed every 5 frames, we ran several
experiments where we increased the number of completely
skipped frames as seen in Fig. 4. On the x-axis we plot
the different conﬁgurations. The ﬁrst number speciﬁes after
how many new frames the 3D reconstruction is executed,
the second after how many the 2D semantic segmentation
and the last after how many frames the 3D reﬁnement is
done. For the 3D dense CRF we only update points that
accumulated new predictions in the previous 10 frames that
we actually evaluated. This means the window size is twice
the number of the skipped frames. As it can be seen, results
decrease at ﬁrst, but then seem to be stable. The accuracy
when only generating semantic segmentations for each 6
th
frame is very similar to that of the single-image baseline.
However, the speed improves by a factor of more than ﬁve,
improving the frame rate of0.75Hz to a frame rate around
4Hz (see the lower part of Table I). Lower subsampling
rates also result in a signiﬁcantly higher frame rate. To the
best of our knowledge, no other methods create semantic 3D
reconstructions of indoor scenes at such speed.
Our code can further be optimized and currently does not
exploit parallelism to its full potential. The three processes
2636
1 3 5 7 9 11 13 15
55
60
65
70
Accuracy (%)
Number of trees
 
 
Global
Class?avg
8 12 16 20 24 28 32
55
60
65
70
Accuracy (%)
Maximum tree depth
 
 
Global
Class?avg
10 30 50 70 90
60
62
64
66
68
70
Accuracy (%)
Minimum number of samples in leaf nodes
 
 
Global
Class?avg
1 2 3 4 5 6
x 10
6
60
62
64
66
68
70
Accuracy (%)
Number of training samples
 
 
Global
Class?avg
0 0.2 0.4 0.6 0.8 1
45
50
55
60
65
70
75
Accuracy (%)
p
 
 
Global
Class?avg
(a) (b) (c) (d) (e)
Fig. 6: RDF parameter experiments. The graphs show both the class-average and global classiﬁcation accuracy. (a) The number of trees in the RDF.
(b) The maximum tree depth. (c) The minimum number of samples needed further splitting of a node. (d) The number of training samples used during
training. (Each tree is trained on a subset). (e) The weight for the new and old class distribution, when repassing equally sampled data through the trees.
currently still run sequentially and thus block the progress
of the other processes. By actually running the processes in
parallel, we are optimistic that we can achieve higher frame
rates and believe that video frame rates are within reach.
D. RDF Experiments
To gain a better understanding on how RDFs work for
indoor semantic segmentation we investigate the inﬂuence of
several parameters. We do this by varying a single parameter
at a time. As seen in Fig. 6 certain parameters have a big
inﬂuence. Both the number of trees and the maximum tree
depth converge to a stable range. We limited our experiments
to 8 trees for efﬁciency reasons (Fig. 6a). The performance
of the RDF changes signiﬁcantly with the allowed tree depth,
peaking at a tree depth of 24 levels (Fig. 6b). Closely related
is the minimum number of samples required in a node to
consider further splitting (Fig. 6c). This parameter has a
less signiﬁcant effect. The number of training samples has
a small effect on the performance. Fig. 6d suggests that
larger training sets increase classiﬁcation accuracy slightly,
however the training time increases linearly as well. Passing
equally sampled training data through the tree has the most
signiﬁcant impact. To show the effect, we add the original
and the new class distribution, weighted with (1 p) and
p respectively, where p2 [0; 1] (Fig. 6e). While the global
classiﬁcation accuracy is reduced by 3%, the class-average
accuracy increases by 15% when using p = 1. Other
parameters, such as the number of considered splits or
speciﬁc features parameters, had a less signiﬁcant impact. In
general all RDF parameters seem robust to minor changes
when values from the stable ranges are used.
E. 2D Semantic Segmentation Experiments
Finally, we show how our single-image segmentation ap-
proach (RDF followed by 12 iterations of the 2D dense CRF)
compares to other 2D semantic segmentation approaches, by
evaluating it on both NYU Depth datasets. Here we use the
preprocessed images, thus being able to label every pixel.
1) NYU Depth version 1: We compare our 2D segmenta-
tion approach to the Extended Textonboost approach (based
on the publicly available ALE library [16]). We only use
the 12 core classes deﬁned in [6]. We clearly improve the
results, as can be seen in Table II. Even when we only use
the RDF, the quantitative results are better for several classes
already. This yields rather noisy results, but the dense CRF
removes most of the noise (see Figs. 7d and 7e). Compared
to the Extended Textonboost approach, the results are better
aligned to actual image contours (see Figs. 7c and 7e). It
should be noted that these results are not directly comparable
to our 3D experiments as they are also evaluated on pixels
without a valid depth (see Fig. 7f).
Two other approaches provide results on these datasets,
but they use an additional background class. This contains
all labels in the dataset that could not be mapped to one of
the 12 classes. We did not use this class in our experiments,
as we think it contains no useful information. For a fair
comparison we train an additional classiﬁer using 13 classes.
Silberman and Fergus [6] only provide their class-average
accuracy of 56:6%, which we improve by almost 3%. Ren et
al. [7] obtain better results for all classes, but their complex
approach takes over a minute per image.
2) NYU Depth version 2: For all experiments we use the
same parameters as for the ﬁrst dataset. Couprie et al. [17]
group all classes in the dataset into 13 semantic classes.
Silberman et al. [1] provide results for the segmentation
based on 4 structural classes. Quantitative results for both
class sets are shown in Table III. We obtain better results for
9 of the 13 semantic classes and improve both the global and
class-average classiﬁcation accuracy. Gupta et al. [9] also
report results on the second NYU Depth dataset, however
they use different evaluation measurements. Similar to Ren
et al. [7], their approach is more accurate than our 2D
segmentation, but it runs signiﬁcantly slower. To our current
knowledge, only Couprie et al. [17] evaluated their approach
with regards to runtime. We outperform their approach both
in accuracy and speed. On downscaled images (320 240)
they need 0.7 seconds per frame. This means that our single-
image approach is almost three times faster.
VIII. CONCLUSION
We have introduced a novel way to create a semantic 3D
reconstruction of indoor scenes. We show how 2D semantic
segmentation of RGB-D images can be achieved efﬁciently,
using RDFs and 2D dense CRFs. We then apply our 2D-
3D label transfer to obtain a semantic segmentation of a 3D
reconstruction. Finally, we show that for our approach it is
not needed to label every frame in a sequence and we analyze
how this affects the quality of the 3D reconstruction. In future
work, we plan to change our implementation to fully exploit
parallelism, which we believe will further increase the speed
of our approach.
Acknowledgments: This work has been funded, in parts,
by EU projects STRANDS (ICT-2011-600623) and ROVINA
(ICT-2011-600890).
2637
(a) (b) (c) (d) (e) (f)
Fig. 7: Qualitative results for the ﬁrst NYU Depth dataset. (a) Color Images. (b) Ground truth annotations. (c) Extended Textonboost results, using the
dense CRF. (d) Intermediate RDF results. (e) Final results, after applying the 2D dense CRF. (f) Results from the 3D reconstruction experiments, if we
only consider points with a spatial uncertainty up to a certain threshold. See Table II for label colors.
TABLE II: NYU Depth dataset version one. (upper half) Comparison to the Extended Textonboost approach [16]. (lower half) Comparison of our
single-image semantic segmentation to two other approaches. We improve the class-average accuracy of Silberman and Fergus [6]. However, we cannot
compete with the method of Ren et al. [7] yet.
Label color Bed
Blind
Booksh.
Cabinet
Ceiling
Floor
Picture
Sofa
Table
TV
Wall
Window
Backgr.
Global
Average
Extended Textonboost (no CRF) 14.6 3.5 56.1 34.5 58.8 73.8 47.7 31.8 45.7 53.3 88.7 12.2 - 67.1 43.3
Extended Textonboost (CRF) 16.4 1.2 53.3 38.4 64.8 80.6 30.8 33.3 52.8 53.4 92.1 13.4 - 70.4 44.2
Extended Textonboost (dense CRF) 14.1 3.2 57.2 34.5 59.2 75.9 48.6 33.8 47.0 56.3 90.3 11.4 - 68.2 44.3
Ours (RDF only) 51.5 41.6 48.5 54.1 88.3 87.2 62.1 50.0 40.0 73.4 69.6 18.9 - 65.0 57.1
Ours (full 2D segmentation) 57.6 57.3 67.5 58.2 92.7 88.5 56.6 66.7 45.7 82.0 77.6 17.2 - 71.5 64.0
Silberman and Fergus [6] - - - - - - - - - - - - - - 56.6
Ren et al. [7] 85 80 89 66 93 93 82 81 60 86 82 59 35 - 76.1
Ours (full 2D segmentation) 50.7 57.6 59.8 57.8 92.8 89.4 55.8 70.9 48.4 81.7 75.9 18.9 13.5 44.4 59.5
TABLE III: NYU Depth dataset version 2. (left) Using classes deﬁned by [17]. (right) Using the structural classes deﬁned by [1]. We improve both the
results from Silberman et al. [1] and Couprie et al. [17]. Especially when using the semantic classes, we get improved results for several classes.
Label color Bed
Obj.
Chair
Furnit.
Ceiling
Floor
Deco.
Sofa
Table
Wall
Window
Booksh.
TV
Global
Average
Ground
Furnit.
Props
Struct.
Global
Average
Silberman et al. [1] - - - - - - - - - - - - - - - 68 70 42 59 58.6 59.6
Couprie et al. [17] 38.1 8.7 34.1 42.4 62.6 87.3 40.4 24.6 10.2 86.1 15.9 13.7 6.0 52.4 36.2 87.3 45.3 35.5 86.1 63.5 64.5
Ours (full 2D segmentation) 68.4 8.6 41.9 37.1 83.4 91.5 35.8 28.5 27.7 71.8 46.1 45.4 38.4 54.2 48.0 97.4 61.8 40.9 76.1 68.1 69.0
REFERENCES
[1] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor Segmenta-
tion and Support Inference from RGBD Images,” in ECCV, 2012.
[2] A. Anand, H. S. Koppula, T. Joachims, and A. Saxena, “Contextually
Guided Semantic Labeling and Search for 3D Point Clouds,” IJRR,
vol. 32, no. 1, pp. 19–34, 2013.
[3] A. N¨ uchter and J. Hertzberg, “Towards semantic maps for mobile
robots,” RAS, vol. 56, no. 11, pp. 915–926, 2008.
[4] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics (Intelligent
Robotics and Autonomous Agents). The MIT Press, 2005.
[5] P. Kr¨ ahenb¨ uhl and V . Koltun, “Efﬁcient Inference in Fully Connected
CRFs with Gaussian Edge Potentials,” in NIPS, 2011.
[6] N. Silberman and R. Fergus, “Indoor scene segmentation using a struc-
tured light sensor,” in ICCV Workshop on 3D Repr. and Recognition,
2011.
[7] X. Ren, L. Bo, and D. Fox, “RGB-(D) scene labeling: Features and
algorithms,” in CVPR, 2012.
[8] J. C. C. Valentin, S. Sengupta, J. Warrell, A. Shahrokni, and P. H. S.
Torr, “Mesh Based Semantic Modelling for Indoor and Outdoor
Scenes,” in CVPR, 2013.
[9] S. Gupta, P. Arbelaez, and J. Malik, “Perceptual Organization and
Recognition of Indoor Scenes from RGB-D Images,” CVPR, 2013.
[10] M. Johnson and J. Shotton, “Semantic texton forests,” in Computer
Vision: Detection, Recognition and Reconstruction. Springer, 2010.
[11] G. Floros and B. Leibe, “Joint 2D-3D temporally consistent semantic
segmentation of street scenes,” in CVPR, 2012.
[12] R. Triebel, R. Paul, D. Rus, and P. Newman, “Parsing Outdoor
Scenes from Streamed 3D Laser Data Using Online Clustering and
Incremental Belief Updates,” in AAAI, 2012.
[13] H. Hu, D. Munoz, J. A. Bagnell, and M. Hebert, “Efﬁcient 3-D Scene
Analysis from Streaming Data,” in ICRA, May 2013.
[14] J. St¨ uckler, N. Biresev, and S. Behnke, “Semantic mapping using
object-class segmentation of RGB-D images,” in IROS, 2012.
[15] J. Shotton, J. M. Winn, C. Rother, and A. Criminisi, “TextonBoost
for Image Understanding: Multi-Class Object Recognition and Seg-
mentation by Jointly Modeling Texture, Layout, and Context,” IJCV,
vol. 81, no. 1, pp. 2–23, 2009.
[16] L. Ladicky, C. Russell, P. Kohli, and P. H. S. Torr, “Associative
hierarchical CRFs for object class image segmentation,” in ICCV,
2009.
[17] C. Couprie, C. Farabet, L. Najman, and Y . LeCun, “Indoor Semantic
Segmentation using depth information,” CoRR, vol. abs/1301.3572,
2013.
[18] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake, “Real-time human pose recog-
nition in parts from single depth images,” in CVPR, 2011.
[19] J. Gall, A. Yao, N. Razavi, L. J. V . Gool, and V . S. Lempitsky, “Hough
Forests for Object Detection, Tracking, and Action Recognition,”
PAMI, vol. 33, no. 11, pp. 2188–2202, 2011.
[20] P. Geurts, D. Ernst, and L. Wehenkel, “Extremely randomized trees,”
Machine Learning, vol. 63, no. 1, pp. 3–42, 2006.
[21] A. S. Huang, A. Bachrach, P. Henry, M. Krainin, D. Maturana, D. Fox,
and N. Roy, “Visual Odometry and Mapping for Autonomous Flight
Using an RGB-D Camera,” in ISRR, 2011.
[22] J.-H. Park, Y .-D. Shin, J.-H. Bae, and M.-H. Baeg, “Spatial Uncertainty
Model for Visual Features Using a Kinect
TM
Sensor,” Sensors,
vol. 12, no. 7, 2012.
[23] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W. Bur-
gard, “An Evaluation of the RGB-D SLAM System,” in ICRA, 2012.
2638
