Reinforcement Learning with Multi-Fidelity Simulators
Mark Cutler, Thomas J. Walsh, Jonathan P. How
Abstract— We present a framework for reinforcement learn-
ing (RL) in a scenario where multiple simulators are available
with decreasing amounts of ﬁdelity to the real-world learning
scenario. Our framework is designed to limit the number of
samples used in each successively higher-ﬁdelity/cost simulator
by allowing the agent to choose to run trajectories at the lowest
level that will still provide it with information. The approach
transfers state-action Q-values from lower-ﬁdelity models as
heuristics for the “Knows What It Knows” family of RL
algorithms, which is applicable over a wide range of possible
dynamics and reward representations. Theoretical proofs of the
framework’s sample complexity are given and empirical results
are demonstrated on a remote controlled car with multiple
simulators. The approach allows RL algorithms to ﬁnd near-
optimal policies for the real world with fewer expensive real-
world samples than previous transfer approaches or learning
without simulators.
I. INTRODUCTION
Simulators play a key role as testbeds for robotics control
algorithms, but deciding when to use them versus collecting
real-world data is often treated as an art. For instance, several
reinforcement learning (RL) algorithms use simulators to
augment real robot data [1]–[3], but none of those agents
actively decide when to sample simulated or real-world
data. Likewise, the transfer learning (TL) community [4]
has sought to more seamlessly transfer information from
simulators to the real world [5], but TL methods for RL
also do not prescribe theoretically justiﬁed rules for when an
agent should move from simulation to the real world (e.g.
[5]).
Deciding to sample from different simulators or the real
world is important considering the relative cost of experience
at various levels of ﬁdelity. For example, an ODE simulation
can take more time than a simple Newtonian inverse kinemat-
ics model, but it is still typically much less costly (and less
risky) than running trials in the real world. Thus an efﬁcient
learning agent could greatly beneﬁt from actively choosing
to collect samples in less costly, low ﬁdelity, simulators.
This paper introduces, analyzes, and empirically demon-
strates a new framework, Multi-Fidelity Reinforcement
Learning (MFRL), depicted in Figure 1, for performing re-
inforcement learning with a heterogeneous set of simulators
(including the real world itself). The framework combines
ideas from both multi-ﬁdelity optimization [6] and advances
in model-based RL that have yielded efﬁcient solutions
to the exploration/exploitation dilemma. More speciﬁcally,
heuristics from lower-ﬁdelity simulators and adjustments
Laboratory of Information and Decision Systems, Massachusetts In-
stitute of Technology, 77 Massachusetts Ave., Cambridge, MA, USA
fcutlerm,twalsh,jhowg@mit.edu
Exploration heuristics
?
2
, ?
2
?
2
  
High 
Fidelity/
Cost
(real world) 
 ?
D
Low 
Fidelity/
Cost   
?
1
Learned (certain) model parameters
?
1
, ?
1
Fig. 1. MFRL architecture: a multi-ﬁdelity chain of simulators and learning
agents. Agents send exploration heuristics to higher-ﬁdelity agents and
learned model parameters to lower ﬁdelity agents. The environments are
related by state mappings 
i
and optimism bounds 
i
. Control switches
between learning agents, going up when an optimal policy is found, and
down when unexplored regions are encountered.
from high-ﬁdelity data (common techniques in multi-ﬁdelity
optimization) are instantiated in MFRL using the successful
“optimism in the face of uncertainty” heuristic and the
“Knows What It Knows” (KWIK) model-learning framework
from RL [7]. The result is an agent that both:
 uses information from lower ﬁdelity simulators to per-
form limited exploration in its current simulator, and
 updates the learned models of lower ﬁdelity agents with
higher-ﬁdelity data.
Unlike unidirectional methods that transfer heuristics only
once to the real-world agent [5], the MFRL framework also
speciﬁes rules for when the agent should move up to a higher
ﬁdelity simulator, as well as moving down in ﬁdelity before
over-exploring in a more expensive simulation. We show
that these rules, and the transfer of values and data, provide
theoretical guarantees on convergence and sample efﬁciency.
Speciﬁcally, the framework (1) does not run actions at high
levels that have been proven suboptimal below, (2) minimizes
(under certain conditions) the number of samples used in the
real world and (3) limits the total number of samples used
in all simulators. In addition, MFRL without resets provably
uses no more (worst case) samples at the highest ﬁdelity
level than unidirectional transfer approaches.
We showcase MFRL in both bandit learning and multi-
state RL. In addition, our theoretical results hold for a large
class of representations such as linear and Gaussian-noise
dynamics covered by the KWIK learning framework [7].
We test our algorithms in benchmark domains and real-
world remote controlled (RC) car control problems. Our
RC car experiments show that near-optimal driving policies
can be found with fewer samples from the real car than
unidirectional transfer methods or without using simulators.
Our main contributions are (1) introducing the MFRL
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3888
framework for learning with multiple simulators, (2) a the-
oretical analysis of the framework’s sample complexity and
(3) several demonstrations of efﬁcient learning on an RC car
with fewer real-world data points than unidirectional transfer
or learning without simulators. These results demonstrate
MFRL is an efﬁcient manager of the low and high quality
simulators often available for robotics tasks.
II. RELATED WORK
In RL, simulators are often used to train learning agents,
with real-world experience used later to update the simulator
or the agent’s policy (e.g. [2]). However, such systems
require practitioners to decide when to run policies in the
simulator/real world and do not guarantee efﬁcient explo-
ration. Another approach is to always execute actions in the
real world but use a low-ﬁdelity simulator to help compute
policy search gradients [1], [3]. However, these approaches
are speciﬁc to policy search algorithms and again do not
provide exploration guarantees.
Multi-ﬁdelity models have been considered for a single
agent that learns different policies for varying observability
conditions [8]. Instead, our algorithm has control over what
level of ﬁdelity it runs trajectories in and targets a single
policy for an observable real-world task. Multi-ﬁdelity mod-
els have been used in the multi-agent context to combine
data from people performing tasks in different simulators
[9]. However, these policies were learned from traces through
supervised learning (not RL).
Our MFRL framework can be viewed as a type of transfer
learning [4]. In TL, values or model parameters are typically
used to bootstrap learning in the next task (e.g. [4]). By
contrast, we use values from lower-ﬁdelity models as heuris-
tics guiding exploration, and our agent (rather than nature)
controls which simulator it is using, including the capability
to return to a lower-ﬁdelity simulator. We also differ from
transfer learning between environments with different action
sets [10] because our simulators can have different dynamics
or rewards, rather than just different available actions.
A similar approach to ours is Transferred Delayed Q-
Learning (TDQL) [5]. Like us, TDQL transfers the value
function unidirectionally from a source learning task as a
heuristic initialization for the target learning task. Our ap-
proach is different because our algorithm moves not only up,
but also down the multi-ﬁdelity simulator chain. Also, TDQL
is designed only for tabular representations while we tie our
approach to a much larger class (KWIK) of base learners.
We show that, when an agent is not reset when returning
to a simulator, our method provably uses no more (worst
case) samples at the highest ﬁdelity level than unidirectional
transfer approaches like TDQL. Our experiments also verify
that MFRL uses signiﬁcantly fewer samples in the real world
compared to unidirectional transfer.
Our work extends techniques in multi-ﬁdelity optimization
(MFO) [6] to sequential decision making problems. In MFO,
an optimization problem, such as setting design parame-
ters of an aircraft [11], is solved using multiple models.
Techniques in MFO include learning model disparities [11]
and constraining search based on results from lower ﬁdelity
models [12]. However, MFO does not consider sequential
decision making (RL) tasks. MFRL borrows lessons from
MFO by updating models with higher-ﬁdelity data and
performing constrained exploration based on lower-ﬁdelity
results.
III. BACKGROUND AND ASSUMPTIONS
In this section we provide background on RL and the
KWIK framework. We also describe the assumptions made
about our multi-ﬁdelity simulators.
A. Reinforcement Learning
We assume that each simulator can be represented by a
Markov Decision Process (MDP) [13], M =hS;A;R;T;i
with states S, actions A, reward function R(s;a)7!< and
transition function T (s;a;s
0
) = Pr(s
0
js;a). The optimal
value function Q

(s;a) =R(s;a) +
P
0
s
T (s;a;s
0
)V

(s
0
)
where V

(s) = max
a
Q

(s;a) is the expected sum of
discounted rewards when taking actiona in states and then
acting optimally thereafter. A deterministic policy :S7!A
is said to be optimal when 

(s) = argmax
a
Q

(s;a).
In reinforcement learning (RL) [13], an agent knowsS,A,
and but notT andR, which it learns from interaction with
the environment. This leads to an inherent tension between
exploration, where an agent seeks out new experiences to
update its model, and exploitation of its current knowledge.
To judge exploration efﬁciency, we follow previous deﬁ-
nitions [7] of sample complexity for an RL agent as a bound
(with probability 1 ) on the number of suboptimal steps
(where V
t
(s) < V

(s) ). The KWIK framework [7]
standardizes sample complexity analysis for model-based RL
by measuring the number of times the learners of T and R
are uncertain in making a prediction. Speciﬁcally, a KWIK
supervised learner is given an input (i.e. state/action pair)
and asked to make a prediction ^ y (e.g. a reward prediction)
of the true output y. If the agent is certain of its prediction
(jj^ y yjj< with high probability) it predicts ^ y. Otherwise, it
must state “I don’t know” (denoted?) and will view the true
label y. A hypothesis class H is said to be KWIK learnable
if an agent can guarantee (with high probability) it will only
predict? a polynomial (in
1

,
1

;jHj) number of times.
In the KWIK-Rmax RL algorithm [7], an approximate
MDP is built with
^
T and
^
R based on the predictions of
KWIK learners for each parameter. If a learner predicts?,
the uncertainty is replaced using the “optimism in the face
of uncertainty” heuristic, speciﬁcally setting the value of the
corresponding uncertain states to
Rmax
1 
. This interpretation
encourages exploration of unknown areas, but not at the
expense of already uncovered dominant policies. It also
guarantees polynomial sample complexity of the resulting
RL agent [7]. The sample complexity of KWIK-Rmax may
be smaller thanjSj ifT andR can be represented compactly
(e.g. as linear functions with onlyn parameters for an inﬁnite
jSj).
3889
B. Simulator Assumptions and Objectives
In this work, we deﬁne a simulator  as any environment
that can be modeled as an MDP. We follow [7] by deﬁning
the complexity of such domains jj as the size of their
corresponding T and R. Since S may differ between 
i
and a higher ﬁdelity 
j
(some variables may be absent from

i
), we follow prior work in TL [4] and assume a transfer
mapping 
i
: S
i
7! S
j
exists. Speciﬁcally, we assume that
S
i
S
j
and that
i
maps states inS
i
to states inS
j
, setting
data uniformly across variables that exist in S
j
, but not in
S
i
. For instance, in our car simulations, the lowest ﬁdelity
simulator (
1
) does not model rotational rate
_
 , so states
in S
1
map to all states in S
2
with the same variable values
except for
_
 . The reverse mapping
 1
i
only applies to states
in S
j
with a single default value of the missing variable
(
_
 = 0 for the car).
We deﬁne ﬁdelity based on how much 
i
overvalues the
state-actions of 
j
. Speciﬁcally, the ﬁdelity of 
i
to 
j
(with
associated 
i
and tolerance 
i
) is
f(i;j;i;i) =
8
>
<
>
:
  maxs;ajQ


i
(s;a) Q


j
(i(s);a)j;
if8s;a[Q


j
(s;a) Q


i
(i(s);a)i]
 1; otherwise
wheres2S
i
. Intuitively, the ﬁdelity of 
i
to 
j
is inversely
proportional to the maximum error in the optimal value func-
tion, given that 
i
never undervalues a state/action pair by
more than. Otherwise, 
i
is considered to have no ﬁdelity
to 
j
. While this may seem restrictive, this relationship is
fairly common in real-life simulators. For instance, in our
car simulators, the lowest ﬁdelity  assumes that actions will
have perfect outcomes, so aggressive maneuvers achieve their
desired results. In higher ﬁdelity simulators, and eventually
the real world, these optimistic values are replaced with
more realistic outcomes/values. Hence, the simulators form
an optimistic chain formally deﬁned as follows:
Deﬁnition 1: An optimistic multi-ﬁdelity simulator chain
is a series ofD simulators ordered 
1
:::
D
, with 
D
being
the target task (real-world model) and f(
i
; 
i+1
;
i
;
i
)6=
 1 for speciﬁed 
i
and 
i
.
We also make the following realistic assumptions about
the cost and accessibility of the simulators.
Assumption 1: A single step from simulator 
i
has the
same cost as a polynomial (inj
i
j) number of samples from
simulator 
i 1
.
Assumption 2: Access to each simulator may be limited
to running contiguous trajectories rather than having random
access to a generative model or the model parameters.
The ﬁrst assumption states that each successively higher
ﬁdelity simulator costs more to run per step than the one
below it, but it is potentially not worth sampling everyhs;ai
at the lower level. The second restriction states that we may
not have access to the simulator parameters or the ability to
sample state/action outcomes generatively. This is the case
in the real world (
D
) and in certain simulators (e.g. most
commercial video games).
Given such simulators, our objectives are the following:
1) Minimize the number of suboptimal steps (learning
samples) taken in 
D
.
2) Ensure that, for any run of the agent with simulator

i
, only a polynomial number of steps (inj
i
j) are
taken before near-optimal behavior (given constraints
from higher ﬁdelity simulators) is achieved or control
is passed to a lower ﬁdelity simulator.
3) Guarantee that there are only a polynomial (injj and
D) number of switches between simulators.
Objective 1 skews the sampling burden to lower ﬁdelity
simulators while objectives 2 and 3 limit the sample com-
plexity of the algorithm as a whole.
IV. MULTI FIDELITY BANDIT OPTIMIZATION
One of the simplest RL settings is thek-armed bandit case,
an episodic MDP with a single state,k actions (called arms),
and = 0. A learner chooses actions to explore the rewards,
eventually settling on the best arm, which it then exploits. We
now present MFRL for the bandit setting, which has many
features of the full MFRL framework presented later.
A. A MF-Reward Learning Algorithm
Consider a chain of bandit simulators: at each level d2
f1:::Dg there arejAj actions with expected rewardsR
d
(a)
R
d 1
(a) +
d
. For a single simulator, we can utilize a base
learner that can update the estimates of each arm’s reward.
Here, we use KWIK reward learners
^
R
d
with parameter m
based on accuracy parameters and.
^
R
d
predicts ^ (a), the
empirical mean payout for arm a, if the number of samples
seen for that arm is greater than m, and otherwise predicts
?, translating into R
max
as a loose upper bound.
Algorithm 1 presents the Multi-Fidelity Bandit Framework
(MF-Bandit) for accomplishing objectives 1-3 using a KWIK
learner that keeps track of reward means and upper bounds
^
U
da
. MF-Bandit also tracks the informed upper bound U
da
,
which is the minimum of
^
U
da
and the heuristic from the
lower level:U
d 1;a
+
d 1
(lines 20 and 25). The algorithm
also keeps track of whether the value of each action has
converged (con
da
), whether an optimal action has been
identiﬁed (closed
d
), and if the learned model has changed
(change
d
) at simulator level d.
Starting in 
1
, the algorithm selects an actiona

greedily
based on U
d
and checks if learning at the current level is
complete (line 8). Before executing the action, it checks
to make sure the action has been tried sufﬁciently at the
simulator below (lines 9). If not, values from d that are
converged are transferred back to level d  1 (lines 12-15)
and control is passed there. Otherwise, if learning at d is
not ﬁnished, the action is taken and  and
^
U are updated
(lines 17-22). Once the optimal action has been identiﬁed,
the algorithm moves up to the level d + 1 (lines 23-27).
Our algorithm differs sharply from unidirectional (only
up) heuristic transfer [5] because it can backtrack to a
lower ﬁdelity simulator when a previously identiﬁed optimal
action performs badly above. Effectively, backtracking asks
the lower learner to ﬁnd a new optimal policy given new
knowledge from higher-ﬁdelity simulators.
3890
Algorithm 1 Multi-Fidelity Bandit Framework
1: Input: A bandit simulator chain h;i, Actions A,
R
max
, Accuracy requirements  and 
2: Initialize: con
da
, change
d
:=false;8a;d
3: Initialize: KWIK learners
^
R
d
(a;  ;

)
4: d := 1
5:
^
U
da
, U
1a
:=R
max
8a
6: for each timestep do
7: Select a

:= argmax
a
U
da
8: closed
d
:=con
da
_a

is deﬁnitely near optimal
9: if d> 1^:con
d 1;a
^change
d
then
10: fsend values back to level d  1g
11: change
d 1
:=false
12: for a2A do
13: if con
da
then
14:
^
R
d 1
:=
^
R
d
fcopy 
d
and
^
U
d
downg
15: con
d 1;a
, change
d 1
:=true
16: d :=d  1
17: else if:closed
d
then
18: Execute a

, Observe r.
19: Update
^
R
d
(a

)fUpdate 
da
and
^
U
da
g
20: U
da
 := min(U
d;a
;
^
U
da
)
21: if
^
R(a

) switched from? to “known” then
22: con
da
, change
d
:=true
23: else if d<n^closed
d
then
24: fchosen action already converged, go upg
25: Where:con
d+1;a
, U
d+1;a
=U
d;a
+
d 1
26: change
d+1
:=false
27: d :=d + 1
An example where this behavior is beneﬁcial on our RC
car is when an optimal conﬁguration of parameters in the
simulator generates a path with tight turns, but data in the
real world proves such settings cause the car to spin out.
In such a scenario there is still information to be gleaned
from the lower-level simulator by exploring policies given
the knowledge of spinning out from above, which is exactly
what transferring the mean values down accomplishes.
B. Bandit Examples
We now present examples to showcase various features
of Algorithm 1. First, we show MF-Bandit can ﬁnd an
optimal policy for 
D
with far fewer samples in 
D
than
an algorithm without multiple simulators. Consider a Bandit
problem withjAj = 5 arms and D = 3 simulators with
bounded reward [0; 1]. The rewards for each simulator are

1
=f0:8; 0:8; 0:8; 0:8; 0:1g, 
2
=f0:8; 0:8; 0:6; 0:6; 0:1g,
and 
3
=f0:8; 0:6; 0:6; 0:6; 0:1g, all with uniform random
noise up to 0:1. Table I (left) shows the results of running
Algorithm 1 in this scenario with our KWIK bandit learner
with m = 20 as well as results with onlyh
2
; 
3
i and
only 
3
. We see that the use of both simulators or just 
2
produces a signiﬁcant reduction in samples from 
3
and that
having 
1
helps limit samples needed from 
2
.
In the scenario above, the algorithm could potentially
avoid backtracking because one of the optimal actions al-
TABLE I
SAMPLES USED FROM SIMULATORS
Sims Used 1 2 3
1 ,2 ,3 100 80 40
2 , 3 100 40
3 100
Sims Used 1 2 
3
0
1 ,2 ,
3
0 100 80 60
Uni-directional 100 60 80

3
0 100
ways remained the same at each level. But consider the
same scenario except with an alternate top level, 
3
0 =
f0:4; 0:4; 0:6; 0:6; 0:1g. Now neither of the optimal actions
in 
2
are optimal in 
3
0. Table I (right) shows the results
of Algorithm 1 in this case along with a version that does
no transfer and a version that only performs unidirectional
transfer (never going back to a lower ﬁdelity simulator)
[5]. We see here that by allowing the algorithm to return
to lower ﬁdelity simulators once the previously considered
optimal action has been disproved at a higher level, valuable
exploration steps in 
D
are saved, and the cost in terms of
steps in the real world is minimized.
C. Theoretical Analysis
We now formalize the intuitive lessons from above with
theoretical guarantees for MFRL when the base learner is the
KWIK bandit learner. We begin by focusing on objectives 2
and 3 from Section III-B: limiting the number of suboptimal
actions at each level and the number of samples overall.
Theorem 1: Algorithm 1 uses only a polynomial num-
ber of samples over all the levels, speciﬁcally using only
O(
AD
2

2
log(
A
2
D

)) samples per run at level d and only
changing d a maximum of AD times.
Proof: Given  and , an application of Hoeffding’s
inequality yields a KWIK algorithm withm =O(
1

2
ln(
1

)).
Setting the base learners at each level with stricter accuracy
requirements:   =

2D
and

 =

A
2
D
yields the desired result
in terms of the maximum number of samples. Each execution
at a level must determine a new parameter before moving
up or down, and once an arm’s value is set from above it
cannot be sampled at the current level. Therefore there can be
at most AD level changes. Applying a Union bound across
actions gives a probability of failure at a speciﬁc level d of

AD
and applying another union bound across the number of
changes yields the desired result.
Now we turn our attention to objective 1, minimizing the
number of samples used in 
D
. We begin with the following
lemma, which is similar to Lemma 1 of [5], stating that no
action is tried at a level beyond which it is dominated by the
value of a

in 
D
.
Lemma 1: Consider action a at level d and let 
d
=
^
R
d
(a) if a has been executed m times at level d, otherwise

d
=U
d
(a). If 
d
<R
D
(a

D
) 
P
D 1

d=d
 
d
  where a

D
is
the optimal action in 
D
, then with probability (1 ), a
will not be attempted at or above level d.
Proof: Set   and

 as above. At each level d
0
d, by
Deﬁnition 1 we have that the expectation on the reward of
a

D
will beR
d
0(a

D
)R
D
(a

D
) 
P
D 1

d=d
 
d
 >
d
based
on the lemma’s assumption and Deﬁnition 1. This means
whenever we enter leveld
0
, actiona

D
will be used beforea.
By Hoeffding’s inequality, pulling arma

D
m times will give
us a mean reward within   ofR
d
0(a

D
) with high probability,
3891
so there will also be no need to pull arm a at level d
0
after
collecting these m samples.
Now we show only actions that must be tested in 
D
are
used there (objective 1 from section III-B).
Theorem 2: With probability 1 , any actiona attempted
in simulator 
D
(the real world) by Algorithm 1 is either
near optimal (within  of R
D
(a

)) or could only be shown
to be suboptimal in 
D
.
Proof: Consider any action a executed in 
D
and its
associated 
D 1
values at the next lower ﬁdelity simulator
as deﬁned in Lemma 1. From Lemma 1, we have 
D 1

R
D
(a

D
)  
D 1
   . Otherwise with high probability a
would have been pruned and not executed in 
D
. If a is
near optimal we are done. If not, the algorithm must have
taken actiona at levelD 1 and with high probability found
U
D
(a) =
^
R
D 1
(a) R
D
(a

D
) 
D 1
   . Therefore the
only way to determine thata is not near-optimal is to execute
it in 
D
.
A corollary of this theorem is that the MF-Bandit algo-
rithm uses provably no more (wost case) samples in 
D
than
unidirectional transfer, which does not ensure that actions in

D
have been vetted in lower ﬁdelity simulators.
V. MULTI-FIDELITY REINFORCEMENT
LEARNING
We now instantiate the principles of generating heuristics
from low-ﬁdelity simulators and sending learned model data
down from high ﬁdelity simulators in the full (multi-state,
cumulative reward) RL case.
A. The MFRL Algorithm
Algorithm 2 shows the MFRL framework, which takes
as input a simulator chain, maximum reward R
max
, state-
mapping between simulators 
1:::D 1
, a planner P (e.g.
Value Iteration [13]) and accuracy requirements  , , and
m
known
, the latter of which determines when to move to a
higher ﬁdelity simulator. MFRL is similar to MF-Bandit but
now the heuristic passed to higher ﬁdelity simulators is the
Q-function, and both the reward and transition functions are
passed down to lower ﬁdelity simulators.
The algorithm begins by initializing the variables d, m
k
and change
d
and the base KWIK learners
^
T and
^
R, with
parameters   and

 described in Section V-C. We use the
shorthand
^
 to denote the MDP induced by
^
T and
^
R and
replacing all ? predictions with a heuristic (in this case
corresponding lower ﬁdelity values).Q-values for the lowest
ﬁdelity simulator are set optimistically using
Rmax
1 
, and the
agent begins choosing actions greedily at that level.
If the selected state/action pair still has uncertainty at level
d  1 (according to the KWIK model learners there), and a
change has been made at the current level, the algorithm
backtracks one layer of ﬁdelity (lines 9-13).
1
Otherwise, the
action is executed and
^
T and
^
R are updated. If the update
changes a parameter from unknown to known, these learned
1
While this “one unknown” backtracking is theoretically correct, in our
experiments we wait until m
unknown
such states are encountered, which
helps control sampling at lower level simulators.
Algorithm 2 MFRL (MF-KWIK-Rmax)
1: Input: A simulator chainh;;i, R
max
, Planner P ,
accuracy parametersh, , m
known
i
2: d := 1 and m
k
:= 0
3: Initialize: change
d
:= false;8d
4: Initialize: 2D KWIK learners
^
R( ;

) and
^
T
d
( ;

)
5: Initialize: Q
0
:=
Rmax
1 
6: Initialize:
^
Q
1
(s;a) :=P (hS
1
;A;
^
R
1
;
^
T
1
;i;Q
0
)
7: for each timestep and state s do
8: Select a

:= argmax
a
^
Q
d
(s;a)
9: if d > 1^ change
d
^ (
^
T
d 1
(
 1
d 1
(s);a

) = ?_
^
R(
 1
d 1
(s);a

) =?) then
10: fsend values back to level d  1g
11:
^
Q
d 1
(s;a) :=P (
^

d 1
;Q
d 2
+
d 2
)
12: m
k
:= 0
13: d :=d  1
14: else
15: Execute a

, Observe r, s
0
.
16: if
^
R
d
(s;a

) =?_
^
T
d
(s;a

) =? then
17: m
k
:= 0
18: Update
^
R
d
and/or
^
T
d
that predict?
19: else
20: m
k
:=m
k
+ 1
21: if
^
R
d
(s;a

) or
^
T
d
(s;a

) went from? to known
then
22:
^
Q
d
(s;a) :=P (hS
d
;A;
^
R
d
;
^
T
d
;i;Q
d 1
+
d 1
)
23: Seth
^
R;
^
Ti
d
0(
 1
d
(s);a

) ford
0
d based on the
new values. Set change
d
0 :=true
24: if d<D^m
k
=m
known
then
25: fGo up to level d + 1g
26:
^
Q
d+1
(s;a) :=P (
^

d+1
;Q
d
+
d
)
27: m
k
:= 0, change
d
:= false
28: d :=d + 1
parameters are transferred to all lower simulators (line 23).
This has the effect of forcing the corresponding agent in
the lower level simulator (if we return there) to explore
policies that might be optimal given the dynamics of the
higher ﬁdelity simulator. If the model parameters change, the
change
d
ﬂag is also set and the planner recalculates the Q
values. However, unlike standard (no-transfer) KWIK-Rmax,
the algorithm uses heuristic values from the lower ﬁdelity
simulator (Q
d 1
) to ﬁll in the Q-values for state/actions
where the KWIK learners are uncertain during planning.
Finally, we have the convergence check (line 24) to see
if MFRL should move to a higher-ﬁdelity simulator. In the
multi-state case, simply encountering a known state does not
indicate convergence, as states that are driving exploration
may be multiple steps away. Instead, Algorithm 2 checks if
the lastm
known
states encountered at the current level were
known according to the base learners. We provide theoretical
guidelines for setting m
known
in Theorem 3.
B. Puddle World with MFRL
We illustrate the behavior of MFRL in a variant of puddle
world (Figure 2) [13] with multi-ﬁdelity simulators. A puddle
3892
(a) No Puddle (b) Some Puddle (c) Full Puddle
Fig. 2. 
1
::: 
3
for puddle world. 
1
has no puddle, 
2
has most of
the puddle but the optimal policy can bypass it.
0 500 1000 1500 2000 2500 3000
Full Puddle Samples
? 1800
? 1600
? 1400
? 1200
? 1000
? 800
? 600
? 400
? 200
0
Average Cumulative Reward
MFRL
UNI
RMAX
No Puddle
Some Puddle
Full Puddle
0
500
1000
1500
2000
2500
3000
3500
Simulation Steps
MFRL
UNI
Fig. 3. Left: During learning, MFRL consistently outperforms unidirec-
tional transfer and no-transfer Rmax at the Full Puddle level. Each point is an
average of 1000 learning runs (standard errors shown). Greedy policies are
evaluated 60 times, each capped at 600 steps. Right: At the top level, MFRL
requires fewer than half the samples needed by unidirectional transfer. This
is accomplished by transfering some learning burden to the other simulators.
Average of 1000 learning runs (standard deviations shown).
world agent moves in one of four directions with Gaussian
noise and a step cost of  1 (0 at the goal) and high
negative rewards in the puddle (increasing with depth). We
implemented our puddle world with diagonal actions (to
illustrate the behavior of the algorithm in 
2
) and  = 0:95
so the optimal policy in 
3
is generally to skirt along the
outer edges of the puddle.
We tested our algorithm in the presence of two lower
ﬁdelity simulators with respect to the “real” puddle world
(Figure 2). 
1
is the same environment without the puddle.

2
contains a large portion of the puddle, but has an
opening in the worst reward region from 
D
. This again
creates a scenario where an optimal policy in the low-
ﬁdelity simulator supports a policy that is poor in 
D
but
still contains signiﬁcant useful information (in this case the
puddle portions in 
2
).
Figure 3 (left) shows learning curves from this experiment.
MFRL performed the best, with some negative transfer at the
beginning from the “shortcut” in 
2
, but as it encounters the
real puddle it passes this information back to the learner in

2
(through several level changes) and forces that lower-
ﬁdelity agent to ﬁnd a way around the puddle. The result is
a consistent and signiﬁcant improvement over unidirectional
transfer throughout learning. In fact, unidirectional transfer
takes almost as long as the no-transfer case to consistently
ﬁnd the optimal policy since it must explore essentially
the whole puddle at the top level. Figure 3 (right) shows
an average of 1000 runs of MFRL with bars showing the
average number samples in each of the 3 simulators. The
MFRL agent relied heavily on 
1
and 
2
, gathering crucial
information through these lower cost simulators to decrease
learning time at the top level.
C. Theoretical Analysis
Most of the theoretical results for multi-state MFRL are
similar to the bandit theorems, so here we note places where
changes to the bandit proofs are made. We assume here
that the variables in each 
i
are the same, that is  is the
identity mapping. The properties can still be made to hold
with missing variables with increases to  where transfer
based on default values may cause under-estimation.
Theorem 3: MFRL withm
known
=
1
1 
ln

4Rmax
 (1 )

and
  =

4(D+1)
and

 =

4(D+2Djj)
has the following properties
with probability 1 . (1) Actions that have been proven to
be suboptimal with respect toQ

D
at leveld will not be tried
above. (2) Actions taken in 
D
will either be near-optimal
or lead to an unknown state not learned about below or that
needs to be learned about in 
D
. (3) The total number of
level changes is no more than (D+2Djj) and the number of
total samples is polynomially bounded inhjj;D;
1

;
1

;
1
1 
i.
Proof: [sketch] Property (1) can be proven similarly to
Lemma 1, with the learned Q values replacing the learned
reward function. Property (2) is a property of the KWIK-
Rmax algorithm (see Lemma 13 and the Theorem 4 of
[7]) with an admissible heuristic. Our transferred Q-values
are (w.h.p.) admissible by Deﬁnition 1 and the accuracy
guarantees of the underlying KWIK learners.
For Property (3), D + 2Djj is an upper bound on the
number of level changes because each backtrack can only
occur when at least one parameter is learned, and the number
of parameters in the system isjjD. The number of “up”
entries can only beD more than he number of down entries,
giving us D + 2Djj level changes.
By instantiating the KWIK learners
^
T and
^
R with   and


we achieve a polynomial bound on the number of encounters
with unknown states, and we know the algorithm can have
at most m
known
  1 samples between these unknown states
without changing d. Since m
known
is also polynomial in
the relevant quantities, we have the desired bound on the
number of samples. Also, based on the analysis of KWIK-
Rmax (Theorem 4 of [14]), if m
known
known states in a
row are encountered, then with high probability we have
identiﬁed the optimal policy at the current level.
Property (2) of the theorem shows that, if the agent is not
reset when returning to a simulator, MFRL will, with high
probability, enter no more unknown states (where
^
T or
^
R
predict?) in 
D
than a unidirectional transfer method with
the same base learner and architecture. Unlike the bandit
case, this does not translate into such steps being necessary
as the base KWIK-Rmax architecture only gives an upper
(not lower) bound on the number of samples needed.
VI. RC CAR RESULTS
RC cars have been popular testbeds for RL algorithms [1],
[3], [15], though none of these approaches chose which
simulator (if any) to run trajectories in. We ran two experi-
ments (one bandit, one multi-state) on an RC car with two
simulators of the car’s dynamics.
3893
Fig. 4. Left: Our RC car and associated variables. Right: The track
conﬁguration and state/actions spaces for the experiments.
A. Experimental Setup
Our MFRL algorithms are experimentally veriﬁed using
an RC car driving on an indoor track. The car is an off-the-
shelf 1=16 scale 4-wheel drive rally car shown in Figure 4.
The position, velocity, and heading angle of the vehicle
are measured using an external motion capture system. The
wheel velocity is measured and ﬁltered using an optical
encoder read by a 16 bit microcontroller.
Figure 4 (right) shows the car task consisting of selecting
different radii and velocities to minimize lap-times on a track
of ﬁxed length. In this scenario, the track consists of straight
and curved segments, each with an associated distance and
velocity parameter. The virtual “cones” are ﬁxed and denote
the length of the path. As the car ﬁnishes each segment, the
commanded radius (r
i;cmd
) and velocity (v
i;cmd
) values for
the next segment are chosen. The reward returned for each
segment is t wheret is the elapsed time for that segment. If
the car drives (or often slips) out of a virtual “drivable” area
around the track, the car resets to a ﬁxed initial condition
and is given a large negative reward. The state variables in
s
i
(Figure 4 (right)) are the body frame forward velocity,
V
x
, rotational rate,
_
 , distance from track center, r, and the
current segment type, c, (straight or curved).
Choosing the next radius, r
i;cmd
, and velocity, v
i;cmd
,
fully deﬁnes the desired path for segment i (note that
straight and curved track segments are forced to alternate).
The car follows this path using a pure pursuit controller
where the look ahead control distance is a function of the
commanded velocity [16]. Running at 50 Hz, the pure pursuit
controller computes the desired forward velocity, rotational
rate and heading angle required to keep the car on the
speciﬁed trajectory. A steering angle command and a desired
wheel velocity is computed using the closed-loop controllers
from [17], where the cross track error term in the steering
angle control law is omitted, as the cross track error is
minimized by the pure pursuit algorithm. The C
y
parameter
in this control law is found by matching measured vehicle
data to input commands.
The steering angle,, and commanded wheel speed,!
cmd
,
are broadcast to the car’s microcontroller over a wireless
serial connection. Steering commands are sent directly to the
servo. Commands to the motor come from a simple closed-
loop controller around the commanded and measured wheel
speed. This proportional-integral wheel speed controller is
used to lessen effects of changing battery voltage on the
velocity dynamics.
The simulation environments for the RC car consist of
a na¨ ıve simulator (
1
) and a dynamics-based simulator
Input Only
Dynamics Sim
Real Car
0
20
40
60
80
100
120
140
Simulation Steps
MFRL
UNI
Input Only
Dynamics Sim
Real Car
0
1000
2000
3000
4000
5000
6000
7000
Simulation Steps
MFRL
UNI
Fig. 5. Samples used by MFRL and unidirectional transfer at each level.
The bandit case (left) and in the state-based case (right) are shown. In both
cases, the MFRL algorithm uses signiﬁcantly fewer samples in the real
world, but converges to an identical policy. Each case is the average of 3
learning runs with standard deviations shown.
(
2
). The na¨ ıve simulator ignores the dynamic model of
the car and returns ideal segment times assuming the car
followed the requested trajectory exactly. The higher ﬁdelity
simulator models the basic dynamics of the car, including
wheel slip, where model parameters such as the “Magic
Tyre Parameters” [18] are estimated using test data collected
on the car. This simulator captures much of the dynamic
behavior of the car, although discrepancies in real world
data and simulator data become more signiﬁcant at higher
velocities (above about 2.0 m/s) and when the wheels slip
signiﬁcantly. Therefore, learning needs to be performed not
only in simulators, but also on the physical car.
B. Experiment Results for Bandit Setting
The ﬁrst RC car experiment takes place in the (single-
state) bandit setting: choosing a single radii and two ve-
locities (one for curves and one for straightaways) at the
beginning of a 3-lap run. We allowed 5 values for radii
(between 0:5 and 1:2 m) and 5 values for velocities (between
2:0 and 3:5 m/s), yielding 125 actions/arms in the bandit
scenario. We evaluated Algorithm 1 in this setting as well as
the unidirectional transfer of Q-value heuristics [5]. Both of
these found an optimal policy within 60 steps on the real car
and so we did not compare to a no-transfer algorithm since it
would need at least 250 trials to identify the optimal policy
2
.
The simulators are deterministic (
2
has only a small amount
of added artiﬁcial noise) and so we setm = 1 at 
1
and 
2
.
To account for real world noise, we set m = 2 in the real
world, meaning 2 tries with a given parameter setting were
needed to determine its value.
Figure 5 (left) depicts the average number of samples
used in each simulator by each algorithm. MF-Bandit uses
fewer than half as many samples at the real-world when
compared to the unidirectional learner. Both MF-Bandit and
unidirectional transfer converged to policies with lap times of
about 3:7 seconds per lap and learned to use higher velocities
on the straightaways than the curves. These lap times are
2
While there are only 125 actions, each action must be tried m = 2
times in the real world before it is know.
3894
similar to the ones found in the state-based setting described
in the next section, although the state-based policy is more
robust to disturbances and noise.
C. Experiments for the State-Based Setting
In the multi-state case, we used the state space described
earlier and allowed the car to pick a radius and velocity
at the beginning of every segment (4 per lap). Because we
are making closed-loop state-based decisions (i.e. changing
velocities and radii in short segments), we can reduce the
action-space from the bandit setting, sincejj =O(jAj
jSj
).
Here we used 3 radii and 3 velocities (jAj = 9). Because
of the discretization in the state space, which makes the
simulation results potentially noisier (from state aliasing),
we used m = 3 in 
2
and the real car.
In the experiment, both MFRL and unidirectional transfer
converged to an optimal policy that just under 3:7 seconds
around the track. Figure 5 (right) shows the average number
of samples used in each level by MFRL and unidirectional
transfer. The MFRL algorithm converges using an average
of 35% fewer samples in the real world when compared to
unidirectional transfer.
The converged policy in the state-based experiments is
different from the bandit case, due to the versatility of
state-based control. Instead of an oval shape, MFRL chose
different values for the radius entering a curve versus a
straightaway. This led to initially wide turns towards the
cones followed by a sharp turn towards the straightaway,
maximizing the time the car could drive fast down the
straight section between cones. Reaching this fairly compli-
cated policy with a reasonable number of real-world samples
was made possible by MFRL’s efﬁcient use of samples from
the previous levels. Particularly, 
1
pruned policies that were
too slow while 
2
pruned policies that were too fast on the
curves. This left 
3
(the real car) to reﬁne the policies in a
noisier environment.
VII. EXTENSIONS AND CONCLUSIONS
More powerful representations than our tabular T and
R models, including linear dynamics and Gaussian-noise
models, are polynomially KWIK learnable [7]. This extends
Theorem 3 to certain continuous MDPs where the discretiza-
tion used in our experiments would not be necessary and will
be empirically investigated in future work.
Another extension is relaxing Assumption 2, which as-
sumes samples can only be obtained by performing trajec-
tories. However, allowing such generative access does not
necessarily make choosing where to sample any easier. For
instance, consider the case where an agent in 
d
encounters
a state that has not been visited in 
d 1
. It is tempting to
simply query 
d 1
at that state. However, these samples
might be ineffective. For example, in our ﬁrst RC car
experiment, optimistic transitions in 
D 1
cause the car to
attempt aggressive maneuvers that fail 
D
. But this does not
mean the system should query the simulator to learn what
happens during an unrecoverable spin. The more prudent
course is to replan in 
D 1
given the transition dynamics
actually encountered in 
D
, which is what MFRL does.
Thus, actively choosing samples in MFRL with a generative
mode is a topic for future work.
We have introduced MFRL, which extends lessons from
the multi-ﬁdelity optimization community to sequential deci-
sion making. MFRL transfers heuristics to guide exploration
in high ﬁdelity (but higher cost) simulators. Unlike previous
transfer learning techniques, our framework also allows the
transfer of learned model parameters to agents in lower
ﬁdelity simulators, a tactic we have shown is crucial for
minimizing sub-optimal steps in the real world. Throughout
this process, our agents retain sample efﬁciency guarantees
over the entire learning process because of our integration of
the KWIK-Rmax framework. Our experiments with an RC
car show that not only is the framework theoretically sound,
but it is also a practical technique for scaling reinforcement
learning algorithms to real-world decision making.
REFERENCES
[1] P. Abbeel, M. Quigley, and A. Y . Ng, “Using inaccurate models in
reinforcement learning,” in ICML, 2006.
[2] P. Abbeel, A. Coates, M. Quigley, and A. Y . Ng, “An application of
reinforcement learning to aerobatic helicopter ﬂight,” in NIPS, 2006.
[3] J. Z. Kolter and A. Y . Ng, “Policy search via the signed derivative,”
in RSS, 2009.
[4] M. E. Taylor, P. Stone, and Y . Liu, “Transfer learning via inter-
task mappings for temporal difference learning,” Journal of Machine
Learning Research, vol. 8, no. 1, pp. 2125–2167, 2007.
[5] T. A. Mann and Y . Choe, “Directed exploration in reinforcement
learning with transferred knowledge,” in European Workshop on
Reinforcement Learning (EWRL), 2012.
[6] T. D. Robinson, K. E. Willcox, M. S. Eldred, and R. Haimes, “Multi-
ﬁdelity optimization for variable-complexity design,” in AIAA/ISSMO
Multidisciplinary Analysis and Optimization Conference, 2006.
[7] L. Li, M. L. Littman, T. J. Walsh, and A. L. Strehl, “Knows what
it knows: a framework for self-aware learning,” Machine Learning,
vol. 82, no. 3, pp. 399–443, 2011.
[8] E. Winner and M. M. Veloso, “Multi-ﬁdelity robotic behaviors: Acting
with variable state information,” in AAAI, 2000.
[9] E. J. Schlicht, R. Lee, D. H. Wolpert, M. J. Kochenderfer, and
B. Tracey, “Predicting the behavior of interacting humans by fusing
data from multiple sources,” in UAI, 2012.
[10] B. Fern´ andez-Gauna, J. M. L´ opez-Guede, and M. Gra˜ na, “Trans-
fer learning with partially constrained models: Application to rein-
forcement learning of linked multicomponent robot system control,”
Robotics and Autonomous Systems, vol. 61, no. 7, pp. 694–703, 2013.
[11] F. A. Viana, V . Steffen, Jr., S. Butkewitsch, and M. Freitas Leal,
“Optimization of aircraft structural components by using nature-
inspired algorithms and multi-ﬁdelity approximations,” Journal of
Global Optimization, vol. 45, no. 3, pp. 427–449, 2009.
[12] A. Molina-Cristobal, P. R. Palmer, B. A. Skinner, and G. T. Parks,
“Multi-ﬁdelity simulation modelling in optimization of a submarine
propulsion system,” in Vehicle Power and Propulsion Conf., 2010.
[13] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning.
Cambridge, MA, USA: MIT Press, 1998.
[14] L. Li, “A unifying framework for computational reinforcement learn-
ing theory,” Ph.D. dissertation, Rutgers University, New Brunswick,
NJ, 2009.
[15] T. K. Lau and Y .-h. Liu, “Stunt driving via policy search,” in ICRA,
2012.
[16] S. Park, J. Deyst, and J. P. How, “Performance and lyapunov stability
of a nonlinear path following guidance method,” Journal of Guidance,
Control, and Dynamics, vol. 30, no. 6, pp. 1718–1728, 2007.
[17] G. M. Hoffmann, C. J. Tomlin, M. Montemerlo, and S. Thrun,
“Autonomous automobile trajectory tracking for off-road driving:
Controller design, experimental validation and racing,” in ACC, 2007.
[18] E. Velenis, E. Frazzoli, and P. Tsiotras, “Steady-state cornering equilib-
ria and stabilisation for a vehicle during extreme operating conditions,”
International Journal of Vehicle Autonomous Systems, vol. 8, no. 2,
pp. 217–241, 2010.
3895
