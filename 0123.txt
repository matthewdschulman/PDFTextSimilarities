Autonomous Acquisition of Generic Handheld Objects in Unstructured
Environments via Sequential Back–Tracking for Object Recognition
Krishneel Chaudhary, Yasushi Mae, Masaru Kojima and Tatsuo Arai
Abstract— Robots operating in human environments must
have the ability to autonomously acquire object representations
in order to perform object search and recognition tasks without
human intervention. However, autonomous acquisition of object
appearance model in an unstructured and cluttered human
environment is a challenging task, since the object boundaries
are unknown in prior. In this paper, we present a novel
method to solve the problem of unknown object boundaries
for handheld objects in an unstructured environment using
robotic vision. The objective is to solve the problem of object
segmentation without prior knowledge of the objects that
human interacts with daily. In particular, we present a method
that segments handheld objects by observing human-object
interaction process, and performs incremental learning on the
acquired models using SVM. The unknown object boundary
is estimated using sequential back-tracking via exploitation of
afﬁne relationship of consecutive frames. The segmentation is
achieved using identiﬁed optimal object boundaries, and the
extracted models are used to perform future object search and
recognition tasks.
Index Terms— Handheld object segmentation, Incremental
Learning, Sequential Back–Tracking (SBT), Support Vector
Machine (SVM)
I. INTRODUCTION
Autonomous learning and acquisition of object appearance
models from the environment with no human intervention
is fundamental for robots operating in human environments.
For example, if a robot can learn from observation of human-
object interactions, it can keep track of the objects being
manipulated and alarm in case of dangerous or hazardous
objects. The learning of new skills is achieved using object
recognition, which allows robots to interact with objects
using visual perceptions. Most research on object recognition
focuses on ofﬂine learning process, where object appearance
models are provided manually in advance to achieve robust
performance, or are acquired from the environment by plac-
ing markers and tags on the objects [1]. Ofﬂine teaching and
learning is a time-consuming process with additional training
effort for new objects, while the placing of markers and tags
on objects is also a time-consuming and unpractical task.
In this work, we consider an alternative to manually
providing of object models by performing autonomous ac-
quisition of object representations via observation of human
actions. The goal is to equip the robot with an autonomous
object acquisition and recognition module, so that the robot
This work was supported in part by a Grant-in-Aid for Scientiﬁc Research
(C) 23500242. K. Chaudhary, Y . Mae, M. Kojima and T. Arai are with Grad-
uate School of Engineering Science, Osaka University. fkrish, mae,
kojima, araig@arai-lab.sys.es.osaka-u.ac.jp
(a) (b)
(c)
Autonomous Acquisition of Handheld Object Segmentation & Modeling
Object Recognition
Unknown Handheld Object RGB–D Frames Handheld Object Boundary Estimated
zg(t+ 0) zg(t+ 1) zg(t+ 2) z
f
(t+ 3) z
f
(t+ 4) zg(t+ 5) 
g(t+ 0) g(t+ 1) g(t+ 2) g(t+ 3) g(t+ 4) Extracted Models
Fig. 1: Autonomous learning by observation of handheld object. a)
Handheld object non-contiguous to the human body. b) Handheld
object contiguous to the human body. c) SBT sequence diagram
can discover new objects, learn their appearance models au-
tonomously and perform object search and recognition tasks
without supervision. This incremental learning capabilities
would allow more robust performance, since errors can be
corrected gradually over time, as more exemplary generic
appearance models are accumulated in the dynamic envi-
ronment. However, autonomous acquisition of object model
poses difﬁculties in performing precise object segmentation,
since the object boundaries and attributes are unknown.
In this paper, we present a novel method to perform au-
tonomous acquisition of handheld object appearance models
via observation of human-object interaction in an unstruc-
tured environment, as an alternative to manually providing
of object models. More precisely, we solve the problem
of unknown object boundaries and performing object seg-
mentation without previous knowledge of the object being
manipulated. Our approach uses the RGB-D data to segment
the potential target object regions in a non-stationary and
unstructured environment. The unknown object boundary is
estimated when the handheld object is not in connection to
the body space, Fig.1(a). The object is segmented and used to
extract unknown handheld objects that are connected to the
body space, Fig.1(b), by SBT i.e., we build generic handheld
object models from future frames in which objects are not in
connection to the body space and sequentially back–track to
segment handheld object from the former frames, as shown
in Fig.1(c), so that object informations are maximized. The
segmented objects are accumulated as an appearance model,
and are used for future object search and recognition tasks.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4953
The remaining of this paper is organized as follows:
Section II gives the overview of the related works. In Section
III, we discuss the proposed Sequential Back-Tracking algo-
rithm. The acquisition of handheld object models connected
to the human body space and that of non-connected are
discussed in Section IV and V respectively. The experimental
results and discussions are provided in the Section VI,
followed by conclusion in Section VII.
II. RELATED WORKS
Much research has been done in the area of computer
vision for learning object models in controlled environments,
but limited literature focuses on learning models from human
actions in an unstructured environment.
The use of semi-supervised learning [2] reduces the need
to provide a complete labeled training data. Welke et al.
[3] presented a method for autonomous segmentation and
modeling of objects placed in the hand of a ARMAR-III
robot operating in a cluttered environment using RGB-D
data. In their approach, the robot models the object held
in its hands by rotating the object in front of its camera.
The limitation is that the object modeling is performed only
when the object is placed in robots hand. Roth et al. [4]
presented a system for incremental learning through tracking
of an unknown hand held object. They used background
subtraction for object boundary detection and to initialize the
MSER tracker. Background subtraction is suitable for stable
objects and thus not robust in a non-stationary environment.
Incremental learning through human demonstration with-
out any prior knowledge of the object has grown in recent
years. Arsenio [5] used developmental learning on humanoid
robots in natural environment for object learning through
tracking and segmentation. The robot performs incremental
learning of objects demonstrated by a human teacher and
uses geometric hashing representation for storage. [6] and
[7] presents an incremental learning of objects in a cluttered
environment using RGB-D data focusing on object represen-
tation and its applications in object recognition on real time
systems. The object’s to be modeled is presented within the
peripersonal space by a human teacher with the closest visual
region selected as candidate region of the object. However,
the limitation is the intentional demonstration of the objects
to the robot, which requires human effort.
In contrast to the aforementioned research, we attempt
to eliminate the need for intentional demonstration of ob-
jects, developing a module that learns and acquires object
representations autonomously from observation of human
actions. This enables a better adaption to the environment
with gradual reduction of errors over time and also eliminates
the need for a human intervention.
III. SEQUENTIAL BACK–TRACKING ALGORITHM
The segmentation of an unknown handheld object, denoted
g(t) at any timet2f1;:::;Tg, is a challenging task, since the
object boundaries are unknown in prior, with added complex-
ity of induced human motion and integration of environmen-
tal and human body cues. On a 2D image spaceg(t) is either
connected to the body space, denotedz
g
(t) or unconnected,
denoted z
f
(t) as shown in Fig.1(b) and (a) respectively.
The estimation of unknown object boundaries are efﬁciently
achieved in z
f
(t), while in z
g
(t) the separation of g(t)
from the human body cues are difﬁcult, because the depth
information of the human body space and the object space
are approximately same and therefore can not be separated
as in z
f
(t). We solve the problem of segmenting g(t +r)
from z
g
(t+r), where r =f0;1;:::;kg
1
by ﬁrst segmenting
g(t+s) fromz
f
(t+s), where s =f1+k;2+k;:::;p+kg
2
,
since the g(t) boundaries are efﬁciently estimated inz
f
(t).
In other words,z
g
(t+r) andz
f
(t+s) are decomposed into
two separate temporal problems and segmentation is ﬁrst
performed for z
f
(t) followed by z
g
(t) over different time
frame via back–tracking as shown in Fig.1(c).
The acquisition of object appearance model fromz
f
(t+s)
(see Sec.IV) is achieved using the RGB-D data and the 3D
skeleton hand joint position,x
(x;y;z)
. The human-object region
denoted l
f
(t+s), are extracted, skin visual cues subtracted
(Fig.2(b)), the boundaries of g(t +s) are estimated using
kNN and g(t +s) is accumulated as the object appearance
models of object Y
i
(Fig.2(c)). The autonomous acquisition
of g(t +r) from z
g
(t +r) (see Sec.V) is realized using
sequential back-tracking, which is deﬁned as: all acquired
framesz
g
(t+r) with unknown object boundaries are solved
for g(t+r) using g(t+s) acquired fromz
f
(t+s) via back–
projection for estimation of unknown object boundaries in
z
g
(t+r) of objectY
i
. The back–tracking is achieved via ex-
ploitation of afﬁne relationship between consecutive frames
and adequate handheld object segmentations are performed.
The sequence diagram of SBT is shown in Fig.2.
In other words, g(t +s), are extracted from l
f
(t +s)
and accumulated as an appearance model of object Y
i
at
t+8s. The feature descriptors are computed from the ac-
quired object appearance models and clustered to Bag of
visual Word (BOW) model [8], which is trained using SVM
(Fig.2(d)). Next, l
g
(t +r) are extracted from z
g
(t +8r)
and categorized. In this work, we use an assumption,
¯
A
that consecutive frames, i.e j
t+1
and j
t
are closely related
by afﬁne transformation for same object Y
i
. Therefore the
distinction between z
f
(t+(1+k)) and z
g
(t+k) are rela-
tively small. Based on
¯
A we used g(t+s) for estimating the
optimal boundary ofg(t+r) inl
g
(t+r) as shown in Fig.2(f).
The estimated boundary is used for extracting g(t+r) using
graph-cut segmentation, and the accumulated models are
used to perform incremental learning as shown in (Fig.2(g)).
IV. GENERIC MODEL EXTRACTION FROMz
f
(t+s)
In this section, we describe the method for boundary
estimation and segmentation of unknown handheld object,
g(t+s), when the object is not in connection to the human
body space. The acquired object appearance models g(t+s)
are then used in estimating the boundaries of g(t+r) from
1
k is the total number of frame ofz
g
(t), with unknown object boundaries
2
p is the total frame ofz
f
(t), and p + k = n is the total number of frame
in the human action observation sequence.
4954
z
g
(t+r) &z
f
(t+s) acquisition from Kinect z
f
(t+s) solved for handheld object Handheld object boundary estimation Object model feature extraction
RGB-D data and skeleton joints Human object region Model acquisition Training using SVM
Bag of Word
clustering
SURF Features
Extraction of human object
region fromz
g
(t+r)
First
(a) (b) (c) (d)
Handheld object region is unknown Object extraction in queued frames Accumulation of segmented models Search using acquired models
Queue unknown frames SBT using future model Modeling and learning Object Recognition
Estimation of
Boundary
g(t+s)
Application
(e) (f) (g) (h)
Fig. 2: Flow diagram of the Sequential Back-Tracking Method. a) Acquisition of RGB–D frame from Kinect with unknown objects. b)
Extraction of human-object regions, from sequentially acquired frames. Objects connected to the body are queued (e) and solved for object
boundaries after models are segmented from objects not connected to body space. c) Unknown boundary estimation and object appearance
models segmentation. d) Feature extraction and training of all acquired models. f) Estimation of object boundaries in the unknown frames
in (e) using SBT. g) Accumulation and learning of segmented object appearance models. h) Object recognition using the acquired models.
z
g
(t+r) for a complete sequence of Y
i
via back-tracking,
such that no preceding knowledge of the object is lost.
A. Detection and Extraction of Human–Object Region
The skeleton tracking capabilities of Kinect sensor are
used to detect the presence of a human in the environment.
Skeleton information in Kinect SDK is described by the
length of the links and the joint angles. Speciﬁcally, it
provides the three dimensional Euclidean coordinates, (x, y,
z dimensions) and the orientation matrix of each joint with
respect to the sensor.
The depth data refers to the distance of discrete pixels
from the sensor, and is used for extracting the human-object,
V
f
(t+s), region from the depth space, which is an efﬁcient
method of separating the cluttered environment, compared to
RGB data where the color distribution has to be known. The
extractedV
f
(t+s) is equivalently mapped to RGB space, and
the corresponding human–object region, l
f
(t +s) in RGB
space is extracted. The skin visual cues q
ij
are subtracted
from l
f
(t +s) region and equivalently from V
f
(t +s), as
shown in Fig.2(b).
r
f
(t+s)
depth
=V
f
(t+s) q
ij
(1)
In this work, the skin color subtraction was performed using
the Skin Probability Maps described in [9].
B. Estimation of Handheld Object Boundary using kNN
The nearest neighbor classiﬁer is one of the simplest
methods of performing general, non-parametric classiﬁcation
[10]. We use kNN classiﬁcation algorithm to estimate the
optimal boundary on the depth data, r
f
(t+s), belonging to
theg(t+s). ThekNN classiﬁer simply looks at thek points in
the training set,P that are nearest to the test pointx
(x;y)
,(only
known variable in RGB-D space) and counts the number of
each class in the set and returns a mathematical estimate of
x
(x;y)
as measured by the distance metric.
p(v =cjx
(x;y)
;P;k)=k
 1
å
ieN
(x
(x;y)
;P)
G(v
i
=c) (2)
where v identiﬁes the class c, c2f1;:::;Gg, to which x
(x;y)
belongs. N are k nearest points to x
(x;y)
inP.G is a boolean
indicator function.
g(t +s) is identiﬁed to the correct contour in r
f
(t +s)
belonging to the hand-object region using kNN. The training
set vector is represented as:
P
i
2<(x
1
;y
1
);(x
2
;y
2
);:::(x
n
;y
n
)>
where (x
r
;y
r
) denotes the rth contour coordinate in the
image domain. k is experimentally evaluated by choosing
different values of k on randomly selected frames based on
x
(z)
, and angular human variation for possible human poses.
k = 3 with lowest error rate was used in the experiment.
C. Feature Extraction and Learning
The features are extracted using the Speed Up Robust
Feature (SURF) [11] local feature descriptors. The SURF
descriptor is preferred over other descriptors due to its
concise descriptor length, scale invariance and robustness
to rotation. The SURF descriptor deﬁnes a 128-dimensional
feature vector for each interest point in the image. The
feature vectors of g(t +s) and l
g
(t +r) are computed by
SURF and used for estimating the unknown hand–object
boundaries in l
g
(t+r), as shown in Fig.2(f).
The extracted local features, ¯ h(
~
i; j), from the images are
vector quantized into a BOW model [8], so that each image is
represented using a ﬁxed length feature vector. In this work,
we built a dictionary size of 150 clusters, such that each
image is represented by a feature vector, F(
~
i; j) of length
[150 x 1]. The extracted feature vectors, F(
~
i; j) are trained
using SVM [12], [13]. The Radial Basis Function (RBF), in
4955
(3) was chosen as a kernel function of SVM based on the
good optimized results compared to other kernel functions
(only results of RBF are shown due to space limitation).
K(x;x
0
)= exp( 
kx x
0
k
2
2s
2
) (3)
where s is the degree of generalization applied to the
training set. As the incremental learning of the object models
increases, the s is reduced. Next, all g(t+s) are categorized
and labeled. Labeling in this work is done numerically and
is incremented for every new object not in the database.
V. OBJECT MODEL SEGMENTATION FROMz
g
(t+r)
The human–object region, l
g
(t +r) are extracted from
z
g
(t +r) and used for estimating the unknown boundary
of g(t+r) connected to the human body space.
A. Estimation of Unknown Object Boundary in l
g
(t+r)
After classiﬁcation, the corresponding boundary, y
i
of
g(t +r) in l
g
(t +r) is determined using Random Sample
Consensus (RANSAC) algorithm [14]. RANSAC algorithm
is used to exclude outliers from a large data set and randomly
select the best inliers of model g(t +s) to l
g
(t +r). Note,
we deﬁne a square search space of length ¯ c (4) on l
g
(t+r)
centered at x
(x;y)
circumscribing hand–object neighborhood
using knowledge of g(t+s) for better boundary estimation.
¯ c = 2max(l;w) (4)
where l is the length of g(t+s) and w is width of g(t+s).
Based on assumption
¯
A, the region of g(t +r) in l
g
(t +r)
corresponding to the group of good match inliers is derived
using perspective transformation, and the respective object
model, g(t+r) is segmented.
However, the extracted model may be integrated with
visual cues of human body region or the estimatedy
i
maybe
larger than the optimal object boundary, as shown in Fig.3(a).
Such integrations increases the misclassiﬁcation rates when
used for recognition, therefore are eliminated. The difﬁculty
lies in the identiﬁcation of the true object and the true
integrated cue region for multi-colored objects that are to
be separated. To solve this, we treat the segmented model
as an energy minimization problem [15], which is efﬁciently
solved using graphcut segmentation approach.
B. Learning Object Color Model and Integrated Cues
The back–tracking method exploits the afﬁne relationships
between the frames based on assumption
¯
A, therefore the
color distribution ofg(t+s) consisting of pixels a
f
(i;j)
can be
characterized by a Gaussian Mixture Model (GMM) (5).
p(xjl)=
M
å
i=1
w
i
g(xjm
i
;å
i
) (5)
where x is the feature vector, M is the number of Gaussian
components, w
i
is the weight component, m
i
and å
i
are mean
and covariance matrix respectively. The foreground G
i
= 1,
and the backgroundL
i
= 0, color models are built ing(t+r)
(a) (b) (c) (d)
Fig. 3: a) Segmentation using initial estimated boundary. b) Ob-
ject color model construction using GMM. c) Optimal boundary
estimation. d) Segmentation using optimal boundary
by learning the RGB color distribution of a
f
(i;j)
in g(t +s)
using the GMM, shown in Fig.3(b). The learning process is
performed once for every new object and achieved using an
Expectation Maximization algorithm [16]. Since the majority
of pixels, a
f
(i;j)
ing(t+r) belongs to the handheld object, the
classiﬁcation ofG
i
andL
i
is based on the assumption that the
larger vector contains the handheld object model distribution.
However, sudden changes in illumination in the uncontrolled
environment may affect the RGB color distribution ing(t+s)
and l
g
(t+r), which may result in false clustering of a
f
(i;j)
to G
i
or L
i
.
Consequently, when the segmented boundary y
i
is larger
than the optimal object boundaryn
i
with the integrated cues,
incorrect segmentation occurs due to erroneous minimiza-
tion. To solve the aforementioned problem, we used SURF
to abetment graphcut in eliminating additional cues while
maintaining g(t+r) boundaries. Basically, SURF is used to
redeﬁne the y
i
when the initial y
i
is larger than the n
i
, such
that erroneous cues are minimized as shown in Fig.3(c).
C. Segmentated Object Model Evaluation
Features ¯ h
f
(
~
i; j) ofg(t+s) are computed and matched ﬁrst
with features ¯ h
G
(
~
i; j) of G
i
. This is because our assumption
is that G
i
contains the object model. The evaluation of the
feature match between the 2 images is deﬁned as a ratio,
~
W:
~
W
i
=
k
i
miss
k
i
match
(6)
where k
miss
is the features with different labels in the 2
images and k
match
is the matched features in the 2 images.
If
~
W is greater than the threshold, t than the object model is
in L
i
, hence the features ¯ h
f
(
~
i; j) are matched to features of
L
i
, ¯ h
L
(
~
i; j) and the new boundary is estimated, the object
segmented and the method is repeated until the optimal
object boundary is achieved, i.e when
~
W
i 1
=
~
W
i
. The object
model is segmented using the computed n
i
, see Fig.3(d).
VI. RESULTS
A. Setup and Initialization
The experiment was performed using Microsoft Kinect
sensor for data acquisition; it outputs RGB-D frame of 640
x 480 pixels at 30Hz with a workable range of 1-3m. The
experimental data was acquired in 2 rooms with different
structure and illumination intensities as shown in Fig.6. Note
that none of the objects were known to the system in prior.
4956
0.92
   
0.01
0.01
0.29
   
   
0.04
0.89
0.16
0.14
   
0.63
0.06
   
   
0.81
0.05
0.10
   
0.03
0.04
0.01
   
0.79
0.02
   
0.12
   
0.01
   
   
0.52
   
0.10
   
0.09
   
0.01
   
0.37
   
   
   
0.02
   
0.07
   
0.69
Object 1
Object 2
Object 3
Object 4
Object 5
Object 6
Object 7
Object 1
Object 2
Object 3
Object 4
Object 5
Object 6
Object 7
0.10
   
0.03
0.02
0.03
   
0.02
   
0.27
   
   
0.01
0.24
   
0.06
0.08
0.31
0.03
0.03
0.13
   
   
   
0.20
0.54
   
   
0.07
0.23
0.16
0.03
0.04
0.31
   
0.02
   
   
   
   
   
0.04
   
0.61
0.49
0.42
0.37
0.62
0.59
0.89
Object 1
Object 2
Object 3
Object 4
Object 5
Object 6
Object 7
Object 1
Object 2
Object 3
Object 4
Object 5
Object 6
Object 7
0.78
0.15
0.21
0.32
   
0.83
   
   
0.05
0.02
0.63
0.02
0.18
   
0.16
0.67
Object 1
Object 3
Object 4
Object 8
Object 1
Object 3
Object 4
Object 8
0.35
0.05
   
0.07
0.12
0.47
   
0.04
   
   
0.69
0.08
0.53
0.48
0.31
0.81
Object 1
Object 3
Object 4
Object 8
Object 1
Object 3
Object 4
Object 8
(a) Room 1 light on (b) Room 1 light off (c) Room 2 light on (d) Room 2 light off
Fig. 4: Leave-one-out cross-validation confusion matrix of the handheld objects acquired using sequential back-tracking from: (a) Lab
room 1 with lights on. (b) Lab room 2 with the illumination intensity of approx. 40%. (c) Lab room 2 with lights on and (d) Lab room
2 with illumination of approx. 60%. All acquired objects representations shown in Fig.5 are labeled numerically.
Fig. 5: Object appearance model samples acquired using sequential back-tracking. These are the 8 objects used in the experimentation
Comic book Dryer box Cereal box Snack packet Can Cookie box Fruit Juice Frying P box
Object 1 Object 2 Object 3 Object 4 Object 5 Object 6 Object 7 Object 8
Fig. 6: Sample frames showing the environmental structure where
the experiment was performed.
B. Evaluation of Acquired Objects Models
First the experiment was performed with all lights turned
on, and then we turned some lights off so that the gener-
ality of the implemented system could be shown. During
experimentation, the segmented models of size greater then
30 x 30 pixels (condition a) were acquired, since it contains
distinctive features for good modeling performance. Samples
of selected models accumulated via SBT are shown in Fig.5.
The acquired models were used for validation. However,
during autonomous acquisition of the handheld object, some
acquired models satisﬁed condition a, but were results of
partial occlusion or incorrectly calculated object bound-
ary therefore acquiring truncated object appearance models.
These models create ambiguity among other object models,
signiﬁcantly degrading performance. For the purpose of this
work, model selection was done basically by selecting mod-
els with features greater than threshold q
i
for each object.
q
m
=
max(8
p
(¯ h
p
(
~
i; j)))
2
(7)
where m is the number of different objects and p is the
number of acquired models of the same label m.
The validation using a confusion matrix constructed from
leave-one-out cross validation of the acquired generic hand-
held object appearance models is shown in Fig.4. Note
in this section for validation of acquired object models,
the features were computed using SURF and RGB color
1 1.5 2 2.5 3
0
20
40
60
80
100
Room 1
Distance from Kinect (meters)
True Positive Rate (%)
 
 
Object 1
Object 2
Object 3
Object 4
Object 7
1 1.5 2 2.5 3
0
20
40
60
80
100
Room 2
Distance from Kinect (meters)
True Positive Rate (%)
 
 
Object 1
Object 3
Object 4
Object 8
Fig. 7: Object recognition performance using the acquired handheld
object models with lights-on in: left) Room 1 where 5 objects were
used, right) Room 2 where 4 objects were used.
histogram descriptors. Matrix (a) in room 1 with the light on,
the large misclassiﬁcation of object 6 is evident due to few
distinguishable features. Other models sustains distinctive
features, making exemplary object representations. Matrix
(b) with reduced illumination in room 1 the misclassiﬁcation
among objects increased signiﬁcantly. The partial reﬂective
material of object 4 reﬂects light making it distinctive from
other objects while, object 7 in reduced illumination is more
distinctive from other objects due to its color composition.
Matrix (c) with light on in room 2, the ambiguity between
object 1 and 8 is evident as both objects have similar
color and texture characteristics. Matrix (d) with reduced
illumination in room 2, object 8 was more distinctive while
misclassiﬁcation of objects 1, 2 increased similar to room 1
C. Object Search and Recognition
The acquired models were used to perform recognition
of the objects in the cluttered environment. We evaluated
the success rate at different distance of the object from the
sensor as shown in Fig.7. Objects 5 and 6 was not used in
recognition, as the success rate was very low due to small ob-
ject size and very few distinctive features as distance varied.
As shown in Fig.7, the success rate in both rooms degrades
4957
0 10 20 30 40
0
20
40
60
80
100
Number of Acquired Model
Error Rate (%)
Room 1
 
 
Object 1
Object 2
Object 3
Object 4
0 10 20 30 40
0
20
40
60
80
100
Number of Acquired Model
Error Rate(%)
Room 2
 
 
Object 1
Object 3
Object 4
Object 8
Fig. 8: Incremental learning error rate of ﬁrst 40 models of each
objects acquired using Sequential Back–Tracking in 2 rooms.
signiﬁcantly as the distance from the sensor increases. This
results as increase in distance increases ambiguity between
the true object region and the environmental clutters, since
the object size decreases, hence increase in misclassiﬁca-
tion rate. Note that at distance of 2–3m, experiment was
performed on a less cluttered environment. Fig.8 shows the
result of errors as model acquisition increases. Objects with
high feature attain signiﬁcant drop in error rate while smaller
objects had gradual but slow decrease in error.
D. Discussion
Overall, the performance of SBT was better when the
lights were turned on with average success of 73% in
correctly estimating the unknown object boundaries without
truncation for each handheld object.
In this work, the human action based object modeling
was performed for a one individual in the environment with
only one handheld object. We also made an assumption that
the object and the human clothing color distribution are not
same. In case of similar color distribution, the object cannot
be separated from the background and therefore the optimal
object boundary cannot be determined. This is because the
addition cues are considered as part of the object and hence
the energy minimization function fails to correctly identify
the object boundary from the integrated cue’s.
Moreover, in the current implementation we also placed a
constrain of condition a. As part of the future prospect, we
plan to reduce the parameter of condition a, so that smaller
household objects such as cups, could be accumulated as
from human actions at variable distances. Furthermore, we
plan to extend autonomous acquisition module of SBT to
acquire appearance models from multiple persons as well as
accumulation of multiple objects in human hands.
VII. CONCLUSION
In this paper, we presented a novel method for autonomous
acquisition of object appearance models by observation of
handheld objects in an unstructured environment for object
recognition. We considered the problem of ofﬂine training
for object recognition that is not only time consuming but
also models have to be provided manually. In our approach,
we focus on performing incremental learning of objects
autonomously over time. The problem of unknown object
boundaries was solved using kNN for objects not connect
to the human body space, while objects connected to the
human body space was solved using proposed sequential
back–tracking. We performed experiments using unknown
objects in 2 different rooms with different structure and
illumination conditions. The acquisition rates were higher
with the lights turned on and for objects with rich textures.
On average SBT successful estimated 73% of unknown
boundaries without truncation. The acquired models were
used for object recognition in cluttered environments; at
closer distances, the recognition rates were higher, while
at increased distances, the environment clutters affected the
recognition rate signiﬁcantly. Finally we believe that the pro-
posed system is a powerful tool for autonomous learning of
object models by robots for search and recognition purposes,
since no previous knowledge about the object is required.
REFERENCES
[1] J. Wu, A. Osuntogun, T. Choudhury, M. Philipose, and J. Rehg,
“A scalable approach to activity recognition based on object use,”
in Computer Vision, 2007. ICCV 2007. IEEE 11th International
Conference on, 2007, pp. 1–8.
[2] C. Penaloza, Y . Mae, K. Ohara, T. Takubo, and T. Arai, “Generic object
classiﬁers based on real image selection from the web,” in Pattern
Recognition (ACPR), 2011 First Asian Conference on, 2011, pp. 239–
243.
[3] K. Welke, J. Issac, D. Schiebener, T. Asfour, and R. Dillmann, “Au-
tonomous acquisition of visual multi-view object representations for
object recognition on a humanoid robot,” in Robotics and Automation
(ICRA), 2010 IEEE International Conference on, 2010, pp. 2012–
2019.
[4] P. M. Roth, M. Donoser, and H. Bischof, “On-line learning of unknown
hand held objects via tracking,” in Int. Conf. on Computer Vision
Systems, 2006.
[5] A. Arsenic, “Developmental learning on a humanoid robot,” in Neural
Networks, 2004. Proceedings. 2004 IEEE International Joint Confer-
ence on, vol. 4, July 2004, pp. 3167–3172 vol.4.
[6] H. Wersing, S. Kirstein, M. Gtting, H. Brandl, M. Dunn, I. Mikhailova,
C. Goerick, J. J. Steil, H. Ritter, and E. Krner, “Online learning of
objects in a biologically motivated visual architecture.” Int. J. Neural
Syst., vol. 17, no. 4, pp. 219–230, 2007.
[7] C. Goerick, I. Mikhailova, H. Wersing, and S. Kirstein, “Biologically
motivated visual behaviors for humanoids: Learning to interact and
learning in interaction,” in Proceedings of the IEEE/RSJ International
Conference on Humanoid Robots (Humanoids 2006), Genoa, Italy.
IEEE/RSJ, 2006.
[8] G. Csurka, C. R. Dance, L. Fan, J. Willamowski, and C. Bray, “Visual
categorization with bags of keypoints,” in In Workshop on Statistical
Learning in Computer Vision, ECCV, 2004, pp. 1–22.
[9] G. Gomez and E. F. Morales, “Automatic feature construction and a
simple rule induction algorithm for skin detection,” in In Proc. of the
ICML Workshop on Machine Learning in Computer Vision, 2002, pp.
31–38.
[10] K. P. Murphy, Machine Learning: A Probabilistic Perspective. Mit
Press, 2012.
[11] H. Bay, T. Tuytelaars, and L. V . Gool, “Surf: Speeded up robust
features,” in In ECCV, 2006, pp. 404–417.
[12] O. Chapelle, P. Haffner, and V . Vapnik, “Support vector machines
for histogram-based image classiﬁcation,” Neural Networks, IEEE
Transactions on, vol. 10, no. 5, pp. 1055–1064, 1999.
[13] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector
machines,” ACM Transactions on Intelligent Systems and Technology,
vol. 2, pp. 27:1–27:27, 2011.
[14] M. A. Fischler and R. C. Bolles, “Random sample consensus: a
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395,
June 1981.
[15] Y . Boykov and M.-P. Jolly, “Interactive graph cuts for optimal bound-
ary amp; region segmentation of objects in n-d images,” in Computer
Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International
Conference on, vol. 1, 2001, pp. 105–112 vol.1.
[16] T. Moon, “The expectation-maximization algorithm,” Signal Process-
ing Magazine, IEEE, vol. 13, no. 6, pp. 47–60, 1996.
4958
