Important: -You might have to include '-Xmx2048m' as a 
		   VM argument to insure that there is enough
		   heap memory space.
		   -the entire process takes a bit of time to 
		   run for all provided documents. the results 
		   of running with all provided documents can be found
		   in this directory. See the 'fileComparisons.txt'
		   file for a print out of the final similarity matrix
		   
Uses: itextpdf jar files: itext-pdfa-5.5.4-javadoc.jar, 
						  itext-pdfa-5.5.4-sources.jar, 
						  itext-pdfa-5.5.4.jar, 
						  itext-xtra-5.5.4-javadoc.jar, 
						  itext-xtra-5.5.4-sources.jar, 
						  itext-xtra-5.5.4.jar,
						  itext-pdf-5.5.4-sources.jar
						  itext-pdf-5.5.4.jar,
						  itextpdf-5.5.4-javadoc.jar
						  
Assumes: -all pdf files are in the "inputPdfs" folder and are all
         named with the convention number.pdf
         -the 'highest' number file is 2500.pdf (note that this
         can be changed with the maxNumOfInputFiles variable
         -Only words of length greater than 2 that are in the 
         English dictionary are considered for similarity 
         comparisons. I couldn't find the Google Unigram Corpus
         mentioned in the assignment parameters, so I created
         my own English dictionary instead to check the validity
         of each word.

To Run: No arguments needed. Running the main method in 
		PDFTextSimilarity.java produces two output files for
		each pdf: 1) a text file containing all of the words
		in the pdf and 2) a text file containing all of the 
		word frequency counts for all words in the english
		dictionary with length greater than 2. Additionally 
		outputs a file called 'fileComparisons.txt' that
		includes the similarity matrix. 
		
Testing: PDFTextSimiliarityTest.java contains unit tests
		 for all methods in PDFTextSimilarity.java
		 
Runtime: 1) createEnglishDictionary: runs through every single
			word once, so O(n) time where n is the number
			of words in the English dictionary
		 2) converting the input PDF files to text files: O(n)
		 	where n is the number of files
		 3) finding the frequency counts for each file: runs through
		 	each file. For each file with n words, it makes a single pass over
		 	every word--O(n)--adding each word to the hashmap for that file.
		 	It then iterates through the hashmap and prints it to
		 	standard out--O(n). For m files, this whole process
		 	is O(mn).
		 4) Finding the similarity between two documents: the code
		    iterates through the words keyset of one of the two documents
		    and checks how many of the words are in the words keyset
		    of the second document. If the file contains n words, this
		    runs in O(n) time.
		 5) Finding the similarity between all documents: For a total of 
		 	m files, there are m^2/2 comparisons (the upper right
		 	half of the matrix) to make which can be approximated as
		 	m^2. As stated in step 4, each comparison is O(n). Thus,
		 	the total runtime is O(nm^2).
		 6) Total runtime: The most expensive step is comparing all
		 	the documents to create the similarity matrix (step 5).
		 	Thus, the total runtime is O(nm^2)
		 	
Considerations: -I thought of sorting the word frequency mappings
  				 so that the most frequent words appear as the first
  				 entries in the maps. However, the assignment simply
  				 asked for similarity comparisons based on the number
  				 of the same words, so this would have been unnecesarry
  				 and added another runtime cost. That being said, documents'
  				 similarity could be better approximated by considering
  				 the proportional frequency for words rather than just
  				 the existence of a word in a document.
  				-In Nets150, we coded tf-idf matrices to comapre documents.
  				 This would be a fun extension of this assignment.
         