Calibration Method for Multiple 2D LIDARs System
Mengwen He

, Huijing Zhao

, Jinshi Cui

, Hongbin Zha

Abstract— Many robotic and mobile mapping systems have
been developed using multiple 2D LIDARs (brieﬂy multi-
LIDAR system) to sense environment. In such systems, extrinsic
calibration of all LIDARs is essential for making collaborative
use of the data from different sensors. This research aims at
developing a calibration method for multi-LIDAR systems at
the general scene, such as an outdoor place or an underground
parking-lot, without modiﬁcation to environment by putting
calibration targets. In this paper, the calibration method is
proposed by aligning the 3D data of different LIDARs. They
are concerned at two-levels: 1) reference calibration, i.e. ﬁnding
the transformation from a reference LIDAR to the platform
frame; 2) multi-LIDAR calibration, i.e. ﬁnding the LIDARs’
relative geometries by referring to the reference one. The
method is examined in calibrating the multiple 2D LIDARs on
an intelligent vehicle platform POSS-V , where the data collected
through a driving in an underground parking-lot are registered
to ﬁnd sensors’ geometry. Calibration accuracy is examined by
comparing with a CAD model of the scene, which was measured
by using a total station.
I. INTRODUCTION
Nowadays, 2D LIDAR sensors, such as SICK LMS,
Hokuyo URG and UTM, have been widely exploited in
robotics and mobile mapping platforms for 3D modeling
[1], [2], SLAM (Simultaneous Localization and Mapping)[3],
object detection [4], [5], [6], autonomous navagation [7],
[8] etc. As a 2D LIDAR sensor proﬁles at the environment
on only a certain plane, in order to acquire more complete
knowledge to the robot’s surroundings, some sensing systems
[9], [10] have been developed using a number of 2D LIDARs
that possess much different viewpoints and scanning planes
(referred to as the multi-LIDAR system). In such systems,
LIDAR sensors need to be well calibrated so as to integrate
the data of different sensors to a global coordinate system,
and make collaborative use of them.
Calibration of multi-LIDAR system (more speciﬁcally,
extrinsic calibration) is often poorly documented. Among
the documented works, there are generally two types of
calibration: 1) calibrating a LIDAR to a reference frame
deﬁned by such as a camera [11], [12], [13] or the ego-
vehicles odometry center [14] or 2) ﬁnding the relative ge-
ometries among different LIDARs [15], [16]. These calibra-
tion methods usually require some modiﬁcation of the scene
by introducing landmarks that are visible (or detectable) by
LIDARs, such as speciﬁcally placed reﬂective targets [15],
poles [17], checker boards [18], which introduce laborious

M. He, H. Zhao, J. Cui and H. Zhao are with the State Key Lab of
Machine Perception (MOE), School of EECS, Peking University, Beijing,
P.R. China alexanderhmw at pku.edu.cn
This work is partially supported by the Hi-Tech Research and De-
velopment Program of China [2012AA011801], the NSFC-ANR Grants
[61161130528] , and the NSFC Grants [91120010].
Fig. 1. The POSS-V platform that is used to study the calibration of a
multiple 2D LIDAR system.
ﬁeld work, or rely on special facilities [16]. Furthermore,
some methods make use of the visible laser [19] or require
that the LIDAR can measure reﬂectivity [15], which put
strong restriction to the adaptiveness of the methods on a
broad range of systems. Calibration of a 3D LIDAR system
that possesses multiple laser beams has also been studied
[16], [20]. As their methods put some assumptions to the
scene, such as contiguous surfaces, further elaborations are
required to adapt the methods to a more general scene. In a
multi-LIDAR system, the scanning planes and viewpoints
of 2D LIDARs could be greatly different, yielding much
occlusions between the data of different sensors, and making
it a big challenge in generating data correspondences. Thus
issues are not well studied in literature.
This research studies the calibration method of a multi-
LIDAR system, where a number of 2D LIDARs are rigidly
mounted on a robotic platform at different locations and
directions. The experimental platform used in this research
is shown in Fig.1. However the method developed is not
restricted to such a system, as a more general purpose method
is aimed. A two-level calibration method is concerned, which
ﬁnd 1) the transformation from a reference LIDAR to the
platform frame, which is usually deﬁned by the localization
module, and 2) the geometric relationships between the
reference LIDAR and other LIDARs. We aim at calibration
in a general urban environment, such as an outdoor place or
a parking-lot, without any modiﬁcation of the environment
nor requiring special facilities. In the authors previous work
[21], a pairwise calibration method that ﬁnds the relative
geometry between two LIDARs was proposed. This paper is
an extension of the previous work, where novel contributions
are 1) a method that calibrating a reference LIDAR to the
platform, and 2) a method that simultaneously calibrating
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3034
Fig. 2. The underground parking lot in our experiment. The yellow and
green lines are the designed V-shape pathes.
multi-LIDAR with the help of a well-calibrated reference
LIDAR in global level.
An experiment was conducted in an underground parking-
lot. whereL4 in Fig.1 is selected as the reference one,L2 and
L3 are calibrated with L4 to achieve a more complete and
consistent LIDAR sensing to the environment. LIDAR data
are collected by driving the platform along a V-shape path (as
shown in Fig.2), which is theoretically proved in section 3,
and demonstrated efﬁcient in experiment. Geometric features
(e.g. the planes in parking lot scene) are extracted from the
3D point-clouds of each LIDAR, and are aligned to ﬁnd a
geometric transformation from the reference LIDAR (i.e.L4
in Fig.1) to the platform frame, and from the other LIDARs
(i.e.L2,L3 in Fig.1) to the reference one. The underground
parking lot was measured using a total station to ﬁnd a 3D
model of the environment as the ground truth, which are
used to evaluate the calibration accuracy by comparing the
integrated LIDAR data with the 3D model. We drove the
platform to collect data for test along a C-curve path at the
corner of the parking-lot (Fig.6). The C-curve is different
with the V-shape, because it has no straight path, and the
straight path is a requirement for our calibration method.
So using the data collected on the C-curve path to test our
calibration result obtained on the V-shape path is complete
and reliable.
Below we give an overview to the calibration method
in section 2, present the methods of the reference LIDAR
calibration and the multiple LIDARs calibration in section
3 and 4 respectively, and address the experimental results at
section 5, followed by conclusion and future work at section
6.
II. CALIBRATION METHOD OVERVIEW
The calibration method for multi-LIDAR system is to
calculate each LIDAR’s extrinsic calibration parameters,
which contain the LIDAR’s rotation and translation in the
platform’s coordinate. In this paper, we notate the LIDAR’s
calibration parameters as a transformation matrix M
L
, then
notate the rotation term as matrix R
L
and the translation
term as vector T
L
. The rotation matrix in this paper is
parameterized using Euler angles.
With the localization data of the platform, we can generate
the 3D world point-cloudfp
i
g of the scanned area from
the 2D local point-datafq
i
g collected by the LIDAR. If
we notate the localization data as a series of transformation
matricesfM
V
i
g corresponds to the 2D point-datafq
i
g, then
the generation of 3D point-cloud is as Eq.1.
p
i
=M
V
i
M
L
q
i
(1)
But in reality, we always use the estimated LIDAR’s
calibration parameters to generate the 3D point-cloud. In
this paper, we notate the LIDAR’s estimated calibration
parameters as the estimated matrix
^
M
L
of M
L
, then no-
tate the estimated rotation term as
^
R
L
and the estimated
translation term as
^
T
L
. The estimated LIDAR’s calibration
parameters must contain calibration error, which will deviate
the true 3D point-cloud fp
i
g to the estimated 3D point-
cloudf^ p
i
g (Eq.2). The calibration error is regarded as a
error transformation M
e
to connect
^
M
L
and M
L
as Eq.3,
then notate its rotation term as R
e
and translation term as
T
e
.
^ p
i
=M
V
i

^
M
L
q
i
(2)
^
M
L
=M
e
M
L
(3)
In our research, we calibrate the LIDARs by minimizing
its calibration error (making M
e
! I), which means the
estimated calibration parameters
^
M
L
!M
L
, as well as the
alignment betweenf^ p
i
g andfp
i
g. But to directly minimize
the unknown calibration error is difﬁcult, so our strategy is to
minimize a strongly related objective function, which is the
alignment distance between the generated 3D point-clouds,
by tuning the estimated calibration parameters
^
M
L
.
Our calibration method for multi-LIDAR system is divided
into two sequential stages: reference and multiple LIDARs
calibration. The reference calibration chooses one LIDAR as
reference for other LIDAR’s calibration and then calibrates
it to the platform frame. After reference calibration, the mul-
tiple LIDARs calibration calibrates the other target LIDARs
with the help of the reference. Finally all the LIDARs are
properly calibrated to the platform frame. These two stages
are almost same except the composition of the optimization
formula for minimizing the alignment distance between 3D
point-clouds (will be discussed in section 3 and 4).
In this research, we refer to the author’s previous work [21]
and use the distance between multi-type geometric features
to represent the alignment distance between 3D point-clouds.
The geometric feature G is deﬁned as Eq.4, wherefp
k
g is
the feature’s 3D points; ;f

are the geometric type and
formula (in this paper, we only use plane features, which is
the only available features in the underground parking-lot);
f;
2
g is the feature’s reliability measurement. The distance
D
G
(G
1
;G
2
) between the features G
1
and G
2
is deﬁned as
Eq.5.
3035
G =ffp
k
g; = plane;f

;f;
2
gg (4)
D
G
(G
1
;G
2
) =
X
i
(f

1
(p
i
2
) 
1
)
2

2
1
(5)
The minimized distance between two sets of geometric
feature is obtained by tuning estimated calibration param-
eters
^
M
L
, which will regenerate the 3D point-cloud P as
well as its geometric featuresfG
j
g. For convenience we
deﬁne a function for the geometric feature regeneration as
Eq.6, a sum of geometric feature distances between two 3D
point-clouds P
1
and P
2
as Eq.7, and an expression of the
relationship between a regenerated 3D point-cloudP and its
new geometric featuresfG
j
g caused by tuning the estimated
calibration parameters
^
M
L
as Eq.8. In [21], the author had
demonstrated the accuracy and robustness of the alignment
of geometry features in 3D point-clouds.
G(
^
M
L
) =ffp
k
g(
^
M
L
);;f

(
^
M
L
);f;
2
g(
^
M
L
)g (6)
D
P
(P
1
;P
2
) =
X
j
D
G
(G
j
1
;G
j
2
) (7)
P (
^
M
L
))fG
j
(
^
M
L
)g (8)
III. REFERENCE LIDAR CALIBRATION
The reference LIDAR calibration stage is to choose a
LIDAR as a reference for other LIDARs’ calibration and
then calibrate it to the platform frame. In our research,
we calibrate the reference LIDAR without requiring any
measured calibration targets, but only by aligning 3D point-
clouds generated by the reference LIDAR.
A. Reference LIDAR Calibration Method
If we drive the platform in two different directions to
collect two 3D point-clouds of a scene, say P
1
and P
2
,
by using a reference LIDAR in the same conﬁguration.
The existence of calibration error will make these two 3D
point-clouds misaligned. Thus our strategy for the reference
LIDAR calibration is to align these two 3D point-clouds to
eliminate the calibration error. So the optimization formula
for the alignment ofP
1
andP
2
using their geometric features
is deﬁned as Eq.9.
^
M

L
= arg min
^
M
L
D
P
(P
1
(
^
M
L
);P
2
(
^
M
L
))
= arg min
^
M
L
P
j
D
G
(G
j
1
(
^
M
L
);G
j
2
(
^
M
L
))
(9)
In our experiment, we use 2D SLAM for localization. In
order to guarantee the localizations are in the same frame, we
continuously drove the platform along a V-shape path (Fig.2).
In generated 3D point-clouds P
1
and P
2
, we manually
extracted the matched plane features for alignment. Then
we minimize the objective function deﬁned in Eq.9 for the
reference LIDAR calibration (the detail of the optimization
process refers to [21]). The tuned variables are 3 rotation
angles and 3 translation values of estimated calibration
parameters. Finally after alignment, if there is no mirror
reﬂection occurs, then the calibration error is eliminated
in theory. But we only use 2D SLAM for localization,
so the calibration translation error on Z-axis cannot be
eliminated, we just used a measured value for instead. This
only inﬂuences the absolute height of LIDARs in the ﬁnal
calibration results but the relative height can still be hold.
B. Reference LIDAR Calibration Theory
The author’s previous work [21] has demonstrated that if
the platform drives straightly, then the calibration error M
e
will generate an invertible afﬁne transformation M
A
from
the true 3D points

p
i
	
to the estimated 3D points

^ p
i
	
(Eq.10). This deviation transformation is decided by three
factors: the normal of the LIDAR’s scan plane in the platform
frame n, the calibration error M
e
and the direction of the
straight driving R
V
.
^
~ p
i
=M
A
(n;M
e
;R
V
) ~ p
i
(10)
Assume there are two 3D point-clouds P
1
and P
2
, which
are collected by the reference LIDAR on two pathes with
different directionsR
V1
6=R
V2
, then there are two deviation
transformation matrices M
A1
and M
A2
. If the P
1
and P
2
are aligned, then M
A1
= M
A2
. In Appendix A, we have
demonstrated that if M
A1
=M
A2
, the calibration error will
be eliminated (M
e
=I).
IV. MULTIPLE LIDARS CALIBRATION
After reference LIDAR calibration, the multiple LIDARs
calibration stage is to simultaneously calibrate other target
LIDARs to the platform frame with the help of reference
LIDAR. In our research, we calibrate target LIDARs by
aligning their 3D point-clouds to the constant 3D point-cloud
of the reference LIDAR. In order to improve the result in
global level, the alignment among the 3D point-clouds of
the target LIDARs is also concerned.
A. Multiple LIDARs Calibration Method
The existence of the calibration error of one target LIDAR
L
t
will make its generated 3D point-cloud P
t
misaligned
to the constant 3D point-cloud P
r
of the reference LIDAR
L
r
. So our strategy for calibrating one target LIDAR is
to align the 3D point-cloud P
t
to the 3D point-cloud P
r
to make their calibration errors are in the same level. So
the optimization formula for the alignment of P
t
and P
r
using their geometric features is deﬁned as Eq.11, which is
referred to as pairwise LIDARs calibration. We can combine
all the pairwise calibration to do simultaneous optimization
as Eq.12, which is referred to as multiple LIDARs calibration
^
M

Lt
= arg min
^
M
L
t
D
P
(P
r
;P
t
(
^
M
Lt
))
= arg min
^
M
L
t
P
j
D
G
(G
j
r
;G
j
t
(
^
M
Lt
))
(11)
f
^
M
k
Lt
g =arg min
f
^
M
k
L
t
g
X
k
D
P
(P
r
;P
k
t
(
^
M
k
Lt
)) (12)
3036
But this optimization performs bad in global level, which
is manifested in the misalignment of 3D point-clouds gen-
erated by the target LIDARs. So the optimization formula
for the alignment among target LIDARs’ 3D point-clouds,
deﬁned as Eq.13, is added to Eq.12 to perform global
optimization.
f
^
M
k
Lt
g =arg min
f
^
M
k
L
t
g
X
l6=m
D
P
(P
l
t
(
^
M
l
Lt
);P
m
t
(
^
M
m
Lt
)) (13)
In our experiment, we simultaneously used all LIDARs to
collect the data on the same straight path. In the generated 3D
point-clouds, we manually extracted the plane features and
minimize the objective function Eq.12+Eq.13 for multiple
LIDARs calibration . Assume there aren target LIDARs, the
tuned variables are 3n rotation angles and 3n translation
values of the estimated calibration parameters. Finally after
alignment, the calibration errors of the target LIDARs are in
the same level with the reference LIDAR.
B. Multiple LIDARs Calibration Theory
For the ﬁrst term of the optimization formula for multiple
LIDARs calibration in Eq.12, assume there are two 3D
point-clouds P
r
and P
t
, which are collect by the well-
calibrated reference LIDAR and the target LIDAR on the
same path, then according to Eq.10, there are two deviation
transformation matrices M
Ar
and M
At
. If the P
r
and P
t
are aligned, then M
Ar
= M
At
. In Appendix B, we have
demonstrated that if M
Ar
=M
At
, the calibration error will
be eliminated M
et
=M
er
=I.
For the second term of the optimization formula for
multiple LIDARs calibration in Eq.13, it is a punishment
term that compensates the ﬁrst term. Because the reference
LIDAR’s 3D point-cloud cannot cover all the space around
the platform, the ﬁrst term cannot work well in its uncov-
ered space. The second term concerns the alignment in the
reference’s uncovered space, then the calibration error will
be reduced in the global level.
V. EXPERIMENT RESULT
The vehicle platform POSS-V (Fig.1) was used in this
research to evaluate the calibration method for a multi-
LIDAR system. An experiment was conducted by using the
data collected through a driving along a V-shape path (Fig.2),
where theL
4
, scanning vertically to the right of the vehicle,
is regarded as the reference LIDAR; theL
2
, scanning upward
to the ahead ceiling surface, and L
3
, scanning downward
to the ahead road surface, are calibrated with platform by
referring to L
4
. As the experiment was conducted at a ﬂat
underground parking lot, the horizontal LIDAR, i.e. L
1
,
was used to perform localization through a LIDAR-based
2D SLAM (Fig.3), and its frame is regarded as that of the
platform.
Below we present the experimental results in reference and
multi-LIDAR calibrations, which are conducted by aligning
the plane features of the 3D points from each LIDAR. We
manually set seeds for region-grow based planes extraction
Fig. 3. The SLAM result of the designed V-shape path in the underground
parking lot.
Fig. 4. The 3D point-clouds of Path 1 and 2. The ﬂoor, wall and ceiling
parts are separately present as highlight for clear viewing.
and selected a coarse calibration parameters as initial value.
We evaluated the result’s accuracy by comparing the sensing
results to the measured 3D scene model from a total station.
A. Reference LIDAR calibration
The data for reference LIDAR calibration was collected by
driving the platform along the V-shape path. The 3D point-
clouds and their plane features are shown in Fig.4. After
alignment, we can get aligned 3D point-clouds, shown in
Fig.5. According to the reference LIDAR calibration theory,
if the 3D point-clouds are aligned and there is no mirror
reﬂection occurs, the calibration error is eliminated except
the translation error along the Z-axis.
In order to demonstrate the reference LIDAR calibration
result is acceptable, we test the result in the test area shown
in Fig.6, which is a corner of the underground parking-lot.
We used a total station to build a CAD model of the test
area and special focus is cast on the planes’ direction in this
scene. And we drove the platform along a C-curve path to test
whether the generated 3D point-cloud is consistent with the
measured CAD model. The SLAM result and the generated
3D point-cloud are shown in Fig. 6. The numerical analysis
of the calibration result is to calculate the angles between
extracted planes in the 3D point-cloud and the result is shown
in Table.I. Take the table’s ﬁrst row for example, the angle
between the Wall1 and Wall2 (Fig.6) is calculated by the
angle between the normal vectors of the Wall1 and Wall2.
From Table.I, the most improvement is the angle between
two vertical walls (Wall1 and Wall2, 86:6127

! 89:9894

),
3037
Fig. 5. Alignment of the 3D point-clouds in 3 viewpoints. The red 3D
point-cloud is from path 1 and the blue 3D point-cloud is from path 2. Left
is before alignment and right is after alignment.
Fig. 6. Upper left:the test area. Upper right: the SLAM result. Lower: the
generated 3D point-cloud from L
4
. The green planes are used to test the
calibration result.
which indicates that the initial calibration parameters are
bad in rotation around Z-axis. Except the result between
the Floor and Wall2 (last row in Table.I), the rest errors
of calibration result are reduced. The difference between the
ground-truth and the calibrated 3D point-cloud is no more
than 0.4 degree. Taking localization error, sensing error of
LIDAR and measurement error of ground-truth into account,
the reference LIDAR calibration result is acceptable.
B. Multiple LIDARs calibration
Firstly we calibrated the LIDARsL
2
andL
3
in the absence
of the second optimization term Eq.13. Fig.7 shows the
extracted features, whose 3D points are collected on the
path 1 in Fig.2, for the ﬁrst optimization term Eq.12. After
alignment, shown in Fig.8, the alignment result around the
wall side, where the features are extracted, is acceptable, but
the alignment result around the vehicle side is bad, which is
manifested in the misalignment of 3D point-clouds.
Then we calibrated the LIDARs using both optimization
TABLE I
REFERENCE LIDAR CALIBRATION RESULT IN TEST AREA (FIG.6)
Planes Before After Ground- Eval.
Angle Calibration(

) Calibration(

) truth(

) ( Change)
Wall1 86.6127 89.9894 89.9476 Excellent
Wall2 ( = 3:3349) ( = 0:0418) (###)
Ceiling 0.7385 0.4955 0.0685 Good
Floor ( = 0:6700) ( = 0:4270) (#)
Ceiling 88.8720 89.7993 89.9279 Excellent
Wall1 ( = 1:0559) ( = 0:1286) (##)
Ceiling 89.9643 89.9606 89.8380 Excellent
Wall2 ( = 0:1263) ( = 0:1226) (#)
Floor 89.3787 89.4310 89.8669 Good
Wall1 ( = 0:4882) ( = 0:4359) (#)
Floor 89.8134 89.4050 89.8070 Good
Wall2 ( = 0:0064) ( = 0:4020) (")
Fig. 7. The 3D point-clouds and their plane features for multiple LIDARs
calibration.
terms. The features from target LIDARs L
2
and L
3
for the
second optimization term Eq.13 are shown in Fig.9. After
alignment, shown in Fig.10, the alignment result around the
vehicle side improved.
Finally we compared these two calibration results to the
ground-truth to demonstrate that 1) the second optimization
term Eq.13 in multiple LIDARs calibration optimization
formula improves the result in the global level, and 2) the
multiple LIDARs calibration result is acceptable.
The ﬁrst demonstration is conducted by testing the angle
between the ceiling plane from the LIDAR L
2
and the ﬂoor
plane from the LIDAR L
3
in the calibration area shown in
Fig.7 and the numerical result is shown in Table.II. We found
that with the second optimization term Eq.13, the calibration
error changes from 1:2390 to 1:0465 and gets closer to the
reference LIDAR’s calibration error 0:9403. With the Fig.8
and the Fig.10, we demonstrated that the second optimization
term Eq.13 improves the calibration result in the global level.
The second demonstration is conducted by testing the
angles between planes from the LIDARs L
2
and L
3
in
the test area shown in Fig.6. The Fig.11 shows the 3D
point-clouds of L
2
and L
3
in the test area and the Fig.12
shows the alignment of these 3D point-clouds. The numerical
analysis is shown in Table.III. The column of error gap is
the difference between the multiple calibration result and
the reference calibration result, and it is an index to show
3038
Fig. 8. The alignment results of multiple LIDARs calibration without
the second optimization term Eq.13. The green 3D point-cloud is from the
reference LIDARL
4
, the blue and the red ones are from the target LIDARs
L
2
and L
3
. The last row shows the misalignment.
Fig. 9. All the plane features for multiple LIDARs calibration using both
optimization terms.
Fig. 10. The alignment result of multiple LIDARs calibration with the
second optimization term Eq.13. The color is same with Fig.8. The last row
shows the improvement in the global level.
TABLE II
EFFICIENCY OF EQ.13 IN CALIBRATION AREA (FIG.7)
Angle Between Without With Reference Ground-
Two Planes Eq.13 Eq.13 Result truth
Ceiling(L
2
)-Floor(L
3
) 1.2390

1.0465

0.9403

0.0453

Fig. 11. 3D point-clouds ofL
2
andL
3
in the test area. Highlighted parts
are the extracted planes for testing.
the difference of the calibration error level. Form Table.III,
the difference between the reference LIDAR calibration error
and the target LIDARs calibration errors is no more than 0.5
degree, so the multiple LIDARs calibration result is in the
same level with the reference LIDAR and is also acceptable.
The straight pathes are required in calibration theory, but
in reality, a lightly curved path can be seen as straight in
a short path segment. The demonstration of the calibration
method on a curved path is our next work.
VI. CONCLUSION
The research proposed a calibration method for multi-
LIDAR systems, which is conducted in general environment,
without modiﬁcation to the environment by such as putting
targets, nor relying on special facilities. The calibration
method is divided into two sequential stages: 1) reference
calibration, i.e. ﬁnding the transformation from a reference
LIDAR to the platform frame; 2) multi-LIDAR calibration,
i.e. ﬁnd the LIDARs’ relative geometries by referring to
the reference one. After the platform’s driving along a V-
shape path, which has been theoretically proved in this paper
and demonstrated efﬁcient in experiment, the data of each
sensor are aligned using their geometric features to ﬁnd the
transformations from the reference LIDAR to the platform
and from other LIDARs to the reference one. Efﬁciency and
Fig. 12. Left: before calibration. Right: after calibration. The edges view
shows the DoN (Difference of Normal) features.
3039
TABLE III
MULTIPLE LIDARS CALIBRATION RESULT IN TEST AREA (FIG.12)
Angle Between Multiple Reference Ground- Error
Two Planes Result Result truth Gap
Ceiling-Floor 0.8786

0.4955

0.0685

0.3831

Wall1(L
2
)-Wall1(L
3
) 0.3866

- - 0.3866

Wall2(L
2
)-Wall2(L
3
) 0.5045

- - 0.5045

Ceiling-Wall1(L
2
) 89.5609

89.7993

89.9279

0.2384

Floor-Wall1(L
3
) 89.5176

89.4310

89.8669

0.0866

Ceiling-Wall2(L
2
) 89.7607

89.9606

89.8380

0.1999

Floor-Wall2(L
3
) 89.4806

89.4050

89.8070

0.0756

TABLE IV
NOTATIONS
Var. Comment
M
A
deviation transformation from true point to estimated point
R
A
linear map matrix term of M
A
T
A
translation term of M
A
M
V
i
platform’s transformation at time i in the world frame
R
V
i
rotation term of M
V
i
R
V
platform drives straightly (R
V
i
=R
V
)
T
V
i
translation term of M
V
i
M
L
geometric transformation of the LIDAR in the vehicle frame
R
L
rotation term of M
L
T
L
translation term of M
L
Me calibration error transformation
Re rotation term of Me
Te translation term of Me
^
M
L
estimated M
L
,
^
M
L
=MeM
L
^
R
L
rotation term of
^
M
L
^
T
L
translation term of
^
M
L
t vehicle forward direction in the vehicle frame ((0; 1; 0)
T
)
n LIDAR scan plane’s normal in the vehicle frame ((;;)
T
)
6= 0 requirement of invertible afﬁne deviation transformation.
O
L
i
position of the LIDAR’s origin in the world frame at time i
accuracy has been demonstrated through an experiment at an
underground parking-lot in Peking University. Future work
will address on an automated feature extraction and selection
algorithm, which is a basis in extending the method to an
online based calibration.
VII. APPENDIX
A. Reference LIDAR Calibration Theory Derivation
The detailed deviation transformation M
A
in Eq.10 is as
Eq.14. The detailed notations are listed in Table.IV.
M
A
=

R
A
T
A
0 1

;M
e
=

R
e
T
e
0 1

(14)
where
8
>
>
>
>
<
>
>
>
>
:
R
A
= E + (I E)J
T
A
= T
0
V
 R
A
O
0
L
+R
V

^
T
L
E = R
V
R
e
R
T
V
J = R
V

tn
T
n
T
t
R
T
V
^
T
L
= R
e
T
L
+T
e
For the two 3D point-clouds P
1
and P
2
mentioned in
section 3, there are two deviation transformations M
A1
and
M
A2
. If M
A1
= M
A2
then P
1
and P
2
are aligned. If P
1
and P
2
are aligned and each contains more than 4 non-
deteriorate points, then M
A1
= M
A2
. If we assume the
rotation axis of R
V
is Z-axis, then we demonstrated that
ifR
A1
=R
A2
=R
0
A
holds, there are only two solutions for
R
0
A
and R
e
:
Solution 1

R
0
A
= I
R
e
= I
Solution 2
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
R
0
A
=
2
4
1 0 0
0 1 0
0 0  1
3
5
(notate it as I
Z
)
R
e
=
2
4
1  2
2
 2  2
 2 1  2
2
 2
2 2 2
2
  1
3
5
Proof:
Assume R
A
=R
V
KR
V
T
and J =R
V
J
0
R
V
T
.
Then K =R
e
+ (I R
e
)J
0
and J
0
=
tn
T
n
T
t
.
Assume R
V
=R
T
V2
R
V1
.
Then R
A1
=R
A2
, R
V
K R
V
T
=K
* n = (;;)
T
and t = (0; 1; 0)
T
.
)
J
0
=
2
4
0 0 0


1


0 0 0
3
5
Assume
R
e
=
2
4
r
11
r
12
r
13
r
21
r
22
r
23
r
31
r
32
r
33
3
5
Then
K =
2
4
r
11
 


r
12
0 r
13
 


r
12
r
21
 


(r
22
  1) 1 r
23
 


(r
22
  1)
r
31
 


r
32
0 r
33
 


3
5
(15)
*
R
V
KR
V
T
=K, (R
V

R
V
)vec(K) =vec(K)
) vec(K) is the eigenvector of R
V

 R
V
, and the
eigenvalue is 1.
Assume the rotation axis of R
V
is  = (0; 0; 1)
T
Then the eigenvector (eigenvalue is 1) of R
V

 R
V
is
vec(K) = a

0 0 0 0 0 0 0 0 1

0
+
b
2

1  i 0 i 1 0 0 0 0

0
+
c
2

1 i 0  i 1 0 0 0 0

0
(16)
From Eq.15 and Eq.16. We get b = 1 and c = 1.
)
K =
2
4
1 0 0
0 1 0
0 0 a
3
5
(17)
From Eq.15 and Eq.17. We get
r
11
= 1 +


r
12
r
13
=


r
12
r
21
=


(r
22
  1) r
23
=


(r
22
  1)
r
31
=


r
32
r
33
=


r
32
+a
3040
)
R
e
=
2
4
1 +


r
12
r
12


r
12


(r
22
  1) r
22


(r
22
  1)


r
32
r
32


r
32
+a
3
5
* R
e
is an orthogonal matrix
)
R
e
R
e
T
=I)
8
>
>
>
>
<
>
>
>
>
:
r
12
=

0
 2
r
22
=

1
1  2
2
r
32
= ( a
p
a
2

2
 a
2
+ 1)
For all cases of r
12
, r
22
and r
32
, with two constraints: 1)
R
e
is orthogonal matrix and 2)jR
e
j = 1, we can only get
two solutions
Solution 1

K = I
R
e
= I
Solution 2
8
>
>
<
>
>
:
K = I
Z
R
e
=
2
4
1  2
2
 2  2
 2 1  2
2
 2
2 2 2
2
  1
3
5
*R
A
=R
V
KR
T
V
andR
V
’s rotation axis is = (0; 0; 1)
T
) This theorem is proved.
Because R
0
A
in solution 2 is a mirror reﬂection transfor-
mation about XY-plane for 3D point-cloud, it can be easily
detected and restrained. So the solution 1 is the unique
feasible solution and it indicates that the calibration rotation
error is eliminated.
When R
e
=I and T
A1
=T
A2
, from Eq.14, we can get:
R
V
T
e
=T
e
Because the rotation axis of R
V
is (0; 0; 1)
T
, the cali-
bration translation error is eliminated on XY-plane but there
is no restriction on Z-axis for T
e
. This can be solved by
choosing other R
V1
and R
V2
, whose rotation axis is not
(0; 0; 1)
T
to establish restriction on Z-axis for T
e
.
B. Pairwise LIDARs Calibration Theory Derivation
For the two 3D point-clouds P
t
and P
r
mentioned in
section 4, there are two deviation transformations M
At
and
M
Ar
. IfP
t
andP
r
are aligned and each contains more than
4 non-deteriorate points, then M
At
= M
Ar
. As reference
LIDAR calibration guarantees that R
er
= I and R
Ar
= I,
we demonstrated that if R
At
=R
Ar
=I, then R
et
=I.
Proof:
Assume J =R
V
J
0
R
V
T
. Then J
0
=
tn
T
n
T
t
.
* R
At
=R
Ar
) R
et
+ (I R
et
)J
0
t
=R
er
+ (I R
er
)J
0
r
) (I R
et
) (I J
0
t
) = (I R
er
) (I J
0
r
)
When R
er
=I, then (I R
er
) (I J
0
r
) = 0
* rank(I J
0
t
) = 2 and rank(I R
et
) = 2 or 0;
) R
et
=I is the only solution.
When R
et
=I and T
At
=T
Ar
, from Eq.14, we can get
T
et
=T
er
, which contains error along Z-axis.
REFERENCES
[1] J. Levinson and S. Thrun, “Robust vehicle localization in urban
environments using probabilistic maps.” in IEEE Int. Conf. Robotics
and Automation (ICRA), 2010, pp. 4372–4378.
[2] S. Hu and A. Zhang, “3D laser omnimapping for 3D reconstruction
of large-scale scenes,” in IEEE Urban Remote Sensing Event, 2009,
pp. 1–5.
[3] A. Segal, D. H¨ ahnel, and S. Thrun, “Generalized-ICP.” in Robotics:
Science and Systems, J. Trinkle, Y . Matsuoka, and J. A. Castellanos,
Eds. The MIT Press, 2009.
[4] B. Douillard, A. Brooks, and F. Ramos, “A 3D laser and vision
based classiﬁer,” in IEEE Intelligent Sensors, Sensor Networks and
Information Processing (ISSNIP), 2009, pp. 295–300.
[5] F. Endres, C. Plagemann, C. Stachniss, and W. Burgard, “Unsupervised
discovery of object classes from range data using latent Dirichlet
allocation.” in Robotics: Science and Systems, J. Trinkle, Y . Matsuoka,
and J. A. Castellanos, Eds. The MIT Press, 2009.
[6] K. Lai and D. Fox, “3D laser scan classiﬁcation using web data and
domain adaptation.” in Robotics: Science and Systems, J. Trinkle,
Y . Matsuoka, and J. A. Castellanos, Eds. The MIT Press, 2009.
[7] J. Guivant, E. Nebot, and S. Baiker, “Autonomous navigation and
map building using laser range sensors in outdoor applications,” Int.
J. Robotic Systems, vol. 17, no. 10, pp. 565–583, 2000.
[8] A. Kelly, “An intelligent predictive control approach to the high-speed
cross-country autonomous navigation problem,” Ph.D. dissertation,
Robotics Institute, Carnegie Mellon University, 1995.
[9] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron,
J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann et al., “Stanley:
The robot that won the darpa grand challenge,” Int. J. Field Robotics,
vol. 23, no. 9, pp. 661–692, 2006.
[10] J. Underwood, “Reliable and safe autonomy for ground vehicles in
unstructured environments,” Ph.D. dissertation, University of Sydney,
2009.
[11] D. Scaramuzza, A. Harati, and R. Siegwart, “Extrinsic self calibration
of a camera and a 3D laser range ﬁnder from natural scenes.” in
IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), 2007, pp.
4164–4169.
[12] G. Lisca, P. Jeong, and S. Nedevschi, “Automatic one step extrinsic
calibration of a multi layer laser scanner relative to a stereo camera,” in
IEEE Int. Conf. Intelligent Computer Communication and Processing
(ICCP), 2010, pp. 223–230.
[13] Q. Zhang and R. Pless, “Extrinsic calibration of a camera and laser
range ﬁnder (improves camera calibration).” in IEEE/RSJ Int. Conf.
Intelligent Robots and Systems (IROS), 2004, pp. 2301–2306.
[14] J. P. Underwood, A. Hill, T. Peynot, and S. Scheding, “Error mod-
eling and calibration of exteroceptive sensors for accurate mapping
applications.” J. Field Robotics, vol. 27, no. 1, pp. 2–20, 2010.
[15] C. Gao and J. R. Spletzer, “On-line calibration of multiple LIDARs
on a mobile vehicle platform.” in IEEE Int. Conf. Robotics and
Automation (ICRA), 2010, pp. 279–284.
[16] M. Sheehan, A. Harrison, and P. Newman, “Self-calibration for a 3D
laser,” Int. J. Robotics Research, vol. 31, no. 5, pp. 675–687, 2012.
[17] J. P. Underwood, A. J. Hill, and S. Scheding, “Calibration of range
sensor pose on mobile platforms.” in IEEE/RSJ Int. Conf. Intelligent
Robots and Systems (IROS), 2007, pp. 3866–3871.
[18] R. Unnikrishnan and M. Hebert, “Fast extrinsic calibration of a laser
rangeﬁnder to a camera,” Robotics Institute, Tech. Rep. CMU-RI-TR-
05-09, 2005.
[19] O. Jokinen, “Self-calibration of a light striping system by matching
multiple 3-D proﬁle maps,” in IEEE 3-D Digital Imaging and Mod-
eling, 1999, pp. 180–190.
[20] J. Levinson and S. Thrun, “Unsupervised calibration for multi-beam
lasers,” in International Symposium on Experimental Robotics, 2010.
[21] M. He, H. Zhao, F. Davoine, J. Cui, and H. Zha, “Pairwise LIDAR
Calibration Using Multi-Type 3D Geometric Features in Natural Scene
,” in IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), 2013.
3041
