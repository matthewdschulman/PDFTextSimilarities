Human-Guided Robotic Manipulation: Theory and Experiments
X. Li and C. C. Cheah
Abstract— Emerging applications of robot systems that in-
volve close physical interaction with human have opened up
new challenges in robot control. For these applications, it is
important to consider the stability and coordination of human-
robot interaction. While various control techniques have been
developed for human-robot interaction, existing methods do not
take the advantages of human ability in responding and adapt-
ing to unknown environment. In this paper, a human-guided
manipulation problem which is able to take advantages of both
the human knowledge and the robot’s ability, is formulated
and solved. The workspace is divided into a human region,
where human play a more active role in the manipulation task,
and a robot region, where the robot is more dominant in the
manipulation. The proposed formulation allows the involvement
of human control action to deal with unforeseen changes
or uncertainty in the real world. We present a theoretical
foundation that allows the stability and coordination of the
human-guided manipulation problem to be analyzed. Based on
the human region and the robot region, an adaptive tracking
controller is developed. Experimental results are presented to
illustrate the performance of the proposed control method.
I. INTRODUCTION
Rapid advances in robotic technologies have led to the
emergence of a diversity of initiatives that involves physical
interactions with human, such as rehabilitation robots [1]–[3]
and exoskeleton or wearable robots [4], [5]. The rehabilita-
tion robots are developed to help stroke patients to restore
the impaired functionalities of limbs, and the wearable robots
are used to enhance the strength or complement the ability
of human. The development of human-robot interaction
systems widens the potential applications of traditional robot
manipulators and also creates systems that are beyond the
physical capability of human body.
Though much progress has been achieved in understanding
motion control of robot manipulator [6]–[13], most results
are limited to the case of isolated robot systems that do
not involve physical interactions with human. By using
Lyapunov method, Takegaki and Arimoto [6] showed that a
simple PD controller with gravity compensation was effective
for setpoint control, despite the nonlinearity of robot dynam-
ics. To deal with trajectory tracking control with parametric
uncertainty, several adaptive control schemes [7], [8] were
proposed for robot manipulator. Various task-space sensory
controllers have also been proposed for robotic manipulator
with uncertainties in both kinematics and dynamics [9]–[11].
In addition, the concept of task-space region control was
proposed in [12], where the desired objective can be speciﬁed
The authors are with the School of Electrical and Electronic Engineering,
Nanyang Technological University, Singapore, 639798. The work was sup-
ported by the Agency For Science, Technology And Research of Singapore
(A*STAR), (reference 1225100002).
as a region, instead of a desired position or trajectory.
Recently, a new task-space control method using regional
feedback information was also proposed in [13] such that the
global dynamic stability with the consideration of singularity
issues and limited sensing zones was guaranteed.
For applications that involve close interaction and cooper-
ation with human, several works have been reported in the
literature to address the issues of safety and performance
in physical human-robot interaction, such as the use of
lightweight robots [14], passive compliant systems [15], and
variable stiffness actuation (VSA) [16], [17]. Bicchi and
Tonietti [18] considered the problem of designing joint-
actuation mechanisms, which not only guarantees low injury
risk but also allows the fast and accurate operation of
the robot. In [19], a non-model based method with less
predictable interactions between patients and devices was
developed for rehabilitative systems. To handle the reaction
of robotic system to environmental forces, a novel concept
of impedance control was ﬁrstly brought up by Hogan et
al. [20], [21] and is now commonly used in tasks involving
physical human-robot interaction. In impedance control, the
desired performance of human-robot interaction is speciﬁed
by a desired impedance between the motion of robot and
the forces of interaction. By specifying the performance of
robot system with a target impedance, an iterative learning
impedance controller was proposed in [22]. An impedance-
based control algorithm was developed in [23], for an-
kle rehabilitation using parallel robot. In [24], an active
impedance controller was proposed to increase the Cartesian
stiffness range of VSA. An adaptive impedance scheme was
developed in [25] to compensate unmodeled uncertainty in
a collaborative task that requires sharing of a load by two
partners. In [26], an adaptive controller was presented for
upper-limb rehabilitative robotic systems, where a position-
dependant stiffness was introduced to resolve the possible
conﬂicts between patients and robots.
However, existing control techniques for human-robot
interaction do not provide much ﬂexibility for cooperative
manipulation tasks that require human’s guidance and assis-
tance, and the advantages of human ability in responding and
adapting to unknown environment are not fully explored or
utilized in the control method. It is interesting to observe that
humans are able to react intelligently to unforeseen changes
in the real world and perform skillful manipulation tasks.
Industrial robot manipulators, on the contrary, are designed
to operate in a structured environment and is able to carry
heavy loads that are beyond the capability of human.
In this paper, we formulate a human guided manipulation
problem that is able to take advantages of both the human
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4594
knowledge and the robot’s ability in a stable manner. We
present a theoretical foundation that allows the analysis
of the human-guided manipulation problem. To formulate
the co-manipulation tasks, the workspace is divided into
a human region, where human plays a more active role
in the manipulation task, and a robot region, where the
robot is more dominant in the manipulation. The human
region allows the involvement of human control action to
take advantages of human knowledge for manipulation in
an unknown environment, while the robot region allows the
robot to take control in cases when manipulation task is
well deﬁned. Based on these regions, an adaptive tracking
controller is developed so as to take the advantages of both
the human knowledge and the ability of robot. It is shown
that the stability of closed-loop system is guaranteed. Exper-
imental results are presented to illustrate the performance of
the proposed control method.
II. ROBOT KINEMATICS AND DYNAMICS
Letx denotes the position of the end effector in task space
[9], such as Cartesian space or image space. Ifx is speciﬁed
in Cartesian space, we havex=r2<
nr
which is the position
of the end effector in Cartesian space. If x is speciﬁed in
image space, we have x =x
I
2<
2
which is the position
of image feature. The Cartesian-space velocity of the end
effector _ r is related to the joint-space velocity _ q as [27]:
_ r =J
r
(q)_ q; (1)
where q2<
n
is the vector of joint variables, and J
r
(q)
is the Jacobian matrix from joint space to Cartesian space.
The image velocity vector _ x is related to the Cartesian-space
velocity and the joint-space velocity as [11], [13]:
_ x
I
=J
I
(r)_ r =J
I
(r)J
r
(q)_ q =J(q)_ q; (2)
where J
I
(r) is the interaction matrix or image Jacobian
matrix, andJ(q)=J
I
(q)J
r
(q)2<
2£n
is the Jacobian matrix
of the mapping from joint space to image space.
When the robotic manipulator interacts with human, the
dynamic model of robot is described as:
M(q)Ä q+[
1
2
_
M(q)+S(q; _ q)]_ q+g(q)+d=¿+¿
e
; (3)
where M(q) is an inertia matrix which is symmetric and
positive deﬁnite, S(q; _ q) is a skew-symmetric matrix, g(q)
denotes a vector of gravitational force, d is a unmodeled
torque,¿ denotes a vector of control inputs, and¿
e
denotes
the torque exerted on the robot by the human. If an external
force f
e
is exerted on the end effector, then ¿
e
=J
T
(q)f
e
.
The dynamic model described by equation (3) is linear in
a set of parameters µ
d
= [µ
d1
;¢¢¢;µ
dn
d
]
T
as [27], [28]:
M(q)Ä q+[
1
2
_
M(q)+S(q; _ q)]_ q+g(q)= Y
d
(q; _ q; _ q; Ä q)µ
d
,
where Y
d
(q; _ q; _ q; Ä q)2<
n£n
d
is a dynamic regressor.
III. HUMAN REGION AND ROBOT REGION
Since human is able to react intelligently to unforeseen
changes in the real world while robot is able to carry
heavy loads that are beyond the capability of human, the
human-robot co-manipulation can take advantage of both the
human knowledge and the robot’s ability. In this section,
a theoretical foundation is presented for the analysis of
the human-robot co-manipulation problem. To formulate
the co-manipulation tasks, the workspace is divided into a
human region, where human play a more active role in the
manipulation task, and a robot region, where the robot is
more dominant in the manipulation.
A. Scenarios of Human-Robot Co-manipulation
Based on the human region and robot region, we consider
two scenarios of the co-manipulation task, where human play
an active role in the beginning of the task and the robot
only takes the control in the end, or the robot is active
in the beginning and human guide the robot to carry out
manipulation tasks in the end.
Manipulator
Camera
Human region
FOV
Robot region
Desired
trajectory
Human region
External force
from human
Fig. 1. The human region is speciﬁed outside the view of camera while
the robot region is speciﬁed within the view. Human guide the end effector
to enter the robot region and then the robot takes the control to drive the
end effector to the desired position with visual feedback.
The ﬁrst scenario is illustrated in Fig. 1. In Fig. 1, a camera
is employed to measure the position of the end effector in
image space, and the end effector is controlled to move to
a desired position with visual feedback. However, due to
limited ﬁeld of view (FOV) of the camera, the robot does
not have any visual information of the environment that is
not within the view. The unknown environment of the robot
workspace that is outside the view of the camera is thus
deﬁned as the human region. Human are expected to take
control ﬁrst and guide the end effector through the unknown
environment towards robot region that is within the view of
the camera. After the end effector enters the robot region,
human can reduce or release the control and the robot then
plays a more active role and drives the end effector to the
desired position. In this scenario, the path of the end effector
in the beginning is unknown or the environment is uncertain.
The second scenario is illustrated in Fig. 2, where the
ending stage of the task is uncertain or unknown. In a com-
plex environment with cluttered background, it is difﬁcult to
identify the desired position of the target and the positions
of the obstacles in the workspace by using sensors such as
vision systems. The positions of the target and obstacles are
therefore unknown or uncertain. The uncertain environment
of the workspace is therefore deﬁned as the human region,
and the region where the robot has the exact knowledge about
the environment is deﬁned as the robot region. The robot
takes control ﬁrst and drives the end effector from the robot
region to the human region. After the end effector enters
4595
the human region, the robot becomes passive, and human
identify the desired position and obstacles and guide the end
effector to move to the desired position. In this scenario, the
path of the end effector in the ending stage is unknown or
the desired position and obstacles are uncertain.
Desired position
External force
from human
Human region
Robot region
Fig. 2. It is difﬁcult to identify the desired position due to the obstacles
around it. Human identify the desired position and guide the end effector
such that the position of the end effector converges to the desired position.
B. Region Function and Potential Energy
As seen from the previous subsection, the human region
and the robot region should be speciﬁed such that human or
the robot only takes control within the corresponding region,
which allows the involvement of a more active control action
when the other becomes passive. The region function of the
human region and the robot human is speciﬁed as:
f(x)=
(x1¡x
d1
)
nx
(x
b1
¡x
d1
)
nx
+¢¢¢+
(xp¡x
dp
)
nx
(x
bp
¡x
dp
)
nx
¡1·0; (4)
wherex2<
p
is the position of the end effector in task space,
the vectorx
d
=[x
d1
;¢¢¢;x
dp
]
T
denotes the desired position,
x
b
=[x
b1
;¢¢¢;x
bp
]
T
represents the bound of the region, and
n
x
is the order. The region function in equation (4) can be
speciﬁed as a circle, an oval, or a rectangle with rounded
corners by varying the order n
x
, for different robot tasks.
The regions where f(x)·0 and f(x)>0 correspond to
the human region and the robot region separately, which can
also be opposite depending on the speciﬁc scenario. Next, the
potential energy for the region function is to be developed.
1) : Consider the ﬁrst scenario in the previous section,
the human region is speciﬁed wheref(x)>0 while the robot
region is speciﬁed where f(x)·0. Therefore, the potential
energy is speciﬁed as:
P
t
(x)=
kt
N
f1¡[min(0;f(x))]
N
g; (5)
where k
t
is a positive constant, N¸4 is an even integer, and
the order of the region function in equation (4) isn
x
=2. The
bottom point of P
t
(x) corresponds to the desired position,
but the top contour ofP
t
(x) is varying as the desired position
x
d
is time-varying, which cannot match the ﬁxed robot
region and human region. To solve the problem, another
potential energy function with a high-order reference region
is introduced as:
P
h
(x)=
k
h
N
2
fmin[0;[min(0;f
h
(x))]
N
¡(·
n
h
h
¡1)
N
]g
N
;(6)
where f
h
(x) is the high-order reference region speciﬁed as:
f
h
(x) =
(x1¡x
d1
)
n
h
(x
b1
¡x
d1
)
n
h
+¢¢¢ +
(xp¡x
dp
)
n
h
(x
bp
¡x
dp
)
n
h
¡1· 0, and n
h
is the order of the function, k
h
is a positive constant, and
0<·
h
<1 is a constant. It is seen that P
h
(x) is smooth and
lower bounded by zero, and the potential energy remains
constant where f
h
(x)>0.
Therefore, the overall potential energy is deﬁned as the
summation of P
h
(x) and P
t
(x) as:
P
H¡R
(x)=P
t
(x)+P
h
(x): (7)
where the notation of H¡R denotes the scenario that human
play an active role in the beginning and the robot only takes
the control in the end. The ﬁxed top contour of P
H¡R
(x)
corresponds to the high-order region f
h
(x), and its bottom
part is the desired position x
d
.
Partial differentiating the potential energy function de-
scribed by (7) with respect to x¡x
d
yields:
(
@P
H¡R
(x)
@(x¡x
d
)
)
T
=(
@Pt(x)
@(x¡x
d
)
)
T
+(
@P
h
(x)
@(x¡x
d
)
)
T
4
=¢"
H¡R
; (8)
The vector ¢"
H¡R
denotes the region error. When the
end effector is inside the human region where f(x)>0,
¢"
R¡H
=0, and human can take control to guide the end
effector to enter the robot region. After the end effector enters
the robot region where f(x)·0, the region error ¢"
H¡R
is
activated to drive the end effector to the desired position.
2) : Consider the second scenario in the previous sub-
section, the robot region is speciﬁed where f(x)>0 while
the human region is speciﬁed where f(x)·0. Therefore, the
potential energy is speciﬁed as:
P
R¡H
(x)=
kx
N
[max(0;f(x))]
N
; (9)
where k
x
is a positive constant, and the notation of R¡H
denotes the scenario that the robot is active in the beginning
and human guide the robot to carry out manipulation tasks
in the end. It is seen that P
R¡H
(x) is smooth and lower
bounded by zero. The bottom contour of the potential energy
corresponds to the region function f(x), and the potential
energy naturally reduces to zero when the end effector
transits from the robot region where f(x)>0 to the human
region where f(x)·0.
Partial differentiating the potential energy function de-
scribed by (9) with respect to x¡x
d
yields:
?
@P
R¡H
(x)
@(x¡x
d
)
´
T
=k
x
[max(0;f(x))]
N¡1
?
@f(x)
@(x¡x
d
)
´
T
4
=¢"
R¡H
; (10)
where ¢"
R¡H
denotes the region error. When the end
effector is inside the robot region where f(x)>0, ¢"
R¡H
6=
0. When the end effector enters the human region where
f(x)·0, the region error ¢"
R¡H
automatically reduces
to zero from equation (10). That is, the region error ¢" is
employed to drive the end effector to transit from the robot
region to the human region, and it reduces to zero after the
end effector enters the human region.
IV. ADAPTIVE TRACKING CONTROL FOR
HUMAN-ROBOT CO-MANIPULATION
Based on the proposed human region and robot region, we
can now proceed to develop the adaptive tracking control
strategy for human-robot co-manipulation. The proposed
4596
controller plays an active role inside the robot region, and
act passively inside the human region so as to allow the
involvement of human control action.
First, a reference vector is proposed as:
_ q
r
=J
+
(q)_ x
a
¡®J
+
(q)¢"; (11)
where J
+
(q) is the pseudo-inverse matrix of J(q), ® is a
positive constant, the region error can be speciﬁed as either
¢"=¢"
R¡H
or ¢"=¢"
H¡R
according to the speciﬁc
scenario of co-manipulation, and _ x
a
represents a refer-
ence vector deﬁned as: _ x
a
=[_ x
d1
x
b1
¡x1
x
b1
¡x
d1
;¢¢¢; _ x
dp
x
bp
¡xp
x
bp
¡x
dp
]
T
where _ x
d
=[_ x
d1
;¢¢¢; _ x
dp
]
T
is the desired velocity. Next, the
sliding vector is introduced as:s= _ q¡ _ q
r
. Using the sliding
vector, the robot dynamics in equation (3) is written as:
M(q)_ s+[
1
2
_
M(q)+S(q; _ q)]s+
Y
d
(q; _ q; _ q
r
; Ä q
r
)µ
d
=¿+¿
e
¡d: (12)
The human-guided manipulation controller is proposed as:
¿=¡J
T
(q)¢"¡K
s
s+Y
d
(q; _ q; _ q
r
; Ä q
r
)
^
µ
d
; (13)
where K
s
is a positive deﬁnite matrix, and
^
µ
d
is a vector
of estimated dynamic parameters which is updated by the
following update law:
_
^
µ
d
=¡L
d
Y
T
d
(q; _ q; _ q
r
; Ä q
r
)s whereL
d
is a positive deﬁnite matrix. Note that ¢" is smooth with
continuous partial derivatives, and thus the controller is also
continuous without hard-switching.
When the end effector is inside the robot region, the region
error ¢" is nonzero, and the proposed controller drives the
robot to move to the desired position within the robot region
or move from the robot region to the human region. When
the end effector is inside the human region, the region error
¢" reduces to zero such that¿=¡K
s
s+Y
d
(q; _ q; _ q
r
; Ä q
r
)
^
µ
d
where s= _ q¡J
+
(q)_ x
a
. That is, only the velocity control
term works, while the position control term x¡x
d
is not
included in the control input. Since human take control of the
manipulator in the human region, the directional information
towards the desired position x
d
is provided by human, and
the robot dynamics is also compensated by human.
Substituting equation (13) into equation (12), we have the
following closed-loop equation:
M(q)_ s+[
1
2
_
M(q)+S(q; _ q)]s+K
s
s
+J
T
(q)¢"+Y
d
(q; _ q; _ q
r
; Ä q
r
)¢µ
d
=¿
e
¡d; (14)
where ¢µ
d
=µ
d
¡
^
µ
d
. A Lyapunov-like candidate V is then
proposed as follows:
V =
1
2
s
T
M(q)s+P(x)+
1
2
¢µ
T
d
L
¡1
d
¢µ
d
; (15)
where P(x) denotes the potential energy function which is
either speciﬁed as P(x)=P
R¡H
(x) or P(x)=P
H¡R
(x)
according to the speciﬁc scenario of the co-manipulation.
Next, differentiating V with respect to time and substitut-
ing the sliding vector, the update law, and the closed-loop
equation into it, we have:
_
V=¡s
T
K
s
s¡s
T
[J
T
(q)¢"+Y
d
(q; _ q; _ q
r
; Ä q
r
)¢µ
d
]
+(_ x¡ _ x
a
)
T
¢"¡
_
^
µ
T
d
L
¡1
d
¢µ
d
+s
T
(¿
e
¡d)
=¡s
T
K
s
s¡®¢"
T
¢"+s
T
(¿
e
¡d): (16)
Equation (16) can rewritten as:
_
V =¡W +s
T
(¿
e
¡d); (17)
where W =s
T
K
s
s+®¢"
T
¢". Integrating equation (17)
over [0;t] yields:
R
t
0
s
T
(&)[¿
e
(&)¡d(&)]d& =V(t)¡V(0)+
R
t
0
W(&)d&. Since bothV andW are non-negative, we have:
R
t
0
s
T
(&)[¿
e
(&)¡d(&)]d& ¸¡V(0): (18)
The above equation (18) demonstrates the passivity of the
dynamics between the input ¿
e
¡d and output s. We are
now ready to state the following theorem:
Theorem 1: The sliding vector s is bounded if the external
torque¿
e
exerted by human and the unmodeled torqued are
bounded, and K
s
is chosen sufﬁciently large so that:
1
°
2
4
=2¸
min
[K
s
]¡1>0; (19)
where ° >0, and ¸
min
[?] denotes the minimum eigenvalue.
Proof: Note that
R
t
0
s
T
(&)[¿
e
(&)¡d(&)]d& ·
1
2
R
t
0
jjs(&)jj
2
d&
+
1
2
R
t
0
jj¿
e
(&)¡d(&)jj
2
d&: (20)
From equations (17) and (20), we have:
¡V(0)+
R
t
0
W(&)d&
·
1
2
R
t
0
jjs(&)jj
2
d&+
1
2
R
t
0
jj¿
e
(&)¡d(&)jj
2
d&: (21)
Since V >0, substituting W into equation (21) yields:
(2¸
min
[K
s
]¡1)
R
t
0
jjs(&)jj
2
d&+2®
R
t
0
jj¢"(&)jj
2
d&
·
R
t
0
jj¿
e
(&)¡d(&)jj
2
d& +2V(0): (22)
Next, using equation (19), we have:
R
t
0
jjs(&)jj
2
d& ·°
2
R
t
0
jj¿
e
(&)¡d(&)jj
2
d& +k(0); (23)
where k(0) = 2°
2
V(0). Therefore, the sliding vector s
is bounded since the torque ¿
e
exerted by human and the
unmodeled torque d are bounded. 444
It is well known that human are able to adapt to unknown
forces and act intelligently without accurate knowledge of the
environment. Next, we consider the case where the human
force is able to learn or adapt to the unmodeled external
torque such that ¿
e
!d as t!1. The following theorem
states the effects of human adaption on the overall system:
Theorem 2: The closed-loop system described by (14) gives
rise to the convergence of the region error, if the condition
described by (19) is satisﬁed and human are able to adapt
and reach for the desired target.
Proof: In the ﬁrst scenario that human play the active role
in the beginning, if the end effector is guided by human to
move from the human region to the desired robot region,
the desired target is reached. In the second scenario that
human take control in the ending stage, the desired target is
reached if the end effector is guided to move to the desired
position. When the end effector is located at the robot region
in either scenario, the proposed controller (13) is activated
which leads to the closed-loop equation (14).
4597
Next, equation (23) is obtained if the condition described
by (19) is satisﬁed. Equation (23) indicates thats is bounded
since ¿
e
¡d is bounded. From equation (17), we have:
V(t)+
R
t
0
W(&)d&=
R
t
0
s
T
(&)[¿
e
(&)¡d(&)]d&+V(0): (24)
since s is bounded and ¿
e
¡d!0,
R
t
0
s
T
(&)[¿
e
(&)¡d(&)]d&
is bounded, and hence both V and
R
t
0
W(&)d& are bounded.
SinceV is bounded,s,¢µ
d
,P(x) are bounded. The bound-
edness of P(x) ensures the boundedness of f(x). Since
the region function is bounded, x is bounded if the desired
positionx
d
is bounded. Therefore,¢" is bounded. Since¢"
is bounded, _ q
r
is bounded, and _ q is bounded because s is
bounded. The boundedness of _ q guarantees the boundedness
of _ x sinceJ(q) are trigonometric functions ofq. Therefore,
¢_ " is bounded, and ¢" is uniformly continuous. Since
R
t
0
W(&)d& =
R
t
0
(s
T
K
s
s+®¢"
T
¢")d& is bounded, it is
easy to verify that ¢" 2 L
2
(0;1). Therefore, it follows
from [27], [29], [30] that ¢" ! 0, and ¢"=0 implies
that the position of the end effector converges to the desired
position after the end effector enters the robot region in the
ﬁrst scenario of H¡R, or the end effector is controlled
to move from the robot region to the human region in the
second scenario of R¡H. 444
V. EXPERIMENT
The proposed control scheme was implemented on the ﬁrst
two joints of a SONY SRX-4CH industrial manipulator.
A. Human-Guided Visual Tracking
In the ﬁrst experiment, a PSD camera is employed to
measure the position of the end effector in image space
(in the unit of voltage [31]). While the vision ensures the
high-accurate positioning of the end effector, the vision is
not available if the end effector starts from a large initial
position, and the robot does not have any visual information
of the environment that is outside the view.
To solve the problem, the environment of the workspace
that is outside the view of the camera is deﬁned as the human
region. Human guide the end effector to transit from the
human region to the robot region. After the end effector
enters the robot region, the visual feedback is activated and
used to drive the end effector to the desired position. The
region function in equation (4) is speciﬁed as:
f(x
I
)=
(x
I1
¡x
d1
)
2
(x
b1
¡x
d1
)
2
+
(x
I2
¡x
d2
)
2
(x
b2
¡x
d2
)
2
¡1·0; (25)
and the high-order region that matches the FOV is speciﬁed
as: f
h
(x
I
)=
(x
I1
¡x
d1
)
20
(x
b1
¡x
d1
)
20
+
(x
I2
¡x
d2
)
20
(x
b2
¡x
d2
)
20
¡1·0, where x
b1
=
¡3 volt if x
I1
·x
d1
else x
b1
=3 volt; x
b2
=¡3 volt if x
I2
·
x
d2
elsex
b2
=3 volt, and the desired positionx
d
is speciﬁed
as a circle as: x
d1
=¡1+sin(0:4t) volt, and x
d2
= 1+
cos(0:4t)volt. Human take control of the manipulator where
f
h
(x
I
)>0, and the robot becomes active where f
h
(x
I
)·0.
The control parameters in equation (13) were set as:®=50,
k
t
=3:5, k
h
=3:5, ·=0:7, K
s
=diagf0:0001;0:0001g, L
d
=
diagf0:0001;0:0001g, the region error is speciﬁed as ¢"=
¢"
H¡R
since human play an active role in the beginning and
the robot only takes the control in the end. The experimental
results are shown in Fig. 3. As seen from Fig. 3(a) and Fig.
3(b), the end effector transits from the human region to the
robot region and converges to the desired position within
the robot region. As seen from Fig. 3(c), the tracking errors
reduce to zero after the end effector enters the robot region.
B. Human-Guided Setpoint Control
In the second experiment, the robot end effector is con-
trolled to move to a neighborhood around the desired position
by using the Cartesian-space feedback only. A Cartesian-
space region is formulated to enclose the desired position,
where the robot region is speciﬁed outside the Cartesian-
space region and the human region is speciﬁed inside the
Cartesian-space region. After the end-effector transits from
the robot region to the human region, the robot becomes
passive, and human identify the desired position and guide
the end effector to move to the desired position.
The region function in equation (4) is speciﬁed as:
f(r)=
(r1¡r
d1
)
2
0:1
2
+
(r2¡r
d2
)
2
0:1
2
¡1·0; (26)
and the desired positionr
d
is speciﬁed as a setpoint as:r
d1
=
¡0:4 m, and r
d2
= 0:2 m. The robot plays a more active
role where f(r)>0, and the human control action is fully
activated where f(r)·0. The control parameters in equation
(13) were set as: ®=10, k
x
=2, K
s
=diagf0:05; 0:05g,
L
d
=diagf0:0001;0:0001g, and the region error is speciﬁed
as ¢"=¢"
R¡H
since the robot is active in the beginning
and human guide the robot to carry out manipulation tasks
in the end. The experimental results are shown in Fig. 4. As
seen from Fig. 4(a), the end effector ﬁrst moves from the
robot region to the human region, and it is then guided by
human to the desired position within the human region. As
seen from Fig. 4(b), the position errors converge to small
bounds at steady state.
VI. CONCLUSION
In this paper, a human-guided manipulation problem has
been formulated and solved. The proposed method allows
us to take advantages of both the human knowledge and
the robot’s ability in a stable manner. The combination
of the human and robot co-manipulation is illustrated in
various scenarios, and an adaptive tracking controller is then
developed for the human-guided robotic manipulation. It has
been shown that the stability of the closed-loop system that
involves human-robot interaction can be ensured. Experimen-
tal results have been presented to illustrate the performance
of the proposed method in different scenarios of human-robot
co-manipulation. We believe that such formulation would
bridge the gap between traditional robot control and physical
human-robot interaction control.
REFERENCES
[1] H. I. Krebs, N. Hogan, M. L. Aisen, and B. T. V olpe, ”Robot-aided
neurorehabilitation,” IEEE Trans. Rehabilitation Eng., V ol. 6, No. 1,
pp. 75-87, 1998.
[2] H. I. Krebs, J. J. Palazzolo, L. Dipietro, M. Ferraro, J. Krol, K.
Rannekleiv, B. T. V olpe, and N. Hogan, ”Rehabilitation robotics:
performancebased progressive robot-assisted therapy,” Autonomous
Robots, V ol. 15, No. 1, pp. 7-20, 2003.
4598
?0.2 ?0.1 0 0.1
0.35
0.4
0.45
0.5
0.55
0.6
0.65
r
1
(m)
r
2
(m)
 
 
actual
bound
Human region
Initial
position
f
e
Robot region
(a) Path of end effector in Cartesian space
?5 0 5
?6
?4
?2
0
2
4
6
x
I1
(Volt)
x
I2
(Volt)
 
 
actual
desired
bound
Desired
trajectory
Human region
Robot region
(b) Path of end effector in image space
5 10 15 20 25 30
?5
?4
?3
?2
?1
0
1
2
3
4
5
time (s)
Error (Volt)
 
 
x
I1
x
I2
(c) Tracking errors
Fig. 3. Experiment 1: Human exert an external force fe on the end effector and guide it to transit from the human region to the robot region. After the
end effector enters the robot region, the vision-based control is activated and drives the end effector to the desired position.
?0.8 ?0.6 ?0.4 ?0.2 0 0.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
r
1
(m)
r
2
 (m)
 
 
actual
desired
bound
Human
region
initial position
Robot region
f
e
(a) Path of end effector in Cartesian space
0 2 4 6 8
?0.4
?0.2
0
0.2
0.4
0.6
time (s)
Error (m)
 
 
r
1
r
2
region bound
External force f
e
(b) Position errors
Fig. 4. Experiment 2: The end effector is controlled to move from the
robot region to the human region with Cartesian-space feedback only, and
then human exert an external force fe on the end effector and guide it to
move to the desired position after it enters the human region.
[3] H. I. Krebs, B. V olpe, D. Williams, J. Celestino, S. Charles, D. Lynch,
and N. Hogan, ”Robot-aided neurorehabilitation: a robot for wrist
rehabilittion,” IEEE Trans. Neural Syst. Rehabilitation Eng., V ol. 15,
No. 3, pp. 327-335, 2007.
[4] H. Kazerooni, J. L. Racine, L. Huang, and R. Steger, ”On the control
of the Berkeley Lower Extremity Exoskeleton (BLEEX),” IEEE Int.
Conf. Robotics Automat., pp. 4353-4360, 2005.
[5] A. M. Dollar, and H. Herr, ”Lower extremity exoskeletons and active
orthoses: challenges and Stage-of-the Art,” IEEE Trans. Robotics,
V ol. 24, No. 1, pp. 144-158, 2008.
[6] M. Takegaki, and S. Arimoto, ”A new feedback method for dynamic
control of manipulators,” ASME J. Dynamic Syst. Meas. Control, V ol.
102, pp. 119-125, 1981.
[7] J. J. E. Slotine, and W. Li, ”On the adaptive control of robot
manipulators,” Int. J. Robotics Res., No. 6, pp. 49-59, 1987.
[8] G. Niemeyer, and J. J. E. Slotine, ”Performance in adaptive manip-
ulator control,” Int. J. Robotics Res., 10(2), 149-161, 1991.
[9] C. C. Cheah, M. Hirano, S. Kawamura and S. Arimoto, ”Approximate
Jacobian control for robots with uncertain kinematics and dynamics,”
IEEE Trans. Robotics Automat., 19(4), 692-702, 2003.
[10] W. E. Dixon, ”Adaptive regulation of amplitude limited robot ma-
nipulators with uncertain kinematics and dynamics,” IEEE Trans.
Automatic Control, V ol. 52, No. 3, pp. 488-493, 2007.
[11] C. C. Cheah, C. Liu, and J. J. E. Slotine, ”Adaptive tracking controls
for robots with unknown kinematics and dynamic properties,” Int. J.
Robotics Res., V ol. 25, No. 3, pp. 283-296, 2006.
[12] C. C. Cheah, D. Q. Wang, and Y . C. Sun, ”Region-reaching control
of robots,” IEEE Trans. Robotics, 23(6), 1260-1264, 2007.
[13] X. Li, and C. C. Cheah, ”Global task-space adaptive control of robot,”
Automatica, V ol. 49, No. 1, pp. 58-69, 2013.
[14] S. Haddadin, A. Albu-Schaffer, A. De Luca, and G. Hirzinger,
”Collision detection and reaction: a contribution to safe physical
human-robot interaction,” IEEE Int. Conf. Robotics Automat., pp.
3356-3363, 2008.
[15] Y . Yamada, Y . Hirasawa, S. Huang, Y . Umetani, and K. Suita,
”Human robot contact in the safeguarding space,” IEEE/ASME Trans.
Mechatronics, V ol. 2, No. 4, pp. 230-236, 1997.
[16] G. Palli, C. Melchiorri, and A. De Luca, ”On the feedback lineariza-
tion of robots with variable joint stiffness,” IEEE Int. Conf. Robotics
Automat., pp. 1753-1759, 2008.
[17] A. De Luca, F. Flacco, A. Bicchi, and R. Schiavi, ”Nonlinear
decoupled motion-stiffness control and collision detection/reaction
for the VSA-II variable stiffenss device,” IEEE Int. Conf. Robotics
Automat., pp. 5487-5494, 2009.
[18] A. Bicchi, and G. Tonietti, ”Fast and ”soft-arm” tactics [robot arm
design],” IEEE Robotics and Automation Magazine, V ol. 11. No. 2,
pp. 22-33, 2004.
[19] E. D. Fasse, and N. Hogan, ”Control of physical contact and dynamic
interaction,” Int. Symp. Robotics Res., 1995.
[20] N. Hogan, ”Impedance control: an approach to manipulation,” ASME
J. Dynamic Syst. Meas. Control, V ol. 107, pp. 1-24, 1985.
[21] N. Hogan, and S. P. Buerger, ”Impedance and interaction control,”
Robotics and Automation Handbook, CRC Press, 2001.
[22] C. C. Cheah, and D. Wang, ”Learning impedance control for robotic
manipulators,” IEEE Trans. Robotics Automat., V ol. 14, No. 3, pp.
452-465, 1998.
[23] J. A. Saglia, N. G. Tsagarakis, J. S. Dai, and D. G. Caldwell, ”Control
strategies for ankle rehabilitation using a high performance ankle
exerciser,” IEEE Int. Conf. Robotics Automat., pp. 2221-2227, 2010.
[24] F. Petit, and A. Albu-Schaffer, ”Cartesian impedance control for A
variable stiffness robot arm,” IEEE/RSJ Int. Conf. Intelligent Robots
Systems, pp. 4180-4186, 2011.
[25] E. Gribovskaya, A. Kheddar, and A. Billard, ”Motion learning and
adaptive impedance for robot control during physical interaction with
humans,” IEEE Int. Conf. Robotics Automat., pp. 4326-4332, 2011.
[26] J. Zhang, C. C. Cheah, and S. H. Collins, ”Stable human-robot
interaction control for upper-limb rehabilitation robotics,” IEEE Int.
Conf. Robotics Automat., pp. 2201-2206, 2013.
[27] S. Arimoto, Control Theory of Non-Linear Mechanical Systems.
Oxford University Press, 1996.
[28] F. L. Lewis, C. T. Abdallah, and D. M. Dawson, Control of Robot
End-effectors. New York: Macmillan Publishing Company, 1993.
[29] M. W. Spong, and M. Vidyasagar, Robot Dynamics and Control. New
York: John Wiley & Sons, 1989.
[30] J. J. E. Slotine, and W. Li, Applied Nonlinear Control. Englewood
Cliffs, New Jersy: Prentice Hall, 1991.
[31] http://www.jr3.com/index.html
4599
