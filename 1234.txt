 
the starting trigger, arising up a hand, and upper body motion 
data observed by Kinect sensor are extracted as the data 
sequence for HMM motion estimator. The data sequence is 
terminated when the hand of the observed person is put down 
or 30 frame data are obtained.  
 
Fig.5 GUI Window Developed for Upper Body Gesture Observation 
and Estimation 
In the observed data vector O[t] = [o
1
; o
2
; o
3
; o
4
; o
5
; o
6
], o
1
 
denotes facing angle of tracked skeleton, o
2
 and o
3
 denote 
angle of shoulder and elbow joint respectively, o
4
, o
5
 and o
6
 
are X, Y and Z components of hand speed respectively.  
Information of the facing angle is used for estimating the 
degree of human intention on communicating to the robot. It is 
easy to know that shoulder and elbow joint angles are tightly 
related with representations of human gestures for 
communication. Also both angles are able to be obtained from 
skeleton tracking data. Finally, the data set of 3D vector 
velocity of human’s hand is one of the most important 
parameters for identifying human gestures in communication 
or actions on performing some tasks.  
For estimating human intention on commanding and 
controlling robots, five gesture actions: Pointing Position(?
1
), 
Trajectory Pointing(?
2
), Beckoning(?
3
), Retire(?
4
), and 
Stop(?
5
) are defined and implemented in the upper body action 
estimator. Table I shows results of HMM training for these 
five basic actions and other five actions as a control group. As 
the result, to top five basic actions, symbol values log(O/?) of 
the main diagonal are in order of 23, and other values are 
larger than 100.  To other five actions different from trained 
gestures, symbol values are over 100 also. 
Table 1 Result of log(O|?) for HMM based Upper Body Gesture Estimation 
 
Fig.6 shows the five upper body gestures in 3 dimension 
symbol space: Pointing Position (large blue ball), 
Beckoning(large green ball), and Stop(large green ball), 
Trajectory Pointing(middle size blue ball), Retire(middle size 
yellow ball), as well as other untrained action represented as 
five small balls. In the symbol space, the first dimension 
corresponds to the element of low speed of hand motion, the 
second dimension corresponds to the element of high speed of 
hand motion, and the third dimension corresponds to the 
element of the angle of human’s hand.  
 
Fig.6 Symbol Space of Upper Body Gesture 
B. Estimation of Lower Body Motions based on Motion 
Transition 
Observations of lower body motions are performed by 
using two LRF sensors, the sampling rate is 50ms. The 
motion data is generated from three scans of target subjects 
with a tracking algorithm implemented, and motion data is 
obtained in every 150ms and is store in the human motion 
map at every 500ms. Fig.7 is a visualization result of lower 
body observation. 
 
Fig. 7 Example of Observation Result of Lower Body Motion 
The observation data vector of lower body O[t] = [o
1
; o
2
] 
consists of two symbol elements: moving distance of the 
lower body observed (o
1
), and relative distance between body 
and leg which is used for checking the sitting posture ( o
2
). In 
the estimator of lower body motion, body motion patterns are 
defined as three basic motions (Walking, Standing and 
Sitting) and 6 motion transition patterns between two of three 
basic motion elements. Table 2 shows the HMM training 
result of motion transition of lower body. As a result, symbol 
values of the main diagonal are in order of 7, and other values 
are larger. Within them, 6 values are in value around 8 and 9 
which indicates similar motion pattern in the symbol space, 
and many other are over value of 100.  
 ?
1
 ?
2
 ?
3
 ?
4
 ?
5
 
O1: Pointing Position 23.3 118.0 321.4 211.3 167.8 
O2:Trajectory Pointing 118.0 23.2 233.1 301.1 229.5 
O3: Beckoning 247.0 153.1 23.1 251.6 310.1 
O4: Retire 258.0 375.0 272.8 23.2 393.1 
O5: Stop 226.3 183.8 80.2 221.0 23.2 
O6: Waving Hand (S) 193.6 133.5 93.7 215.0 232.4 
O7: Waving Hand (L) 294.6 228.6 240.0 224.1 210.3 
O8: Deskwork 111.7 100.7 175.3 223.4 126.1 
O9: Picking Object 300.7 369.1 437.3 343.4 352.2 
O10: Putting Object 198.8 143.8 151.8 190.4 209.5 
3702
 
Transition Map which denotes motion transition from Walking 
to Standing in blue, and motion transition from Standing to 
Walking in red respectively.  
 From Human Motion Map data, a geographical trajectory 
pattern of straight walking in a corridor and a trajectory 
pattern connecting from two directions in front of the door are 
easily been confirmed. Also from Human Motion Transition 
Map, we can easily confirm that motion transition between 
Walking and Standing happen in front of and in the door way. 
This indicates the motion patterns of body motion when 
subjects are opening a door and changing in-room shoes. This 
kind of motion behavior is related strongly with objects in 
particular location such as, doors, tables and chairs. 
B. Geographical Clustering and Human Motion Behavior 
Analyzing  
For analyzing human motion behaviors and geographical 
relationship, we developed a K-means method based 
geographical clustering package and implemented into the 
Human Motion Mapping system. The geographical clustering 
algorithm is applied to Human Motion Map and Motion 
Transition Map data generated in the previous sub-session. 
The results of clustering in 1-8 clusters are shown as different 
color regions and center positions of clusters in Fig.11 and 
Fig.12. In Fig.11-(a), lines connected between centers of 
neighbor clusters shows walking direction. However, at the 
place near the door, lines with big direction changing angle 
show feasible walking direction, lines with relatively small 
direction changing angle are not feasible, and cannot be used 
for extracting walking behaviors. Results of Fig.11-(b) 
indicate that regions of clusters with small clusters numbers 
(3 in this map) may overlap some areas where people cannot 
walk through, and regions of clusters with a certain large 
number (4 and more clusters) will represent walking regions 
correctly. Also it can be confirmed that sharp and size of the 
cluster’s regions in corridor change much when the cluster 
numbers are change from 4 to 8, but clusters near the door are 
not showing so much changes. Fig.12 shows not only that 
motion transitions between Walking and Standing motions 
mostly are observed near the door, but also two transitions: 
Walking to Standing, and Standing to Walking, are both with 
two regions with center positions are located alternatively on 
the walking direction. This result of geographical clustering 
indicates that human behavior near the door usually switch 
their walking motion for opening a door then changing their 
shoes to in-room shoes, a “Japanese life style”. Therefore, as 
showing by this example, many human behaviors at particular 
locations or around some particular objects can be extracted 
and labeled by applying a computational geographical 
clustering algorithm to Human Motion Map and Human 
Motion Transition Map when we collected enough data.   
C. Experiments in Various Indoor Environments 
Several experiments were conducted in the 18th floor of the 
Building 2 at the CIT (Fig.13), and we especially paid 
attention to obtain motion data in 7 areas where environments 
have different characteristics and persons may show different 
motion behaviors. Multiple subjects were showing daily 
gestures in these areas.  
 
Fig. 13  Experiment Environment at 18th Floor of CIT Building 2
     
(a)  Center Positions of Clusters        (b) Regions of Clusters 
Fig. 11  Results of Geographical Clustering of Human Motion Map   
    
(a)  Center Positions of Clusters        (b) Regions of Clusters 
Fig. 12  Result of Geographical Clustering of Motion Transition Map 
 
3704
