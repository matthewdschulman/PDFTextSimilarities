Experimental Evaluation of Contact-Less Hand Tracking Systems for
Tele-Operation of Surgical Tasks
Yonjae Kim, Peter C. W. Kim, Rebecca Selle, Azad Shademan, and Axel Krieger
Abstract—
This paper reports an evaluation of contact-less hand track-
ing sensors for the use of tele-operation, in particular for
surgical robotics applications. Two hand tracking systems
are investigated: 3Gear Systems interface with the Microsoft
Kinect
TM
sensor, and the Leap Motion sensor system. This
paper reports an experimental evaluation and comparison of the
two systems range, static positioning error, trajectory accuracy
of single ﬁnger and hand motions, and latency. Latency and
trajectory accuracy were found superior using the Leap system.
Kinect
TM
/3Gear was found superior when larger range and
gesture control are necessary. 3Gear was used in a simulated
surgical positioning task and demonstrated an average trans-
lational accuracy of 6.2mm. Given the data we have collected,
we conclude that neither system, at present, possesses the high
level of accuracy and robustness over the required range that
would be a prerequisite for use as a medical robotics master.
I. INTRODUCTION
There are three major limitations prevalent in current
mastercontrollersforsurgicalrobots:1)theirlargefootprint;
2) they are not sterilizable, preventing smooth transition
between manual and robotic operation; 3) the mechanical
linkage limits range of motion and can be cumbersome to
use. As contact-less hand tracking sensors have the potential
to remove these shortcomings, this paper seeks to perform
evaluation of hand tracking systems for use as masters
in medical robotics. In particular, this paper reports an
experimentalevaluationandcomparisonfortwonewpromis-
ing hand tracking systems: The Microsoft Kinect
TM
sensor
(Microsoft Corporation, Redmond, WA) equipped with the
3GearSystems
1
(3GearSystemsInc.,SanFrancisco,CA)en-
vironmentforhandposedetectionandtracking,andtheLeap
Motion Controller
2
(Leap Motion, San Francisco, CA) with
its native ﬁnger tracking software. Effective sensor range,
static positioning error, and trajectory accuracy of ﬁnger and
hand motions, and latency are reported for both systems.
These variables were chosen based on their relevance in
functioning as a surgical robotic master. The 3Gear hand
trackingsystem was furtherevaluatedin a simulated surgical
laparoscopic positioning task.
A. Prior Art
The current state of the art in surgical robotics is based
on a master-slave control paradigm, best represented by
Authors are with the Sheikh Zayed Institute for Pediatric Surgical
Innovation, Children’s National Medical Center, 111 Michigan Ave. N.W.,
Washington, DC20010.{yokim, pkim, rselle, ashadema,
akrieger}@cnmc.org
1
http://www.threegear.com/
2
https://www.leapmotion.com/
the da Vinci
TM
robot (Intuitive Surgical Inc, Sunnyvale,
CA) [1]. The da Vinci
TM
robot is used for a wide range of
surgical procedures including urology, gynecology, cardio-
thoracic, and general surgery. MiroSurge is another general-
purpose medical robotic system developed at the Institute
of Robotics and Mechatronics, German Aerospace Centre
(DLR)[2].Anothermaster-slavesystemistheRavensurgical
robot developed at the University of Washington [3]. These
medicalrobots(slaves)arecontrolleddirectlybythe surgeon
operating a master console.
For medical robotics to continue to evolve and gain ac-
ceptance in more applications,developersshould continueto
explore promising novel technologies to adapt and integrate
into medical platforms. Among these, hand tracking as
master control is especially appealing for medical robotic
application because its contact-free operation allows for
interaction without compromising sterility and its smaller
footprint could allow the surgeon to use the robot at the
patients bedside.
Kinect was originally designed for whole-body tracking
for indoor gaming applications, but over time researchers
have attempted to use the Microsoft Kinect
TM
for general
robotic and virtual reality control [4], [5] and the sensor
speciﬁcations of the Kinect
TM
have been analyzed by
severalauthors.Khoshelham[6]characterizesdepthdataand
shows that error in measured depth increases with increasing
distance to the sensor from a few millimeters to a few
centimeters over the range of the sensor. Dutta [7] reports
root-mean-squared errors in coordinate measurements to be
6.5 mm and 5.7 mm in planes perpendicularto the view axis
and 10.9 mm along the view axis (depth). Kvalbein [8] also
analyzes depth and planar accuracy.
The Kinect
TM
has been used in several robotic appli-
cations including Human-Robot-Interaction (HRI) interface
design. In the majority of the Kinect
TM
-based HRI lit-
erature, the whole-body human skeleton is tracked using
the OpenNI
3
and Microsoft Kinect
4
Software Development
Kits (SDK) (e.g. Perez et al. [9] for a recent HRI interface
basedonKinect
TM
). Insteadof trackingwhole-bodymotion,
an ideal contact-less interface in an operating room (OR)
would require minimal gross motions, utilizing ﬁne motions
and gestures to perform the necessary task. Hand tracking
appears to ﬁt these criteria. Researchers have previously
used the Kinect
TM
sensor to implement hand tracking [4],
3
http://www.openni.org/
4
http://www.microsoft.com/en-us/kinectforwindows/
develop/overview.aspx
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3502
Fig. 1. Schematic drawing of the setup for accuracy evaluation.
and hand gesture tracking (e.g. Ren et al. [10] and Oikono-
midis et al. [11]).
This paper reports an evaluation of two promising new
hand tracking systems for potential use in medical robotics:
the 3Gear System and the Leap Motion. 3Gear System is
a hand and gesture tracking system developed by 3Gear
Systems, using the Kinect
TM
as depth camera. The Leap
is a new sensor developed by Leap Motion, Inc. with ﬁnger
and hand tracking available natively.
This paper provides the research and development com-
munity with preliminary data to assist in incorporatingthese
and similar hand-based motion controllers into their robotic
applications.
II. METHODS
A. Robotic Testbed
In order to test the positional consistency and accuracy of
the sensor systems, an artiﬁcial hand was used rather than
a human hand to produce precise motions and trajectories.
A wooden right hand mannequin (CW301, Alvin and Co,
Windsor, CT) was mounted onto a KUKA lightweight robot
(LWR) (KUKAAG,Augsburg,Germany).TheKUKA LWR
robots manual lists a positional repeatability of±0.05 mm,
and its base frame coordinates were used as the standard
reference for the two systems’ movement data. A nitrile
glove was placed on the wooden hand to emulate a surgeons
handandtoremoveerrorcausedbyshapedifferencebetween
the wooden and human hand. The hand was mounted onto
the robot so that the back of the hand would face the active
sensor. This was done because an increase in noise was
observed due to the shape of the wooden hand on the front
side, particularly a steep angle in the distal ﬁnger joint.
The ﬂatter backside of the wooden hand allowed for cleaner
sensordataapproximatingahumanhand.Thehandwasﬁxed
into a pointing pose (see Figure 2) for all tests because both
systems had difﬁculty detecting the ﬁngers that were tightly
Fig. 2. Photo of the experimental setup showing KUKA arm with the hand
and arm sleeve. (Left) Kinect
TM
sensor equipped with the 3Gear Systems
software. (Right) Leap Motion sensor secured to the table.
Fig. 3. Drawing of end points of trajectories in the workspace.
spaced when extended. A wooden left hand was mounted in
the ﬁeld of view for the tests using 3Gear because the 3Gear
systems recommended usage is with two hands.
Figure 1 shows a schematic drawing of the general test
setup and depicts the orientation of the sensors in respect to
the KUKA. Figure 2 shows the experimental setup. For the
tests, the robot was programmed to move linearly between
points in the cubical workspace shown in Figure 3. The
cube’s sides are formed by axial translations by the KUKA
between the Cartesian values described in Table I, and the
length of the sides are 125mm?150mm?150mm. Points
1-3/A-C were placed to approximately line up with the
centralYZ-planeofthesensors,andpoints4-8/D-Hoccupied
the operators right side of the sensors. Assuming that the
sensors’ performancesare approximatelysymmetrical across
their YZ-plane,the size of the evaluatedarea would be about
125mm?300mm?150mm, making it approximately half
of the da Vinci
TM
master’s maximum range (measured to
be approximately 250mm?650mm?300mm). The Leap
3503
Fig. 4. 3D plots of axial trajectories measured by Leap and 3Gear.
KUKA frame x
min
xmax y
min
ymax z
min
zmax
3Gear 400 525 100 250 550 700
Leap 450 575 100 250 700 850
TABLE I
THE LOCATIONS OF THE AXIAL AND DIAGONAL TRAJECTORY
ENDPOINTSINKUKA COORDINATE FRAME.
was placed approximately 17.5cm below point 2, and the
Kinect
TM
was mounted approximately67cm above point 2.
B. Contact-less Hand Tracking Systems
The Leap Motion Controller hardware tested in these
experimentswasthedeveloper’sversionv.06.5withsoftware
v0.7.4. The Leap API implements a listener class that re-
ceives a frame containing the positions and orientation of all
pointersandhandsdetectedbytheLeap.Thisisapre-release
hardware and software for developers. The 3Gear system
was tested using a single Kinect
TM
for Windows sensor
with the 3Gear software version v0.9.21, also a pre-release.
The 3Gear API analyzes each frame of the Kinect
TM
and
constructs multiple messages with different sets of data that
consistofjointframe,position,andorientationofﬁngersand
hands. The API then sequentially receives these messages
througha listener class. Both the Leap and the 3Gear system
return pointer and hand objects, where a pointer contains
the 3D position, yaw, and pitch of the tip of the index
ﬁnger (and any other elongated object in the ﬁeld for the
Leap), and the hand contains the 3D position, yaw, pitch,
and roll of the palm or back of the hand. Thus, ﬁngers are
tracked with 5 DOF, and hands are tracked with 6 DOF.
To collect data, we developed recording programs to store
every frame received from the listener class, and the data
was laterextractedforanalysisusingMATLAB(Mathworks,
Natick, MA). The 3Gear system required calibration before
usage. A calibration pattern
5
was placed approximately 2.5
feet below the Kinect
TM
to follow the recommended setup.
For use with the robotic system, human hands were used
for shape calibration, as the wooden mannequins ﬁngers
could not be spread as needed. After shape calibration,
ﬁnger scale parameters were found in the calibration ﬁles
5
http://www.threegear.com/latest/doc/4x5-1.5.pdf
and proportionally modiﬁed to reﬂect the longer ﬁngers of
the manikins. The mannequin was then shaped into the 6
poses utilized by the 3Gear system (pinch, OK, handshake,
microphone,L, and pointing)to calibratethe poses. Through
all tests, only one sensor was active in the ﬁeld to prevent
interference.We tested both systems for a numberof criteria
that motion-control systems should fulﬁll to be acceptable
for use as a medical robot master.
C. Sensor Range
The largerthe ﬁeld of view or rangeof a sensor,the easier
a system can track hand movements made during surgery
without the need for clutching. The operating range for both
systems was experimentally estimated by moving the hands
slowly towards the edge of the sensors’ range until the data
showed either: 1) a quick, signiﬁcant, persistent change in
orientation; 2) visibly signiﬁcant increase in frequency and
amplitude of noise; or 3) complete or frequent loss of hand
tracking. The angular ranges of both systems were also
estimated by slowly rotating a hand at the center of the
workspaceusingyaw,pitch,androllmotionsuntilthesystem
visibly ceased to track the orientation correctly.
D. Static Position Error
Medical robot masters should have as little noise interfer-
ing with its operation as possible to prevent the slave from
executing an involuntary motion and to reduce the need for
ﬁlteringthat mayreducethe responsivenessofthe system. In
order to test the noise produced by the sensor in absence of
movement,thewoodenhandwasheldbytherobotatpoint2,
in the middle of the sensors’ range, and the sensor data was
collected for ten seconds. To test the practical resolution of
thesystems, therobotwas movedfrompoint2incrementally
+1 mm in each axial direction and the recording program
storedall incomingdataat eachpointfor10secondsinorder
to measure static position error. The recorded point cloud
at eleven static positions was recorded with each sensor.
Standard deviationsof the static position errors in x, y, and z
were calculated for hand and pointer position for the eleven
positions for both sensors.
E. Trajectory Error
A human operator is able to correct for small distance er-
rorsandscalingthatmaybepresentinaroboticsystemusing
visual feedback. Therefore, rather than precise displacement
accuracy, motion trajectory tracking and consistency is more
important as it provides the surgeon with intuitive control
of the robot. To test if the displacement and trajectory
sensing would hold through movements in different areas
of the workspace, the robot was used to move the hand
linearly,while maintaininga single rotational frame, through
18 axial trajectories as shown in Figure 4. Each trajectory
was traveled 5 times back and forth at a speed of 0.02m/s.
Positional errors were calculated for the movements in form
of absolute distance between end points and in deviation of
points to the line through endpoints. The positional errors
were calculated for pointer and hand positions for both
sensors.
3504
Fig.5. 3Dplotsofeleven static point cloudsalong adiagonal. Thetopmost
point cloud was gathered at point 2 according to Figure 3. Each subsequent
point cloud was gathered at +1mm x +1mm x +1mm in KUKA coordinates
away from the previous point.
F. Latency
Latency is an important factor for surgical robotics, since
surgeonsneedtoreactquickly,inparticulartotargetsthatare
oftenmobileinamoving,breathingpatient.Thelatenciesfor
both systems were experimentally measured. The recording
programstored a computertimestamp alongwith eachframe
and message as it recorded a ﬁnger clicking on a mouse but-
ton that was oriented perpendicularly to the sensor’s z-axis.
The cursor of the mouse was placed over a Qt-framework
QPushButton
6
that would store the computer timestamp on
button release. We estimated latency by analyzing the delay
between the click timestamp and the frame where movement
away from the mouse is detected. The pointer z-value of
each frame was plotted against time, and a line was ﬁtted
into points before and after each mouse-click timestamp.
The timestamp of the frame/message that was closest to the
intersectionofthetwolineswas chosenastheresponsetime.
G. Surgical Positioning Task
Four trained surgeons from Childrens National Medical
Center operated a robotic simulation using the 3Gear Kinect
systemtofurtherinvestigatethecapabilitiesandpotentialsof
the 3Gear Kinect control system in medical robotics. 3Gear
was selected over Leap for this test due to its larger range
and gesture control capabilities.
The simulation was designed in Robot Operating System
(ROS)
7
. Figure 6 on the left displays the simulated robot,
whichis modeledafteranexperimentallaparoscopicsurgical
robot used at Childrens National Medical Center [12]. It
consists of a KUKA LWR robot arm and a custom designed
6
http://qt-project.org/
7
http://wiki.ros.org/rviz
articulated stapling tool. The simulated robot has nine DOF
in total, seven in the KUKA LWR arm and two additional
+/- 90 degrees rotational DOF in the tool. The tip of the
tool consists of a pinching mechanism, a needle driver, and
a stapling mechanism. The robot was situated as if mounted
toanoperatingtableoverapatient.The3GearKinectcontrol
system was used to track the surgeons right hand and move
the robot simulation accordingly in translation and rotation.
Additionally, the tool was constrained to a remote center of
motion (RCM) representing a laparoscopic entry port. The
surgeons were asked to place staples at random locations
within the workspace of the robot. A target, a translucent
copy of the end piece of the robot tool encased in a
translucent orange cylinder, was displayed in the simulation.
Thesurgeonsgoalwastomanipulatetherobotsothattheend
piece of the robot aligned with the target in translation and
rotation.Once the surgeonwas satisﬁed with the positioning,
they pressed a foot pedal, imitating placement of a staple.
Once the foot pedal was pressed, a new target would appear.
The surgeonthen tried to align the robotwith the new target.
The experiment consisted of thirteen targets, the ﬁrst three
for practice, and the following ten to be recorded to measure
accuracy and time needed to reach each target. Figure 6
on the right displays the tool tip and shows the accuracy
measurements for the experiment. Each time the foot pedal
was pressed the transformationmatrixof the robottool tip to
the target tool tip was recorded.The magnitude of the vector
fromtherobottooltiptothetargettooltipisthetranslational
error. Roll, pitch, and yaw angles were determined from
the rotational matrix of the recorded transformation matrix
according to Figure 6.
III. EXPERIMENTAL RESULTS
Experiments were designed to assess contact-less motion
control of surgical robots. The criteria that we consider
important for such a system include range of motion and
accurate 6 degrees-of-freedom (DOF) tracking to control
a surgical tool as a master. The accuracy and precision
of positioning is particulary important because of the ﬁne-
motion requirements of surgical tools and the potential harm
surgical robots can inﬂict.
Since the surgeon must be able to respond quickly to the
changing surgical environment, we also consider low laten-
cies to be necessary. In this section, we report performance
results of the two sensor systems under these criteria.
A. Range Results
The approximate effective work space of Leap and 3Gear
are depicted in Figure 7. Table II lists the approximate XZ
ranges at the lower, maximum, and upper Y values, where
the maximum corresponds to the height with greatest XZ
range. The angular ranges in the center of the workspace are
shown in Table III. Assessing the effective working range,
the 3Gear system had a signiﬁcantly larger work volume
with a maximum XZ range of 780mm?700mm with a Y
range of 900mm, compared to the maximum XZ range of
350mm ?270mm with Y range of 275mm for Leap. The
3505
Fig. 6. Screenshots of the rviz simulation program: Picture of the simulated robot (left) for the surgical positioning task. Picture displaying the tool tip
with error measurements (right).
Fig. 7. Approximate effective workspace ranges of 3Gear and Leap.
3Gear system also had a larger overall angular range, having
a smaller pitch rangeof±40 comparedto Leap’s pitch range
of±50, but having a larger yaw and roll ranges of±60 each
over Leap’s yaw and roll ranges of ±40 each. In addition,
the Leap was unable to differentiate between the palm and
the back of the hand, while the 3Gear system was able to
do so, effectively doubling 3Gear’s unique roll measurement
range while the Leap does not.
B. Static Position Results
Figure 5 shows a 3D plot of the hand and pointer coor-
dinates measured by 3Gear and Leap for eleven consecutive
static points along a diagonal.
Table IV lists the average XYZ standard deviations for
each system. The standard deviations of Leap’s static data
were signiﬁcantly lower than 3Gear’s, with Leap’s axial
standard deviations ranging from 0.01mm to 0.04mm com-
pared to 3Gear’s axial standard deviations that range from
0.35mm to 1.66mm. The elongated nature of 3Gear’s point
cloud was most likely caused by 3Gear’s hand tracking
algorithm rapidly switching between extending and ﬂexing
the non-pointer ﬁngers when the hand was still, causing the
measured index ﬁnger and hand position to shift back and
forth erroneously. To test this, ellipsoids were ﬁtted into
3Gear’s point clouds, and the average of their major axis
vectors was found to be only 3.2 degrees different from the
direction vector of the pointer, supporting this theory.
Y [mm] X Range [mm] Z Range [mm]
Leap
Y
lower
50 80 90
Ymax 250 350 270
Yupper 325 200 200
3Gear
Y
lower
-550 1150 552
Ymax 0 780 700
Yupper 350 275 343
TABLE II
APPROXIMATEXZ RANGES AT DIFFERENT Y VALUES (HEIGHT) FOR
LEAP AND 3GEAR.Ymax IS THE Y VALUE WITH THE LARGESTXZ
RANGES.
3506
C. Linear Translation Results
The point clouds generated by each motion were in-
spected, and the points corresponding to the pauses in the
robots movements at each endpoint were identiﬁed. The
averages of these endpoint sets were used to construct a
line that represents the ideal linear trajectory between the
measured endpoints. The deviation of the non-endpoint sets
of each axial trajectory for the large movements are plotted
in Figure 8. The linear trajectories are labeled according
to their respective endpoints according to Figure 3. For
example, a trajectory along the X axis from point A to
point 2 is called x
A1
. The analysis data for combined axial
movements are listed in Table V. Both systems’ deviations
varied signiﬁcantly depending on the location of the tra-
jectory in the workspace. In most trajectories, the 3Gear
systemhadgreaterdeviationsthanLeapforbothpointersand
hands, with 3Gear’s mean deviations rangingfrom 2.2mmto
4.8mm, while Leap’s mean deviations ranged from 0.37mm
to 2.76mm. However, Leap’s hand deviation showed much
greater outliers in certain trajectories, going as high as
455.2mm, compared to the 3Gear’s maximum of 22.2mm.
Leap’s pointer tracking did not have this problem, as its
maximum deviation was only 6.7mm compared to 3Gear
pointer’s maximum deviation of 17.1mm. This was likely
caused by the fact that the trajectories got too close to the
edge of the effective workspace ranges of the Leap hand-
tracking algorithm, as seen by the fact that Leap was not
able to track the hand for parts of a few trajectories.
Overall, we measured lower errors in trajectory distance
with 3Gear, which had a minimum average distance error of
-0.2mm and the maximum distance error of 8.1mm, while
the Leap had a minimum distance error of -0.5mm and a
maximumdistanceerrorof-19.2mm.ThissuggeststhatLeap
requires calibration to better match its coordinates to real-
world distances.
D. Latency Results
TableVI showsthe latencymeasurementresults.Leaphad
a lower latency of 34.9 milliseconds compared to 3Gear’s
Sensor Yaw Pitch Roll
Leap ±40 ±50 ±40
3Gear ±60 ±40 ±60
TABLE III
APPROXIMATEEFFECTIVE ANGULAR RANGES OF3GEAR AND LEAP IN
DEGREES.
Pointer Hand
Sensor zstd xstd ystd zstd xstd ystd
3Gear 1.66 0.35 0.61 1.06 1.27 0.84
Leap 0.02 0.01 0.03 0.04 0.01 0.01
TABLE IV
AXIAL STANDARD DEVIATIONSIN MM OF THE STATIC POINTER AND
HAND POSITIONSMEASURED BY 3GEAR AND LEAP.
latency of 74.9 milliseconds. Leap also had a much greater
frame rate of 111.4 frames per second over 3Gear’s 30.0
frames per second.
E. Surgical Positioning Results
Time andaccuracyresults ofthe foursurgeonsperforming
the surgical positioning task in form of translational and
angular positioning errors are displayed in Table VII. The
average tanslational error was 6.2mm and angular errors
were 8.2, 8.1, and 33.7 degrees for yaw, pitch, and roll
respectively. The large errors in roll were most likely due to
the difﬁculty to visually discern the target roll on the screen.
The four surgeons needed an average of 50.4 seconds to
position the simulated tool per target. Surgeons cited limited
training on the system and poor visualization and lack of
depth perception as contributors to errors.
IV. CONCLUSION AND DISCUSSION
This paper reportedan evaluationoftwo contact-less hand
tracking systems Leap Motion controller and the Kinect-
based3Gearsystemtoassesstheirpotentialforuseincontrol
of medical robots.
The range of 3Gears allows enough space to make the
samemotionssurgeonswouldmakeonthedaVinci’smaster.
3Gear’slongy-axisrangealso allowsmountingofthe sensor
away from the surgeon, eliminating issues with sterility. The
Leap’s range was signiﬁcantly shorter, which also increases
the possibility that a surgeon may accidentally touch the
device during use. The larger angular range of the 3Gear
system also allows for the surgeon to control the pose of the
robotic tool more freely compared to Leap.
Leap’s static noise for both pointer and hand, measured in
the center of the sensor range, was signiﬁcantly lower than
3Gear’s. This could potentially allow for very ﬁne tool posi-
tion control that would not be possible with 3Gear’s system.
However, when considering movements further away from
thecenterofthesensor’s range,errorsincreasedsigniﬁcantly
for both sensors.
Overall, we measured lower errors in trajectory distance
with 3Gear compared to Leap, suggesting that Leap requires
calibration to better match its coordinates to real-world dis-
tances. In terms of deviation away from the trajectory, both
systems’ deviations varied signiﬁcantly depending on the
location of the trajectory in the workspace. While generally,
3Gear showed larger deviations compared to Leap, Leap’s
measured hand data had much greater deviations in some
speciﬁc trajectories compared to any deviations produced by
3Gear.
Angulardegreesoffreedomare measuredmoreaccurately
with Leap compared to 3Gears. Finger/pointer are more
Sensor Avg. Frame Rate (fps) Avg. Latency (ms)
Leap 111.4 34.9
3Gear 30.0 74.9
TABLE VI
FRAME RATE AND LATENCY MEASUREMENTSFOR3GEAR AND LEAP.
3507
Fig. 8. Box plot of pointer and hand measurement deviation from linear trajectories. *Leap had two outlier deviations of 455.17mm and 225.24mm in
Z
1,2
that are not displayed in the boxplot. **End points could not be obtained for marked Leap trajectories due to hand tracking loss near one of the
endpoints.
Pointer Hand
Direction Sensor ? ? m d ? ? m d
z
3Gear 2.24 0.96 6.33 -2.09 3.59 1.73 12.38 -0.43
Leap 0.38 0.36 2.52 -9.01 0.37 2.85 455.17 -15.02*
x
3Gear 2.24 1.68 11.60 -1.19 4.65 1.77 10.96 8.07
Leap 0.68 0.75 5.72 -10.31 2.76 7.60 140.29 -0.48*
y
3Gear 3.97 1.94 17.10 -0.16 4.78 2.30 22.17 0.42
Leap 0.73 1.18 6.67 -16.27 1.36 2.83 57.85 -19.23*
TABLE V
LINEAR TRANSLATION ERRORS FOR AXIAL MOTIONS IN MM FOR POINTER AND HAND.MEAN (?), STANDARD DEVIATION(?), AND MAXIMUM (m)
DEVIATION FROM LINEAR TRAJECTORY,AS WELL AS ERRORS IN DISTANCE(d) BETWEEN ENDPOINTSARE SHOWN.*END POINTS COULD NOT BE
OBTAINED FOR ONE LEAP TRAJECTORY PER AXIS DUE TO HAND TRACKING LOSS NEAR ONE OF THE ENDPOINTS.THE DEVIATION FOR THESE
TRAJECTORIESWERE CALCULATED AGAINST THE LINEAR TRAJECTORY OF THE CORRESPONDINGPOINTER MEASUREMENTS.THE MARKED
DISTANCE BETWEEN END POINTS ARE AVERAGES OF FIVE TRAJECTORIES,AS OPPOSEDTO SIX.
accurate compared to hand tracking for position as well
as yaw and pitch for both sensors. Since only hand data
provides roll, and all 6DOF are necessary for tool control,
the best strategy is to use pointer for position control, yaw,
and pitch. Hand data should be only used for roll.
The measured frame rate and latency of the Leap was
superiortothe3Gearsystem,whichislimitedbytheKinect’s
frame rate of 30 frames per second versus 111 frames per
second for the Leap.
While we have not formallycomparedgesture recognition
of both tracking systems for this paper, it was evident
when working with both systems, that 3Gear has a distinct
advantage over Leap in this area.
3Gear was successfully used in a simulated surgical posi-
3508
Error Translation [mm] Yaw [degrees] Pitch[degrees] Roll [degrees] Time [sec]
Surgeon 1 [Avg±Stdv] 10.0±3.3 12.9±10.2 9.8±7.9 24.3±10.0 49.7±23.9
Surgeon 2 [Avg±Stdv] 6.6±2.1 5.7±7.5 6.6±3.8 39.0±26.5 32.6±19.9
Surgeon 3 [Avg±Stdv] 4.1±1.6 4.8±2.8 8.5±3.8 26.5±18.5 19.9±65.3
Surgeon 4 [Avg±Stdv] 4.1±1.9 9.7±6.6 7.5±6.8 54.1±24.2 53.9±27.0
Average [Avg±Stdv] 6.2±2.8 8.3±3.7 8.1±1.3 33.7±15.9 50.4±13.5
TABLE VII
TRANSLATIONALAND ROTATIONALERRORS IN SURGICALPOSITIONINGTASK
tioning task by four surgeons and demonstrated an average
translational accuracy of 6.2mm. Increased amount of train-
ing and better depth perception in the visualization would
help improve these results.
Given the data we have collected, we conclude that
neither system, at present, has the high level of accuracy
and robustness over the required range that would be a
prerequisite for use as a medical robotics master. However,
it should be noted that both of these systems are still in the
process of undergoing optimizations and improvements, and
we expect the performance of both systems to improve over
time. In addition, post-processing of sensory data in form
of added ﬁltering and smoothing and down scaling motions
could further improve the performance for both systems.
ACKNOWLEDGEMENTS
The authors would like to thank Dr. Johnny Costello, Dr.
Hope Jackson, Dr. Kyle Wu, and Dr. Craig Peters for their
participation in the experiment and Dr. Simon Leonard for
his help in operating the KUKA LWR.
REFERENCES
[1] da vinci surgery. [Online]. Available: http://www.intuitivesurgical.
com/specialties/
[2] U. Hagn, R. Konietschke, A. Tobergte, M. Nickl, S. J¨ org, B. K¨ ubler,
C. Passig, M. Gr¨ oger, F. Fr¨ ohlich, U. Seibold, L. Le-Tien,
A. Albu-Sch¨ affer, A. Nothhelfer, F. Hacker, M. Grebenstein, and
G. Hirzinger, “DLR MiroSurge: a versatile system for research
in endoscopic telesurgery,” Int J Comput Assist Radiol Surg,
vol. 5, no. 2, pp. 183–193, June 2009. [Online]. Available:
http://www.springerlink.com/content/30m6101651r823pm/
[3] M. J. H. Lum, D. C. W. Friedman, G. Sankaranarayanan, H. King,
K. Fodero, R. Leuschke, B. Hannaford, J. Rosen, and M. N. Sinanan,
“The RAVEN: design and validation of a telesurgery system,” Int.
J. Robot. Res., vol. 28, no. 9, pp. 1183 –1197, 2009. [Online].
Available: http://ijr.sagepub.com/content/28/9/1183.abstract
[4] M. Van den Bergh, D. Carton, R. De Nijs, N. Mitsou, C. Landsiedel,
K. Kuehnlenz, D. Wollherr, L. Van Gool, and M. Buss, “Real-
time 3D hand gesture interaction with a robot for understanding
directions from humans,” in RO-MAN, 2011 IEEE, 2011, pp.
357–362. [Online]. Available: http://ieeexplore.ieee.org/stamp/stamp.
jsp?arnumber=6005195
[5] V. Frati and D. Prattichizzo, “Using Kinect for hand tracking
and rendering in wearable haptics,” in World Haptics Conference
(WHC), 2011 IEEE, 2011, pp. 317–321. [Online]. Available:
http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945505
[6] K. Khoshelham, “Accuracy analysis of Kinect depth data,” in Interna-
tional Archives of the Photogrammetry, Remote Sensing and Spatial
Information Sciences (ISPRS), vol. XXXVIII-5/W12, 2011.
[7] T. Dutta, “Evaluation of the Kinect
TM
sensor for 3-D kinematic
measurement in the workplace,” Applied Ergonomics, vol. 43, no. 4,
pp. 645–649, July 2012.
[8] M. Kvalbein, “The use of a 3D sensor (Kinect
TM
) for robot motion
compensation: The applicability in relation to medical applications,”
Master’s thesis, University of Oslo, Oslo, Norway, May 2012.
[9] C. Perez Quintero, R. Tatsambon Fomena, A. Shademan, N. Wolleb,
T. Dick, and M. Jagersand, “SEPO: Selecting by pointing as an
intuitive human-robot command interface,” in Proc. IEEE Int. Conf.
Robot. Automat., 2013.
[10] Z.Ren,J.Yuan,and Z.Zhang,“Robust hand gesture recognition based
on ﬁnger-earth movers distance with a commodity depth camera,” in
Proceedings of the 19th ACM international conference on Multimedia.
ACM, November 2011, pp. 1093–1096.
[11] I. Oikonomidis, N. Kyriazis, and A. Argyros, “Efﬁcient model-based
3D tracking of hand articulations using Kinect,” in Proceedings of the
British Machine Vision Conference (BMVC), 2011.
[12] S. Leonard, K. L. Wu, Y. Kim, A. Krieger, and P. C. Kim, “Smart
tissue anastomosis robot (STAR): A vision-guided robotics system for
laparoscopic suturing,” IEEETransactions onBiomedical Engineering,
Preprint.
3509
