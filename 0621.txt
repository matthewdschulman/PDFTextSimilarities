Learning from Demonstrations
with Partially Observable Task Parameters
Tohid Alizadeh
1
, Sylvain Calinon
1,2
and Darwin G. Caldwell
1
AbstractÑ Robot learning from demonstrations requires the
robot to learn and adapt movements to new situations, often
characterized by position and orientation of objects or land-
marks in the robotÕs environment. In the task-parameterized
Gaussian mixture model framework, the movements are con-
sidered to be modulated with respect to a set of candidate
frames of reference (coordinate systems) attached to a set of
objects in the robot workspace. Following a similar approach,
this paper addresses the problem of having missing candidate
frames during the demonstrations and reproductions, which
can happen in various situations such as visual occlusion, sensor
unavailability, or tasks with a variable number of descriptive
features. We study this problem with a dust sweeping task in
which the robot requires to consider a variable amount of dust
areas to clean for each reproduction trial.
I. INTRODUCTION
Robot programming by demonstration provides a way of
teaching the robot to acquire a new skill and perform a
speciÞc task, without requiring the end user to be a robotic
expert or to have professional programming skills [3].
In most tasks, the motion depends on a set of coordinate
systems, or task parameters representing the locations of
intermediary via-points or targets (virtual or real) that can
locally inßuence the shape, amplitude, direction and timing
of movements. Thus, approaches that take into account
the dependence of the motion with respect to external
task parameters are needed. In this way, after providing
demonstrations with associated task parameters, the robot
can generalize the task to a new set of task parameters. This
process should rely on a small number of demonstrations
from the user. Several approaches towards this aim have been
proposed [2], [4]Ð[13]. These approaches use a variety of
tools including Gaussian mixture regression, hidden Markov
models, dynamic movement primitives, Gaussian process re-
gression and reinforcement learning, see [1] for an overview
of the available approaches.
In the above approaches, the task parameters (query points or
style variables), which are used for the demonstrations and
reproductions are considered to be always available. This can
be a limiting factor in situations where some task parameters
could be missing in the reproduction phase. This problem
is studied here with an approach based on an extension of
the task-parameterized Gaussian mixture model [1] able to
reproduce trajectories when some task parameters are not
available.
1
Department of Advanced robotics (ADVR), Istituto Italiano di Tecnolo-
gia (IIT), Via Morego 30, 16163 Genova, Italy. T. Alizadeh is also with the
Universita degli Studi di Genova. name.surname@iit.it
2
Idiap Research Institute, CH-1920 Martigny, Switzerland.
Fig. 1: Snapshots of a demonstration of the dust sweeping task
through kinesthetic teaching with a gravity-compensated robot.
A dust sweeping task using a broom is considered to il-
lustrate the applicability of the proposed model, see Fig.
1. In this task, the position and orientation of a set of
dust areas and the dust pan are used to construct the task
parameters. Some of these task parameters could be available
or missing during demonstrations and/or reproductions. For
each set of provided task parameters, the task is performed
by the demonstrator and the trajectory data are recorded. The
learned model is used to re-generate new trajectories for new
task parameters, by considering cases when some of them are
unavailable.
The paper is organized as follows. Section II summarizes
the proposed extension of the task-parameterized GMM to
potentially missing task parameters. Section III presents the
experimental setup and the achieved results. Section IV
concludes the paper and presents future work.
II. TASK-PARAMETERIZED GAUSSIAN MIXTURE MODEL
WITH POTENTIALLY MISSING TASK PARAMETERS
Each task parameter is considered here as a candidate
frame of reference, represented by a positionb and a transfor-
mation matrixA, which is composed of a set of basis vectors
{e
1
,e
2
,...}. The learning problem is set as maximizing
the log-likelihood of the observations in different candidate
frames, under the constraint that these observations are
generated by the same source. Namely, each framej observes
the same training datapoint ?
n
from its own perspective
through local projection. Similarly to the estimation of the
parameters of a standard GMM, deriving this constrained
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3309
TABLE I: Overall process.
1. Task demonstration
- Determine P (maximum number of frames)
- Perform the following loop during demonstration
- Record data point {?
n
}
N
n=1
- Determine the available task parameters and store them
as indices setsV n andW
j
- Record available task parameters {{A
n,j
,b
n,j
}
n?W
j
}
j?V n
2. Model Þtting
- Determine K (number of components of GMM)
- Initialize model parameters
{¹
i
,{Z
?
i,j
,Z
?
i,j
}
P
j=1
}
K
i=1
- Estimate model parameters with EM (Eq. (3-4))
3. Reproduction
- Determine which are the input I and output O variables
- Determine the available task parameters,V n
- Perform the following computation iteratively
- Collect/select ?
I
n
and {A
n,j
,b
n,j
}
j?V n
- Estimate temporary GMM parameters {¹
i
,?
n,i
,?
n,i
}
K
i=1
modeling the joint distribution of?
I
n
and?
O
n
as
?
I
n
,?
O
n
?
K
X
i=1
¹
i
N(?
n,i
,?
n,i
) (Eq. (4))
- Use GMR to retrieve ?
O
n
as
?
O
n
|?
I
n
?N(ö ?
n
,
ö
?n) (Eq. (7))
optimization problem results in an expectation-maximization
(EM) algorithm that guarantees to improve the likelihood of
the model at each iteration.
The original task-parameterized GMM is formulated by con-
sidering that all task parameters are available. Here, instead,
we study the case where some frames are unobservable
during demonstrations and reproductions.
When cleaning a table, the available visual information are
used to form a path and move through a set of uncleaned
area. If the same task is done in the dark while only some
parts of the table are visible, this path will be modiÞed
according to the missing information. In complete darkness,
an average path covering the table would be produced. We
aim to achieve similar behaviours with our model. Therefore,
when a task parameter is not accessible, it will be simply
ignored and the model will be estimated or will retrieve
movements based on the available task parameters.
We will assume that each demonstration m ? {1,...M}
contains T
m
datapoints forming a dataset of N datapoints
{?
n
}
N
n=1
with N =
P
M
m
T
m
. Each datapoint ?
n
=
[t
n
,x
n
] ?R
D+1
(e.g. D = 4 for Cartesian position in 3D
and orientation angle) is associated with the observed task
parameters {A
n,j
,b
n,j
}
j?V n
that represent the available
candidate frames of reference (at most P ). Here n and j
refer to time step and frame of reference, respectively. V
n
is a set of indices corresponding to the observable frames
of references at time step n. For example, in the case of a
task with 4 frames of references in which the second frame
is missing, we haveV
n
={1,3,4}.
The parameters of the task-parameterized GMM with K
components are {¹
i
,Z
?
i,j
,Z
?
i,j
}, representing respectively
the mixing coefÞcients, centres and covariance matrices for
each frame j and mixture component i. At time step n,
during both learning and reproduction phases, the resulting
center ?
i,n
and covariance matrix ?
i,n
of each component
i correspond to products of linearly transformed Gaussians
N(?
n,i
,?
n,i
)?
Y
j?V n
N

A
n,j
Z
?
i,j
+b
n,j
,A
n,j
Z
?
i,j
A
?
n,j

,
(1)
computed as
?
n,i
=
?
?
X
j?V n

A
n,j
Z
?
i,j
A
?
n,j

?1
?
?
?1
,
?
n,i
=?
n,i
X
j?V n
(A
n,j
Z
?
i,j
A
?
n,j
)
?1
(A
n,j
Z
?
i,j
+b
n,j
). (2)
The difference between Eqs. (1-2) and the original one lies
in the indices used for the summation and multiplication. In
the original one, the indices includes all frames of references,
while here it includes only the availabe ones. The parameters
of the model are iteratively estimated with the following EM
procedure:
E-step:
?
n,i
=
¹
i
N(?
n
|?
n,i
,?
n,i
)
P
K
k=1
¹
k
N(?
n
|?
n,k
,?
n,k
)
. (3)
M-step:
¹
i
=
P
N
n=1
?
n,i
N
,Z
?
i,j
=
X
n?Wj
?
n,i
A
?1
n,j
[?
n
?b
n,j
]
X
n?Wj
?
n,i
,
Z
?
i,j
=
X
n?Wj
?
n,i
A
?1
n,j
[?
n
? ÷ ?
n,i,j
][?
n
? ÷ ?
n,i,j
]
?
A
??
n,j
X
n?Wj
?
n,i
,
(4)
with ÷ ?
n,i,j
= A
n,j
Z
?
i,j
+b
n,j
. The set W
j
contains the
time steps indices in which the frame j is available,
W
j
={n = 1áááN|j ?V
n
}.
Compared to the original EM formulation for task-
parameterized GMM [1], the indices include only the the
datapoints in which the frame of reference is available during
the demonstration, instead of encompassing all the data
points for all the frames of references.
The model parameters are initialized by clustering the data
with equal time intervals. After the model is learnt for the
provided demonstration data, it can be used to produce new
trajectories, given a set of new task parameters. At Þrst
step, the input and output variables, ?
I
and ?
O
, should be
speciÞed (here, we consider the time as the input variable,
but in general, any variable or set of variables could be
considered as the input of the system). Accordingly, ?
n.i
and?
n,i
will be partitioned into input and output parts as:
?
n,i
=

?
I
n,i
?
O
n,i

, ?
n,i
=

?
I
n,i
?
IO
n,i
?
OI
n,i
?
O
n,i

Then, for each time step, the set of accessible task parameters
is provided as {A
n,j
,b
n,j
}
j?V n
. A GMM is built for the
3310
input and output variables at time stepn, based on the learnt
task-parameterized GMM parameters as:
?
I
n
,?
O
n
?
K
X
i=1
¹
i
N(?
I
n
,?
O
n
|?
n,i
,?
n,i
), (5)
in which ?
n,i
and ?
n,i
are computed using Eqs. (1-2).
Note that the unobservable frames do not affect the centers
and covariance matrices. Then Gaussian mixture regression
(GMR) is applied to estimate the value of the output variable,
?
O
n
given the value of the input variable,?
I
n
:
?
O
n
|?
I
n
?N(ö ?
n
,
ö
?
n
), (6)
in which
ö ?
n
=
K
X
i=1
?
i
(?
I
n
)

?
O
n,i
+?
OI
n,i

?
I
n,i

?1

?
I
n
??
I
n,i


,
ö
?
n
=
K
X
i=1
?
2
i
(?
I
n
)

?
O
n,i
??
OI
n,i

?
I
n,i

?1
?
IO
n,i

,
?
i
(?
I
n
) =
¹
i
N

?
I
n
|?
I
n,i
,?
I
n,i

P
K
k=1
¹
k
N

?
I
n
|?
I
n,k
,?
I
n,k
 (7)
The estimated output in Eq. (7) encapsulates variation and
correlation information in the form of a probabilistic ßow
tube [15], continuously differentiable in time.
Afterwards, the estimated ö ?
n
is used by the controller as the
best estimate of?
O
n
at this time step. We can see with (2) and
(7) that, when a task parameter is missing, the corresponding
Z
?
i,j
andZ
?
i,j
will still allow the estimation of ö ?
n
and
ö
?
n
.
Model selection is compatible with the techniques ap-
plied for the conventional GMM. The Matlab and C++
source codes of the proposed model are available at
http://programming-by-demonstration.org.
A summary of the proposed approach is provided in Table.
I. In the next section, the proposed approach is demonstrated
using a robotic dust sweeping experiment.
III. ROBOTIC DUST SWEEPING EXPERIMENT
A. Experimental setup
In order to test the applicability of the proposed approach,
the task of sweeping dust areas on a table is considered.
The aim is to sweep a set of dust spots with a broom on a
table and collect the dust inside a dust pan. For this task,
the shape of the movement is modulated by the position
and orientation of the dust spots and the dust pan. The task
requires to move the broom attached to the end-effector of
the robot from an initial pose in the robot workspace towards
a Þrst marker representing a dust spot, then move towards
a second marker representing a second dust spot and Þnally
move towards the dust pan, with suitable orientation (if the
dust spots and the dust pan are approximated by an ellipse
with two main directions in 2D, the broom will be oriented
in the direction of the principal axis of the ellipse. In this
way the dust pieces will be collected inside the dust pan (the
broom is oriented according to the shape of the dust pan).
Fig. 2: Snapshots of a reproduction attempt of the dust sweeping
task after model learning.
(a) Obs. from 1
st
frame (b) Obs. from 2
nd
frame
0.3 0.4 0.5 0.6 0.7 0.8
?0.25
?0.2
?0.15
?0.1
?0.05
0
0.05
0.1
x
1
 (m)
x
2
 (m)
?0.6 ?0.4 ?0.2 0 0.2 0.4
?0.8
?0.6
?0.4
?0.2
0
(c) Obs. from 3
rd
frame (d) Obs. from 4
th
frame
?1 0 1 2
?3
?2.5
?2
?1.5
?1
?0.5
0
?1.5 ?1 ?0.5 0 0.5 1
?1.5
?1
?0.5
0
Fig. 3: Demonstration data observed from different frames of
references and the obtained model after learning (the movements
start from the black squares).
In this experiment two dust areas are considered and there-
fore there will be 4 frames of references as the task param-
eters (P = 4):
1. The robot frame (which is Þxed all the time),
2. The dust pan frame,
3. The dust piece #1 frame,
4. The dust piece #2 frame.
The task parameters are here constant during the movement
for each demonstration. The experiment is implemented
in a Barrett WAM torque-controlled 7 DOFs manipulator.
The robot is gravity-compensated during demonstrations and
reproductions. The orientation of the end-effector is kept
perpendicular to the worktop, with the orientation along
the vertical axis determined by the learning approach. The
demonstrated trajectories are tracked by a virtual spring-
damper system. Forces acting as gravity compensation are
superposed to the tracking forces, resulting in a safe con-
3311
0.4 0.6 0.8
?0.4
?0.2
0
0.2
x
1
(m)
x
2
(m)
50 100 150 200
0
0.5
1
time step
F
n,j
Fig. 4: Top: samples of the demonstrations, Middle: the correspond-
ing reproductions, and Bottom: the relative importance of the frames
during the reproduction, after model learning in the presence of all
the frames (task parameters). The task parameters are shown as
frames of references (Black, blue, red and green correspond to the
Þrst, second, third and fourth frame of reference, respectively).
Fig. 5: Top: samples of the reproductions for the same set of param-
eters as the demonstration in the case of missing frames. Bottom:
corresponding importance of the frames during the reproduction
(See also Fig. 4 for the legend of the graphs).
troller for the user, who can exploit the redundancy of the
robot and the redundancy of the task during demonstration
and reproduction, see [14] for details of the controller. For
example, when sharing the workspace with the robot, the user
can during reproduction change the position of the robotÕs
elbow to have more space.
The information regarding the position, size and orientation
of the dust pieces and the dust pan are tracked by a
camera using color-based image processing and homography
transformation to unwrap the camera image. Task parameters
for each frame are then built using this information. Fig. 1
shows the experimental setup. All demonstrations start from
nearly the same position and orientation of the broom. The
motions are described by 4 variables (D = 4), representing
the Cartesian position of the broom (x
1
,x
2
,x
3
) and its angle
Fig. 6: Top: reproductions for new set of task parameters. Bottom:
the corresponding importance of the frames during the reproduction,
in the presence of all the task frames (task parameters). The task
parameters of the demonstrations are depicted using smaller and
thinner frames of references (See also Fig. 4 for the legend of the
graphs).
Fig. 7: Top: reproductions for the new set of parameters in the
case of missing frames. Bottom: the corresponding importance
of the frames during the reproduction. Only the available task
parameters are shown in the Þrst row. The task parameters used for
the demonstrations are depicted using smaller and thinner frames
of references (See also Fig. 4 for the legend of the graphs).
with respect to a vertical axis. Therefore, we have
b
n,1
=0,b
n,j
=
?
?
0
p
n,j
?
n,j
?
?
,A
n,1
=I,A
n,j
=
?
?
1 0 0
0 R
n,j
0
0 0 1
?
?
in whichp
n,j
,?
n,j
andR
n,j
refer to the Cartesian position,
the angle and the orientation (as full rotation matrix) of the
dust pan and two dust pieces at time step n, respectively.0
is a vector or a matrix of 0s of appropriate size andI is the
identity matrix. In total, 6 demonstrations are provided with
different locations of the dust pieces and dust pan. Models
with K = 6 components are considered in the experiment
(selected empirically).
B. Experimental results
Snapshots of the reproduction of the task by the robot
is presented in Fig. 2. Please refer to the accompanying
video of the experiment for a better insight. Fig. 3 presents
the demonstration data observed form the point of view of
frames 1-4 (in 2D, on the horizontal plane). The obtained
model after learning is also depicted with Gaussian com-
ponents with centers Z
?
i,j
and covariance matrices Z
?
i,j
for
3312
each frames of references. From the point of view of the
1
st
frame, the trajectories are close to each other at the
beginning of the movements and therefore the corresponding
Gaussian component has smaller covariance matrix than
the other components. In the case of the 2
nd
frame, since
all the movements end at this frame, the trajectories are
close to each other at the end of the movements, and
therefore, the corresponding Gaussian component has a small
covariance matrix, while in the beginning of the movements,
the trajectories are diverse and the corresponding Gaussian
component has a larger covariance matrix. The 3
rd
and the
4
th
frames of references have small Gaussian components at
different timing in the middle of the movement and bigger
ones at the beginning and the end of the movement.
In Fig. 4, the demonstration samples are depicted in the
Þrst row (in 2D, from the top view). In the second row,
reproductions for the same set of task parameters as the
demonstrations are presented, in which all the frames of
references are available. The orientation of the dust broom
is shown by segments along the trajectories. The reproduced
data follows reasonably the trajectory and the orientation of
the dust broom. In the third row of Fig. 4, the importance of
the frames during the reproduction is shown. The importance
of framej at time stepn,F
n,j
is computed as the ratio of the
precision matrix determinant for a given frame with respect
to the other frames at time step n, deÞned as:
F
n,j
=
|?
?1
n,j
|
P
X
k=1
|?
?1
n,k
|
, (8)
where
?
n,j
=
K
X
i=1
?
i
(?
I
n
)
h
?
O
n,i,j
??
OI
n,i,j
(?
I
n,i,j
)
?1
?
IO
n,i,j
i
, (9)
and
?
n,i,j
=A
n,j
Z
?
i,j
A
?
n,j
. (10)
These results show that the Þrst frame is the most impor-
tant in the beginning of the movement, then gradually the
fourth (1st dust area), third (2nd dust area) and the second
(dust pan) frames become important.
In Fig. 5, reproductions in the case of missing frames are
presented. In the Þrst row, reproductions for the same set
of demonstrated task parameters are presented but one of
the frames is removed. Only the available task parameters
are shown in the image and the importance of the frames
are shown below the reproduction trajectories. Note that
the importance of the missing frame is always 0, and the
importance pattern for the available frames shares similarities
with the full frame availability condition in case in Fig. 4.
We can see that, the role of the missing frame is shared
by the neighbouring frames. These results show that the
proposed approach tackles the absence of task parameters
in a natural way by reproducing the movement based on
the available task parameters. The reproduction results show
that the model can retrieve appropriate motions from, with
the dust piece collected in the dust pan at the end of the
motion.
Similar reproduction results are illustrated in Figs 6 and 7 for
new sets of task parameters, demonstrating the generalization
capability of the approach. Reproductions in the case of new
(a) (b)
0 50 100 150 200
0
2
4
6
8
x 10
6
time step
P
n
0 50 100 150 200
0
2
4
6
8
10
x 10
6
(c) (d)
0 50 100 150 200
0
1
2
3
4
5
6
x 10
6
0 50 100 150 200
0
1
2
3
4
5
6
7
x 10
6
Fig. 8: Average precision for reproductions in different situations.
In (c) and (d), the blue, red and green graphs refer to the case in
which3
rd
,4
th
and both3
rd
and4
th
frames is missing, respectively.
See text for additional description.
task parameters in which one of the frames is missing are
depicted in Fig. 7. We can see that, the approach is able to
tackle the absence of the task parameter and at the same time
generalize to new situations.
In Fig. 8 a measure of precision is shown for reproductions
in different situations. This measure is calculated as the
determinant of the precision matrix retrieved by GMR, at
each time step (P
n
= |
ö
?
?1
n
|). Fig. 8 (a) presents the
evolution of this variable for the reproductions in which all
the frames of references are available and are the same as
the demonstrations, averaged over 6 reproductions. Fig. 8 (b)
shows the evolution for the new set of task parameters (all
accessible), averaged over 4 reproductions. We can observe
0.4 0.6 0.8
?0.4
?0.2
0
0.2
x
1
 (m)
x
2
 (m)
Fig. 9: Reproduction (black, thick line) for the case in which only
the robot frame is available. For comparison, the demonstrations
are plotted using thinner (blue) lines.
3313
similar patterns in (a) and (b). The two main peaks in both
graphs correspond to time intervals in which the trajectory
passes close to the 4
th
and the 3
rd
frames of references,
respectively. Fig. 8 (c-d) correspond to the reproduction
conditions respectively depicted in Figs. 5 and 7. It can
be seen from these graphs that when a task parameter is
not accessible, there is a natural smoothing effect on the
trajectory around the corresponding time interval, where the
movement is retrieved less precisely in that region. This
behaviour is predictable: when the task parameter is missing,
there is no need to be precise for the time intervals which
correspond to the corresponding frame of reference.
In the extreme case where only the robot frame is available
(namely, without vision tracking), the approach is still able
to perform an appropriate average movement, as depicted in
Fig. 9. The retrieved movement corresponds to an average
behavior based on the previous experiences in the dataset.
IV. CONCLUSIONS
An extension of the task-parameterized GMM model was
presented to handle partially observable task parameters, both
in the demonstration and the reproduction phases. This is
achieved by building the model and retrieving output variable
based only on the available task parameters at each time
step. The proposed extension is simple and effective, and its
performance is demonstrated using a robotic dust sweeping
task.
The approach that we propose is well suited for problems
in which the task parameters can be represented in the
form of coordinate systems, and where it is desirable to
have a reasonable movement when a task parameter is
missing. The approach retrieves a conÞdence measure on the
estimated path, which could be exploited in future work to
automatically determine if the movement can be reproduced
when some frames are missing (e.g., partial occlusions),
and wait for the occlusion to disappear if the measure of
conÞdence on the retrieved movement is too low.
The proposed approach considers only the cases in which the
number of task parameters in the reproduction is smaller or
equal to the number of task parameters in the demonstration.
In other words, each task parameter should be present
at least once during the demonstrations. In order to deal
with excessive task parameters, other approaches should be
developed to automatically discard redundant and irrelevant
frames from larger sets of candidate frames.
Another way of dealing with the missing task parameters,
that could be investigated in future work, is to use a similar
strategy as in the problem of matrix and higher order tensor
completion algorithms [16].
Investigating the similar case of missing task parameters
using other learning approaches could be another line of the
future works to consider.
REFERENCES
[1] S. Calinon, T. Alizadeh, and D. G. Caldwell, ÒOn improving the
extrapolation capability of task-parameterized movement models,Ó in
Proc. IEEE/RSJ Intl Conf. on Intelligent Robots and Systems (IROS),
Tokyo, Japan, November 2013, pp. 610Ð616.
[2] S. Calinon, Z. Li, T. Alizadeh, N. G. Tsagarakis, and D. G. Caldwell,
ÒStatistical dynamical systems for skills acquisition in humanoids,Ó
in Proc. IEEE Intl Conf. on Humanoid Robots (Humanoids), Osaka,
Japan, 2012, pp. 323Ð329.
[3] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, ÒRobot program-
ming by demonstration,Ó in Handbook of Robotics, B. Siciliano and
O. Khatib, Eds. Secaucus, NJ, USA: Springer, 2008, pp. 1371Ð1394.
[4] M. Brand and A. Hertzmann, ÒStyle machines,Ó in Proc. ACM Intl
Conf. on Computer graphics and Interactive Techniques (SIGGRAPH),
New Orleans, Louisiana, USA, July 2000, pp. 183Ð192.
[5] S. Calinon, F. Guenter, and A. Billard, ÒOn learning, representing and
generalizing a task in a humanoid robot,Ó IEEE Trans. on Systems,
Man and Cybernetics, Part B, vol. 37, no. 2, pp. 286Ð298, 2007.
[6] D. Forte, A. Gams, J. Morimoto, and A. Ude, ÒOn-line motion
synthesis and adaptation using a trajectory database,Ó Robotics and
Autonomous Systems, vol. 60, no. 10, pp. 1327Ð1339, 2012.
[7] F. Stulp, G. Raiola, A. Hoarau, S. Ivaldi, and O. Sigaud, ÒLearning
compact parameterized skills with a single regression,Ó in Proc. IEEE
Intl Conf. on Humanoid Robots (Humanoids), 2013.
[8] T. Matsubara, S.-H. Hyon, and J. Morimoto, ÒReal-time stylistic
prediction for whole-body human motions,Ó Neural Networks, vol. 25,
pp. 191Ð199, 2012.
[9] ÑÑ, ÒLearning parametric dynamic movement primitives from mul-
tiple demonstrations,Ó Neural Networks, vol. 24, no. 5, pp. 493Ð500,
2011.
[10] J. Kober, A. Wilhelm, E. Oztop, and J. Peters, ÒReinforcement
learning to adjust parametrized motor primitives to new situations,Ó
Autonomous Robots, vol. 33, no. 4, pp. 361Ð379, 2012.
[11] A. Ude, A. Gams, T. Asfour, and J. Morimoto, ÒTask-speciÞc gen-
eralization of discrete and periodic dynamic movement primitives,Ó
Robotics, IEEE Transactions on, vol. 26, no. 5, pp. 800Ð815, 2010.
[12] V . Krueger, D. L. Herzog, S. Baby, A. Ude, and D. Kragic, ÒLearning
actions from observations: Primitive-based modeling and grammar,Ó
IEEE Robotics and Automation Magazine, vol. 17, no. 2, pp. 30Ð43,
2010.
[13] K. Kronander, M. S. Khansari-Zadeh, and A. Billard, ÒLearning
to control planar hitting motions in a minigolf-like task,Ó in Proc.
IEEE/RSJ Intl Conf. on Intelligent Robots and Systems (IROS), San
Francisco, California, USA, September 2011, pp. 710Ð717.
[14] S. Calinon, I. Sardellitti, and D. G. Caldwell, ÒLearning-based control
strategy for safe human-robot interaction exploiting task and robot
redundancies,Ó in Proc. IEEE/RSJ Intl Conf. on Intelligent Robots and
Systems (IROS), Taipei, Taiwan, October 2010, pp. 249Ð254.
[15] D. Lee and C. Ott, ÒIncremental kinesthetic teaching of motion
primitives using the motion reÞnement tube,Ó Autonomous Robots,
vol. 31, no. 2-3, pp. 115Ð131, 2011.
[16] M. Signoretto, R. Van de Plas, B. De Moor, and J. A. Suykens, ÒTensor
versus matrix completion: a comparison with application to spectral
data,Ó Signal Processing Letters, IEEE, vol. 18, no. 7, pp. 403Ð406,
2011.
3314
