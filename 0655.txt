A concurrent real-time biologically-inspired visual object recognition
system
Andreas Holzbach
1
and Gordon Cheng
2
AbstractÑIn this paper, we present an biologically-motivated
object recognition system for robots and vision tasks in general.
Our approach is based on a hierarchical model of the visual
cortex for feature extraction and rapid scene categorization.
We modify this static model to be usable in time-crucial real-
world scenarios by applying methods for optimization from
signal detection theory, information theory, signal processing
and linear algebra. Our system is more robust to clutter
and supports object localization by approaching the binding
problem in contrast to previous models. We show that our
model outperforms the preceding model and that by our mod-
iÞcations we created a robust and fast system which integrates
the capabilities of biological-inspired object recognition in a
technical application.
I. INTRODUCTION
Object recognition in technical systems is still conÞned to
speciÞc scenarios and very limited in performance outside
their intended scope. In order to solve the problem of
object recognition, it makes sense to follow the biological
example for two reasons. First we donÕt have any other
examples of an universal working vision system and second
biological systems exceed the capabilities of any existing
technical system by far. Humans are capable of detecting
and recognizing objects under the most complex circum-
stances.Theycaneasilyidentifyobjectsundermostlightning
conditions, orientation, color or size. Even objects in clutter
pose little problems, in contrast to state-of-the-art computer-
based object recognition systems, which struggle to perform
adequatelyundervaryingsituations.Therefore,itonlymakes
sense Ð and maybe is the only successful way Ð to analyse
how the visual system in biological systems works and use
that knowledge for modelling those mechanisms to build a
more likely effective and robust object recognition system.
Only recently researchers began to look into possible
architectures which process information similar to its bio-
logical prototype [1], [2], [3]. These models cover a sub-
functionality of the vision processing performed by the
brain; like visual attention, object recognition, tracking or
learning. Especially in the area of object recognition, models
have been built as a proof-of-concept with little effort in
situating them in the real-world, mainly because they aim
on biologically accurateness and the plausible modelling
of neural processing. So naturally these models are slow,
*This work was supported (in part) by the DFG cluster of excellence
Cognition for Technical systems CoTeSys of Germany, and also (in part)
BMBF through the Bernstein Center for Computational Neuroscience Mu-
nich (BCCN-Munich).
Andreas Holzbach
1
and Gordon Cheng
2
are with the Institute for
Cognitive Systems, Technische Universit¬ at M¬ unchen, Karlstr. 45/II, 80333
M¬ unchen, Germany. Email available at www.ics.ei.tum.de.
Fig. 1: Responses for entropy (top left and right), gabor Þlter
(middleleft)andobjectlocalization(bottomleftandmiddle).
inefÞcient and hardly applicable in robotics. So far little
effort has been put into modifying and enhancing those
models to be usable in time-crucial applications in uncertain
environments. With our work we contribute to solve this
issue.
II. RELATED WORK
In the last couple of years there has been an increase in
biologically-inspired hierarchical models for object recogni-
tion,duetoadeeperunderstandingofinformationprocessing
inthebrain[4],[5],[2].Someofthesemodelshavealsobeen
applied to enhance common techniques like face recognition
by using biologically-inspired features [6]. Some research
draw more attention to active-vision systems, which have
been used to solve different vision problems like: object
recognition [7], [8], [9], [10], [11]; visual search [12], [13];
visual attention [14]; or visual tracking [15]. It has also
been investigated how to integrate object recognition [16],
[17] and visual attention also with a focus on the aspect
of computational complexity [18]. Especially the HMAX
model [19] has been investigated and modiÞed in multiple
publications [20], [21], [22], [23].
In this paper we speciÞcally focus on the optimization of
biologically-inspired object recognition for technical appli-
cations to encourage further investigations in this promising
research Þeld.
III. HMAX
The object recognition module presented in this paper is
built on Serre et al.Õs HMAX [24], which presents a feed-
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3201
Fig. 2: Functional Overview of the architecture.
forward model of the visual cortex described by Riesenhuber
and Poggio [19]. An overview is given in Figure 2. Each
layer in the classical model consists of four alternating layers
of simple cells (S1, S2) and complex cells (C1, C2) [25].
S1 Layer: The Þrst layer is based on a representation of
simple cells which react to oriented edges and bars in the
receptiveÞeld.Theresponseofthesecellsarequitesimilarto
Gabor Þlters. The Gabor Þlters are created using the function
G
?,?,?,?,?
(x
?
,y
?
) =exp

?
x
?2
+y
?2
?
2
2?
2

cos

2¹
x
?
?
+?

(1)
with
x
?
=xcos? +ysin? (2)
and
y
?
=?xsin? +ycos? (3)
where ? controls the orientation of the Þlter, ? the phase
offset, ? the variance of the Gaussian, ? the spatial aspect
ratio and ? represents the wavelength of the sine function.
The edge-sensitive cells contribute to the rotation invariance
of the recognition system by the sensitivity to edges and
bars of different orientations.
C1 Layer: Complex cells have a larger receptive Þeld
than simple cells and add some degree of spatial invariance
and shift tolerance to the system. S1 cells of same scale
band, same orientation and adjacent Þlter size are connected
to a complex cell. The functionality can be described as a
kind of max pooling operation; The maximum value of two
adjacent Þlters of different sizes is calculated by using a
sliding window approach.
S2 Layer: In the third layer small patches are chosen
from random positions in the receptive Þeld of C1. Each
patch set consists of 4 patches, assembled by taking each
patch in the set from a C1 response of different orientation
(0
?
,45
?
,90
?
,135
?
) but same position and same scale band.
Serre et al. use different sizes of patch sets: patch sets
which contain patches of size 4; patch sets with patches of
size 8; with size 12 and with size 16. These patch sets are
then used for two different cases
Before the training or classiÞcation case, a dictionary of
patch sets needs to be built. In the standard HMAX system
these patch sets are chosen randomly over multiple images.
The S2 cell response is similar to a Gaussian radial basis
function and can be calculated as follows
r
i,k
= exp(??||X
i
?P
k
||
2
) (4)
where ? is the sharpness of the tuning. X
i
is one of the
patch sets created in the S2 layer and P
k
is one of the
ÒmemorizedÓ patch set in the earlier created dictionary. The
radial basis function is calculated for all patches i in the set
of patch sets of S2 and for all patch setsk in the dictionary.
C2 Layer: Like in C1, the complex cells in the C2 layer
now again perform a max operation over all the responses.
For each element in the dictionary the maximum response
for equation 4 is calculated using all the RBF responses of
the patch sets of equal size. Using equation 4 this leads to
3202
f
k
= max(exp(??||X
i
?P
k
||
2
));?i (5)
which builds the feature vector F = {f
0
,f
1
,...,f
d
}
for all k in the dictionary, with d being the length of the
dictionary. The feature vector can now be further used for
training a classiÞer. For comparison reasons we used a SVM
classiÞer as Serre et al. with a radial basis function kernel
[24].
IV. IMPROVEMENTS
We enhanced the standard HMAX model to be applicable
in real-world scenarios in terms of speed, object recognition
performance and object localization (see Þgure 1).
A. Gabor Filter
Gabor Þlters have been shown to provide a good estimate
for the response of cortical simple cells and so they are used
in all of the HMAX-like implementations. The model pre-
sented in [20] uses four different orientations with different
sizes and parameters resulting in 64 different Þlters. Mutch
and Lowe [22] use a slightly different approach by applying
12 different orientations but with a sparse representation
to a pyramid-based model. The different orientations are
supposed to contribute to the systemÕs orientation invariance.
However, those models create n-dimensional patches - with
n being the number of different orientations - at stage S2
by sampling over random positions. These patches are used
for creating a feature vector for classiÞcation by applying
a radial basis function, which calculates the norm of the
difference of the n-dimensional patches. Consequently the
result of the RBF function is quite different if the patches
are rotated, which indicates, that orientation invariance is
in fact very limited. Therefore we argue, that Gabor Þlter
of different orientations can be combined by creating an
orientation-free Gabor Þlter:
G
?,?,?,?
(x,y) =
exp

?
x
2
+y
2
?
2
2?
2

cos
 
2¹
p
x
2
+y
2
?
+?
!
(6)
This approach creates a much Þner representation of edges
than ordinary Gabor Þlters, as all possible orientations are
covered (see Þgure 3). In addition it reduces the computa-
tional cost of convolution from n dimensions to one - in
our case from 64 to 12. Another beneÞt of a orientation-
free Gabor Þlter is that it is separable, which would make it
computationally more effective. But in the HMAX model
the Þlter is only deÞned within a circular area as it is
more accurate to a simple cellsÕ anatomy, which makes it
non-separable. We tested non-circular Gabor Þlters against
circular ones and got better deÞned edges using the original
approach. Using singular value decomposition (SVD) we are
still able to factorize a circular Gabor Þlter into separable
matrices. The SVD of the Gabor Þtler matrix takes the
following form:
(a) 0
?
(b) 45
?
(c) 90
?
(d) 135
?
(e) Orientation-free Gabor Þlter
0
5
10
15
20
0
5
10
15
20 -0.002
-0.001
0
0.001
0.002
0.003
0.004
(f) Error matrix SVD
Fig. 3: ModiÞcation of applied Gabor Þlters
Fig. 4: Speed comparison between Image Filtering with non-
separable and separable kernel using CPU and GPU for
different kernel sizes.
G =USV
T
=
j
X
i=1
u
i
s
i
v
T
i
(7)
We can precalculate the separable Þlters and create the
convolved image J from image I by using
J =
j
X
i=1
I?(u
i
Ã
s
i
)+I?(v
T
i
Ã
s
i
) (8)
We achieve almost similar results for j ³ 3 compared to
the original Þlter with an average error rate of 9.5? 10
?5
over the whole Þlter (see Þgure 3f) - and still are faster by
applying the separable Þltering for j = 3 than using the
non-separable Þlter.
We compared the computation speed for convolution with
different Þlter sizes on CPU and GPU for the separable Þlter
and the non-separable Þlter in Þgure 4 for a image size of
320?240. Using our separable Þlter approach we achieve a
constant processing time on GPU of under 1 ms for j = 3
on all kernel sizes. The average computation time of the
S1 layer using our approach with 16 orientation-free Gabor
Þlters takes under 16 ms on GPU compared to about 256 ms
for 64 Þlters on CPU with the standard system (see table I).
This is a speed up of about 16.
3203
(a) Original (b) Entropy
(c) Std Dev (d) Min Max
Fig. 5: Approximations for Entropy Calculation
B. Entropy
In[26]and[27]weenhancedtheHMAXmodelbyadding
an information theoretic aspect of neural processing - the
maximization of information along the pathway. Our system
incorporates the information entropy in the S2 layer of the
system. It is sensible in regard to the information a single
patch carries and adaptively rejects patches which donÕt
account for the overall information gain. We calculate the
entropy of each patch by applying:
H(X) =?
M
X
m=1
p
m
logp
m
(9)
withp
m
beingtherelativefrequencyofbrightnessvaluem
within the patch. This approach Þlters out patches that show
an almost plain distribution of intensities. In order to further
reduce the computation time of the system we tested two
additional approaches to approximate the entropy in a patch:
1. The standard deviation of the patch and 2. The difference
of the maximum and minimum occurring intensity in the
patch T:
H(X)Åmax(T)?min(T) (10)
The intensity difference and the standard deviation ap-
proach were both equally fast but about 1.5? faster than
the entropy approach, with similar results (see Þgure 5).
For our system we choose the intensity difference approach,
because the threshold parameter is more intuitive than the
other approaches.
C. Radial Basis Function
The feature vector is calculated using the radial basis
function (see equation 4). This means the relative L
2
-norm
ofthedifferenceoftwopatches,theexponentialfunctionand
an exponent has to be calculated. The computation time of
this step highly depends on the number of sampled patches
and the size of the dictionary. A dictionary size of 2000 and
e.g. 500sampledpatcheswouldrequire 1.000.000RBFcalls.
We approximate the RBF function response by applying a
simpler L
1
-norm using:
r
i,k
Å 1?
||X
i
?P
k
||
L1
?
(11)
with ? being the maximum possible value a L
1
-norm can
have for the speciÞc patch size. Hereby we normalizer from
a range from [0;1] with 1 meaning identical patches. This
speeds up the computation by a factor of 2 over the normal
approach.
D. Dictionary
In the standard HMAX implementation, the dictionary is
created by randomly selecting patches as artiÞcial neurons
from a set of responses in C1. This approach bears the risk
to select a non-optimal set with over-represented and redun-
dant features. Especially in image data sets, where image
categories are presented in clutter for training and testing it
is uncertain if the applied algorithm actually classiÞes the
object itself or just the surroundings. The category car in the
Caltech101 database is for example such a case: The actual
object only takes a fraction of the image, whereas objects
like trees or houses take up most of the space. Therefore it
is uncertain, if the presented algorithms actually recognize
the class car or mainly the background, as the patches are
randomly selected over the whole image.
Todealwiththisproblemourmethodfollowsanapproach,
whichisbasedonneuraltuning.Cellsinthebrainselectively
represent speciÞc sensory patterns. Applying our orientation-
free Gabor Þlter approach enables us to assign patches to
speciÞc object classes due to the higher complexity of the
generated image after convolution. Each class is represented
by an own sub-dictionary, that is created by keeping only
patches which occur to a certain degree in all the training
images. Hereby we want to achieve, that the created dictio-
nary represents the actual object instead of itÕs surroundings.
A car tire probably will appear in all images for example,
however a tree might not, therefore patches containing the
tree will most likely be Þltered out.
After the sub-dictionaries are created, we apply an ap-
proach derived by lateral inhibition appearing in neural
processing. For each patch in a sub-dictionary we calculate
the response of each patch of each other sub-dictionary.
If a patch exists, which reacts above a certain threshold
to patches in all sub-dictionary, then these patches are
completely removed. That way the sub-directories are even
more conÞned to their speciÞc class.
Mathematically,wecandescribethesetofsub-dictionaries
as a partition of dictionary D
[
Di?D
D
i
=D (12)
with
D
i
={x|?x?D
i
: ?y?D
j
,i6=j :r(x,y)>?} (13)
3204
with ? being a threshold of the response of our ap-
proximated radial basis function r of Equation 11. Pseudo-
Algorithm 1 displays how a sub-dictionary is created.
Algorithm 1: Create Object SpeciÞc Dictionary
Data: Sub-Dictionary D
i
; Set of training images T; Set
of patches C; Threshold ?
Create New Set Of patches(T
1
, D
i
);
forall s> 1 do
Create New Set Of patches(T
s
, C);
forall k do
forall p do
if f(D
ip
, C
k
) < ? then
delete(D
ip
);
break;
end
end
end
end
E. Object Localization
Biologically-inspired computational models have mostly
applied a simple sliding window approach to localize spe-
ciÞc objects in an image, which makes the system rather
inefÞcient, especially in a fast-changing environment. The
patches in the sub-directories are object-speciÞc enough that
they allow us to deduce the object location to a certain
degree using the patches maximum response occurrences in
theimage(seeÞgure6).Thisapproachrequiresnoadditional
calculation, as the maximum responses are anyway needed
to be calculated by the system in order to create the feature
vector for the classiÞer. We create a saliency map by adding
the maximum response values for each patch in the sub-
dictionarytothelocationinthesaliencymapwherethepatch
from the test image was sampled that created this highest
response.
V. RESULTS
A. Processing Speed
As already shown in Þgure 4, we were able to speed up
the gabor Þltering by a factor of 4. Compared to our CPU
implementation of the standard HMAX model with nonsep-
arable gabor Þlters, our system speeds up the computation
using GPUs and separable orientation-free gabor Þlters by a
factor of Å16.8 (see table I).
TABLE I: Processing speed of S1 layer in HMAX vs our
system (averaged over 100 cycles; CPU: i7, GPU: Geforce
670 GTX).
HMAX Our System
CPU GPU CPU GPU
Non-separable Þlter 252 ms 98 ms 63 ms 24 ms
Separable Þlter 177 ms 60 ms 44 ms 15 ms
In table II we show the computation speed for the next
layer C1. Again we compared the speed of the original
HMAX system against ours.
(a) Input Image (b) Saliency Map for
Object Subdirectory
(c) Saliency Map for
different Subdirectory
Fig. 6: Object Localization. A saliency map of maximum re-
sponses to the object subdirectories. The map which belongs
to the object in a) is shown in b); c) shows the response of a
different object subdirectory. First three images were taken
from the Caltech101 database, the others were taken from
the UIUC car dataset.
TABLE II: Processing speed C1
HMAX Our System
CPU GPU CPU GPU
MAX Operation 140 ms 37 ms 35 ms 9.25 ms
Table III shows processing speed for a dictionary of size
2000 with a sampling rate of 200 patches per patch size per
C1 layer. Our system speeds up the overall processing for
the S2 Layer by a factor of Å 8.6. As our system creates
a very efÞcient representation of an object within the sub-
directories, we already achieved good results with a sub-
directory size of about 100.
B. ClassiÞcation Performance
We tested our system against the Caltech-101 database.
Foreachrun,werandomlychoseatrainingandtestingimage
set and computed results with different numbers of positive
training examples (1, 3, 15, 30 and 40) and 50 negative
training examples. Our approach outperforms the original
system in regard to the classiÞcation accuracy (e.g. for the
3205
TABLE III: Processing speed S2 (For a dictionary size of
2000 and a sample rate of 200 per layer
HMAX Our System
Patch Size RBF Approx.RBF RBF Approx.RBF
4 1.06s 0.53s 0.26s 0.13s
8 1.59s 0.79s 0.41s 0.19s
12 2.17s 1.09s 0.55s 0.26s
16 2.86s 1.25s 0.71s 0.31s
Sum 7.68s 3.66s 1.92s 0.89s
 50
 60
 70
 80
 90
 100
 10 20 30 40
Classification Performance [%]
Training Examples
Faces
Our Approach
Standard HMAX
 10 20 30 40
Training Examples
Airplanes
Our Approach
Standard HMAX
 10 20 30 40
Training Examples
Cars
Our Approach
Standard HMAX
Fig. 7: Comparison of classiÞcation results for faces, air-
planes and cars of the Caltech image database between the
standard HMAX and our approach.
airplanes dataset 92% compared to 86%; faces: 96% to 90%,
see Þgure7; cars 96% to 94%) or is at least of equal result.
VI. CONCLUSION
In this paper, we have presented a biologically-inspired
object recognition system, which applies methods for opti-
mization from signal detection theory, information theory,
signal processing and linear algebra. With our modiÞcations
we were able to speed up the computation time while
outperforming the original classiÞcation performance, which
creates a system that integrates the potential of biologically-
inspired hierarchical models into a technical application. We
also enhanced the model to be object location sensitive with-
out performance loss by making use of object subdirectories,
which adds a crucial aspect to a vision system.
REFERENCES
[1] N. Kruger, P. Janssen, S. Kalkan, M. Lappe, A. Leonardis, J. Piater,
A. Rodriguez-Sanchez, and L. Wiskott, ÒDeep hierarchies in the
primatevisualcortex:Whatcanwelearnforcomputervision?ÓPattern
Analysis and Machine Intelligence, IEEE Transactions on, vol. 35,
no. 8, pp. 1847Ð1871, 2013.
[2] T. Poggio, ÒThe Computational Magic of the Ventral Stream,Ó Nature
Precedings, 2012.
[3] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado,
J. Dean, and A. Y. Ng, ÒBuilding high-level features using large scale
unsupervised learning,Ó arXiv preprint arXiv:1112.6209, 2011.
[4] M. Thomure, W. Landecker, and M. Mitchell, ÒRandom prototypes in
hierarchical models of vision,Ó Learning, no. 1998, p. 2010, 2010.
[5] T. Serre, G. Kreiman, M. Kouh, C. Cadieu, U. Knoblich, and T. Pog-
gio, ÒA quantitative theory of immediate visual recognition.Ó Progress
in brain research, vol. 165, pp. 33Ð56, Jan. 2007.
[6] E.MeyersandL.Wolf,ÒUsingBiologicallyInspiredFeaturesforFace
Processing,Ó International Journal of Computer Vision, vol. 76, no. 1,
pp. 93Ð104, July 2007.
[7] S. Chen, Y. Li, and N. M. Kwok, ÒActive vision in robotic systems: A
survey of recent developments,Ó The International Journal of Robotics
Research, vol. 30, no. 11, pp. 1343Ð1377, 2011.
[8] A. Andreopoulos, S. Hasler, H. Wersing, H. Janssen, J. K. Tsotsos,
and E. Korner, ÒActive 3d object localization using a humanoid robot,Ó
Robotics, IEEE Transactions on, vol. 27, no. 1, pp. 47Ð64, 2011.
[9] C. Goerick, H. Wersing, I. Mikhailova, and M. Dunn, ÒPeripersonal
space and object recognition for humanoids,Ó in Humanoid Robots,
2005 5th IEEE-RAS International Conference on. IEEE, 2005, pp.
387Ð392.
[10] H. Wersing and E. K¬ orner, ÒLearning optimized features for
hierarchical models of invariant object recognition.Ó Neural
computation, vol. 15, no. 7, pp. 1559Ð88, July 2003. [Online].
Available: http://www.ncbi.nlm.nih.gov/pubmed/12816566
[11] ÒLearning optimized features for hierarchical models of invariant
object recognition.Ó Neural computation, vol. 15, no. 7, pp. 1559Ð88,
July 2003.
[12] B. Rasolzadeh, M. Bj¬ orkman, K. H¬ ubner, and D. Kragic, ÒAn active
vision system for detecting, Þxating and manipulating objects in the
real world,Ó The International Journal of Robotics Research, vol. 29,
no. 2-3, pp. 133Ð154, 2010.
[13] T. Halverson and A. J. Hornof, ÒA computational model of active
vision for visual search in humanÐcomputer interaction,Ó HumanÐ
Computer Interaction, vol. 26, no. 4, pp. 285Ð314, 2012.
[14] C. Siagian and L. Itti, ÒRapid biologically-inspired scene classiÞcation
using features shared with visual attention,Ó Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 29, no. 2, pp. 300Ð
312, 2007.
[15] V. Mahadevan and N. Vasconcelos, ÒBiologically inspired object
trackingusingcenter-surroundsaliencymechanisms,Ó Pattern Analysis
and Machine Intelligence, IEEE Transactions on, vol. 35, no. 3, pp.
541Ð554, March 2013.
[16] A. Ude, D. Omrÿ cen, and G. Cheng, ÒMaking object learning and
recognition an active process,Ó International Journal of Humanoid
Robotics, vol. 5, no. 02, pp. 267Ð286, 2008.
[17] A. Ude, C. Gaskett, and G. Cheng, ÒSupport vector machines and
gabor kernels for object recognition on a humanoid with active
foveated vision,Ó in Proceedings. IEEE/RSJ, vol. 1, 2004, pp. 668Ð
673.
[18] A. Ude, V. Wyart, L.-H. Lin, and G. Cheng, ÒDistributed visual
attention on a humanoid robot,Ó in Humanoid Robots, 2005 5th IEEE-
RAS International Conference on, 2005, pp. 381Ð386.
[19] M. Riesenhuber and T. Poggio, ÒHierarchical models of object recog-
nition in cortex.Ó Nature neuroscience, vol. 2, no. 11, Nov. 1999.
[20] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio, ÒRobust
object recognition with cortex-like mechanisms.Ó IEEE transactions
on pattern analysis and machine intelligence, vol. 29, no. 3, pp. 411Ð
26, 2007.
[21] P.MorenoandM.J.Mar,ÒAcomparativestudyoflocaldescriptorsfor
object category recognition : SIFT vs HMAX,Ó Pattern Recognition,
no. June, pp. 1Ð8, 2007.
[22] J. Mutch and D. Lowe, ÒMulticlass Object Recognition with Sparse,
Localized Features,Ó 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition - Volume 1 (CVPRÕ06), pp.
11Ð18, 2006.
[23] C. Theriault, N. Thome, and M. Cord, ÒHMAX-S : Deep Scale
Representation for biologically inspired Image Categorization,Ó Image
(Rochester, N.Y.), pp. 3Ð6, 2011.
[24] T. Serre, L. Wolf, and T. Poggio, ÒObject recognition with features in-
spired by visual cortex,Ó in Computer Vision and Pattern Recognition,
CVPR 2005, vol. 2. Ieee, 2006, pp. 994Ð1000.
[25] D. Hubell and T. Wiesel, ÒReceptive Þelds of single neurones in the
catÕs striate cortex,Ó The Journal of Physiology, vol. 148, no. 3, 1959.
[26] A. Holzbach and G. Cheng, ÒEnhancing Object Recognition for
Humanoid Robots through Time-Awareness.Ó in Humanoid Robots,
2013, 13th IEEE-RAS International Conference, October 2013.
[27] ÑÑ, ÒAn information theoretic approach to an entropy-adaptive
neurobiologically inspired object recognition model,Ó Frontiers in
Computational Neuroscience, no. 135, 2011.
3206
