  
? 
Abstract— This paper presents a novel place recognition 
algorithm inspired by the recent discovery of overlapping and 
multi-scale spatial maps in the rodent brain. We mimic this 
hierarchical framework by training arrays of Support Vector 
Machines to recognize places at multiple spatial scales. Place 
match hypotheses are then cross-validated across all spatial 
scales, a process which combines the spatial specificity of the 
finest spatial map with the consensus provided by broader 
mapping scales. Experiments on three real-world datasets 
including a large robotics benchmark demonstrate that 
mapping over multiple scales uniformly improves place 
recognition performance over a single scale approach without 
sacrificing localization accuracy. We present analysis that 
illustrates how matching over multiple scales leads to better 
place recognition performance and discuss several promising 
areas for future investigation. 
I. INTRODUCTION 
Robotic mapping and localization systems typically operate 
at either one fixed spatial scale, or over two, combining a 
local and a global scale [1-3]. In contrast, recent high profile 
discoveries in neuroscience have indicated that animals, 
such as rodents, navigate the world using multiple parallel 
maps, with each map encoding the world at a specific spatial 
scale [4, 5]. The multi-scale rodent mapping system consists 
of neurons that encode areas ranging from several square 
centimetres to several square meters, with many 
intermediate scales represented in-between. Unlike hybrid 
metric-topological multi-scale robot mapping systems, 
rodent maps are homogeneous, distinguishable only by 
scale.  Although theoretical studies have highlighted 
computational benefits of a multi-scale mapping system [6, 
7], no real world experiments have been done to investigate 
these principles.  
In this paper, we present a biologically-inspired multi-
scale mapping system mimicking the rodent multi-scale 
map. Our approach utilizes multiple arrays of Support 
Vector Machines, with each array trained to perform place 
recognition at a specific spatial scale, and a process for 
combining place recognition hypotheses from these different 
 
ZC, AJ and MM are with the School of Electrical Engineering and 
Computer Science at the Queensland University of Technology, Brisbane, 
Australia, zetao.chen@student.qut.edu.au. UE and MH are with Center for 
Memory and Brain and Graduate Program for Neuroscience at Boston 
University. This work was supported by an Australian Research Council  
Discovery Project DP1212775 awarded to Michael Milford. 
spatial scales (Figure 1). Unlike traditional probabilistic 
robotics methods, where spatial specificity is passively 
determined by sensor observation models, our approach 
intentionally creates parallel training systems to map the 
sensor input to the environment at different spatial scales. 
 
Figure 1. Illustration of our multi-scale place recognition system. 
Multiple parallel SVMs are trained to recognize places at different 
spatial scales and filter out hypotheses not supported by all scales. In 
this example, the number one ranked match at the highest spatial 
precision (yellow) is not supported by matches at lower spatial 
precisions. In contrast, the second ranked match is supported at all 
scales and is consequently correctly chosen as the place match. 
We conduct experiments on three real world datasets and 
compare single- and multi-scale place recognition 
performance. We extend an initial pilot study [8] by 
presenting a new, adaptive method for combining multi-
scale spatial hypotheses based on SVM firing score, rather 
than the manual approach described in [8]. For the first time 
the method is able to achieve a uniform improvement across 
all presented studies, improving the recall rate at 100% 
precision by an average factor of 77%. We present a new 
visualization method that illustrates how place hypotheses at 
different scales are combined and present for the first time 
results on the benchmark 70 km Eynsham dataset. 
The paper is organized as follows. Section II discusses 
related place recognition and mapping techniques. In Section 
III we describe the components of the multi-scale place 
learning system. Experiments are presented in Section IV, 
with results shown in Section V. Finally we conclude the 
paper in Section VI by discussing ongoing and future work. 
II. BACKGROUND 
In this section, we summarize single- and two-scale robotic 
Multi-scale Bio-inspired Place Recognition  
Zetao Chen, Adam Jacobson, U?ur M. Erdem, Michael E. Hasselmo and Michael Milford 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1895
  
spatial scales at varying recall rates. 
Multi-scale matching consistently improves the 
performance. Using Gist features, the recall rate at 100% 
precision improves by a factor of nearly 3 from 8% using a 
single-scale to 28% when using three scales. The 
corresponding improvement using PCA features is from 
14% to 23%. Using two scales results in an intermediate 
performance improvement. Matching coverage is relatively 
evenly distributed throughout the dataset, with the largest 
gap at 100% precision measuring approximately 2 km in 
length. 
Although our focus is on the improvement potential 
offered by adopting a multi-scale approach, we provide 
absolute comparison metrics here. The maximum recall rate 
of 28% at 100% precision is superior to the baseline FAB-
MAP performance, comparable to the motion-model FAB-
MAP performance and less than the approximately 50% 
recall achieved with FAB-MAP using both a motion model 
and epipolar geometric verification, and comparable to the 
20 frame SeqSLAM implementation [18].  
 
Figure 6. Precision recall curves demonstrating the single- (“200 meter 
system”) and multi-scale ((“combined 200, 400 meter system” and 
“combined 200, 400 and 800 meter system”) place recognition 
performance using gist feature on the Eynsham dataset 
 
Figure 7. Error rates using (from left to right in each bar cluster) one, 
two or three spatial mapping scales at various recall rates using Gist 
features.  
B. Ground Truth Plots 
Figure 11a-b presents ground truth plots showing the true 
positives (green circles), false positives (blue squares) and 
false negatives (red stars) output by the single and three-
scale systems for the Eynsham dataset at an identical recall 
rate. Straight lines connect the matching segments. The 
introduction of multiple matching scales removes most of 
the false positive matches from the single scale results. 
Further analysis of the multi-scale false positive match is 
provided in the following subsection. 
 
Figure 8. Place recognition coverage at 100% precision and 28% recall 
on the Eynsham dataset. Coverage is generally distributed with a worst 
case gap approximately 2 km long. 
 
Figure 9. Precision recall curves demonstrating the single- (“200 meter 
system”) and multi-scale (“combined 200, 400 meter system” and 
“combined 200, 400 and 800 meter system”) place recognition 
performance using PCA feature on the Eynsham dataset. 
 
Figure 10. Error rates using (from left to right in each bar cluster) one, 
two or three spatial mapping scales at various recall rates using PCA 
features. 
C. Multi-hypothesis Combination Plots 
Figures 11c-f show examples of how place match 
hypotheses at varying scales are combined together. In 
general, a large number of false positives at the smallest 
spatial scale (bottom yellow row) are eliminated due to lack 
of support from larger spatial scales. The examples in (c-d) 
show how secondary ranked spatially specific matches are 
correctly chosen as the overall place match due to support 
from other spatial scales. In (e) the best ranked spatially 
specific match is correctly supported by the other spatial 
scales, while (f) shows a failure case where the incorrect 4
th
 
ranked spatially specific match is more strongly supported 
1899
  
by the other spatial scales than the 1
st
 ranked and correct 
spatially specific match.  
D. Campus and Rowrah Dataset  
In these two datasets, the proposed method improves the 
recall rate by an average factor of 74.79% across all 
experiments at 100% precision. This performance represents 
a 34% improvement in the recall rate at 100% precision over 
that presented in the original study [8]. A larger 
improvement is achieved using Gist than PCA; at 100% 
precision, the recall rate for Gist was improved by an 
average of 81.7% over all experiments, versus 67.9% for 
PCA.  
VI. DISCUSSION AND FUTURE WORK 
We have demonstrated that implementing a multi-scale 
place recognition system improves place recognition 
performance by combining the output from parallel mapping 
frameworks, each trained to recognize places at a specific 
spatial scale. Although this paper presents a specific visual 
pre-processing techniques and learning mechanisms, we 
believe that the novel multi-scale combination concept 
should generalize to other sensor types, sensor processing 
schemes and learning methods. In this section we discuss 
several areas of current and future work. 
The current system assumes that the camera is moving at 
a constant speed during the training and testing stages. 
Incorporating an odometry source will allow the system to 
allocate segments directly based on spatial distances 
travelled rather than (in effect) time. Moreover, 
incorporating odometry information will enable us to expand 
our current system to two-dimensional unconstrained 
movement in large open environments. Testing the system in 
open field environments will be more analogous to many 
current rodent experiments and may increase the likelihood 
of generating neuroscience insights.  
The next step beyond odometry-driven segmentation is 
data-driven segmentation, where an environment is 
segmented based on local self-similarity. Such an approach 
would avoid inefficient representations of large bland spaces 
with small spatial scale maps. Furthermore, in large open 
spaces, precise localization is often not possible; in such a 
situation it may be possible to fall back to a less spatially 
specific place recognition estimate that uses broader visual 
cues. It may also be possible to improve the algorithm’s 
efficiency by performing selective hypothesis validation 
 
Figure 11. Ground truth plots for the (a) single and (b) multi-scale Eynsham dataset. (c-d) show examples of secondary-ranked spatially 
specific place matches (yellow) that became the primary overall place match hypothesis due to support from other spatial scales. In (e) 
the first ranked spatially specific match is supported, while (f) shows a failure case where a secondary ranked spatially specific match is 
incorrectly chosen as the overall match due to more significant support from the other spatial scales than the correct, first ranked spatially 
specific match. 
1900
  
using a “top-down” approach; only searching for finer scale 
place matches in areas of a map that are matched at the 
broadest level. 
Recent work using RatSLAM has shown that biologically 
inspired algorithms can perform online sensor fusion to 
enable place recognition in changing environmental 
conditions, such as over day-night cycles [28, 29]. An 
obvious extension to this research would be to use a multi-
scale mapping framework to exploit the variable spatial 
specificity of different sensor modalities, such as cameras, 
range finders and WiFi. By integrating these multi-sensor 
fusion systems with a biologically-inspired, multi-scale 
mapping framework, it may be possible to combine their 
functional capabilities to produce a highly capable, general 
purpose robot mapping and navigation system. 
REFERENCES 
[1] M. Bosse, P. Newman, J. Leonard, M. Soika, W. Feiten, and S. Teller, 
"An atlas framework for scalable mapping," in International 
Conference on Robotics and Automation, Taipei, Taiwan, 2003, pp. 
1899-1906. 
[2] B. Kuipers, J. Modayil, P. Beeson, M. MacMahon, and F. Savelli, 
"Local Metrical and Global Topological Maps in the Hybrid Spatial 
Semantic Hierarchy," in International Conference on Robotics and 
Automation, New Orleans, USA, 2004. 
[3] B. Kuipers and Y. T. Byun, "A Robot Exploration and Mapping 
Strategy Based on a Semantic Hierarchy of Spatial Representations," 
Robotics and Autonomous Systems, vol. 8, pp. 47-63, 1991. 
[4] H. Stensola, T. Stensola, T. Solstad, K. Froland, M. Moser, and E. 
Moser, "The entorhinal grid map is discretized," Nature, vol. 492, pp. 
72-78, 2012. 
[5] T. Hafting, M. Fyhn, S. Molden, M.-B. Moser, and E. I. Moser, 
"Microstructure of a spatial map in the entorhinal cortex," Nature, vol. 
11, pp. 801-806, 2005. 
[6] Y. Burak and I. R. Fiete, "Accurate path integration in continuous 
attractor network models of grid cells," PLoS Computational Biology, 
vol. 5, 2009. 
[7] P. E. Welinder, Y. Burak, and I. R. Fiete, "Grid cells: the position 
code, neural network models of activity, and the problem of learning," 
Hippocampus, vol. 18, pp. 1283-1300, 2008. 
[8] Z. Chen, A. Jacobson, and M. Milford, "Bio-inspired Place 
Recognition over Multiple Spatial Scales," presented at the 
Australasian Conference on Robotics and Automation, Sydney, 
Australia, 2013. 
[9] G. Dissanayake, P. M. Newman, S. Clark, H. Durrant-Whyte, and M. 
Csorba, "A solution to the simultaneous localisation and map building 
(SLAM) problem," IEEE Transactions on Robotics and Automation, 
vol. 17, pp. 229-241, June, 2001 2001. 
[10] M. Cummins and P. Newman, "Highly scalable appearance-only 
SLAM - FAB-MAP 2.0," in Robotics: Science and Systems, Seattle, 
United States, 2009. 
[11] M. Cummins and P. Newman, "FAB-MAP: Probabilistic Localization 
and Mapping in the Space of Appearance," International Journal of 
Robotics Research, vol. 27, pp. 647-665, 2008. 
[12] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, "MonoSLAM: 
Real-Time Single Camera SLAM," IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 29, pp. 1052-1067, 2007. 
[13] D. Ball, S. Heath, J. Wiles, G. Wyeth, P. Corke, and M. Milford, 
"OpenRatSLAM: an open source brain-based SLAM system," 
Autonomous Robots, pp. 1-28, 2013/02/21 2013. 
[14] M. Milford and G. Wyeth, "Persistent Navigation and Mapping using a 
Biologically Inspired SLAM System," International Journal of 
Robotics Research, vol. 29, pp. 1131-1153, 2010. 
[15] M. J. Milford, Robot Navigation from Nature: Simultaneous 
Localisation, Mapping, and Path Planning Based on Hippocampal 
Models vol. 41. Berlin-Heidelberg: Springer-Verlag, 2008. 
[16] M. Milford and G. Wyeth, "Mapping a Suburb with a Single Camera 
using a Biologically Inspired SLAM System," IEEE Transactions on 
Robotics, vol. 24, pp. 1038-1053, 2008. 
[17] K. Konolige and M. Agrawal, "FrameSLAM: From Bundle 
Adjustment to Real-Time Visual Mapping," IEEE Transactions on 
Robotics, vol. 24, pp. 1066-1077, 2008. 
[18] M. Milford, "Vision-based place recognition: how low can you go?," 
International Journal of Robotics Research, vol. 32, pp. 766-789, 
2013. 
[19] M. Milford and G. Wyeth, "SeqSLAM: Visual Route-Based 
Navigation for Sunny Summer Days and Stormy Winter Nights," in 
IEEE International Conference on Robotics and Automation, St Paul, 
United States, 2012. 
[20] Peter Biber and T. Duckett, "Experimental analysis of sample-based 
maps for long-term SLAM," The International Journal of Robotics 
Research vol. 28, pp. 20-33, 2009. 
[21] J. G. Heys, K. M. MacLeod, C. F. Moss, and M. E. Hasselmo, "Bat 
and rat neurons differ in theta frequency resonance despite similar 
coding of space," Science, vol. 340, pp. 363-367, 2013. 
[22] I. T. Jolliffe, Principal Component Analysis, 2 ed.: Springer, 2002. 
[23] I. Biederman, "Aspects and extension of a theory of human image 
understanding.," Computational processes in human vision: An 
interdisciplinary perspective, 1988. 
[24] M. C. Potter, "Meaning in visual search," Science vol. 187, pp. 965-
966, 1975. 
[25] A. Oliva and A. Torralba, "Modeling the shape of the scene: A holistic 
representation of the spatial envelope," International journal of 
computer vision, vol. 42, pp. 145-175, 2001. 
[26] V. Vapnik, "The support vector method of function estimation," 
Nonlinear Modeling, pp. 55-85, 1998. 
[27] N. Cristianini and S. T. John, An introduction to support vector 
machines and other kernel-based learning methods: Cambridge 
university press, 2000. 
[28] A. Jacobson, Z. Chen, and M. Milford, "Brain-based Sensor Fusion for 
Navigating Robots," in IEEE International Conference on Robotics 
and Automation, Karlsruhe, Germany, 2013. 
[29] A. Jacobson and M. Milford, "Towards Brain-based Sensor Fusion for 
Navigating Robots," in Proceedings of the 2012 Australasian 
Conference on Robotics & Automation, 2012. 
 
 
1901
