Novel Uniaxial Force Sensor based on Visual Information
for Minimally Invasive Surgery
A. Faragasso, J. Bimbo, Y . Noh, A. Jiang, S. Sareh, H. Liu, Member, IEEE
T. Nanayakkara, H.A. Wurdemann and K. Althoefer, Member, IEEE
Abstract— This paper presents an innovative approach of
utilising visual feedback to determine physical interaction forces
with soft tissue during Minimally Invasive Surgery (MIS). This
novel force sensing device is composed of a linear retractable
mechanism and a spherical visual feature. The sensor mecha-
nism can be adapted to endoscopic cameras used in MIS. As
the distance between the camera and feature varies due to the
sliding joint, interaction forces with anatomical surfaces can
be computed based on the visual appearance of the feature in
the image. Hence, this device allows the measurement of forces
without introducing new stand-alone sensors.
A mathematical model was derived based on validation
data tests and preliminary experiments were conducted to
verify the model’s accuracy. Experimental results conﬁrm the
effectiveness of our vision based approach.
I. INTRODUCTION
In recent years, Minimally Invasive Surgery (MIS), also
called laparoscopic or keyhole surgery, has become a well-
established and preferred approach to a growing number of
major surgeries ranging from relatively simple procedures
such as prostatectomy [1], cholecystectomy [2], and cystec-
tomy [3] to more complex surgical operations which are very
difﬁcult to conduct laparoscopically such as coronary artery
revascularization and mitral valve repair [4] [5]. MIS proce-
dures are performed using instruments that are inserted via
12-15 mm incisions called Trocar ports which allow surgeons
to bring surgical tools and sensors into the patient’s body.
MIS offers a number of advantages over traditional open
surgery including improved therapeutic outcome, shortened
postoperative recovery, reduced immunological stress re-
sponse of the tissue and tissue trauma, and less postoperative
pain; MIS is also is cost-effective [6]. However, it is reported
that the absence of physical tissue interaction is a major
limitation of MIS compared to traditional open surgeries [7].
During open procedures, surgeons have direct access to soft
tissue of organs and are able to manually palpate. Surgeons
can directly investigate the force-displacement response to
*The work described in this paper is partially funded by the Seventh
Framework Programme of the European Commission under grant agreement
287728 in the framework of EU project STIFF-FLOP, as well as by the
National Institute for Health Research (NIHR) Biomedical Research Centre
based at Guy’s and St Thomas’ NHS Foundation Trust and King’s College
London. The views expressed are those of the authors and not necessarily
those of the NHS, the NIHR or the Department of Health.
A. Faragasso, J. Bimbo, Y . Noh, S.Sareh, H. Liu, T. Nanayakkara,
H.A. Wurdemann and K. Althoefer are with the Centre for
Robotics Research, Department of Informatics, King’s College
London, London, WC2R 2LS, UK angela.faragasso,
joao.bimbo, yohan.noh, allen.jiang,
sina.sareh@kcl,hongbin.liu, thrish.antha,
helge.wurdemann, k.althoefer@kcl.ac.uk
Fig. 1: Novel Vision-based Single Axial Force Sensor (CAD
Drawing)
acquire distributed tactile information. Hence, haptic (force
and tactile) feedback has become essential in MIS: In pal-
pation procedures for tumor localization, clinicians press
their ﬁngers on the patient’s anatomical surfaces to assess
tool-tissue interaction forces to diagnose tissues as normal
or abnormal using tactile feedback [8]. Tissue areas that
are stiffer than the surrounding tissue can be recognized as
potentially abnormal and tumourous for instance [9]. Since
surgical robotic devices are improving incisively the perfor-
mance of the operations, they have been developed beyond
the investigational stage. Currently, these device continue
to evolve as they become more ergonomic [10]. Although
vision has been improved in MIS through the introduction of
high-deﬁnition 2D and 3D vision systems, methods of direct
palpation and haptic feedback in MIS are still in their infancy.
The force applied to soft organs can only be estimated
through visual feedback by observing the deformation of
the tissue in the transmitted camera images. Performing safe
surgeries in limited space and dynamic environments where
surgeons have a restricted view and no sense of touch have
created a growing demand on surgical vision techniques and
sensor developments in order to retrieve tactile feedback
similar to traditional open surgery. Analysis shows that a
new design is required to address these problems that occur
with the current equipment [7].
We propose a new low-cost vision-based force sensing device
as shown in Figure 1. The paper is organised as follows:
Section II reviews the current state of the art in sensing
approaches for MIS. The design of the proposed sensing
portotype is described in Section III. Section IV presents
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1405
the real-time visual algorithm and initial vision-stress tests.
From this, the mathematical model is derived (Section V).
The experimental results are reported in Section VI.
II. BACKGROUND
Trejos et al. [11] reviewed the current state of the art in
force sensing technologies in order to underline the current
limitations and evaluate the beneﬁts of haptic information for
surgical tasks in MIS. Recently, a number of sensors have
been developed to address the issue of force feedback for
keyhole surgery. In order to provide surgeons with haptic
feedback, researchers have integrated surgical instruments
with sensing capability to feed back the sense of touch
when indenting or grasping soft tissue [12]. One approach
of force measurements is to use light modulation techniques.
In [13], a miniaturized ﬁber optic sensor suitable for MR-
guided cardiac catheterization uses the variation in distance
and orientation between a reﬂective surface and a ﬁber optic
cable to estimate the axial and lateral force applied on a
surgical tip while the orientation is retrieved through image
analysis. The same principle was used to develop the triaxial
force sensor in [14]. This device can compute the orthogonal
component of the force applied to a compliant structure.
A distal force sensor suitable for minimally invasive tissue
palpation was developed in [15]. This sensor uses the optical
ﬁbre principle to compute the tissue interaction force and
can measure forces in a range of 3 N in axial and 1:5 N in
radial direction. In [16], elastomer elements were used to
develop an MR-compatible uniaxial force sensor for mitral
valve annuloplasty utilised within a beating heart. It is
waterproof and electrically passive. In [17] and [18], an
indentation depth sensor was developed and tested during
a rolling indentation procedure. The wheeled device can
compute tissue reaction forces as well as identify tissue
stiffness distribution visualised in form of a stiffness map. A
robotic master-slave teleoperated system with force reﬂection
capabilities that can be incorporated in MIS was developed
in [19]. The haptic master interface can reﬂect forces in all
directions. On the slave side, an actuation mechanism and
force/torque sensors were used within a disposable tip.
So far, tactile and force sensors have been applied to surgical
tools in MIS to measure local tissue properties. Providing
this feedback supports the surgeon operating with remote
mechanisms. Many studies use commercially available force
sensors such as the ATI Nano17 to measure force and torque
accurately. However, constraints on size, geometry, costs,
bio-compatibility and sterelisability make some of these
approaches not suitable for MIS. In [20], a force sensor
was integrated with the tip of an existing robotic instrument
for instance considerably increasing costs. Hence, another
research direction is to estimate forces applied to soft tissue
without using any traditional stand-alone force sensors.
Due to the limitations of the above technologies, this paper
proposes a new low-cost vision-based force sensing device
as shown in Figure 1. The advantages of this method are:
1) The sensor is small in size and can be miniturised for
MIS.
2) The current fabrication of the device provides MR-
compatibility, water and corrosion resistance, and leads
to a lightweight sensing structure.
3) The sensor principle is based on feature tracking
combining vision and a spring mechanism. The work
takes inspiration from the method used for wheeled
robot navigation in [21].
4) The sensing range and resolution can easily be cus-
tomised by changing the inside spring. ”Softer” springs
enable high force resolution sacriﬁcing range and vice-
versa.
The presented device can be attached to an endoscopic
camera allowing to measure forces along one 1 axis during
MIS procedures.
III. VISION-BASED FORCE SENSOR DESIGN
The prototype of the developed sensory device is shown in
Figure 2. Figure 2(a) gives an overview of the disassembled
parts. The camera spring force sensor consists of a spring-
driven linear shaft which is inserted into a cylindrical housing
in order to prevent lateral movement of the compression
spring. A small body (here: a spherical feature) is used to
”visualize” interaction forces. This assembled mechanism is
attached to a vision system as it can be seen in Figure 2(b).
Linear Module
Feature
Spring
Sensor Housing
Camera
(a)
Linear Module
Feature
Spring
Sensor Housing
Camera
(b)
Fig. 2: (a) Disassembled and (b) Assembled Force Sensor
Prototype
1406
Fig. 3: HSV Colour Space
The design allows the feature to be in the ﬁeld of view of the
camera. As the sphere interacts with soft tissue, the distance
between the camera and end-effector is modulated.
The size of the sensor is compared to a 50 pence sterling
coin. The spherical feature has a diameter of 5 mm, the
spring length is 35 mm and the linear shaft is 15 mm
long. The used camera is a USB camera with an outer
diameter of 7 mm, resolution of 640x480 and a frame
rate of 30 frames=s. The feature can be mounted directly
on the linear sliding shaft or on the end-effector having
direct contact with the surrounding objects. The model was
designed using SolidWorks and manufactured with a rapid
prototyping machine (Project HD-3000 Plus, 3D Systems).
The Project rapid prototyping machine employs a large
number of printing jets to print objects in 3D. Using this
machine to manufacture the novel sensing mechanism allows
miniaturisation.
The resolution and range of this design are customisable,
depending on the spring constant of the inner spring. The
maximum force the sensor can measure is limited by the
maximum size of the spring that can be integrated with
a laparoscopic tool that ﬁt through a Trocar port. The
resolution is variable, as it depends on the current distance
between the camera and the feature. It corresponds to the
minimum observable change in force, which is the required
force to increase the radius of the tracked feature by 1 pixel.
This value is larger when the feature is further away from
the camera and decreases as it comes closer.
IV. INTRINSIC RELATION BETWEEN VISUAL FEEDBACK
AND FORCE
A. Real-time Image Processing
Image processing is performed using OpenCV , an open
source computer vision system interfaced in ROS (Robot
Operating System). It is based on the detection and tracking
of a colored sphere in an image. Here, we use the HSV
colour channel for colour detection. The HSV colour space
is a model that describes a colour (Hue-value) in terms of
its shade (Saturation or amount of gray) and brightness (in-
tensity value or luminance). In its cylindrical representation
(see Figure 3), the angle around the z-axis corresponds to
the Hue value, the distance to the z-axis to the Saturation
value which deﬁnes the colour’s impurity and purity, and
the distance along the z-axis to the intensity (V value). A
script was created to select the feature’s HSV values. This
allows to determine the colour interval online and adapt the
algorithm in case of different feature colours.
For initial tracking, OpenCV’s implementation of the Hough
Transform is utilised to detect the feature. The performance
of this method results in an inaccurate recognition as the
feature is not able to be detected as a perfect circle. This
effect occurs due to light disturbances in the environment
affecting thresholds of the Canny edge detector which is
performed intrinsically. To overcome this problem, a more
sophisticated algorithm is implemented. Our method applies
the morphological operator in a black/white image to select
equivalent pixels in the HSV interval. Thus, it is possible to
deﬁne the properties of areas in the image. Inner fragmentary
regions are ﬁlled and bays along the corners eliminated.
This is obtained through a sequence of two morphologi-
cal primitives: dilate and erode. Noise and false positives
affecting the HSV ﬁlter are removed using the Gaussian
blur algorithm. Hence, the algorithm successfully detects the
Fig. 4: Feature Detection using OpenCV HoughCircle (in
red) and the proposed Algorithm (in orange)
Fig. 5: Vision Performance during Occlusions
1407
Single Axial 
Force Sensor
Motorised Linear Module 
ATI Nano 17 
Sensor
Data Acquisition Software
Feature Analysis Algorithm
Fig. 6: Setup
minimum enclosing circle that contains the white points in
the image.
Figure 4 shows the comparison between the HoughCircle
transformation and the improved algortithm. The perfor-
mance difference can be clearly observed between the red
and orange feature detection result. The developed algorithm
is also robust to occlusions as shown in Figure 5. The
image in the bottom left illustrates the transformation of
the input image into HSV colour space. In the top right
image, the output of the morphological operation and, in the
bottom right image, the blurred image is shown. The ﬁeld
of view is occluded, however, the orange circle represents
the successful feature detection in the top left image. This
algorithm allows to accurately compute the radius of the
feature which will be mapped to any applied force.
B. Computation of the Spring Parameter
In this section, stress test are performed using a ATI
Nano 17 force/torque sensor (SI-12-0.12, resolution 0:003 N
with a 16-bit data acquisition card) to validate the proposed
method and determine the stability of the system. During
the experimental tests, the device is ﬁxed on a motorised
linear module. The ATI Nano17 sensor applies linear forces
to the feature while moving along the module. The force
Fig. 7: Distance versus Force
Fig. 8: Pinhole Camera Model
sensor is connected to a PC running the LabView software
in order to compute the spring constant. Image data is
simultaneously recorded during this test. Figure 6 shows the
entire experimental setup.
Figure 7 shows the results. The spring constant, that deﬁnes
the relation between the displacement and the force, is equal
to 177:9 N=m and was computed using a linear ﬁtting.
V. MODELLING FORCE VERSUS FEATURE RADIUS
A. Mathematical Model
The pinhole camera model in Figure 8 deﬁnes the mathe-
matical relationship between 3D point coordinates and their
projection onto an image plane of an ideal pinhole camera.
This ideal model can be used as a ﬁrst order approximation
of mapping a 3D scene to a 2D image. In this model, the
focal length f represents the distance between the originO
of a 3D coordinate system and the image plane. Considering
a pointP in the world coordinate frame atP = (x
1
;x
2
;x
3
)
and its projection Q = (y
1
;y
2
) in the camera frame, it is
possible to deﬁne the relation between the 3D coordinate of
P and the coordinates ofQ in (Y
1
;Y
2
) as shown in Figure 8.
From the proprieties of the similar triangles, it follows that:

y
1
y
2

= 
f
x
3

x
1
x
2

(1)
We considered the relation between the sphere dimension
into the 2D image and the distance computed by the sensor
during the contact. As the feature has a spherical shape and
the embedded sensor allows movements only along a single
axis that is perpendicular to the camera, the variation of
sphere’s radius in the image can be related to the distance.
If r represents the sphere’s radius, x the distance between
the sphere position and the camera, and h the projection
of the radius in the image plane, the following will result
considering Equation 1:
h
x
=
r
f
(2)
Considering the initial position x
0
and radius r
0
of the
sphere, from Equation 2 is possible to express every new
1408
Fig. 9: Sphere’s Projection in the Image Plane
position in function of these known variables, shown in
Figure 9:
x =
hf
r
=
x
0
r
0
r
(3)
We obtain the expression of x:
x =x
0
 
r
0
r
x
0
(4)
The model that expresses force in function of the circle radius
was obtained using Equation 4 in Hooke’s law for the force
response of springs:
F (r) =Kx
0

1 
r
0
r

(5)
Here, K represents the spring constant. The initial position
of the spherical feature x
0
is equal to 42 mm and the initial
radius r
0
is 94 pixels. The mathematical model depends on
the design of the sensor and the visual feature, i.e. initial
radius r
0
and feature-camera distance x
0
, so an adjustment
is required for any new design of the sensor. In addition,
the sphere’s radius can be substituted with other geometrical
parameters and this make its validation independent of the
visual feature shape . To evaluate the performance of the
model, the Root Mean Squared Error (RMSE) was used
which is deﬁned as:
RMSE =
r
P
n
t=1
(y F (r))
2
n
(6)
The resulting RMSE for this model was 0.0404. Figure 10
plots the results of the experimental data and the mathemat-
ical model. It can be seen that the model does not ﬁt exactly
with the experimental data.
Fig. 10: Mathematical Model Fitting
Fig. 11: Experimental Model Fitting
B. Experimental Model
The next approach is to ﬁnd a mathematical function that
directly expresses forces measured in terms of the feature’s
radius. This equation is derived using Matlab’s Curve Fitting
Tool, where the best ﬁt to the data points is found. Figure 11
shows the ﬁtted curve which is obtained using the rational
function in Equation 7. Since the mathematical model was
derived in the previous section, there is no risk of overﬁtting.
F (r) =a
1
(1 
a
2
r
) (7)
The parameters found werea
1
= 7.143 anda
2
=91.94. The ﬁt
has a prediction interval of 95 % and its RMSE is 0:03.
VI. EXPERIMENTAL RESULTS
Experimental tests were performed to validate the models
described on the previous two sections. Our sensor was
manually pushed against the Nano 17 force/torque sensor.
The results in Figure 12 shows that both models perform
reasonably well, failing only when the force goes above
the sensors range, which is lower than the benchmarking
sensor, saturating it. Another issue was the response speed
of the sensor, which is affected by the existence of friction.
Fig. 12: Model Results
1409
Since the desired application of this sensor is MIS, very fast
response speed is not an essential requirement. In terms of
accuracy when the force is inside the sensor’s range, the
RMSE was 0.1535 for the mathematical model and, for the
experimental model 0.1355.
The spring used in this experiment allowed a range of
0   1:96 N and a variable resolution between 0:0439 N and
0:0787 N.
VII. DISCUSSION AND CONCLUSIONS
In this paper a new force sensor mechanism has been
proposed which utilises visual information to compute force
exerted by tracking a feature. The sensor consist of a
normal camera and a compression spring, and can be used
in combination with an endoscopic camera. The resulting
device is, therefore, low-cost and can be regarded as a
disposable instrument. Here, we have explained our approach
for the design and the implementation of a new sensing
device which, to the best of authors’ knowledge, is the
ﬁrst implementation of force sensing device which only
relies on visual information. In this work, we have also
developed an image tracking algorithm that relates the radius
of the spherical feature into the values of the exerted force.
This tracking algorithm can be generalised to be used with
various geometries of the feature. In order to validate our
design and modelling approach we have obtained the force-
radius relationship through experiments. The experimental
results shown that the mathematical model derived presents
a good approximation of the experimental data obtained
through benchmarking with a very accurate commercial force
sensor. The limitations and inaccuracies of the sensor are
mostly due to lens distortion and friction. These issues
should be addressed in future designs. Further development
of this sensing device will also consider miniaturisation and
optimisation studies on the size and the geometry of the
visual feature. Problems that can effect the performance of
the vision algorithm, i.e. occlusions due to blood and smoke
will also be regarded investigating new kind of materials and
vision processing algorithms.
REFERENCES
[1] T. Wilson and R. Torrey, “Open versus robotic-assisted radical prosta-
tectomy: which is better?,” Current Opinion in Urology, vol. 21 (3),
pp. 200–205, 2011.
[2] W. Ji, Z. Zhao, J. Dong, H. Wang, F. Lu, and H. Lu, “One-stage
robotic-assisted laparoscopic cholecystectomy and common bile duct
exploration with primary closure in 5 patients,” Surgical Laparoscopy
Endoscopy Percutaneous Techniques, vol. 21 (2), pp. 123–126, 2011.
[3] P. Zehnder and I. S. Gill, “Cost-effectiveness of open versus laparo-
scopic versus robotic-assisted laparoscopic cystectomy and urinary
diversion,” Current Opinion in Urology, vol. 21 (5), pp. 415–419,
2011.
[4] M. E. Currie, J. Romsa, S. Fox, W. Vezina, C. Akincioglu, J. War-
rington, R. McClure, L. Stit, A. Menkis, W. Boyd, and B. Kiaii,
“Long-term angiographic follow-up of robotic-assisted coronary artery
revascularization,” The Annals of Thoracic Surgery, vol. 93 (5),
p. 142631, 2012.
[5] S. Masroor, C. Plambeck, and M. Dahnert, “Complex repair of a
barlows valve using the da vinci robotic surgical system,” ISOs
Work on Guidance for Haptic and Tactile Interactions, vol. 19 (5),
p. 593595, 2010.
[6] J. Whynott, “Micro Molding: Meeting the Challenges of Design-
ing Medical Devices for MInimallu Invasive Surgery,” in 2008 Mi-
cro/Nano Conference & Exhibits Society of Manufacturing Engineers,
no. 313, 2008.
[7] M. van Veelen, E. Nederlof, R. Goossens, C. Schot, and J. Jakimowicz,
“Ergonomic problems encountered by the medical team related to
products used for minimally invasive surgery.,” Surg Endosc, vol. 17,
no. 7, pp. 1077–81, 2003.
[8] T. R. Coles, D. Meglan, and N. W. John, “The role of haptics in
medical training simulators: A survey of the state of the art,” IEEE
Transactions on Haptics, vol. 4, no. 1, pp. 51–66, 2011.
[9] H. Stassen, J. Dankelman, and C. Grimbergen, “Developments in
Minimally Invasive Surgery and Interventional Techniques (MISIT),”
Conference on human decision making and manual control, pp. 212–
218, 1997.
[10] D. M. Herron and M. Marohn, “A consensus document on robotic
surgery.,” Surgical endoscopy, vol. 22, pp. 313–25; discussion 311–2,
Feb. 2008.
[11] A. L. Trejos, R. V . Patel, and M. D. Naish, “Force sensing and
its application in minimally invasive surgery and therapy: a survey,”
Proceedings of the Institution of Mechanical Engineers, Part C:
Journal of Mechanical Engineering Science, vol. 224, pp. 1435–1454,
Jan. 2010.
[12] G.-P. Haber, M. A. White, R. Autorino, P. F. Escobar, M. D. Kroh,
S. Chalikonda, R. Khanna, S. Forest, B. Yang, F. Altunrende, R. J.
Stein, and J. H. Kaouk, “Novel robotic da vinci instruments for
laparoendoscopic single-site surgery,”Urology, vol. 76, no. 6, pp. 1279
– 1282, 2010.
[13] P. Polygerinos, P. Puangmali, T. Schaeffter, R. Razavi, L. D. Senevi-
ratne, and K. Althoefer, “Novel miniature mri-compatible ﬁber-optic
force sensor for cardiac catheterization procedures,” in Robotics
and Automation (ICRA), 2010 IEEE International Conference on,
pp. 2598–2603, IEEE, 2010.
[14] P. Puangmali, P. Dasgupta, L. D. Seneviratne, and K. Althoefer,
“Miniaturized triaxial optical ﬁber force sensor for mri-guided min-
imally invasive surgery,” in Robotics and Automation (ICRA), 2010
IEEE International Conference on, pp. 2592–2597, IEEE, 2010.
[15] P. Puangmali, H. Liu, L. D. Seneviratne, P. Dasgupta, and K. Althoefer,
“Miniature 3-axis distal force sensor for minimally invasive surgical
palpation,” IEEEASME Transactions on Mechatronics, vol. 17, no. 4,
pp. 1–11, 2011.
[16] M. C. Yip, S. G. Yuen, and R. D. Howe, “A robust uniaxial force
sensor for minimally invasive surgery.,” IEEE transactions on bio-
medical engineering, vol. 57, pp. 1008–11, May 2010.
[17] H. Liu, P. Puangmali, D. Zbyszewski, O. Elhage, P. Dasgupta, J. S.
Dai, L. Seneviratne, and K. Althoefer, “An indentation depthforce
sensing wheeled probe for abnormality identiﬁcation during minimally
invasive surgery,” Proceedings of the Institution of Mechanical Engi-
neers, Part H: Journal of Engineering in Medicine, vol. 224, pp. 751–
763, June 2010.
[18] H. Liu, J. Li, X. Song, L. Seneviratne, and K. Althoefer, “Rolling
indentation probe for tissue abnormality identiﬁcation during mini-
mally invasive surgery,”Robotics,IEEETransactionson, vol. 27, no. 3,
pp. 450–460, 2011.
[19] M. Tavakoli, R. V . Patel, and M. Moallem, “A Force Reﬂective Master-
Slave System for Minimally Invasive Surgery,” no. October, pp. 3077–
3082, 2003.
[20] U. Seibold, B. Kubler, and G. Hirzinger, “Prototype of instrument
for minimally invasive surgery with 6-axis force sensing capability,”
Robotics and Automation, ... , no. April, pp. 498–503, 2005.
[21] N. Alt, Q. Rao, and E. Steinbach, “Haptic exploration for navigation
tasks using a visuo-haptic sensor,” in Interactive Perception Workshop,
ICRA 2013, (Karlsruhe, Germany), May 2013.
1410
