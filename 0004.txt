Visibility Analysis for Autonomous Vehicle Comfortable Navigation
Yoichi Morales, Jani Even, Nagasrikanth Kallakuri, Tetsushi Ikeda, Kazuhiko Shinozawa,
Tadahisa Kondo and Norihiro Hagita
Intelligent Robotics and Communication Laboratories
Advanced Telecommunications Research Institute International
Abstract—This work introduces a 3D visibility model for
comfortable autonomous vehicles. The model computes a vis-
ibility index based on the pose of the wheelchair within the
environment. We correlate this index with human navigational
comfort(discomfort)andwediscusstheimportanceofmodeling
visibility to improve human riding comfort. The proposed
approach models the 3D visual ﬁeld of view combined with
a two-layered environmental representation. The ﬁeld of view
is modeled with information from the pose of the robot, a 3D
laser sensor and a two-layered environmental representation
composed of a 3D geometric map with traversale area infor-
mation. Human navigational discomfort was extracted from
participants riding the autonomous wheelchair. Results show
that there is fair correlation between poor visibility locations
(e.g., blind corners) and human discomfort. The approach
can model places with identical traversable characteristics but
different visibility and it differentiates visibility characteristics
according to traveling direction.
I. INTRODUCTION
In human navigation, the walking style (e.g., position
within a corridor and velocity) while following a path is
inﬂuenced by the traversable characteristics of the environ-
ment and the good/poor visibility at a certain pose. In the
caseofblindcorners,peopletendtoslowdownandsmoothly
overshoot while turning as they do not know what to expect
ontheotherside.Inthecaseofacornerwithsamegeometric
characteristics but with good visibility, people do not need
to slow down nor to overshoot on turns. Figure 1 shows and
example of two corners in an environment which present the
same geometric traversable conﬁgurations, however, because
of their composition, the visibility is quite different. The left
side of Figure 1, shows an example of a corner with good
visibility,therefore,thepersononthewheelchairknowswhat
to expect around the corner. In the right image in Figure 1,
because of poor visibility, the person on the wheelchair does
not know what to expect around the corner. In the latter case,
even if no obstacle is present, the wheelchair driver would
be careful to slow down in case that evasive action would
need to be taken.
This paper points out the importance of modeling the
visibility of an environment for comfortable navigation
on a passenger vehicle. We discuss the relation between
good/poor visibility conditions while driving a wheelchair,
we present a model of environment visibility and discuss
This research was supported by the Ministry of Internal Affairs and
Communications with a contract entitled ‘Novel and innovative R&D
making use of brain structures.” All the authors are with the Advanced
Telecommunications Research Institute International, Kyoto, Japan
Fig. 1. Wheelchair turning right in a corner. In both cases the person faces
similar traversable characteristics, however, the visibility characteristics (in
red) are different. In the left image the passenger is aware of what to expect
after turning while in the right, the passenger does not know what to expect
after turning the corner
its importance towards computation of human comfortable
trajectories. The visibility analysis is implemented using 3D
range information from a laser sensor and a two-layered
environmental representation.
Previous work [1] presented an approach to build a
Human-Comfort Factor Map (“HCoM”) which allows a
planner to compute comfortable paths. The limitation of the
work is that the approach can only be extended to model
straight path environments. In this work we extend the study
and include the modeling of turns. We propose to compute
a visibility index which could be used for computing paths
for autonomous vehicles.
Thispaperintroducesastudywithhumanparticipantswho
manually drove the wheelchair in an indoor loop environ-
ment. Based on this study we extracted human tendencies
for approaching blind and non-blind turns. Then, the same
participants rode the wheelchair in autonomous mode and
pressedabuttongiventotheminthecasetheyfeltdiscomfort
or stress. We present a discussion of the correlation between
environmental visibility and navigational comfort.
A. Comfort
Previousworksaddressedhumancomfortorientedtowards
seat ergonomics [2] where subjective and objective methods
tomeasurecomfortwerediscussed.Subjectivemethods(ask-
ing people how comfortable they are) are direct, whereas ob-
jective methods (electromyography or stress measurements)
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2197
are indirect but less time consuming and less prone to error.
Nevertheless, it is difﬁcult to measure the origin of the mea-
surements obtained. In other works, vehicle comfort factors
were classiﬁed into dynamic (vibrations, shocks), ambient
(thermal,noise)andergonomics(passenger’sposition)[3].In
[4], vehicle ride comfort is measured objectively considering
mechanical vibration, shock and jerk as one of the main
source of discomfort.
Inthisworkwestudytherelationbetweenvehicleridedis-
comfortandenvironmentvisibility.Discomfortwasextracted
directly asking humans to press a button. We argue that
visibility, which depends on the surrounding characteristics,
plays an important role in the navigational style of humans
and conclude that if environment visibility from the present
location is not modeled, then, comfortable navigation is not
feasible. This assertion is discussed in the evaluation section
and throughout the paper.
B. Visibility Concept in Mapping and Robot Systems
There are previous works related to visibility modeling.
Chown et al. [5] deﬁned the term gateway to refer to
the locations with major changes in visibility, where there
is visual separation between neighboring areas. Occlusion
ﬁnding using virtual line models on laser sensor data to ﬁnd
gateways was proposed in [6]. In cognitive robot mapping,
gateways were also used as the directional places which sep-
arate environments [7]. In the work presented in this paper,
we modeled the visibility of the passenger as centered on the
seat of the wheelchair. and human comfort while driving a
wheelchair. We argue about the existing correlation between
environment visibility and human navigational comfort.
C. Wheelchair Navigation
Robotic wheelchairs provide users with mobility auton-
omy and safe navigation [8] and have been product of
previous research [9], [10], [11]. In these previous works, the
visibility model of the passenger was not speciﬁcally mod-
eled and related to the navigational style. Human-wheelchair
collaboration works have been presented in the literature. A
system to detect user’s needs and intentions using multiple
hypotheses method and dynamically generating safe trajec-
tories was proposed in [12]. Wang J. et al., in [13] proposed
to adapt a wheelchair assistance to variations of user perfor-
mance and environmental changes evaluating safety, comfort
(comfort is measured as function of the jerk of the vehicle)
and obedience in real time. Vander Poorten et al., in [14]
proposed a haptic guidance algorithm to provide assistance
for electric wheelchair navigation through narrow spaces.
Differently to the previous works on wheelchair navigation
assistance, the main point in the approach presented in this
paper is the modeling of visibility for safe comfortable
navigation.
In our experiments we present an analysis of 30 different
wheelchair participants in an indoor corridor loop environ-
ment containing a couple of blind corners and a couple
of corners with good visibility. Experimental results show
moderate correlation between the visibility characteristics of
the environment and discomfort of the participants.
II. VISIBILITY MODELING
The visibility model is composed of three main parts: a
robotic wheelchair, 3D data information and an environment
representation. The output is a visibility index which is an
indicator of the visible traversable areas in the environment
from the point of view of the robotic wheelchair.
In the restof the section the 3D ﬁeld of view of the robotic
wheelchair is explained, then, the two-layered environment
representation is presented. Finally, the approach to combine
the ﬁeld of view with the environmental representation to
compute the visibility index is detailed.
A. Field of View
Human eye visual ﬁeld extends beyond 90 degrees in the
transverse direction; this is the ﬁeld where something can
be seen. With both of the eyes the horizontal ﬁeld can be
extended up to 200
o
[15]. In this work, the ﬁeld of view
was modeled as a ﬁeld of 180
o
horizontal and 20
o
vertical.
The range data coming from an omnidirectional 3D laser
sensor was used to model the ﬁeld of view and only the data
within the aforementioned thresholds was used. The range
was limited to r
Max
= 8 m which we found a reasonable
distance corresponding to 5 seconds ahead of the position of
the wheelchair at its maximum velocity of 1.6 m/sec. For
reference see Figure 2.
The reason to select a vertical ﬁeld of view of 20
o
was
to avoid laser hits corresponding to the roof or the ﬂoor. As
the wheelchair localization module has inaccuracies where
a small drift in angle would produce a big error as distance
increases, the range distance was thresholded.
8m
Fig. 2. 3D ﬁeld of view of the wheelchair. Visibility is modeled as 180
o
horizontal and 20
o
vertical.
The ﬁeld of view is attached to the wheelchair and moves
and turns with it. It is projected on the environmental
model to compute the visible and non-visible areas of the
environment.
B. Environment Representation
The environment model has two layers. One layer contains
the three-dimensional occupancy voxel map and the other
one contains the traversable areas of it. The 3D voxel map
provides the framework to perform ray-casting with the
laser sensor and the 2D occupancy grid map to provide
the traversable areas in the environment. Overall, this repre-
sentation allows to differentiate between visible areas from
traversable areas and their combinations (Figs. 1, 4 and 6 ) .
2198
C. Visibility Index Computation
The visibility index V
index
is a number from 0 to 1
that represents the ratio of ray-casted laser beams that are
traversable and visible. The index is a function f(r
i
,m) =
[0,1] of all the i
th
laser beams r
i
within the ﬁeld of view
that are projected into the environmental map m. The ratio
reaches its maximum if all the traversable areas within the
ﬁeld of view are visible; the ratio reaches its minimum when
the traversable areas are not within the visible area. A ray-
casting technique is applied on every laser beam of the
ﬁeld of view towards the environmental model. The visibility
analysis is summarized in Fig. 3 and the process is detailed
below:
Visibility
Index
Robot
Pose
Environment
Model
3D Data
Visibility
Analysis
Fig. 3. Visibility analysis block diagram. The inputs of the model are the
robot pose, 3D visibility data information and the geometric representation.
The output is the visibility index.
Starting from the origin r
Or
, the ray is casted until its
maximumrange r
Max
;dependingontheregioninwhicheach
raysegmentfallsitisclassiﬁedinoneoffourcategories(Fig.
4):
1) Visible and traversable (green in Figs. 4(a) to 4(d)):
the ray goes from r
Or
to r
Max
without any hit.
2) Visible but not traversable (green in top of Figs. 4(c)
and 4(d)): the ray may hit or not hit a wall but falls in
a non-traversable region.
3) Not visible but traversable (red in Figs. 4(a) to 4(d)):
the ray starts from a previous hit r
Hit
?1
and ends in
another hit r
Hit
or the max range r
Max
and is located
on a traversable region.
4) Not visible and non traversable (yellow in Figs. 4(a),
4(b)): the ray starts from a previous hit r
Hit
?1
and ends
in another hit r
Hit
or the max range r
Max
and is located
on a non-traversable region.
Finally the visibility index V
index
is given by
V
index
=
R
VT
R
VT
+R
NT
(1)
where R
VT
=∑
k
S
VTk
is the sum of the length of all the k
visible and traversable laser ray segments S
VT
and R
NT
=
∑
j
S
NT j
is the sum of all non-visible but traversable j seg-
ments S
NT
. As shown in experimental results, the proposed
index is capable to model good visibility corners from blind
corners.Moreover,itproperlymodelsintersectionsaccording
to the direction of travel (Fig. 8).
(a) Visibility region in blind corner. (b) Visibility region in blind corner.
(c) Visibility in non-blind corner. (d) Visibility in non-blind corner.
Fig. 4. Visibility analysis on a blind corner on the top and a good visibility
corner on the bottom. The visible ray segments are shown in green, the non-
visible but traversable are in red and the non-visible and non-traversable are
in yellow. The ﬁgure presents snapshots of our data viewer (in OpenGL)
showing real experiment data.
III. SYSTEM OVERVIEW AND IMPLEMENTATION
A. Robotic Wheelchair and Sensor Framework
Experiments were conducted using an electric wheelchair
from Imasen Engineering Corporation. The wheelchair could
be controlled with a joystick and at the same time it is
interfaced with a laptop computer using USB communica-
tion. The wheel encoder data was received by the program
on the laptop and velocity commands could be sent to
the wheelchair. All the software used for controlling the
wheelchair motion was executed on this laptop computer.
The wheelchair was equipped with a Velodyne HDL?32E
laser scanner, two Hokuyo UTM?30LX 2D laser scanners
and a Crossbow VG440 Inertial Measurement Unit (IMU).
The placement of the sensors is shown in Fig 5.
B. Building the Environment Model
The maps were built off-line with logged data. The build-
ing process is brieﬂy explained below.
1) 3D Map Building: The robotic wheelchair is equipped
with wheel encoders which were used to obtain odometry
data and a velodyne laser sensor was used to obtain the
rangeinformation.Tobuildthemap,wedrovethewheelchair
with the joystick while the odometry and the laser sensor
information were recorded. We used an iterative closest point
(ICP) based SLAM to correct the trajectory of the robot
and align the laser sensor scans using the 3DToolkit library
framework [16]. After obtaining the point cloud map, we
down-sampled into 3D voxels ordered in an octree structure
2199
Fig. 5. Robotic wheelchair equipped with a 3D laser sensor, two 2D laser
sensors, an IMU, a bluetooth controller and wheel encoders.
2m
0m
33m
32m
Fig. 6. Environment representation. The occupancy grid map is in the
center where traversable areas are in white and non-traversable areas are in
black and gray. The corners of the 3D map are shown where the coloring
represents the different height of the voxels. The corners on the left have
good visibility and the ones on the right have poor visibility.
.
using the octomap library framework of [17] where the voxel
resolution was set to 0.1m x 0.1m x 0.1m.
2) 2D Map Building: The occupancy grid map was built
using the gmapping framework from the robot operative
system (ROS) [18]. The map is shown in Fig. 6 and share the
same coordinate frame as the 3D map. The ﬁeld of view was
combined with the occupancy grid map to perform a check
of the traversable areas that are present. Traversable areas are
the free cell areas (in white) and the non traversable areas are
the occupied cells and the unknown cells (black and gray).
C. Wheelchair Localization
The wheelchair localizes itself towards the map using
a particle ﬁlter approach based on laser data ray-casting
[19]. Each particle contains a pose given by state vector
ˆ x={x,y,?}, with position x and y and orientation ?.
IV. HUMAN COMFORT EXTRACTION
In this work we assume that comfortable navigation is
achieved when the user of the wheelchair does not feel in
danger or stressed, i.e., not in discomfort. Hence, we extract
the time stamped locations where participants felt stressed,
uneasy or in discomfort. The objective of the experiment was
to extract discomfort under the following conditions:
• All participants have used the wheelchair: before the
experiment, participants were allowed to practice and
drive the wheelchair.
• Participant discomfort was extracted while they rode the
wheelchair during autonomous navigation:
– Through a couple of the participant’s own replayed
trajectories without knowledge of it.
– Through a couple of trajectories from an expert
driver (common to all participants). The expert
drove the wheelchair with few zig-zag effects and
velocities close to maximum.
• During navigation participants were provided a wireless
button which they pressed in case of feeling in discom-
fort or danger.
A. Experimental Procedure
We called 30 Japanese people with an average age of
21.5 years (15 females and 15 males) who were payed
for their participation and did not have knowledge of the
content or objective of the experiments. The experiment was
held in an indoor corridor environment with four corners,
two with poor visibility (Fig. 6 top and bottom right) and
two with good visibility (Fig. 6 top and bottom left). The
total length of the path was of approximately 130 m. The
experiments were divided in two phases: self driving and
autonomous ride, where participants traversed the environ-
ment the same amount of time clockwise and counter-
clockwise (CW/CCW). We recorded all the time stamped
positions, linear/angular velocities, accelerations, laser data
and stress button control (only for autonomous navigation).
The experiments had three steps:
1) Getting familiar with the wheelchair: Before starting
the experiment, each participant was asked to drive the
wheelchair in an empty room for some minutes in order to
get used to the wheelchair control. Once they felt comfort-
able with the joystick control, they were asked to drive the
wheelchair in the corridor loop shown in Fig. 6.
2) ManualDriving: Eachparticipantdrovethewheelchair
manually in the corridor making 8 loops (4 CW + 4
CCW).Theydroveattheirowncomfortabledrivingstyleand
velocities. The path of the wheelchair during their manual
control was recorded.
2200
3) Autonomous Driving: Participants were asked to sit on
the wheelchair while it navigated autonomously replaying
two of the trajectories (CW and CCW) they followed during
the last lap of their manual driving and two of the trajectories
(CW and CCW) recorded by an expert who drove at high
velocities and few jerking. The order of the four different
autonomous runs were jumbled for all participants.
B. Extracting Comfort Information
In order to obtain the subjective feedback from the partic-
ipants about the comfort on the wheelchair navigation, they
were given a wireless controller and were asked to press a
button whenever they felt it as uncomfortable or uneasy. We
performed an analysis of the places in which participants
were pressing the controller and related it with different
discomfort effects:
• Poor visibility of the environment (blind corners or
intersections).
• Zig-zag effects are known to have effect in navigational
comfort [4] and are out of the scope of this work.
V. EVALUATION AND DISCUSSION
This section presents an evaluation of the proposed visi-
bility analysis in the four corner regions of the experimental
environment of Fig. 6. The histogram of the number of
observations of a speciﬁc visibility index and the histogram
of the number of button clicks are presented. Then their
relationisdiscussed.Wecomputedthecorrelationcoefﬁcient
between discomfort (stress button clicks) and visibility index
value. Then experimental results show the difference of the
visibility index when driving CW and CCW. Finally we
discuss the system limitations.
A. Correlation between visibility and discomfort
Using the data of the 30 participants while riding the
wheelchair autonomously (four runs per participant) we built
the histograms of Figure 7 which show the relation between
the visibility index and the number of times that the stress
button was pressed. The visibility index is shown on the
horizontal axis of the histograms in a range from 0 to
1 divided in 50 bins (0.02 per bin). The top histogram
shows the total number of observations (N
Observation
) of a
determined visibility index. The middle histogram shows
the total number of times in which the passengers pressed
the button (N
Button
) when they felt stress or discomfort. In
the bottom histogram, the vertical axis shows the visibility-
observation ratio (R) of the number of times that the stress
button was pressed in each bin divided by the total number
of observations. The visibility-observation ratio for the i
th
bin is given by expression (2):
Ratio=
N
Button
N
Observation
(2)
Figure 7 shows that the histogram had a maximum ratio
value R
i
at V
index
= 0.6 and as V
index
increases the ratio
values decreased. In these experiments, the wheelchair kept
enough distance from the wall and obstacles, therefore,
the visibility index became bigger than 0.6. The Pearsons
correlation coefﬁcient of the Ratio value and the visibility
index is ?0.627 (p = 0.002). This can be interpreted as
the participants feeling less comfortable at lower values of
visibility. As V
index
increased the rate in which participants
pressed the button decreased as the visibility was better.
For our experimental data set, as the results in Fig. 7
show, at low visibility the ratio reaches its maximal value
1. It is expected to have higher ratio as V
index
decreases,
however, reaching the maximal value is not expected if
similar experiments are held with a different data set.
Fig. 7. Environment visibility histograms where the horizontal represents
the visibility index in a range from 0 1 divided in 50 bins. The top shows the
histogram with the total number of observations N
Observation
. In the middle
the total number of times in which the button was pressed N
Button
. The
bottom shows the visibility-observation ratio (R) .
B. Visibility modeling
The approach presented in this work is capable of mod-
eling environmental visibility according to the direction of
motion. Results of visibility index while driving CW and
CCW are shown in Figs. 8(a) and 8(b). It is notable that
in the corner at the bottom right of Figs. 8(a) and 8(b) the
visibility index score when going CW is very low as the
corridor coming from the right is not visible, on the contrary,
when going CCW the corridors coming from the top and
right are visible, therefore the visibility score is high.
Fig. 9 (left) shows how people driving in blind corners
tend to “over” turn in order to gain visibility while turning.
On the contrary, Fig. 9 (right) shows how in non-blind
corners (good visibility like Fig. 1 left side) the participant
drove even closer to the corner.
C. Discussion
The visibility index is a value between 0 and 1, however,
results show that for our environment values under 0.5 were
not found. In other types of environments lower values could
be computed, therefore we decided not to shift the index to
theleft(Fig.7).Asourexperimentalenvironmentwassimple
there was no need to perform a connectivity analysis (e.g.,
usingatopologicalmap)tocomputewhatnextpossiblepaths
are traversable in the case of intersections.
2201
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
CW
(a) Running clockwise.
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
CCW
(b) Running counter clockwise.
Fig. 8. Visibility index in a robotic wheelchair run. It can be appreciated
how the visibility is modeled differently when running clockwise than that
counter-clockwise.Thisismorenoticeableatthebottomrightofthegraphs.
CW
CCW
Turning at Blind Corner
CW
CCW
Turning at Corner with good Visibility
Fig. 9. Trajectories of a participant driving the wheelchair clockwise (CW)
and counter clockwise (CCW). It can be seen how people try to increase
the visibility while turning in the blind corners.
Experimentalresultsdidnotofferevidenceofdifferencein
discomfortbetween driving CWor CCW.Alsono conclusive
difference between expert and self driving runs was found.
We believe that passengers get habituated to the navigation
of the vehicle after few runs; this is, the more experienced
theybecomethelessstressedtheyfeel.Passengerhabituation
effects during navigation need to be observed and modeled.It
is left for future work to study habituation effects in the
correlation coefﬁcient between stress and visibility index.
In this work we used a robotic wheelchair as experi-
mental platform and did not perform experiments with real
wheelchair users. This work is not intended to be vehicle
speciﬁc and its results should be useful for other types of
personal mobility devices.
VI. CONCLUSIONS
This paper presented a novel approach for modeling the
visibility of a passenger on a vehicle. Experimental results
show that human participant discomfort during navigation in
corners has moderate correlation to environmental visibility.
Thevisibilityanalysispresentedinthisworkcandifferentiate
between corners with same traversable characteristics but
different visibility (corners in Fig. 8). Furthermore, results
conﬁrm that the proposed approach offers different visibility
index at the same location of the environment depending on
the driving direction (bottom left corner of Figs. 8 (a) and
(b)). As future work we plan the implementation of a path
planner which uses the visibility index as a parameter to
compute comfortable paths according to the characteristics
of the environment. We are working in the integration of the
visibilitymodeltoahumancomfortmap(HCoM)[1]tocom-
pute human comfortable paths based on the environmental
visibility in turns, corners and intersections.
REFERENCES
[1] Y. Morales, N. Kallakuri, K. Shinozawa, T. Miyashita, and
N. Hagita, “Human-comfortable navigation for an autonomous robotic
wheelchair,” in Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ
International Conference on, 2013.
[2] M.deLooze,L.Kuijt-Evers,andJ.vanDien,“Sittingcomfortanddis-
comfort and the relationships with objective measures.” Ergonomics,
vol. 46, no. 10, pp. 985–97, 2003.
[3] O. Ormu, Krunoslav; Mufti, “Main ambient factors inﬂuencing pas-
senger vehicle comfort,” Proceedings of 2nd International Ergonomics
Conference, vol. 2, pp. 77–84, 2004.
[4] K. Strandemar and K. tekniska h¨ ogskolan, On Objective Measures
for Ride Comfort Evaluation, ser. Trita-S3-REG, 2005. [Online].
Available: http://books.google.co.jp/books?id=loectgAACAAJ
[5] E. Chown, S. Kaplan, and D. Kortenkamp, “Prototypes, location, and
associative networks (plan): Towards a uniﬁed theory of cognitive
mapping,” Cognitive Science, vol. 19, no. 1, pp. 1–51, 1995.
[6] D. Schr¨ oter, T. Weber, M. Beetz, and B. Radig, “Detection and
classiﬁcationofgatewaysfortheacquisitionofstructuredrobotmaps,”
in DAGM-Symposium, 2004, pp. 553–561.
[7] P. Beeson, J. Modayil, and B. Kuipers, “Factoring the mapping
problem: Mobile robot map-building in the Hybrid Spatial Semantic
Hierarchy,” International Journal of Robotics Research, vol. 29, no. 4,
pp. 428–459, 2010.
[8] E.W¨ astlund,K. Sponseller,andO. Pettersson, “Whatyouseeis where
you go: testing a gaze-driven power wheelchair for individuals with
severe multiple disabilities,” in Proceedings of the 2010 Symposium
on Eye-Tracking Research Applications. New York, NY, USA: ACM,
2010, pp. 133–136.
[9] E. Prassler, D. Bank, and B. Kluge, “Key technologies in robot
assistants: Motion coordination between a human and a mobile robot,”
Transactions on Control, Automation and Systems Engineering, vol. 4,
2002.
[10] Y. Kobayashi, Y. Kinpara, T. Shibusawa, and Y. Kuno, “Robotic
wheelchair based on observations of people using integrated sensors,”
in Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ Inter-
national Conference on, oct. 2009, pp. 2013 –2018.
[11] A. Argyros, P. Georgiadis, P. Trahanias, and D. P. Tsakiris, “Semi-
autonomous navigation of a robotic wheelchair,” Journal of Intelligent
and Robotic Systems, vol. 34, 2001.
[12] T. Carlson and Y. Demiris, “Human-wheelchair collaboration through
prediction of intention and adaptive assistance,” in in Proc. of IEEE
International Conference on Robotics and Automation, 2008, pp.
3926–3931.
[13] Q. Li, W. Chen, and J. Wang, “Dynamic shared control for human-
wheelchair cooperation,” in Robotics and Automation (ICRA), 2011
IEEE International Conference on, 2011, pp. 4278–4283.
[14] E. Vander Poorten, E. Demeester, E. Reekmans, J. Philips, A. Hunte-
mann, and J. De Schutter, “Powered wheelchair navigation assis-
tance through kinematically correct environmentalhaptic feedback,” in
RoboticsandAutomation(ICRA),2012IEEEInternationalConference
on, 2012, pp. 3706–3712.
[15] D. Henson, Visual ﬁelds, 2nd edition. Butterworth-Heinemann, 2000.
[16] A. Nuchter, K. Lingemann, J. Hertzberg, and H. Surmann, “6d slam-
3d mapping outdoor environments.” Journal of Field Robotics, vol. 24,
no. 8-9, pp. 699–722, August 2007.
[17] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and
W. Burgard, “OctoMap: An efﬁcient probabilistic 3D mapping
framework based on octrees,” Autonomous Robots, 2013, software
available at http://octomap.github.com. [Online]. Available: http:
//octomap.github.com
[18] G. Grisetti, C. Stachniss, and W. Burgard, “Improved techniques for
grid mapping with rao-blackwellized particle ﬁlters,” Robotics, IEEE
Transactions on, vol. 23, no. 1, pp. 34–46, 2007.
[19] F. Dellaert, D. Fox, W. Burgard, and S. Thrun, “Monte carlo localiza-
tion for mobile robots,” in IEEE International Conference on Robotics
and Automation (ICRA99), May 1999.
2202
