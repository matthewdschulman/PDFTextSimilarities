Vision-Based Motion Control of a Flexible Robot for Surgical
Applications
Alessandro Vandini, Antonino Salerno, Christopher J. Payne, Guang-Zhong Yang, Fellow, IEEE
Abstract— In recent years, continuum robots have gained
signiﬁcant momentum in terms of technological maturity and
clinical application. Their ﬂexibility allows complex treatment
sites to be reached with minimal trauma to the patient.
However the reliable control of continuum robots is still an
ongoing research issue in the robotics community because
their deformable structure makes the modeling of these devices
difﬁcult. This motivates the use of external sensors or vision
to achieve accurate control. In this paper, a motion control
framework based on a vision sensor is proposed in order
to perform accurate and controlled movements of a ﬂexible
robot that is mounted to an anthropomorphic robotic arm.
The vision sensor, which relies on a single camera, provides
accurate 3D shape reconstruction and spatial localisation of
the ﬂexible robot. This information is used to provide feedback
for the real-time control of the ﬂexible robot. The vision
sensor detects the robot ﬁrst in an image stream by modeling
its appearance using compressed visual features in an online
learning framework. This is combined with the kinematics
information from the anthropomorphic robotic arm in order to
accurately reconstruct and localise the 3D shape of the ﬂexible
robot by minimizing an energy function. Detailed analysis of
the framework and a validation are presented in order to
demonstrate the practical value of the proposed method.
I. INTRODUCTION
Minimally invasive surgery (MIS) has gained increasing
popularity as an alternative to open surgery. It has the advan-
tages of reduced blood loss, minimized risk of infection and
shorter patient recover times. However, such surgery must be
performed through small incisions in the body which makes
the surgery technically challenging. One such challenge is the
access constraint imposed by the incision. This problem has
inspired many researchers to develop new ﬂexible robotic
tools that can navigate to the surgical site from a remote
access point [1]. These designs can use articulated linkages
[2], [3], [4], [5] or be of a continuum nature [6], [7],
[8], [9]. Continuum-based systems can manoeuvre through
smooth trajectories and tight bending radii that is an essential
requirement for some MIS procedures, but their robotic
control is not a trivial task because their motion is determined
by smooth, continuous deformations of the robot structure
than movements of rigid links and joints [10].
Continuum-based ﬂexible robots typically used a tendon-
driven approach and the modeling of these systems suffers
from inaccuracies resulting from friction forces, hysteresis
effects and backlash that are challenging to model [11].
These continuum systems are also sensitive to external loads
that alter their kinematics [7]. This motivates the use of other
A. Vandini, A. Salerno, C. J. Payne and G. Z. Yang are with the Hamlyn
Centre for Robotic Surgery, Imperial College London, SW7 2AZ, London,
UK (e-mail: a.vandini12@imperial.ac.uk)
external sensors to provide meaningful and reliable shape
information of the robot. This shape measurement forms
the input to the robot control loop, in case of vision-based
shape sensing techniques, this becomes a visual servo control
system [12], [13].
The use of optical ﬁber sensing has been investigated in
the last decade in order to accurately estimate the shape
of ﬂexible devices [14], [15]. An accurate 3D shape re-
construction of a ﬂexible needle using ﬁber bragg grating
sensors was proposed in [16]. Optical ﬁbre-based methods
can achieve accurate measurements of the shape and are
physically compact but their integration into a continuum
robot is not always trivial and they can be expensive to
produce.
Alternative shape sensing techniques that adopt vision-
based methods are advantageous because they do not require
any hardware modiﬁcations to the robot and surgical imaging
equipment is already an essential component of the operating
theatre. Moreover vision-based techniques can accurately
measure the continuum robot shape without impeding the
robots ﬂexibility or interfering with the robot kinematics. The
3D shape reconstruction of a continuum robot based on self-
organizing maps applied to stereo vision is proposed in [17].
This method does not require orthogonal cameras or ﬁducials
applied to the body of the robot. The estimation of the pose
of a continuum robot is also achieved in [10] by using learnt
visual features which are robust to limited occlusions. These
features are learnt ofﬂine and they link the shape conﬁgu-
ration of the robot with its visual features. The 3D shape
of a robotic catheter is reconstructed and localised using
appearance priors of the catheter together with optimal C-arm
rotations in [18]. In [19] the shape of the robot is calculated
using a deformable surface parametrization combined with
optimal viewpoints of a robotic C-arm. Although the results
are promising, the method is validated only using simulated
data. A fast and accurate shape estimation algorithm which
models the robot deformations with circles passing through
ﬁducials is proposed in [20]. The results obtained using the
vision sensors were more precise than those estimated using
internal measurements of the continuum robot; the method
considers only planar motion of the robot. This approach was
then extended in [11] where planes created by feature points
extracted along the robot were used together with image
correspondences of the robot and its forward kinematics in
order to estimate the shape of the robot. This method was
tested on simulated data. In [21] three orthogonal cameras are
used to reconstruct the shape of a ﬂexible manipulator using
a voxel-carving approach. Although the method reaches
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6205
millimetre level accuracy, the requirement of three cameras
makes it difﬁcult to integrate into an operating room.
In this paper, a motion control framework based on a novel
and robust vision sensor is proposed to allow accurate and
controlled guidance of a ﬂexible robot that is mounted to an
anthropomorphic robotic arm. The vision sensor estimates
the 3D shape of the ﬂexible robot and its spatial location
using only a single camera and the kinematics information
derived from the anthropomorphic robotic arm. It is shown
that the vision sensor is robust with inhomogeneous back-
grounds. The continuous and real-time shape measurement
is the input for the real-time control of the ﬂexible robot by
providing feedback of its instantaneous shape conﬁguration.
The ﬂexible robot is ﬁrst detected in an image stream by the
vision sensor that models the appearance of the robot using
compressed visual features in an online learning framework.
The 3D shape of the ﬂexible robot adopted for its real-
time control is then reconstructed and localised by fusing
the detection performed on the image to the kinematics
information from the anthropomorphic robotic arm. This
information fusion is performed by minimizing an energy
function which mainly depends upon the distance between
projections of a 3D model of the robot and its detected
projection in the image, ensuring an accurate and robust
estimation of the robot shape. This is then used to produce
the control variable, referred to as the bending angle, which
the control system uses to control the motion of the robot.
This is achieved by comparing the angle measured  with a
desired bending angle 
d
as shown in Sec. II.D.
Since this motion control framework is based on a vision
sensor, it does not affect the ﬂexible robot maneuverability
nor is it prone to the friction, hysteresis or backlash issues
seen in open-loop approaches, allowing for much greater
overall accuracy of the system. Furthermore, the framework
relies on a single camera which makes it easy to integrate
in the clinical workﬂow in order to assist surgeons during
minimally invasive surgery. A detailed validation of the
framework was performed to illustrate its potential practical
value and robustness.
The paper is structured as following: Sect. II describes
the vision algorithm and the general control architecture,
Sect. III presents the main results and the experimental setup
used to validate the proposed framework and ﬁnally Sect. IV
concludes the work with a discussion of the obtained results
and directions for future works.
II. METHODS
In this section, a vision-based motion control of a ﬂexible
robot is proposed. The framework consists of: ﬂexible robot
centreline extraction, 3D shape reconstruction, control of
the macro robot and control of the ﬂexible robot. Fig. 1(a)
shows a workﬂow of the whole proposed framework and the
workﬂow of the vision algorithm can be found in Fig. 1(b).
As shown in Fig. 1(a), the control architecture is based on
the feedback loop retrieved by the estimation of the bending
angle  of the ﬂexible robot using the vision sensor.  is
then compared with a desired bending angle 
d
in order to
Fig. 1. In (a) the vision-based motion control algorithm of a ﬂexible robot
is shown. The workﬂow of the vision sensor is illustrated in (b)
produce the input command to the controller. This generates
the command torques 
M
for the motion control of the
ﬂexible robot. In the following sections we will consider
a ﬂexible device with 1 degree-of-freedom (D.o.F.) mounted
on the end-effector of an anthropomorphic robotic arm (with
7 D.o.F.) as the system to be controlled. The ﬂexible section
of the robot is mounted at the tip of a rigid shaft that is rigidly
afﬁxed to the macro robot. The purpose of the macro robot
in Fig. 1(a), is twofold: (i) to provide the gross positioning
of the ﬂexible robot and (ii) to provide both the position of
the base of the ﬂexible sectionp
c
and the unit vectors ^ x
c
, ^ z
c
identifying the bending plane of the ﬂexible tip to the vision
algorithm.
A. Flexible Robot Centreline Extraction
The a priori estimation of a robust appearance model
for the ﬂexible robot is challenging due to the varying
illumination conditions, deformations of the object, pose
variations and occlusions. To overcome these issues, online
object tracking methods can be used to learn an adaptive
appearance model of the object updating it using instant
observations [22]. Although these methods have shown great
results, some limitations have still to be addressed such as
the limited data available for the learning process and the
tracking drift problem.
In our framework, a modiﬁed version of the Compressive
Tracking method (CT) [22] is used to track the continuum
robot. CT models the appearance of the object by compress-
ing samples of the background and foreground with a sparse
measurement matrix R. R is used to project the samples
from the high-dimensional image feature space into a lower-
dimensional space. These projected samples are then used
as positive or negative samples of the object to train a naive
Bayesian classiﬁer online. In order to estimate the location
of the object in the current frame, samples of the image are
6206
Fig. 2. An example of results from each step of the ﬂexible robot centreline
extraction is shown. In the ﬁrst row the original image together with the
heatmap is shown. In the second row, the ﬂexible robot is detected and the
centreline extracted (shown in red).
acquired close to the previous object location. From these
samples the one that has the maximum classiﬁcation score
is chosen as the location of the object in the current frame.
The sampling performed in [22] that was used to train the
classiﬁer is based on a rectangle patch, also called bounding
box. This patch contains the object in the case of a positive
sample and the background in the case of a negative sample.
In our case, the object that has to be tracked is a continuum
robot which has a long thin shape as shown in Fig. 5. If
only a rectangular bounding box was used to sample the
robot in the image, it would contain a great area of the
background. As a result, the learning of the appearance
model would be negatively affected. Therefore, instead of
considering the whole robot shape as our target object in
the tracking framework, the robot is sampled along its body
using small adjacent rectangles. Each rectangle is centred
along the centreline of the robot and its size corresponds to
the width of the robot shape in the image. In our framework,
since the robot is described by multiple positive samples a
heat map is estimated using the score of the classiﬁer. An
empirically-deﬁned threshold is then applied to the heat map
to segment the robot from the background, as shown in Fig.
2. Finally, the centreline on the ﬂexible robot, deﬁned asc is
extracted by applying a fast thinning algorithm [23] on the
segmented area belonging to the robot.
B. 3D Shape Reconstruction
The mechanical design of the ﬂexible robot ensures its
movements are restricted to a calculable plane, the motion
plane which is described by two unit vectors ^ x
c
and ^ z
c
.
This ensures that the domain of possible solutions for the
3D shape reconstruction is limited to  as it is shown in
Fig. 3.
The shape of the continuum robot at time t is deﬁned as
Fig. 3. The two pictures represent the same scene viewed from two different
points of view. The bending angle  together with the vectors ^ xc and ^ zc
that describe the motion plane are shown. Moreover the shapesS
t 1
and
St are described.
S
t
and it is described as a set of equidistant 3D points. In
order to estimate S
t
an extended version of the algorithm
proposed in [24] is used. The 3D shape of a deformable
body is estimated by calculating the 3D displacements of the
points of S
t 1
that minimize the following energy function:
E(u) =D(u)+S
l
(u)+S
D
(u): (1)
D is the image-based difference measure between the 2D
object skeleton extracted at time t and the projections of
S
t 1
displaced of u. D is deﬁned as:
D =
1
q
q
X
i=1
M
2
(d(y
i
)) (2)
wherey
i
is the image projection of thei
th
point ofS
t 1
after
being displaced by u
i
, q is the number of points of S
t 1
,
and M is the distance map estimated using the 2D object
skeleton. S
D
is the smoothness term and S
l
is the length
preserving term [24]. The coefﬁcients  and  are weights
and were empirically chosen to be 4 and 0.2, respectively.
In [24], the recovery of the 3D shape is performed using
multiple views and allowing the displacement of the points
that compose the shape in the entire 3D space. Although
only a single view is available in our case, it is known that
the shape of the ﬂexible robot lies on . Therefore u can
be deﬁned in term of ^ x
c
and ^ z
c
as u = ^ x
c
z
x
+^ z
c
z
y
, where
z = (z
x
;z
y
) is the displacement ofS
t 1
on. This restricts
the space of solutions to  and decreases the number of
unknowns from 3q to 2q.
In our case the 2D object skeleton calculated at timet isc
t
,
namely the centreline of the ﬂexible robot. Moreover, since
a 2D to 3D correspondence between c
t
and S
t 1
is known,
the distances of the projections of the displaced endpoints of
S
t 1
used to ﬁndD are calculated considering the endpoints
of the c
t
instead of M.
The estimatedS
t
is used to calculate the bending angle
which is the angle between the unit vector ^ x
c
and the unit
vector
^
b.
^
b is deﬁned as the directional vector that describes
the line that connects the base of the ﬂexible robot section
p
c
with the tip of the ﬂexible robot, namely the last point of
S
t
, as shown in Fig. 3.
6207
C. Control of the Macro Robot
In order to guarantee a safe human robot interaction during
surgery, the Cartesian impedance control has been chosen
as the controller of the macro robot since it performs the
gross positioning of the ﬂexible robot in situ. The impedance
control law can be found in details in [25] and [26].
The control law can be expressed as

c
=J
T
(q)K(x
d
 x(q))+D(d
c
)+F
d
(q; _ q; q) (3)
where, J(q)2<
6n
is the Jacobian matrix depending on
the conﬁguration q for a robot with n joints, K 2<
6n
is a diagonal stiffness matrix, x = [p ']
T
is the actual
pose in terms of position p and orientation ', D(d
c
) is a
damping vector depending on the damping coefﬁcients d
c
andF
d
(q; _ q; q) is the robot dynamics compensation including
the terms of the inertia matrix, centrifugal and Coriolis
torques, friction and the gravitational torque vector. The
vectors of joints position, velocity and acceleration areq; _ q; q,
respectively. The desired posex
d
for the gross positioning of
the ﬂexible robot has been computed by means of a minimum
jerk trajectory planner relying on a ﬁfth order polynomial
function as arch length [27], [28]. The vision-based control
system of the ﬂexible robot controls the ﬁne tip placement.
The main beneﬁt of the above control approach is that
the robot’s visco-elastic properties during interaction with
the patient’s tissue can be regulated by tuning the robot
stiffness K and damping the D matrices according to the
robot behaviour like a passive mass-spring-damper system.
With reference to Fig. 4, the pose vector x of the tool
frame fCg with respect to the base frame fBg can be
retrieved by the forward kinematics in terms of the following
homogeneous transformation matrix:
T
b
c
(q) =

R
b
c
p
b
c
0
T
1

(4)
where R
b
c
2 SO(3) and p
b
c
are respectively the rotation
matrix and origin position of the tool framefCg with respect
to the base framefBg. As shown in Fig. 4, the reference
framefCg is placed at the beginning of the ﬂexible tip and
the structure can be considered with rigid links and rigid
joints up to the point C, thus the positionp
b
c
and the unitary
vectors ^ x
c
, ^ z
c
provide the input for the shape computation
of the ﬂexible tip. ^ x
c
and ^ z
c
are respectively the ﬁrst and the
third column of the rotation matrix R
b
c
.
D. Control of the Flexible Robot
To guarantee high disturbance rejection and enhanced tra-
jectory tracking capabilities, a decentralized control structure
has been adopted to control the motion of the ﬂexible tip
[29].
The command torque 
M
at motor level can be summa-
rized as

M
=K
P
e+K
D
_ e+K
I
Z
e(t)dt (5)
Fig. 4. Experimental setup with main components and reference frames
placement.
wheree = (
d
 ) is the error signal between the desired
angular position 
d
and the actual angular position  of
the motor, K
P
, K
D
and K
I
are respectively the position,
velocity and integral constants.
Extensive explanation about the control gains in eq. (5),
the transfer function, frequency response and disturbance
rejection factor of the PID controller can be found in [29].
The desired motor velocity
_

d
(and consequently the
position 
d
) can be controlled as
_

d
=
8
<
:
_

N
if  (
d
 h)
 
_

N
if  (
d
+h)
0 if (
d
 h)< < (
d
+h)
where
_

N
is the nominal velocity of the motor in order
to guarantee continuous operation during surgery.
d
, which
is the desired angle, is related to the desired shape to be
reached by the ﬂexible tip and is calculated on the current
shape reconstructed by the vision algorithm in Sect. II.B.
The parameter h acts as threshold which can be tuned
according to the accuracy required and the surgical task to
be performed.
E. The Flexible Robot
The robotic device, which is shown in Fig. 5, incorporates
a proﬁled ﬂexible probe. The probe tip can be deﬂected
using an antagonistic tendon system; it extends the design
initially presented in [30]. The probe is manufactured from
superelastic Nitinol ASTM F 2063 using wire-based electro
discharge machining (EDM) to allow large deﬂections. The
design is intended to have ﬂexibility in a single plane and
adopts a machined proﬁle that ensures stiffness in the plane
orthogonal to bending through the use of a spine of 0.3mm
in width. This spine also gives the probe axial rigidity which
prevents de-tensioning of the tendons during operation which
can lead to hysteresis effects. Stress-relieving ﬁllet features
are incorporated to prevent high stresses in the bending
6208
Fig. 5. Design of the ﬂexible robot.
regions which can lead to fracture and low fatigue strength. A
series of nitinol guides have been fabricated into the proﬁle
of the design to allow the tendons to be routed through.
The tendons terminate at the instrument tip and are routed
back to a pulley system that allows a central channel to be
passed through the bore of the instrument. The tendons are
afﬁxed to a rotatable capstan mechanism in the proximal
actuation system which is coupled to a DC motor with
position encoding (1226 A 012 BK1855, Faulhaber) and
incorporating a 256:1 ratio gearbox (256:1 12/4, Faulhaber).
Hard position limits have also be incorporated to prevent
overloading of the ﬂexible probe.
III. EXPERIMENTS AND RESULTS
A. Experimental Setup
The experimental setup consists of the custom-made ﬂex-
ible instrument attached to the end-effector of a KUKA
Lightweight Robotv 4+ (i.e. LWR), a Storz Tele Pack light
box endowed with a camera, a NDI Aurora tracking system
and a knee phantom as shown in Fig. 4.
The Lightweight Robot 4+ (KUKA Roboter GmbH, Augs-
burg, Germany) is a robotic manipulator. It has 7 revolute
joints i.e. 7-D.o.F., a load-to-weight ratio of approximately
1:1, a tip velocity of 6m=s, a workspace of up to 1:5m
3
and high dynamic performance suitable for interaction with
human tissue. Further details can be found in [26]. The
ﬂexible instrument is extensively described in Sect. II.E. The
selected angular velocity of the motor shaft is 0.0016 rad/s.
The NDI Aurora is an electromagnetic measurement sys-
tem designed speciﬁcally to track the position and shape
of surgical tools and instruments such as needles, catheters,
probes, and scopes. Its accuracy is 0.48 mm and 0.30 deg
for position and orientation measurements respectively. The
Aurora system has been used to validate the vision algorithm
by through integration of the Aurora receiver in to the tip
of the ﬂexible robot. The robustness of the Aurora sensor
to electromagnetic distortions has been empirically tested.
Known movements of the sensor have been performed using
the KUKA LWR. At each movement a measurement of
the electromagnetic sensor has been acquired and compared
to the motion information obtained by the KUKA LWR.
No relevant distortions of the sensor have been measured
compared to its nominal accuracy.
Both control of KUKA LWR and vision-based control
of the ﬂexible instrument run in a desktop PC with the
following features: i7-2600 at 3.40 GHz, 16 GB of RAM and
Windows 7 as operating system. The KUKA LWR, ﬂexible
instrument, camera and Aurora have been synchronized in
the main control program written in C++. It is a multi-
threaded program and is capable of running both the control
of the KUKA LWR at 200 Hz and the vision-based control
of the ﬂexible device at about 3 Hz in parallel. 0.14 s and
0.22 s are the mean computation time for the centreline
extraction and for the 3D shape reconstruction, respectively.
The total computation time is 0.36 s with an unoptimized
C++ implementation.
In order to evaluate the performance of the vision sensor
and the control algorithm, six different positions of the
KUKA LWR workspace have been chosen. The algorithm
has been tested using a white board as a background for
four positions and a phantom knee as background for the
remaining two. The positions have been selected in order
to test the vision algorithm when the ﬂexible robot bending
plane, namely the motion plane , was out-of-plane with
respect to the camera. A calibration between the coordinate
system of the camera, the KUKA LWR and the Aurora was
performed in order to map the coordinate systems of all the
devices in the coordinate system of the KUKA LWR.
A point-to-point motion [29] based on a minimum jerk
arch length has been performed to reach such positions of the
KUKA LWR workspace. The axis/angle technique [29] has
been used to plan the orientation of the robot end-effector
and of the ﬂexible instrument. For each trial, the robotic
arm was commanded to move to a position (with accuracy
0:1mm, as declared by the manufacturer) before the ﬂexible
robot end-effector was actuated.
For each position of the robot end-effector, the control
loop performance was evaluated for a range of desired
bending angles in which 
d
was chosen to be 10

, 20

and 30

, starting from an approximate straight conﬁguration
of the ﬂexible robot. The error in the estimation of  by
the vision sensor is calculated as an absolute value of the
difference between the angle calculate using the tip position
acquired using the Aurora and the angle calculated using tip
of the shape estimated by the vision sensor. The tip error
is estimated by calculating the Euclidean distance between
the tip position provided by the Aurora and the tip of the
shape calculated using the vision sensor. For each position
the centreline extraction algorithm was manually initialised
providing the centreline of the ﬂexible robot on the ﬁrst
frame. Moreover the initial 3D shape used for the 3D shape
reconstruction was chosen to be a straight segment with
the same length of the ﬂexible robot, located on p
c
and
having the same direction of ^ x
c
. For all the experiments the
threshold h was set to 2

.
B. Results
In Fig. 6(a) the angle estimated by the vision sensor (VS
angle) and the angle calculated using the Aurora (EM angle)
during the ﬁrst trial using the knee phantom are shown. The
6209
Fig. 6. The bending angle calculated using the Aurora system (EM angle)
and the vision sensor (VS angle) is reported in (a) for the ﬁrst position
of KUKA LWR using the knee. The control starts at 10th frame and the
desired angle is 30

. In (b) for the same position of (a) the angle errors
which are calculated as the absolute value of the difference between VS
angle and EM angle are reported.
frames reported on the axis represent the frame processed
by the vision algorithm. The control task starts at the 10th
frame. The angle errors for each frame of the same trial are
reported in Fig. 6(b). The vision sensor is able to provide a
reliable and precise bending angle with respect to the Aurora
sensor as it shown in Fig. 6(b). Moreover, from Fig. 6(a) it
is possible to observe that the control of the ﬂexible robot
converges to the desired angle 
d
.
The angle errors calculated for each frame of all the trials
grouped by the desired angle are shown in Fig. 7(a). The
mean value of the error angle is below 1

. The Fig. 7(b)
shows the ﬁnal angle errors, namely the difference between
the measurement acquired at the end of the trial and the
desired angle, for all the trials performed grouped by desired
angle. These results demonstrate that the algorithm converges
at the desired angle with good accuracy.
A summary of the performance of the vision sensor is
shown in Table I where the angle errors calculated for each
frame of all the trials are grouped by the position of the
KUKA LWR.
The vision sensor reaches an accuracy of approximately
1

on the bending angle of the ﬂexible robot and about 1/2
mm on the localisation of the tip. Moreover it is possible to
observe in Fig. 6 and in Table I that the errors are not affected
by employing a non-homogeneous background such as the
knee phantom. The highest mean value of the error shows in
Table I is from the trial number 4 where the motion plane
Fig. 7. The angle errors calculated for each frame of all the trials grouped
by the desired angle are shown in (a). In (b) are shown the ﬁnal angle errors,
namely the difference between the measurement acquired at the end of the
trial and the desired angle, for all the trials performed grouped by desired
angle.
TABLE I
3D LOCALISATION ERRORS IN [MM] FOR THE TIP OF THE FLEXIBLE
ROBOT AND ANGLE ERRORS IN [DEGREE] FOR THE EXPERIMENTS
CONDUCTED WITH A WHITE BOARD AND WITH A PHANTOM KNEE.
White Board Knee
Positions 1 2 3 4 1 2
Mean Tip 1.43 2.98 1.07 2.08 2.23 1.27
StdDev Tip 0.34 0.81 0.89 1.00 0.47 0.63
Mean Angle 1.03 0.95 0.86 1.30 1.02 0.71
StdDev Angle 0.67 0.88 0.88 1.07 0.92 0.59
was about 45

out-of-plane of the camera. This means that
although the motion of the ﬂexible robot was out-of-plane
of the camera, the algorithm was still able to perform with
good accuracy in limiting the localisation and reconstruction
errors.
IV. CONCLUSIONS
In this work, a vision-based motion control framework of a
ﬂexible robot for surgical applications is proposed. The novel
and robust vision sensor provides accurate and online shape
measurements of the ﬂexible robot that feeds its control unit.
The framework is based on a single camera and from position
information acquired from an anthropomorphic robotic arm
where the ﬂexible robot is attached. The results demonstrate
the overall robustness of the framework which is able to
converge to the desired angle purely by the images acquired
from a single camera. Such a conﬁguration is easily satisﬁed
in MIS as an additional endoscope is usually deployed along
with the surgical instrumentation. The vision sensor is able
to provide fast and accurate measurement of the shape also in
6210
the presence of a non-homogeneous background. Moreover
it reaches millimeters accuracy on the detections of the
tip. Finally, the shape of the ﬂexible robot is continuously
estimated without the addition or integration of expensive
and bothersome new hardware in to the ﬂexible robot.
The integration of the vision-based control with dynamic
active constraints and virtual ﬁxtures [31] is envisaged as
a further application of the proposed algorithm in order to
achieve hands-on cooperative control. In this scenario, the
surgeon shares control of the surgical instrument with the
macro robot in which software constraints are applied to
the ﬂexible tool motion in order (i) to guide the instru-
ment to the target anatomy in situ, thus avoiding collisions
with surrounding healthy tissues; (ii) to provide the haptic
feedback to the surgeon; (iii) to autonomously control the
ﬂexible instrument with the purpose of increasing the sur-
geons dexterity and to reduce the number of D.o.F.s that
the surgeon has to directly control. Moreover a study on the
optimal conﬁgurations of the bending plane of the ﬂexible
robot considering the point of view of the camera will be
performed.
ACKNOWLEDGMENT
The authors would like to thank Dr Stamatia Giannarou,
Konrad Leibrandt and Petros Giataganas for their contri-
butions to this research. This work was supported by the
Wellcome Trust and the Engineering and Physical Sciences
Research Council as part of the Medical Engineering Solu-
tions in Osteoarthritis Centre of Excellence [088844/Z/09/Z]
project.
REFERENCES
[1] V . Vitiello, S. Lee, T. Cundy, G. Yang, “Emerging robotic platforms
for minimally invasive surgery.” IEEE Reviews in Biomedical Engi-
neering, vol. 6, pp. 111–126, 2013.
[2] J. Shang, D. P. Noonan, C. Payne, et al., “An articulated universal
joint based ﬂexible access robot for minimally invasive surgery,” in
IEEE International Conference on Robotics and Automation, 2011,
pp. 1147–1152.
[3] J. Shang, C. J. Payne, J. Clark,etal., “Design of a multitasking robotic
platform with ﬂexible arms and articulated head for minimally invasive
surgery,” in IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2012, pp. 1988–1993.
[4] A. Degani, H. Choset, A. Wolf, and M. Zenati, “Highly articulated
robotic probe for minimally invasive surgery,” in IEEE International
Conference on Robotics and Automation, 2006, pp. 4167–4172.
[5] G. Petroni, M. Niccolini, A. Menciassi, et al., “A novel intracorpo-
real assembling robotic system for single-port laparoscopic surgery,”
Surgical endoscopy, pp. 1–6, 2012.
[6] N. Simaan, K. Xu, W. Wei, et al., “Design and integration of a
telerobotic system for minimally invasive surgery of the throat,” The
International journal of robotics research, vol. 28, no. 9, pp. 1134–
1153, 2009.
[7] D. Camarillo, C. Milne, C. Carlson, et al., “Mechanics modeling
of tendon-driven continuum manipulators,” IEEE Transactions on
Robotics,, vol. 24, no. 6, pp. 1262–1273, 2008.
[8] M. Kutzer, S. Segreti, C. Brown, et al., “Design of a new cable-
driven manipulator with a large open lumen: Preliminary applications
in the minimally-invasive removal of osteolysis,” inIEEEInternational
Conference on Robotics and Automation, 2011, pp. 2913–2920.
[9] J. Ding, K. Xu, R. Goldman, et al., “Design, simulation and evaluation
of kinematic alternatives for insertable robotic effectors platforms
in single port access surgery,” in IEEE International Conference on
Robotics and Automation, 2010, pp. 1053–1058.
[10] A. Reiter, R. E. Goldman, A. Bajo, et al., “A learning algorithm
for visual pose estimation of continuum robots,” in International
Conference on Intelligent Robots and Systems, 2011, pp. 2390–2396.
[11] V . Chitrakaran, A. Behal, D. Dawson, and I. Walker, “Setpoint regu-
lation of continuum robots using a ﬁxed camera,” Robotica, vol. 25,
no. 5, pp. 581–586, 2007.
[12] F. Chaumette and S. Hutchinson, “Visual servo control. i. basic
approaches,” IEEE Robotics Automation Magazine, vol. 13, no. 4, pp.
82–90, Dec 2006.
[13] S. Hutchinson, G. Hager, and P. Corke, “A tutorial on visual servo
control,” IEEE Transactions on Robotics and Automation, vol. 12,
no. 5, pp. 651–670, Oct 1996.
[14] R. Posey Jr, G. Johnson, and S. V ohra, “Strain sensing based on
coherent rayleigh scattering in an optical ﬁbre,” Electronics Letters,
vol. 36, no. 20, pp. 1688–1689, 2000.
[15] Y .-L. Park, K. Chau, R. J. Black, et al., “Force sensing robot ﬁngers
using embedded ﬁber bragg grating sensors and shape deposition
manufacturing,” in IEEE International Conference on Robotics and
Automation, 2007, pp. 1510–1516.
[16] M. K. Momen Abayazid and S. Misra, “3d ﬂexible needle steering
in soft-tissue phantoms using ﬁber bragg grating sensors,” in IEEE
International Conference on Robotics and Automation, 2013, pp.
5823–5829.
[17] J. M. Croom, D. C. Rucker, J. M. Romano, et al., “Visual sensing of
continuum robot shape using self-organizing maps,” in IEEE Interna-
tional Conference on Robotics and Automation, 2010, pp. 4591–4596.
[18] A. Vandini, S. Giannarou, S.-L. Lee, and G.-Z. Yang, “3D robotic
catheter shape reconstruction and localisation using appearance priors
and adaptive c-arm positioning,” in MIAR 2013, ser. LNCS, vol. 8090.
Berlin, Heidelberg: Springer-Verlag, 2013.
[19] E. J. Lobaton, J. Fu, L. G. Torres, et al. “Continuous shape estimation
of continuum robots using x-ray images,” in IEEE International
Conference on Robotics and Automation, 2013, pp. 717–724.
[20] M. W. Hannan and I. D. Walker, “Real-time shape estimation for
continuum robots using vision,” Robotica, vol. 23, no. 05, pp. 645–
651, 2005.
[21] D. B. Camarillo, K. E. Loewke, C. R. Carlson, and J. K. Salisbury,
“Vision based 3-d shape sensing of ﬂexible manipulators,” in IEEE
International Conference on Robotics and Automation, 2008, pp.
2940–2947.
[22] K. Zhang, L. Zhang, and M.-H. Yang, “Real-time compressive track-
ing,” in European Conference on Computer Vision. Springer, 2012,
pp. 864–877.
[23] T. Zhang and C. Y . Suen, “A fast parallel algorithm for thinning digital
patterns,” Communications of the ACM, vol. 27, no. 3, pp. 236–239,
1984.
[24] R. Liao, Y . Tan, H. Sundar, and et al., “An efﬁcient graph-based de-
formable 2D/3D registration algorithm with applications for abdominal
aortic aneurysm interventions,” in MIAR 2010, ser. LNCS, vol. 6326.
Berlin, Heidelberg: Springer-Verlag, 2010, pp. 561–570.
[25] A. Albu-Sch¨ affer, C. Ott, and G. Hirzinger, “A uniﬁed passivity-
based control framework for position, torque and impedance control of
ﬂexible joint robots,” The International Journal of Robotics Research,
vol. 26, no. 1, pp. 23–39, 2007.
[26] A. Albu-Sch¨ affer, S. Haddadin, C. Ott, et al., “The dlr lightweight
robot: design and control concepts for robots in human environments,”
Industrial Robot: An International Journal, vol. 34, no. 5, pp. 376–
385, 2007.
[27] K. Kyriakopoulos and G. Saridis, “Minimum jerk path generation,”
in IEEE International Conference on Robotics and Automation, 1988,
pp. 364–369 vol.1.
[28] L. Zollo, A. Salerno, L. Rossini, and E. Guglielmelli, “Submovement
composition for motion and interaction control of a robot manipulator,”
in IEEE RAS and EMBS International Conference on Biomedical
Robotics and Biomechatronics (BioRob), 2010, pp. 46–51.
[29] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo, “Robotics:
Modelling, planning and control,” 2008.
[30] E. Lopez, K.-W. Kwok, C. J. Payne, et al., “Implicit active constraints
for robot-assisted arthroscopy,” in IEEE International Conference on
Robotics and Automation, 2013, pp. 5370–5375.
[31] K.-W. Kwok, K. H. Tsoi, V . Vitiello, et al., “Dimensionality reduction
in controlling articulated snake robot for endoscopy under dynamic
active constraints,” IEEE Transactions on Robotics,, vol. 29, no. 1,
pp. 15–31, 2013.
6211
