W-RGB-D: Floor-Plan-Based Indoor Global Localization
Using a Depth Camera and WiFi
Seigo Ito Felix Endres Markus Kuderer Gian Diego Tipaldi Cyrill Stachniss Wolfram Burgard
Abstract—Localization approaches typically rely on an al-
ready available map to identify the position of the sensor in the
environment.Such maps are usually built beforehandand often
require the user to record data from the same sensor used for
localization.Inthispaper,werelaxthisassumptionandpresent
a localization approach based on architectural ﬂoor plans. In
general, ﬂoor plans are readily available for most man-made
buildings but only represent basic architectural structures.
The incomplete knowledge leads to ambiguous pose estimates.
To solve this problem, we present W-RGB-D, a new method
for indoor global localization based on WiFi and an RGB-D
camera. We introduce a sensor model for RGB-D cameras that
is suitable to be used with abstract ﬂoor plans. To resolve am-
biguities during global localization, we estimate a coarse initial
distribution about the sensor position using the WiFi signal
strength. We evaluate our W-RGB-D localization method in
indoor environments and compare its performance with RGB-
D-basedMonteCarlolocalization.Ourresultsdemonstratethat
the use of WiFi information as proposed with our approach
improves the localization in terms of convergence speed and
quality of the solution.
I. INTRODUCTION
Localization in indoor and thus GPS-denied environments
with low-cost sensors is still an open problem. At the same
time, accurate knowledge about the position of a user offers
the opportunity for a variety of novel services in airports,
shopping malls, conference centers or other environments
where GPS is not available. One popular robust approach
to localization is Monte Carlo localization (MCL), which
employs a particle ﬁlter to estimate the position of the
sensor [1], [2]. While this method provides a high accuracy,
it generally requires a precise and pre-built map of the
environment.
The alternative is wireless-based indoor global localiza-
tion. Common technologies for wireless communication in-
cludeglobalsystemsformobilecommunication(GSM),ultra
wideband (UWB), worldwide interoperability for microwave
access (WiMAX), radio frequency identiﬁcation (RFID),
Bluetooth, or WiFi [3], [4], [5]. Among them, WiFi is
one of the most useful signal for global localization. WiFi
receiversarepresentinalmostalldevicesandonecanreadily
exploit the access points already present in the environment.
A further advantage is that WiFi-based localization does
not depend on an accurate model of the environment and
is resilient to ambiguities due to the unique IDs provided
by the access points. However, the accuracy of WiFi-based
Seigo Ito is with the Information & Communication
Research Division, TOYOTA Central R&D Labs., Inc., Japan.
seigo@mosk.tytlabs.co.jp
The other authors are with the Department of Computer Science, Uni-
versity of Freiburg, Germany.
(a) Input1:Proposedsensormodelforaﬂoorplan:(left)grayscale
image; (middle) extracted point cloud; (right) aligned point cloud
and virtual measurements.
(b) Input 2: WiFi signal strength and WiFi signal maps expressed
by a Gaussian Process. This ﬁgure shows the mean signal map of
one access point.
(c) Output:ComparisonbetweenMCL(top)andW-RGB-D(bottom)
Fig. 1. Examples of inputs (top) and outputs (bottom) of the proposed
algorithm.
localization methods is typically lower than that of laser-
based or camera-based MCL approaches.
In this paper, we address the localization problem when
no previously built map of the environment is available.
Our approach solely relies on already available information
such as the ﬂoor plan and WiFi signal strength information.
Floor plans typically are only a sketch of the actual ge-
ometric structure of the environment and typically do not
contain objects like furniture. This reduces the ability to
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 417
uniquely localize the sensor, as can be seen in Fig. 1(c).
In such situations, MCL requires to explore a large part of
the environment to ﬁnally converge, a phenomenon that is
inconvenient for applications that require to ﬁnd the sensor
location quickly. To resolve this problem, we introduce
W-RGB-D, a novel hybrid approach based on WiFi and
a RGB-D data for global localization in indoor settings.
Our approach employs at its core the established Monte
Carlo localization approach and consists of two main steps.
First, we estimate a coarse global position using WiFi. We
employ a two-dimensional WiFi model to quickly reduce
the number of potential ambiguities (see Fig. 1). Second, we
compute a precise estimate of the camera location based on
the environment ﬂoor plan and the data acquired from the
RGB-D camera. Our approach, in this way, compensates for
the weakness of both sensor modalities, enabling a quick
convergence combined with a good localization accuracy.
II. RELATED WORK
A widely-used approach to indoor global localization is
Monte Carlo localization [1], [2]. This algorithm employs
a particle ﬁlter to estimate a distribution over the potential
states of the sensor. In environments with large ambiguities,
it is well-known that the particle ﬁlter shows a slow conver-
gence and requires to move the sensor until the ambiguities
have been resolved.
Another successful approach for indoor global localization
is the multi-hypotheses tracking (MHT) [6], [7]. The MHT
generates data association hypotheses between features of
the environment (e.g., doors, and walls) and measurements.
Feature-based multi-hypothesis localization [7] uses geomet-
ric constraints to prune the hypothesis tree in global local-
ization settings. This method also shows slow convergence
in ambiguous environments.
Alternative approaches employ global scan matching for
indoor global localization [8], [9]. The method proposed by
Tomono [8] uses geometric hashing to match an input scan
with a reference scan without requiring any initial guess.
The spectral scan matching (SSM) technique [9] employs
the spectral matching technique [10] to obtain a coarse lo-
calization and RANSAC for a more precise alignment. Some
researchers focused on image based localization approaches.
The method proposed by Se et al. [11] represents the
environment as a set of local invariant features. The authors
perform global localization by matching features from the
current image to the feature map. Related to that, Bennewitz
et al. [12] propose vision-based MCL avoiding explicit
data associations for robot localization of a humanoid. One
of the renowned techniques in image-based localization is
Fast Appearance-Based Mapping (FABMAP) [13], which
employsanimageretrievalsystembasedonthebagofwords
approach.
WiFi signals have also been used for global localiza-
tion [3], [4], [5]. The RADAR system [3] builds a detailed
“radio ﬁngerprint” of the available 802.11 access points
and estimates the position of a WiFi device using the
received signal strength. The PlaceLab project [4] presents
Fig. 2. Overview of the proposed approach, consisting of three main
components: measurement extraction, visual odometry estimation, and lo-
calization.
a wide-area WiFi based localization system in the greater
Seattle area with nearly 100% coverage. The WiFi-SLAM
approach[5]addresstheSLAMproblemwithWiFisignalby
employingtheGaussianProcesslatentvariablemodel.These
methods are resilient to environment ambiguity, however, at
the cost of an accuracy of few meters at most [3].
Luo et al. [14] also use the information of an abstract ﬂoor
planformobilerobotnavigation.Theyextractdoorplatesand
passage corners as landmarks to globally localize the robot.
In contrast to their method, W-RGB-D localizes the robot
using WiFi maps of the environment.
III. THE W-RGB-D APPROACH
In this section we describe the main components of
our proposed W-RGB-D system, which are the measure-
ment extraction, the visual odometry estimation, and the
localization. Fig. 2 depicts the major components of our
system and their dependencies. The measurement extraction
process described in Section III-A is responsible to extract
virtual measurements from the depth data. These virtual
measurements are used in a dedicated sensor model that
computes the likelihood of the measurement given the ﬂoor
plan at hand. The visual odometry approach presented in
Section III-B is responsible for the estimation of the ego-
motion of the camera from image and depth data. We adopt
a RANSAC-based approach to robustly estimate the motion
and reject spurious association between consecutive images.
Finally, we combine these components with the WiFi signal
strength information within a particle ﬁlter to robustly deal
with ambiguities and to achieve a localization approach that
can quickly estimate the position (see Section III-C).
A. Measurement Extraction
Floor plans typically represent only the basic structure of
buildings including walls, doorways, and windows. Further-
more, they are only a 2D representation of the 3D envi-
ronment. To be able to correctly derive a sensor model for
this kind of maps, we need to extract virtual measurements
from the 3D point cloud obtained with the RGB-D sensor.
In this work, we mainly focus on wall-like features and rely
on plane extraction techniques. A widely-used method to
418
(a) Original points (b) Transformed points
(c) Line extraction (d) Check of connections
Fig. 3. Example of measurement extraction. Original point cloud in
the camera frame (a). Transformed point cloud using IMU data (b). Blue
points correspond to extracted virtual scans (c). Red lines indicate accepted
connections (d).
extract wall-like features from a point cloud is to employ
RANSAC for plane extraction [15]. However, the RANSAC-
based approach has high computational cost when the size
of the point cloud is large. To reduce computation time, one
can use a voxel grid ﬁlter before the plane extraction to
reduce the number of points to be evaluated. This approach
has the disadvantage that all points within a voxel are
approximated with their centroid, reducing the accuracy of
the plane extraction.
In this paper we propose a different approach depicted
in Fig. 3 with low computational cost. We ﬁrst correct
the orientation of the raw point cloud using an inertial
measurement unit (IMU) 3(b). Then, we extract horizontal
scans at different heights as potential virtual measurements
based on the point cloud. For each point of a horizontal
scan, we connect it to the scans above and below if the
vertical gradient is below a certain threshold. We accept
points connected with at least one other level in the virtual
measurement and project them down onto the 2D plane of
the ﬂoor plan. We call this approach sensor measurement
extraction (SME).
B. Visual Odometry
For visual odometry, we use the technique developed by
Endres et al. [16]. Their approach employs the visual image
of an RGB-D camera to detect keypoints and extract their
respective descriptors. It matches the keypoints extracted
from the current image with the ones of previously recorded
images according to their descriptors. Finally, they employ
RANSAC to reject false correspondences and compute the
camera transformation between the two frames in closed
form [17]. Combined with the depth information, this allows
us to register the point clouds in a common coordinate
system. Fig. 4 shows an example of the keypoint extraction
(a) Extracted SIFT keypoint (b) Estimated visual odometry
Fig. 4. Example of keypoint extraction and visual odometry estimation.
Circles show the extracted keypoints (a). Thick green arrows indicate the
estimated transformations of the inliers, and thin black arrows correspond
to the transformations of the outliers (b).
and visual odometry estimation.
C. Localization
To perform global localization, we employ the Monte
Carlo localization algorithm (MCL) [2] together with an
initialization strategy based on the WiFi signal. The MCL
recursively estimates the posterior of the robot’s pose as
follows:
p(x
t
j z
1:t
;u
0:t 1
)/
p(z
t
j x
t
)
Z
x
0
p(x
t
j x
0
;u
t 1
)p(x
0
j z
1:t 1
;u
0:t 2
)dx
0
;
where u
0:t 1
is the sequence of motion command executed
by the robot. The motion model p(x
t
j x
0
;u
t 1
) denotes
the probability of the robot’s state x
t
given it executes u
t 1
in the previous state x
0
. In our case, no real odometry is
available and we rely on the result of the visual odometry
module for the motion update step. The observation model
p(z
t
j x
t
) denotes the likelihood of the observation z
t
given the pose x
t
. The MCL approximates the belief of the
robot with a set of samples called particles and updates it
iterativelybysamplingthoseparticlesfromthemotionmodel
and compute a weight according to the observation model.
Particles are then resampled according to this weight and the
process iterates.
A widely-used approach for the initialization of particles
istorandomlygenerateinitialparticlesuniformlythroughout
the environment (see Fig. 5(a)). However, the number of
particles depends on the size of the environment and many
particles are required in large environments. Furthermore,
having a uniform prior in ambiguous environments may
result in slow or even incorrect convergence of the ﬁlter.
To speed up the initial global localization phase and to
resolve ambiguities in global localization, we propose an
initialization based on WiFi (see Fig. 5(b)). To realize this
and to cope with the underlying uncertainties, we rely on
Gaussian Processes to represent a signal strength map.
Instead of uniformly sampling from the entire state space,
we bias the sampling process to regions of the environment
with consistent signal strength. To do so, we discretize the
environment and compute a weight w for each discretized
419
(a) Particle ﬁlter initialization using a uniform distribution.
(b) Particle ﬁlter initialization using the WiFi signal.
Fig. 5. Two initialization methods in the particle ﬁlter. The initialization
method based on uniform distribution puts particles over the whole state
space (a), while WiFi based initialization quickly resolves the ambiguity
(b).
position x as follows
w =
n
Y
i=1
w
i
; (1)
where n is the number of currently observed access points.
The weight w
i
corresponding to a received signal strength
s
i
of access point i is given by
w
i
= P(s
i
;
gp
(x;D
i
);
gp
(x;D
i
)); (2)
where
P(s
i
;
gp
;
gp
) =
1
(2)
k
2
j
gp
j
1
2
exp

 
1
2
(s
i
 
gp
)
gp
 1
(s
i
 
gp
)

; (3)
is the probability density function of the received signal
strength s
i
, expressed in terms of the mean 
gp
(x;D
i
) and
covariance 
gp
(x;D
i
) of the Gaussian Process [18]. Fig. 6
shows an example of a mean signal map. The term D
i
is
the training data of WiFi signals and k is the number of
dimension. We sample the particles by ﬁrst sampling the
discretized location according to the weight w and then by
uniformly sampling in the discretized cell.
In the current implementation of our approach, we use the
signal strength only for particle initialization and for check-
ing whether the robot has been kidnapped and whenever
the robot stands still. This is because the signal strength
measurements typically have a varying time delay, most
likely caused by the WiFi driver. This makes the integration
of the signal strength too inaccurate when the robot moves
quickly.
Fig. 6. A mean signal map of an access point expressed by the Gaussian
Process. The color gradient depicts the strength of the WiFi signal.
Fig. 7. Floor plan of the environment. The blue solid lines show the
trajectories taken in the experiments. The points S indicate the starting
positions and the pointsE the ﬁnal positions of each trial.
IV. EXPERIMENTS
We evaluated the approach presented in this paper in a
set of real-world experiments. The experimental evaluation
is designed to demonstrate that (a) information about WiFi
signal strength can boost the performance of global localiza-
tion in ambiguous maps, that (b) the measurement extraction
algorithm is more efﬁcient than RANSAC and is suitable for
real-time localization, and (c) our approach does not require
a map built with an RGB-D SLAM system and rather allows
us to localize an RGB-D camera given only an architectural
ﬂoor plan of the environment.
We perform the evaluation on three sets of experiments.
In the ﬁrst set, we evaluate the beneﬁts of a WiFi bootstrap
in terms of convergence speed. To do so, we compare the
convergence of the particle ﬁlter starting with a uniform
distributionwithandwithouttheWiFiinitializationproposed
in this paper. The second set of experiments evaluates the
runtime of our measurement extraction algorithm and com-
pares it to an alternative approach based on the point cloud
library(PCL).Inthethirdsetofexperiments,weevaluatethe
accuracy of the proposed method with respect to the ground
truth, which we obtained by employing MCL and a laser
range ﬁnder. Fig. 7 shows the ﬂoor plan of the experimental
environment and the ground-truth trajectories.
The system used to carry out the experiments consists of
an ASUS Xtion Pro Live and an iPhone 4S with a gyroscope
and an accelerometer. For our implementation, we modiﬁed
theadaptiveMonteCarlolocalizationalgorithminROS[19].
A. Global Localization with WiFi Maps
In this experiment, we investigate the advantage of the
proposed global localization with WiFi initialization, “W-
RGB-D”, which is the initialization strategy described in
420
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0 5 10 15 20 25 30 35 40
Cumulative probability
Error [meters]
RGB-D itinial
RGB-D final
W-RGB-D initial
W-RGB-D final
Fig. 8. Cumulative histogram of the translational error of the particles
in localization for the different approaches. The initialization with WiFi
information prevents particles to become stuck in local minima.
Section III-C. To this end, we compare it to a particle
ﬁlter without WiFi initialization, “RGB-D”. In both cases,
we initialize the ﬁlter by drawing particles from a uniform
probability density over the three-dimensional state space
(position and orientation) of the camera in the environment.
We performed the experiments at known positions in 10
different rooms. To evaluate the convergence speed, we ﬁrst
evaluated the error just after the initialization. Then, we
turned our hand-held device three times on the spot, by 360
degrees for each turn, and evaluated the localization error of
the ﬁnal estimate.
Fig. 8 shows a cumulative histogram of the error in local-
ization using the two different approaches in all experiments.
For each distance d, it depicts the ratio of particles with an
translational error below d. For “RGB-D”, the cyan dashed
line shows this cumulative distribution for the initialization
and the blue double-dashed line the ﬁnal localization. The
pink chain line and the red solid line show the respective
distributions for initialization and ﬁnal localization of our
W-RGB-D approach. The ﬁgure clearly shows the effect of
the ambiguity of the map. Without the wireless information,
simply rotating on the spot is not enough to disambiguate the
camera location. This is evident when comparing Fig. 5 and
Fig. 1. The camera is located in the middle room, which has
about the same dimension as ﬁve other rooms. After turning,
the system was correctly able to infer that the camera is
located in the middle of a room of that size. In case of WiFi
initialization, we were able to infer exactly in which room
it was located, thanks to the prior information of the WiFi
signal.
The differences in convergence revealed by this experi-
ment show an important issue for global localization. If the
system cannot uniquely localize the camera, the camera has
to be moved through rooms and corridors until the system
can localize its position uniquely, which would be time-
consuming and tedious.
B. Runtime Evaluation
This experiment is designed to analyze the computational
demands of the measurement extraction algorithm proposed
in this paper. We compare our measurement extraction
method to the plane extraction approach implemented in
point cloud library [15] as this is an alternative way to
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0 10 20 30 40 50 60
Runtime [secs]
Time [secs]
SME
PCL plane 1
PCL plane 2
Fig. 9. Runtime of the sensor model extraction for SME and PCL plane.
compute a virtual measurement for our problem. We use the
TUM RGB-D dataset [20] for the runtime evaluation.
The PCL plane extraction method uses a downsampled
cloud of the points using a voxel grid ﬁlter with leaves
of 3cm side length. It then extracts several planes using
RANSAC until 70% of points are covered by one of the
planes. Finally, it projects the points belonging to the ex-
tracted planes from the 3D space to the 2D space. We call
this approach PCL plane 1. In addition to this, we use
the organized multi plane segmentation method in the point
cloud library. We call this approach PCL plane 2.
Fig. 9 shows the runtime of SME (see Section III-A) and
PCL plane 1 and 2. The runtime of PCL plane varies largely,
while our approach is faster and shows less variation in
execution time.
C. Localization Accuracy
To analyze the accuracy of our method, we use a robotic
wheelchair [21] equipped with a 2D laser range ﬁnder.
We mounted the hand-held system on the wheelchair and
manually measured its displacement with respect to the laser
range ﬁnder. As ground-truth, we considered the results
of Monte Carlo localization using the 2D laser scanner
mounted on the wheelchair. Fig. 7 indicates the ground-truth
trajectories, where the points denoted with S refer to the start
positions and the points denoted with E to the end positions
of each trajectory taken by the wheelchair.
Fig. 10 depicts the results of our experiments for both
localizationstrategies.Thesolidlineineachgraphshowsthe
resultsofRGB-DandthedashedlineshowstheresultsofW-
RGB-D. Both lines depict the root mean square (RMS) error
and the bars show the standard deviation to illustrate the dis-
tribution of the localization error. The ﬁgure shows three im-
portant aspects. First, localization based on RGB-D sensors
using ﬂoor plans is possible and also accurate. Second, even
without WiFi information, when the approach converges to
the correct solution, the localization error is close to that
of WiFi localization. Third, the WiFi information in fact
increases convergence speed and convergence ratio.
V. CONCLUSIONS AND FUTURE WORK
In this paper, we presented W-RGB-D, a novel approach
for indoor global localization. The main contributions of this
paper are the descriptions and the analysis of an integrated
system based on RGB-D localization and WiFi localization
421
 0
 5
 10
 15
 20
 25
 30
 0 20 40 60 80 100 120
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(a) Experiment from S1 to E1
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0 20 40 60 80 100 120
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(b) Experiment from S2 to E2
 0
 5
 10
 15
 20
 25
 30
 35
 0 20 40 60 80 100 120
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(c) Experiment from S3 to E3
 0
 5
 10
 15
 20
 25
 30
 0 20 40 60 80 100 120 140
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(d) Experiment from S4 to E4
 0
 5
 10
 15
 20
 25
 30
 0 20 40 60 80 100 120
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(e) Experiment from S5 to E5
 0
 5
 10
 15
 20
 25
 30
 0 10 20 30 40 50 60 70 80 90
RMS error [meters]
Time [secs]
RGB-D
W-RGB-D
(f) Experiment from S6 to E6
Fig. 10. Comparison of the convergence speed and convergence ratio of RGB-D and W-RGB-D in the situations depicted in Fig. 7.
for quick convergence of particles in an ambiguous environ-
ment. Our approach is most useful during global localization
especially when a robot or the user carrying the sensor
platform is not supposed to move much. Given that small
RGB-D cameras for tablet computers or smartphones will
soon be available to the public at a low price, W-RGB-D will
allow people to quickly and accurately localize themselves
in indoor environments. Possible extensions of this work
include the incorporation of an image-based localization
approach.Forexample,abagofwordsapproachcanestimate
new proposal distributions for the particle ﬁlter and resolve
localization ambiguity similarly as a WiFi-based method.
REFERENCES
[1] F. Dellaert, D. Fox, W. Burgard, and S. Thrun. Monte Carlo localiza-
tion for mobile robots, Proc. of the IEEE Int. Conf. on Robotics and
Automation, 1999.
[2] S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics, The MIT
Press, 2005.
[3] P. Bahl, and V.N. Padmanabhan. RADAR: An In-Building RF-based
User Location and Tracking System, Proc. of the IEEE Infocom,
pp.775-784, 2000.
[4] A. Lamarca, Y. Chawathe, S. Consolvo, et al. Place Lab: Device
Positioning Using Radio Beacons in the Wild, Proc. of the Third Int.
Conf. on Pervasive Computing, 2004.
[5] B. Ferris, D. Fox, N. Lawrence. WiFi-SLAM Using Gaussian Process
Latent Variable Models, Proc. of the 20th International Joint Interna-
tional Joint Conference on. Artiﬁcial Intelligence, 2007.
[6] P. Jensfelt, and S. Kristensen. Active Global Localization for a Mobile
Robot Using Multiple Hypothesis Tracking, IEEE Transactions on
Robotics and Automation, Vo.17, No.5, 2001.
[7] K. Arras, J. Castellanos, M. Schilt, and R. Siegwart. Feature-based
multi-hypothesis localization and tracking using geometric constraints,
Robotics and Autonomous Systems, vol. 44, pp. 41-53, 2003.
[8] M. Tomono. A scan matching method using Euclidean invariant
signature for global localization and map building, Proc. of the IEEE
Int. Conf. on Robotics and Automation, pp.866-871, 2004.
[9] S. Park, and S. K. Park. Spectral Scan Matching and Its Application to
Global Localization for Mobile Robots, Proc. of the IEEE Int. Conf.
on Robotics and Automation, 2010.
[10] M.LeordeauandM.Hebert.ASpectralTechniqueforCorrespondence
Problems using Pairwise Constraints, Proc. of the Int. Conf. on
Computer Vision, pp. 1482-1489, 2005.
[11] S. Se, D. G. Lowe, and J. J. Little. Vision-Based Global Localization
and Mapping for Mobile Robots, IEEE Transactions on robotics, vol.
21, No.3, pp. 364-375, 2005
[12] M. Bennewitz, C. Stachniss, W. Burgard, and S. Behnke. Metric local-
ization with scale-invariant visual features using a single perspective
camera. European Robotics Symposium, pages 143–157, 2006.
[13] M.CumminsandP.Newman.Highlyscalableappearance-onlySLAM
- FAB-MAP 2.0, Robotics: Science and Systems, 2009.
[14] R.C.Luo,Y.Lin,andC.Kao.AutonomousMobileRobotNavigations
and Localization Based on Floor Plan Map Information and Sensory
Fusion Approach, Proc. of the IEEE Int. Conf. on Multisensor Fusion
and Integration for Intelligent Systems, 2010
[15] R. B. Rusu, and S. Cousins. 3D is here: Point Cloud Library (PCL),
Proc. of the IEEE Int. Conf. on Robotics and Automation, 2011.
[16] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W.
Burgard. An Evaluation of the RGB-D SLAM System, Proc. of the
IEEE Int. Conf. on Robotics and Automation, 2012.
[17] Shinji Umeyama. Least-squares estimation of transformation param-
eters between two point patterns. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (13), 1991.
[18] C.E.Rasmussen,andC.K.I.Williams.GaussianProcessformachine
learning, MIT Press,2005.
[19] ROS amcl, http://www.ros.org/wiki/amcl
[20] J. Sturm, N. Engelhard, F. Endres, W. Burgard and D. Cremers. A
Benchmark for the Evaluation of RGB-D SLAM Systems. Proc. of
the International Conference on Intelligent Robot Systems, 2012
[21] E. Demeester, E. Vander Poorten, A. Huntemann, et al. Robotic
ADaptation to Humans Adapting to Robots: Overview of the FP7
projectRADHAR,InternationalConferenceonSystemsandComputer
Science, 2012
422
