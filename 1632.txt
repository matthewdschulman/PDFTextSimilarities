A Single Time Scale Visual Servoing System for a High Speed SCARA
Type Robotic Arm
Migara H. Liyanage and Nicholas Krouglicof
Abstract— A high speed image based visual servoing (VS)
technique is developed in this study for a Selective Compliant
Assembly Robotic Arm (SCARA) manipulator with rotary
hydraulic actuators. This study has developed a 2D position
measuring system which comprise a high speed camera with a
position sensitive detector as the image sensor. The input output
interface and the controller for the VS system was implemented
using a ﬁeld programmable gate array (FPGA) providing a
single chip solution for the embedded system. This camera was
capable of providing position measurements of the end effector
(EE) with an accuracy of up to 0.95 mm at a frequency of 1340
Hz. The proposed control strategy produced a better tracking
performance with an EE payload of 12 kg with speeds of up
to 1.3 m/s.
I. INTRODUCTION
This paper considers the issue of tracking the end effector
position of a high speed robotic manipulator interacting with
a large payload. When serial manipulators move at high
speeds, the vibration resulting from the compliance of the
supporting structure combined with the elastic deformation
of the links yields signiﬁcant discrepancies in the end effector
(EE) position with respect to the desired trajectory. This
problem can be alleviated by accurately measuring the actual
position of the EE with respect to (w.r.t.) an external refer-
ence frame and incorporating this information in the robot
control strategy. Visual Servoing (VS) refers speciﬁcally to
the integration of vision based feedback in the robot control
loop in order to mitigate the effects of disturbances and
achieve increased accuracy. The problem of ﬁne position
control of a manipulator is a well-studied research topic
but in practice, it is seldom resolved using visual servoing
techniques. When commercially available image sensors are
used as the exteroceptive sensors in VS, feedback data on the
end effector position is generally acquired at a much slower
rate than data from the proprioceptive sensors (e.g.; joint
encoders). Visual servoing becomes even more challenging
when the end effector (EE) moves at extremely high speeds.
Researchers have devised a number of strategies to circum-
vent the bandwidth and computational challenges including
a multi sensor two time scale approach [1], [2].
Vision-based feedback was used by Jiang and Eguchi [2]
for EE tracking control of a ﬂexible manipulator; however,
their robot is much simpler and moves at much slower speeds
(on the order of 40 mm/s) compared to the manipulator
*This work is supported by the Atlantic Canada Opportunities Agency
(ACOA), Research & Development Corporation of Newfoundland &
Labrador (RDC).
The authors are with Faculty of Engineering & Applied Science, Memo-
rial University of Newfoundland, St Johns, Newfoundland, Canada, A1B
3X5. mhl545 & nickk at mun.ca
employed in this study. Liu et al. [3] proposed a hybrid,
multi-sensor approach for accurate, robot target tracking.
This method used a CCD camera to implement an image
based visual servoing (IBVS) technique for coarse position-
ing and a PSD for the ﬁnal accurate positioning of the EE.
In another study Bascetta et al. [1] adopted a two time scale
visual seroving strategy for ﬂexible manipulators. It uses a
fast controller that employs feedback from proprioceptive
sensors (strain gauges) which operates at 500 Hz. The
visual control law which is executed every 40 ms (25 Hz),
ensures smooth tracking of the desired trajectory. Hitaka et.
al [4] considered a stereo-vision-based method to estimate
the oscillation of a mobile manipulator; however, the stereo
vision system considered in this study is not suitable for
high speed applications. The control problems associated
with visual servoing have been widely studied [5]. Kelly
[6], proposed an image-based controller which is Lyapanov
stable with the uncertainties in radial lens distortions and
camera orientation. In another study Marey and Chaumette
[7] proposed a series of image based visual servoing control
laws. The proposed control laws were based on achieving
global stability of the system. There are also a number of
studies that address the object measurement problem [5];
however, these studies did not consider high speed manip-
ulator systems or the development of appropriate hardware
for industrial applications.
Image based visual servoing (IBVS) requires the estima-
tion of position and orientation of the EE from one or more
cameras. A wide range of commercially available cameras
have been employed for IBVS. High resolution cameras
based on the Charge Coupled Device (CCD) have been used
for various position estimation applications in a number of
studies [8], [9]. Complementary Metal Oxide Semiconductor
(CMOS) cameras that produce high quality images at much
higher speeds have emerged as an alternative to the con-
ventional CCD cameras [10]. In recent years motion capture
system have been considered in various robot positioning
applications. The ARTrack2 [11] motion capture system was
considered for tracking and control of the EE of a light-
weight surgical robot. The Vicon motion capture system [12],
[13] has been widely used for tracking and controlling the
trajectory of quadrotor UA Vs. These systems require more
physical space for setup and their use has been limited to
applications that require low sampling rates.
The main objective of this study is to develop a single
time scale visual servoing system for an extremely high
speed industrial manipulator. The ultimate goal is to realize a
low cost, single chip controller for robotic applications. The
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4153
image sensor selected to acquire visual feedback information
on the EE must be fast and simple to integrate into the robot
controller. Camera systems based on CCD or CMOS tech-
nology are expensive and require signiﬁcant computational
resources. In contrast, Position Sensitive Detectors (PSDs)
provide a simple, efﬁcient, and cost effective solution for
high speed, non-contact position measurement. Furthermore,
these sensors require only simple signal conditioning elec-
tronics, they can operate at high speeds, and the position
estimation is computationally simple. Wang et al. [14] de-
veloped a similar camera system using a 2D PSD. However,
measurements made using this camera contain a signiﬁcant
amount of noise and had to use multiple sensor fusion for
noise elimination. It also did not provide a hardware interface
that could be used for real time controls.
The proposed high speed, single time scale VS system
integrates both a 2D PSD-based optical position sensing
system as the exteroceptive sensor, and high resolution joint
encoders as proprioceptive sensors. A single chip solution
based on reconﬁgurable hardware was adopted for imple-
menting the complete controller architecture including the
interfaces for the sensor suite as well as the computational
elements associated with the kinematics and control laws. For
the optical position sensing system, a 2D PSD was integrated
with an optical assembly, dedicated signal conditioning and
data acquisition electronics, a specially designed active target
marker, and a spectrally matched optical interference ﬁlter. A
rigorous calibration procedure was performed on the optical
position sensing system in order to ensure the accuracy of
the measurements. The entire robot controller was imple-
mented exclusively in a Cyclone III Field Programmable
Gate Array (FPGA). A virtual processor was synthesized and
implemented in the same FPGA to handle the computational
intensive elements of the controller. The proposed VS system
was extensively tested using a high speed hydraulic SCARA
manipulator developed at the Memorial University. This
manipulator is capable of handling payloads in the range
of 10 to 20 kg while operating at speeds of up to 2.1 ms
 1
[15].
This paper is organized as follows. It starts with an
introduction to the study. Section 2 provides the details
of the controller of the VS system. Section 3 outlines the
development of the high speed camera. Section 4 details the
on site calibration algorithm. Section 5 presents the FPGA-
based control system architecture. Section 6 presents the
experimental setup. The experimental results from this study
are presented in Section 7. The conclusions are given in
Section 8.
II. CONTROLLER FOR THE VS SYSTEM
The SCARA type robotic arm considered in this study has
links of lengthl
2
andl
3
. The relative joint angles of the links
will be given by
2
and
3
. The proposed VS system will use
feedback from optical encoders for joint angle and position
of the EE obtained from the high speed camera. A schematic
diagram of the proposed system is shown in Fig. 1.
l
2
l
3
O
w
O
c
O’
E
2
High Speed Camera
Robot End Effector
Actuator 1
Actuator 2
Compliant
Support
?
?
3
Fig. 1. Plan view of the SCARA arm with the VS system
Servo Valves SCARA Robot
PD 
controller
J
-1 PD 
controller
+
+
_
+
_
+
x, y
1d d
x d , y d
? ? 1
,
2
? ? , 2
Fig. 2. A schematic diagram showing the architecture of the proposed
controller
The feedback for controlling the arm will be obtained us-
ing both proprioceptive (encoders) and exteroceptive sensors
(high speed camera). The encoder angles will provide the
relative position of the links. During the high speed motion
of the robot ﬂexing of the links and torsional vibration of
the support column could affect the ﬁnal position of the EE.
Therefore, accurate position control of the EE requires to
consider the absolute position the EE. A schematic diagram
of the proposed controller is shown in Fig. 2.
The control signal (U) for the servo valves will calculated
considering joint angles and EE position. This will be given
by,
U =K
P
:e

+K
D
: _ e

+K
PXY
:J
 1
:e
XY
+K
DXY
:
_
(J
 1
:e
XY
)
(1)
Where, K
P
,K
PXY
are a set of proportional gains, K
D
,
K
DXY
are a set of derivative gains, e

is the difference
between desired and actual link angles, e
XY
is the dif-
ference between the desired and actual position of the EE
in Cartesian coordinates with respect to a world coordinate
system andJ
 1
corresponds to the inverse of the manipulator
Jacobian.
III. THE DEVELOPMENT OF A HIGH SPEED CAMERA FOR
POSITION MEASUREMENT
The high speed imaging device consists of a camera with
the Hamamatsu
c 
(S5991-01) improved tetra-lateral (pin
cushion) type 2D PSD as the image sensor. It comprises one
cathode and four anodes. When a light spot is projected on to
4154
Anode Y1
Anode Y2
Anode X 1
Anode X 2
Cathode
x
y
Fig. 3. The Hamamatsu S5991 2D PSD
the image sensor it would result in currents being induced in
the anodes due to a phenomena known as the photo electric
effect. It has a peak sensitivity to infra red signals. Hence, an
infra red (IR) source is used as the beacon. The magnitude
of these current signals vary depending on the location of
the projected light spot on the image sensor. This PSD has
an active area of 9 x 9 mm. A schematic diagram of this
PSD is shown in Fig. 3.
When the PSD is illuminated by an IR source the magni-
tudes of the induced currents depend on the distance between
the illuminated point and the four anodes. Hence, the x, y
coordinates of the spot of illumination could be calculated
using the equation [16],

X =
(I2 +I3)  (I1 +I4)
I1 +I2 +I3 +I4
(2)

Y =
(I2 +I4)  (I1 +I3)
I1 +I2 +I3 +I4
; (3)
Where I
1
,I
2
,I
3
,I
4
correspond to the induced currents in
the PSD, and

X,

Y are dimensionless relative distances
in  x, y directions. These equations give dimensionless
quantities for both X and Y . These quantities should be
related to a physical position quantity to be used for po-
sition measurement applications. Hence, the PSD should be
calibrated to estimate the nonlinear relationship between

X,

Y and the position of the light spot on the image sensor.
A. Design of the Signal Conditioning Circuit for the Camera
The PSD needs to be reverse biased with the cathode at
+10 V and the four anodes at +5V for operation. The biasing
voltages should be stable. Hence, it will be generated using
a +5V LT1019 precision voltage reference. The voltage that
is induced from photo current contains noise and requires
signal conditioning. A ﬁrst order (low pass) ﬁlter is used for
this. It will attenuate shot noise and pass other signals with
frequencies lower than the cutoff frequency. Experiments
showed that the ﬁlter required an integration time of 84 s
on each anode. This induced voltage is a very weak electric
signal. Therefore, it requires signal ampliﬁcation to transform
the weak current signal to a measurable voltage signal. The
OP495 high precision operational ampliﬁer is used for signal
conditioning and signal ampliﬁcation.
The analog currents induced from the IR signal should be
converted into digital signals for data acquisition. This could
+12 V
1.5M
0.1 
60p
+
_
VREF
IN+
IN-
GND
VCC
CLK
DOUT
CS
+5 V
+5 V
ADS8320
OP295
µ
0.1 µ
Fig. 4. A schematic diagram of the printed circuit board for PSD signal
conditioning and data acquisition
be done with an analog to digital converter (ADC). As this
system is used for making high speed measurements the data
acquisition system should be fast and accurate. Therefore, a
16-bit ADC (ADS8320) is used for data acquisition. This
ADC chip is capable of operating at data rates of up to
100 kHz with a resolution of up to 0.1 mV . The anode
voltages are acquired as 16 bit digital signals through a Serial
Peripheral Interface (SPI) input type interface. A schematic
diagram of the ﬁrst order low pass ﬁlter circuit along with
the ADC and the printed circuit board (PCB) developed for
image sensor is shown in Fig. 4.
B. The design of the Camera
The camera unit is assembled using the PCB with 2D PSD
and electronic circuitry, C-mount lens and an IR ﬁlter. The
C-mount lens has a nominal focal length of 35 mm. This
lens offers a ﬁeld of view covering an area of 200 mm x
200 mm at a distance of approximately 800 mm. Focusing
quality of the lens does not affect the signal quality in PSDs.
The iris of the lens was adjusted to the fully open position
to ensure maximum illumination. An IR ﬁlter which has a
band-pass frequency of 850 nm is used in front of the lens to
eliminate noise and background illumination. A serial cable
is used to facilitate power and data communication from the
camera. An exploded view of the camera is shown in Fig. 5.
The camera will be used to measure the position of an IR
marker. In the proposed system it will be mounted on the
EE of the robotic arm. A high power IR light emitting diode
(LED) from Vishay (VSMY7850X01) is used as the emitting
beacon. This is a high intensity LED which illuminates with
current ﬂows of up to 1 A. The intensity of the signal will
ensure a high signal-to-noise ratio improving the accuracy of
the measurement. It is also a wide angle LED which covers
4155
IR Filter
C-mount
Lens
Camera Housing
Camera Cover
Image sensor &
Signal Conditioning
Circuit
Fig. 5. An exploded view of the PSD camera
a beam angle of 120 degrees allowing the EE to be seen by
the camera over a much wider operating envelope.
C. Calibration of the PSD and the Camera Unit
The proposed camera needs to be calibrated before it is
used for measuring the position. More emphasis was put on
the calibration process since the proposed method uses single
sensor feedback for position measurement as compared to
Wang et al. [14], who used multiple sensor fusion. The
2D PSD consists of non linearities such as the pin cushion
effect. Therefore, it needs to be calibrated to relate the actual
position of the IR spot on the sensor with the position
obtained from induced currents. For calibrating the 2D PSD,
it was mounted onto a precision x-y translation table which
is capable of producing displacements in x and y directions
with accuracies of up to 10 m. An IR illumination source
which consists of a laser LED with an aperture size of
100 m was used as the marker for the calibration. The
sum of four anode currents indicates the lighting level from
the illumination source. A feedback controller was used to
maintain the illumination from the marker at a constant level.
A set of 10 x 10 points covering an area 8 x 8 mm of the
PSD was obtained for analysis.
A Design of Experiments (DOE) based method was used
to analyze the data that was obtained through the calibration.
The response surface methodology (RSM) is a statistical
technique used for developing empirical models from ex-
perimental data. In RSM the input variables are known as
factors and the output variables are known as responses. The
objective of RSM is to optimize a response variable that is
inﬂuenced by several independent factors. In the given case,
responses are the true position of IR spot on the PSD, while
factors are x,y positions calculated from the four currents.
The calibration data was analyzed using the RSM - IV
optimal design method. The best model is determined by
considering various statistical parameters such as p value, F
value, adjusted and predicted R
2
. In statistics, the p value
corresponds to the probability that the null hypothesis is
rejected. The F test provide an indicator for comparing the
statistical models that would provide a ﬁt to the data set. The
best ﬁt model should consist of a lower p value, higher F
value and reasonably close adjusted and predictedR
2
values
high in magnitude. The StatEase v8 software was used to
analyze the data. The calibration results for the 2D PSD are
shown in Fig.s 6,7. The DOE provides the model of best
 1
 0:5
0
0:5
1
 1
 0:5
0
0:5
1
 10
0
10
PSD X
PSD Y
X /(mm)
Fig. 6. The variation between X vs

X(PSDX) and

Y (PSD Y)
 1
 0:5
0
0:5
1
 1
 0:5
0
0:5
1
 10
0
PSD X
PSD Y
Y /(mm)
Fig. 7. The variation between Y vs

X(PSDX) and

Y (PSDY )
ﬁt based on a statistical analysis using the experimental data
obtained during the experiments. The models suggested by
the analysis are as follows.
psdX =f(k1;

X;

X
2
) (4)
psdY =f(k
2
;

Y;

Y
2
;

Y
3
); (5)
Where psdX, psdY are the true x,y positions of the IR spot
on PSD and k
1
, k
2
are constants.
The intrinsic parameters of this camera has be known
prior to using it for position estimation. This camera
needs to be calibrated to estimate these parameters. An
effective calibration process would result in accurate and
robust measurement of the position. Several methods have
been proposed in literature for camera calibration. Out of
them, the methods proposed by Heikkila [17] and Rahman,
Krouglicof [18] were used for calibrating this camera.
Heikkila’s method considers a checker board consisting of
4156
an array of circular markers as calibration targets and uses
a simpliﬁed model for estimating the camera parameters.
The calibration target used in this method does not always
provide the accuracy expected for a camera calibration
process. Since a high degree of precision is required, this
camera was mounted onto a camera calibration setup. It
consists of an x-y table that uses two ball screws driven
by stepper motors. This table could be positioned to an
accuracy of 15 m. This table also provides backlash
compensation improving the accuracy. A high power IR
LED was used as marker for calibration. A predeﬁned
square grid of 150 mm x 150 mm was considered for
marker. The marker was given a translational displacement
of 5 mm for each reading. Three parallel planes were
considered for data collection. The distance between the
planes were 12.7. Each calibration run produced a total of
2700 calibration points.
Once the calibration data was obtained, it was analyzed
using the algorithms proposed in Heikkila [17] and Rahman
et al. [18]. A summary of results from the calibration is
given in Table I. SF corresponds to the scale factor, f
TABLE I
SUMMARY OF THE CALIBRATION RESULTS
Calibration Parameter
Method SF f u0 v0 k1 k2 p1 p2 
Heikkila 0.9972 34.70 1.60 -0.92 -2.6 0 5 -5.4 9.2
Rahman 1.0403 35.92 2.87 -0.93 -3.2 0 2 -9.9 8.2
is the focal length, k
1
, k
2
are the coefﬁcients of radial
distortions in units of 10
 3
mm
 2
and mm
 4
, p
1
, p
2
are
the coefﬁcients of tangential distortions in 10
 4
mm
 1
and
10
 3
mm
 1
units, and (u
0
,v
0
) are the coordinates of the
modiﬁed image center in mm.
Reliability of the calibration depends on the repeatability of
the results. The discrepancy between the original calibration
data and the reconstructed model is indicative of how well
the calibrated model ﬁts the calibration data. This is known
as the standard error of the calibration model. The standard
deviation of the error of the reconstructed model is higher
in Heikkila (9.2 m) compared to Rahman & Krouglicof
(8.2 m). Therefore, the estimated camera parameters from
the method proposed in Rahman & Krouglicof [18] were
selected for developing the camera model. Nevertheless,
multiple calibration runs proved that these calibration results
are repeatable.
IV. ON SITE CALIBRATION OF THE CAMERA
An on site calibration is required to estimate the extrinsic
parameters of the camera. It will provide the parameters
required to transform measurements made by the camera to
a global ﬁxed coordinate system. In the proposed calibra-
tion the EE position is estimated with respect to a global
coordinate frame using encoder angles. This is denoted by
w
P. This could be expressed using homogenous coordinates.
Thus,
w
P =
2
6
4
l2:cos(2) +l3:cos(

2
+2 +3)
l2:sin(2) +l3:sin(

2
+2 +3)
0
0
3
7
5 (6)
The camera provides position measurements with respect
to its coordinate system and this is given by
c
P.
The transformation from the world coordinate frame to
camera’s coordinate frame could be performed by consider-
ing a 4 x 4 transformation matrix (
c
T
w
) and a camera matrix
(C). This is also known as the forward camera model. The
estimated position of the EE with respect to the camera (
c
^
P)
using the encoder data is given by,
c
^
P =C
c
Tw 
w
P (7)
Where C =
"
s:f 0 u
0
0
0 f v
0
0
0 0 1 0
#
and
c
T
w
=
h
c
Rw
c
tw
0 1
i
c
R
w
is obtained considering successive roll ( ), pitch (),
yaw () angles about the  x, y, z axes and
c
t
w
=
[t
x
;t
y
;t
z
]
T
describes the translation between the camera and
the world coordinate frame. f is the effective focal length, s
is the scale factor or aspect ratio and (u
0
;v
0
) is the position
of the image center.
A cost function is formulated to minimize error between
EE position estimated using equation 7 and the actual posi-
tion measured by the camera. If C is the total cost, it will
be given by,
C =argmin
s
X
8i
(
c ^
Pi 
c
Pi)
2
(8)
The Levenberg Marquardt (LM) algorithm [19] is em-
ployed to solve the optimization problem and obtain the
parameters which minimizes the given objective function.
The parameters estimated by the optimization include the
link lengths (l
2
;l
3
), pose (t
x
;t
y
;t
z
) and orientation ( ;;)
of the camera.
V. FPGA-BASED IMPLEMENTATION OF THE PROPOSED
CONTROLLER
The controller was implemented using the Terasic DE0 de-
velopment board. This development board consists of Altera
Cyclone III FPGA. Quadrature decoding and SPI Input for
feedback and the SPI output interface for the control signal
that drives the servo valve were implemented in hardware.
Verilog hardware descriptive language was used to develop
the hardware components. A virtual soft processor was
implemented in the FPGA to implement functions that are
too complex to be implemented in hardware. This processor
is optimal for medium-performance applications that do not
require a large memory for operations. Programming of this
virtual processor is similar to that of a microcontroller or
any other processor. Therefore, coding for this processor
was carried out in C language. The architecture of proposed
single time scale VS system is shown in Figure 8. The
proposed embedded system provides a single chip solution
for controlling the arm.
4157
Quadrature 
Decoder
SPI OUT
Clk
Reset
CH A
CH B
Quadrature 
Decoder
Reset
CH A
CH B
Clk
Enable
Clk
CS1
CS2
Data
Control Signal 2 
Nios [15..0]
Control Signal 1 
Nios [15..0]
CS1
Clk
Data
I 1 [15..0]
CS3
CS2
Clk 50 MHz
CS4
I 2 [15..0]
I 3 [15..0]
I 4 [15..0]
Clk 50 MHz
SPI IN
Encoders
PSD Camera
Servo Valve 
Driver
AVALON BUS
CPU
System ID
On-Chip
RAM
UART
PIO
Timer
NIOS II Embedded Processor IP
?
1
[15..0]
?
2
[15..0]
Fig. 8. A schematic diagram showing the architecture of the proposed
controller
A NIOS II standard type processor was conﬁgured with
a 50 MHz phase-locked loop, on-chip memory, a JTAG
UART, an interrupt timer and input/output (I/O) ports. The
on chip memory comprised 40 kilobytes for running the
controller. The JTAG Uart is a universal asynchronous re-
ceiver/transmitter which facilitates the data transmission and
the communications between the FPGA and a host computer.
The interrupt timer was used to trigger a 1 ms interrupt. The
input output buses were conﬁgured depending on the bit size
of the signals.
The robotic manipulator is instrumented with joint en-
coders to measure the position of joint angles. These en-
coders have a resolution of 8000 pulses per revolution.
The encoders provide the angular position using two pulse
trains. The magnitude of joint angle and direction of rotation
could be estimated by carrying out quadrature decoding of
the two pulse trains. Each encoder requires four ﬂip ﬂops
implemented in hardware to perform this operation. The
position measurement from the PSD camera is obtained as
four digital voltage signals. The ADS8320 ADC requires
a 16 bit SPI with data in protocol. The electronic design
of the circuit used a common data and clock input with
four different chip selects for data acquisition. The SPI input
protocol was implemented in hardware. The data acquisition
from the ADC could be carried out at data rates of up to 12
kHz. In order to eliminate noise from the voltage signals of
the PSD a soft-coded median ﬁlter was used.
The control signal was calculated in the soft processor
using the feedback signals. This signal will be output to a
DAC via 16-bit SPI output interface. It consists of a clock,
data and a chip select signal. This interface was developed in
hardware so that it is compatible with the timing diagram of
the MAX541 DAC converter. The analog voltage from the
DAC is sent to the servo valves via an operational ampliﬁer
circuit which ampliﬁes the signal.
VI. THE EXPERIMENTAL SETUP
The SCARA manipulator considered in study has two
links and an EE. The links of this manipulator are 0:49m
SCA R A 
Manipulator
E nd effector 
with IR Marker
Joint E ncoders
High speed PSD 
Camera
Fig. 9. Experimental setup with the SCARA manipulator
and 0:36m in length. Double vane rotary hydraulic actuators
with integrated hydraulic servo valves are used to drive the
revolute joints of this manipulator. These actuators operate
at a rated hydraulic system pressure of 3000 psi. The second
link incorporates an EE which has a weight of 12 kg. This
SCARA Arm is mounted onto a compliant support with
an
0
I
0
shaped cross section. The joints of this robot are
instrumented with rotary encoders as proprioceptive sensors.
The high speed camera developed in this study is used as
the exteroceptive sensor. It will be mounted opposite to the
EE of the manipulator. The high power IR LED is mounted
on to the EE of the arm. The camera will provide the position
of the EE. The proposed high speed visual servoing system
is shown in Fig. 9.
VII. EXPERIMENTAL RESULTS
A. Accuracy of the High Speed Camera System
The on-site calibration algorithm was presented in section
IV. It uses an iterative scheme with LM method to optimize
a cost function to provide the on-site parameters which
include position and orientation of the camera. A data set
was obtained by moving the EE over a 10 x 10 mm grid.
The data set obtained include the encoder readings and
four voltages from the camera. The position provided by
the joint encoders is considered as the true position. The
voltage signals provided by the camera are median ﬁltered
and converted into position signals taking into account the
non linearities of the image sensor.
Fig. 10 (a) show a plot of the root mean squared error
(RMSE) between the measurements in x and y directions
4158
before calibration. An accurate measurement would result
in points that are concentrated around (0; 0). However, it is
seen that these points are mainly concentrated in 2nd and
3rd quadrants. This offset indicates that the position of the
image center has to be re-estimated. The Fig. 10 (b),(c) show
plots of the absolute value of the RMSE in x,y directions.
It is seen that the error follows a fairly repetitive pattern as
the accuracy of the position measurements decrease with the
distance from image center. It is also seen that the error in
measurements in x direction is higher compared to that in
 y direction.
The results obtained after the calibration is shown in
Fig 11. It could be seen that the measurement error has
rapidly decreased after the on-site calibration. If the extrinsic
parameters are not considered the absolute error could be
as high as 3.7 mm in x  direction and 2.34 mm in y 
direction, respectively. The absolute average RMSE in x 
direction will decrease from 1.43 mm to 0.31 mm and iny 
direction it will decrease from 0.73 mm to 0.37 mm. Once
the on-site calibration carried out the absolute RMSE will
not exceed 1 mm in both x;y directions. For the calibration
points considered the maximum absolute error was 0.93 mm
in x  direction and 0.95 mm in y  direction, respectively.
The on-site calibration produces a signiﬁcant improvement
in the accuracy of the measurement.
The Fig. 10, 11(d) shows the absolute value of the cost
function for each data point. This represents an error in
a distance measurement from the origin of the Cartesian
coordinate frame. The cost function has an average RMSE
of 1.71 mm before the calibration. The RMSE of the cost
function could be as high as 3.71 mm at times. However,
with the calibration the the average RMSE will reduce to
0.54 mm. Therefore, once calibrated this camera is robust
in performance to carry out position measurements. The
experiments show that in x direction this camera is capable
of providing measurements with an average accuracy of 0.31
mm with a standard deviation of 0.26 mm. In y direction
the average accuracy was 0.37 mm with a standard deviation
of 0.24 mm.
B. Testing of the Proposed VS system
The SCARA arm considered in this study is capable
of reaching very high velocities. Since the EE carries a
considerable payload it results in signiﬁcant deﬂection in
the support column. Therefore, the absolute position of the
EE needs to be obtained and to be used in controlling the
arm. The EE was set to follow a desired linear trajectory on
the work proﬁle from (425 mm, 333 mm) to (550 mm, 232
mm). The trajectory following experiment considered two
different cases. In one of the cases it considered only the
encoder feedback (with no VS). Thus, effects of the external
disturbances were not considered in the control algorithm.
In the other case with VS, it considered feedback from the
encoders and the camera.
Experiments showed that the camera developed in this
study was capable of providing position measurements of
up to 1340 Hz. The experimental results showing the desired
0 20 40 60 80 100
0
1
2
3
4
X error /(mm)
 4  2 0 2 4
 4
 2
0
2
4
X error /(mm)
Y error /(mm)
0 20 40 60 80 100
0
1
2
3
4
Y error /(mm)
0 20 40 60 80 100
0
1
2
3
4
RMS error /(mm)
Fig. 10. The error of the camera before calibration
0 20 40 60 80 100
0
0:2
0:4
0:6
0:8
1
X error /(mm)
 4  2 0 2 4
 4
 2
0
2
4
X error /(mm)
Y error /(mm)
0 20 40 60 80 100
0
0:2
0:4
0:6
0:8
1
Y error /(mm)
0 20 40 60 80 100
0
0:2
0:4
0:6
0:8
1
RMS error /(mm)
Fig. 11. The error of the camera after calibration
4159
0.42 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58
0.22
0.24
0.26
0.28
0.3
0.32
0.34
0.36
X?direction /(m)
Y?direction /(m)
Desired
With VS
With no VS
Fig. 12. EE following a given trajectory with and without VS
path of the EE and the actual path which EE traced is shown
in Fig. 12. It is seen from the experimental results that, VS
improved the accuracy of the trajectory that was followed
by the EE. Nevertheless the feedback from the camera and
encoders were considered simultaneously in this single time
scale VS system. During the experiments the average EE
velocity was 1.3 ms
 1
. The control signal was updated at a
frequency of 330 Hz.
VIII. CONCLUSIONS
In this study, we have proposed a method to perform
single time scale VS. It outlined the development of a high
speed camera system for position measurement. This camera
uses a 2D PSD as the image sensor. The electronic circuitry
for performing signal conditioning and data acquisition was
developed. Then the 2D PSD was calibrated to relate the
actual position of the IR spot on the sensor to the position
obtained from induced voltages. Next the camera was as-
sembled to a housing along with the printed circuit board
with the PSD, a C-mount lens with a nominal focal length
of 35 mm and an IR ﬁlter. This camera was mounted on to a
camera calibration setup to estimate the intrinsic parameters
of the camera. A methodology for on site calibration was
developed for estimating the position and orientation of the
camera once is mounted to the application setup. An FPGA
based embedded system was developed for implementing the
controller. It used both hardware and a soft processor for
feedback and executing the control algorithm.
This camera was capable of providing measurements at
frequencies of up to 1340 Hz. The position measurements
provided by the camera was accurate up to 0.93 mm, 0.95
mm inx ,y  directions, respectively. The proposed single
time scale VS system was experimented with a high speed
SCARA type manipulator with servo hydraulic actuators.
The experimental results showed a satisfactory performance
of the proposed VS system at EE speeds of up to 1.3 ms
 1
.
The VS controller operated at a frequency of 330 Hz during
the experiment.
REFERENCES
[1] L. Bascetta and P. Rocco, “Two-time scale visual servoing of eye-in-
hand ﬂexible manipulators,” Robotics, IEEE Transactions on, vol. 22,
no. 4, pp. 818–830, 2006.
[2] Z.-H. Jiang and T. Eguchi, “Vision feedback based end-effector motion
control of a ﬂexible robot arm,” in Systems, Man and Cybernetics,
2007. ISIC. IEEE International Conference on, oct. 2007, pp. 2413
–2419.
[3] Y . Liu, N. Xi, Y . Shen, S. Bi, B. Gao, Q. Shi, X. Li, G. Zhang, and
T. Fuhlbrigge, “High-accuracy visual/psd hybrid servoing of robotic
manipulator,” in Advanced Intelligent Mechatronics, 2008. AIM 2008.
IEEE/ASME International Conference on, 2008, pp. 217–222.
[4] G. Hitaka, T. Murakami, and K. Ohnishi, “An approach to vibration
control by stereo vision system in mobile manipulator,” in Advanced
Intelligent Mechatronics, 2001. Proceedings. 2001 IEEE/ASME Inter-
national Conference on, vol. 1, 2001, pp. 601 –605 vol.1.
[5] W. Song and M. Minami, “Hand to eye vergence dual visual servoing
to enhance observability and stability,” in Robotics and Automation,
2009. ICRA ’09. IEEE International Conference on, 2009, pp. 714–
721.
[6] R. Kelly, “Robust asymptotically stable visual servoing of planar
robots,” Robotics and Automation, IEEE Transactions on, vol. 12,
no. 5, pp. 759–766, 1996.
[7] M. Marey and F. Chaumette, “Analysis of classical and new visual
servoing control laws,” in Robotics and Automation, 2008. ICRA 2008.
IEEE International Conference on, 2008, pp. 3244–3249.
[8] T. Sievers and S. Fatikow, “Visual servoing of a mobile microrobot
inside a scanning electron microscope,” in Intelligent Robots and
Systems, 2005. (IROS 2005). 2005 IEEE/RSJ International Conference
on, aug. 2005, pp. 1350 – 1354.
[9] P. Rousseau, A. Desrochers, and N. Krouglicof, “Machine vision
system for the automatic identiﬁcation of robot kinematic parameters,”
Robotics and Automation, IEEE Transactions on, vol. 17, no. 6, pp.
972 –978, dec 2001.
[10] J. Fischer and O. Pribula, “Precise subpixel position measurement with
linear interpolation of cmos sensor image data,” in Intelligent Data
Acquisition and Advanced Computing Systems (IDAACS), 2011 IEEE
6th International Conference on, vol. 1, sept. 2011, pp. 500 –504.
[11] A. Tobergte, F. Frohlich, M. Pomarlan, and G. Hirzinger, “Towards
accurate motion compensation in surgical robotics,” in Robotics and
Automation (ICRA), 2010 IEEE International Conference on, may
2010, pp. 4566 –4572.
[12] N. Michael, D. Mellinger, Q. Lindsey, and V . Kumar, “The grasp
multiple micro-uav testbed,” Robotics Automation Magazine, IEEE,
vol. 17, no. 3, pp. 56 –65, sept 2010.
[13] M. Windolf, N. G¨ otzen, M. Morlock et al., “Systematic accuracy and
precision analysis of video motion capturing systems–exempliﬁed on
the vicon-460 system.” Journal of biomechanics, vol. 41, no. 12, p.
2776, 2008.
[14] C. Wang, W. Chen, and M. Tomizuka, “Robot end-effector sensing
with position sensitive detector and inertial sensors,” in Robotics and
Automation (ICRA), 2012 IEEE International Conference on, may
2012, pp. 5252 –5257.
[15] M. Liyanage, N. Krouglicof, and R. Gosine, “Development and testing
of a novel high speed scara type manipulator for robotic applications,”
in Robotics and Automation (ICRA), 2011 IEEE International Confer-
ence on, may 2011, pp. 3236 –3242.
[16] “Hamamatsu s5991 data sheet,” Hamamatsu Photonics K.K., Hama-
matsu, Japan.
[17] J. Heikkila, “Geometric camera calibration using circular control
points,” Pattern Analysis and Machine Intelligence, IEEE Transactions
on, vol. 22, no. 10, pp. 1066 – 1077, oct 2000.
[18] T. Rahman and N. Krouglicof, “An efﬁcient camera calibration tech-
nique offering robustness and accuracy over a wide range of lens
distortion,” Image Processing, IEEE Transactions on, vol. 21, no. 2,
pp. 626 –637, feb. 2012.
[19] K. Levenberg, “A method for the solution of certain non-linear
problems in least squares,” Quarterly of Applied Mathematics, vol. 2,
no. 2, p. 164168, 1944.
4160
