Integrated Exploration using Time-based Potential Rails
Renan Maffei
1
Vitor A. M. Jorge
1
Edson Prestes
1
Mariana Kolberg
1
Abstract— Integrated exploration is the most complete task
in mobile robotics, and corresponds to the union of mapping,
localization and motion planning. A powerful integrated explo-
ration solution must take into account decisions that improve
the quality of the map construction, such as closing loops,
at the same time that the environment is explored. Potential
ﬁelds and boundary value problems (BVP) have been used with
success in tasks of planning, localization and exploration, but
not yet in integrated strategies. In this paper, we present an
integrated exploration strategy using a time varying BVP-based
exploration. Our strategy consists of creating potential rails that
guide the robot to regions that are either unexplored or were
visited a long time ago. We also apply local distortions in the
potential ﬁeld to generate a loop closure strategy. Experimental
results demonstrate that our method improves the quality of
the map construction, keeping the balance between revisiting
and exploratory activities.
I. INTRODUCTION
The ability of building the map of the environment is
important for most of the tasks performed by autonomous
mobile robots [1]. An integrated exploration technique must
perform an efﬁcient exploration of the environment while
minimizing the ﬁnal map degradation due to localization
errors. The design of integrated exploration techniques must
balance efﬁciency and map construction effectiveness.
By using an integrated exploration which re-enters known
regions the robot reduces localization errors, while in an
exploration strategy that only favors unknown regions, the
probability of making the correct associations is reduced [2].
Furthermore, localization problems tend to increase as the
robot moves, since odometry errors are cumulative. Thus,
each map associated to a small traversed region is generally
only locally consistent. It was shown that by going back
to loops we can correct the local error. However, by con-
tinuously re-entering unknown regions, strategies based on
probabilistic ﬁltering will loose diversity in terms of global
solutions due to resampling problems. The result is a trade-
off between revisiting and the compromise with previously
obtained solutions [2], [3].
In the same way, once the robot ﬁnishes exploration, the
transition between map construction and map usage should
be smooth. That is, we want the robot, e.g. a patrolling or
exposition robot, to make use of the map as soon as possible.
This also implies that once the ﬁrst part of the job – i.e.
integrated exploration – is ﬁnished, the robot must know
where to go back in the environment to be effective in its
job. Therefore, the robot must return to known areas, but
1
Institute of Informatics, Universidade Federal do Rio Grande do
Sul, Porto Alegre, Brazil rqmaffei, vamjorge, prestes,
mariana.kolberg@inf.ufrgs.br
giving priority to those which were not visited for the longest
period of time.
Potential ﬁelds and boundary value problems (BVP) for
the Laplace Equation were applied to exploration in the work
of Prestes et al. [4], [5]. Prestes et al. [6] also developed a
strategy to distort the potential ﬁeld. This can change the
robot movement while it is performing the gradient descent,
without loosing the qualities of the Laplace Equation (e.g. the
absence of local minima). BVP exploration has additional ad-
vantages, including: smooth movements; easy understanding
and implementation; and use of local windows to improve the
speed of convergence of the potential ﬁeld. However, it is in
essence a greedy exploration what is known to be ineffective
for integrated exploration. Furthermore, once the whole map
is explored the potential quickly converges to the potential
of the walls. Thus, we can no longer use the potential ﬁeld
to move the robot again.
In this paper we present an integrated exploration in terms
of a time varying BVP problem. We use an online con-
structed V oronoi Skeleton as a complimentary time varying
boundary. The Skeleton cells keep track of time of visit
information, guiding the potential ﬁeld to unknown regions
and also to those not visited for the longest time. The global
skeleton information enables the use of a local potential
window, which reduces the computational cost of BVP and
maintains the effectiveness of the method. We propose the
use of potential distortions [6] to direct the robot to close
loops (making turns). Moreover, our method never converges
to a ﬂattened potential ﬁeld – even if there are no more
unexplored boundary cells – since there will always be an
“oldest cell”. When the integrated exploration is ﬁnished, the
robot is immediately guided to those regions not visited for
the longest time in an allegedly perpetual cycle – what can
be interesting for patrol and exposition robots.
This paper is presented as follows. Section II presents
the related work. Section III introduces essential background
information on potential ﬁelds. Sections IV and V discuss
our time-based exploration and our loop closure strategies,
respectively. Section VI shows the experiments where we
compare two strategies using potential ﬁelds: the greedy
strategy and the proposed method. Finally, in Section VII
we discuss our method, draw our conclusions, and present
the future work.
II. RELATED WORK
In this Section we review some relevant work in the
context of our work.
Yamauchi’s [7] introductory greedy strategy explores the
environment by continuously driving the robot towards the
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3694
closest frontier using laser and sonar sensors along with a
reactive obstacle avoidance technique. Makarenko et al. [8]
does it by using a weighted function which considers dif-
ferent factors such as map construction coverage, accuracy,
and exploration speed. The localization quality is measured
applying a Kalman ﬁlter at a few selected positions during
planning. The best positions are used as points of interest.
Prestes et al. [4], [6] reformulate the exploration problem
as a Dirichlet BVP for the Laplace equation. The unex-
plored regions have low potential, while the walls have high
potential. The numerical solution is obtained through the
ﬁnite-differences method [6]. The robot is always attracted
to the larger or closer frontiers. When there is no unexplored
frontiers left, the solution converges to the potential of the
walls and the robot stops. The algorithm is fairly simple,
however it has a few drawbacks: it uses a greedy strategy,
which is not appropriate when there are SLAM errors; it is
not possible to control the revisit of explored regions; and it
can be expensive for large maps. Later [5], the method was
improved by using a local map with size varying according
to the smallest laser reading to reduce the computational
cost of the method. Unfortunately, this is only possible
when there are nearby unexplored frontiers inside the local
window, otherwise the global map is used instead. Shade and
Newman [9] adapt BVP to 3-dimensional (3D) environments.
They developed a parameter free strategy using BVP in an
octree map to reduce the computational cost of the method.
The use of irregular grids makes it run in real-time for 2D
applications such as ours, but unfortunately not enough for
a real-time application in 3D environments.
Stachniss et al. [2] strategy merges a grid-based Rao-
Blackwellized Particle Filter (RBPF) and an active loop-
closing technique, using a topological map. In another work,
they devised a heuristic that considers information gain (i.e.
changes in the particles entropy) in loop closures, revisits and
exploration of unknown regions. The idea is to keep the robot
inside a loop only while the diversity is not degraded [3].
These strategies enable behavior switching using a few
parameters but they are environment dependent and unstable
during the loop closure. Freda et al. [10] adapts a Sensor-
based Random Tree (SRT) for the integrated exploration
problem. They also use information gain and localizability
to evaluate candidate conﬁgurations for exploration. The
strategy naturally merges sensor information and map cor-
rection procedures. Amigoni [1] presents an exploration test
framework. He compares different exploration strategies,
evaluating different parameters. Exploration strategies which
balance utility and cost show better results than those using
only utility. Later, Amigoni et al. [11] blends information
gain and traveling cost to construct maps, using point maps
as safe zones for the robot. Blanco et al. [12] state that loop
closing is key to reduce the robot path uncertainty and to
improve the resulting maps. They rely on the entropy of
the expected map of the RBPF – an integration of the map
hypotheses of all particles – to detect opportunities to close
loops and also to guide the robot during the exploration. Juli´ a
et al. [13] present a hybrid reactive/deliberative method to
multi-robot integrated exploration. They rely on the concepts
of expected safe zone and gateway cell to avoid the presence
of local minima and to decide between exploring the current
zone or changing to other zones. They also take into account
localizability measures to decide if it is necessary to return to
previously explored areas. Carlone et al. [14] propose the use
of the Kullback-Leibler divergence to analyze the particle-
based SLAM approximation. The divergence is adapted to
the notion of expected information gain, which enables the
decision between exploration and revisiting actions. The
technique is compared to naive gain, entropy-based gain, and
the expected map, showing better results to detect and close
loops. In all these works, there is an active change between
exploratory and revisiting behaviors.
In our approach, we take advantage of the theory behind
harmonic ﬁelds and the recently developed strategy to distort
potentials [6] to force loop closing by favoring turns on bifur-
cations, which is a powerful strategy when the environment is
not known a priori. This is done without the explicit need to
switch the robot behavior. The equations we employ and the
underlying boundary conditions, along with the preferences
in front of the robot, naturally mimic these behaviors.
III. EXPLORATION BASED ON BVP APPLYING LOCAL
DISTORTIONS
The exploration based on Boundary Value Problem (BVP)
[6] uses the gradient descent of a potential ﬁeld to smoothly
guide the robot to a goal position in the environment. Local
distortions are applied in the ﬁeld, to increase the preferences
for speciﬁc regions. This potential information is computed
from the numeric solution of
r
2
p(r) =(r)




@p(r)
@x




+




@p(r)
@y





(1)
with Dirichlet boundary conditions, where p(r) is the po-
tential at position r2<
2
and 2< is the intensity of the
perturbation produced in the region deﬁned by r.
To compute Eq. 1 numerically, the environment is divided
into a regular grid, where each cell stores its potential
value and is also associated to a square region. By using
Dirichlet boundary conditions, the cells representing obsta-
cles have a ﬁxed high potential value (p=1), whereas the
cells representing the unexplored areas (goals or sinks) have
low potential value (p=0). The remaining cells have their
potential computed iteratively through the ﬁnite difference
method [6].
The intensity of a perturbation applied over the potential
ﬁeld is deﬁned by . If > 0, the concavity of the potential
ﬁeld increases, while  < 0 the convexity of the ﬁeld
increases. These effects can be observed in the 1-D example
shown in Fig. 1. We set the potentials of the two boundary
points as p(0) = 0 and p(30) = 1, and compute the
intermediate potentials using three different values of epsilon
in the central region of the environment (highlighted). When
 = 0, the potential does not present curvature, as shown by
the straight line b. When  < 0 (curve a), the curve of the
potential is ﬂatter near obstacles (high potential) and steeper
3695
close to the goals (low potential). Thus, this can be seen as
a low preference region, because it is harder for the robot
to enter this region than to leave. On the other hand, a high
preference region is deﬁned when  > 0 (curve c). In this
case, it is easier for the robot to enter this region than to
leave, because the slope in the potential is increased near
obstacles and decreased close to the goals.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 5 10 15 20 25 30
?
b
=0
?
a
=-0.2
?
c
=0.2
a
b
c
Position
Potential
Fig. 1. Changes in the curvature of the potential ﬁeld by varying the
preference () of the central region. Curve b is the standard potential
calculated without preferences. Curve a presents the potential calculated
for a negative preference value, while curve c is the result for a positive
preference value.
By using only two ﬁxed boundary conditions in the ob-
stacles and unexplored cells, the BVP exploration is always
guided to the largest and closest unexplored regions. This
approach works well when there is no uncertainty in the robot
localization. However, an effective integrated exploration
strategy balances the navigation to unexplored regions with
re-visiting tasks in order to improve the localization of
the robot and the quality of the resulting map. Preferences
can be applied on BVP exploration to favor the passage
over speciﬁc regions. This is good, for instance, in sparse
environments, because it is possible to keep the robot close
to the walls and minimize localization errors. Yet, the robot
always moves towards the regions with smaller potential –
even with preferences. As a result, re-visiting actions are
unlikely to happen.
IV. TIME DRIVEN BVP INTEGRATED EXPLORATION
Our integrated exploration approach, called Time-driven
BVP (TDBVP), adds another boundary condition to the BVP
exploration. In addition to the high potentials in obstacles
(p=1), and the low potentials in unexplored areas (p=0),
we set a variable potential in the center of the areas visited
by the robot, generating rails where we want the robot to
move. These three different boundary conditions are used to
compute the potentials of the remaining cells, according to
Eq. 1.
We compute the V oronoi skeleton of the known free-space
(thinning) to extract the center cells [15]. Then, to deﬁne the
potential of those cells, we compute a 1-D harmonic function,
along the V oronoi skeleton, based on the time of the last visit
made by the robot
r
2
p(s) = 0 (2)
where p(s) is the potential of the center cells visited at step
s. The idea is that cells not visited for a long time have lower
potential (i.e. are more attractive) than recently visited cells.
For that reason, the boundary conditions of Eq. 2 are the
potentials of the most recently visited cells (p(s
newest
) = 1)
and the potentials of the oldest cells (p(s
oldest
) = 0). Since
this is a 1D function, its solution is a straight line between
the oldest and the newest cells. Thus, we can compute the
potential of the center cells analytically through
p(s) =
s s
oldest
s
newest
 s
oldest
(3)
The potentials of the center cells not visited yet, such as
recently visualized cells, are set to a ﬁxed negative value
(p = 0:5). Therefore, when the robot has to choose between
an unexplored area and a visited area, it will prefer the
unexplored area (unless we insert distortions in the potential,
as we will discuss in next session). When the robot has
to choose between two visited areas, it will prefer the one
visited the longest time ago. After the robot returns to the
oldest area, it will become the newest while another one will
be the oldest. Hence, in our approach the goal, i.e. the oldest
cell, changes over time and never ceases to exist.
We can infer that the environment will always be entirely
explored, even though the navigation decision is performed
locally by the robot according to the potentials of its sur-
roundings. Firstly, because the potential in the visited area
is still higher than the potentials of unexplored areas. And
secondly, because the only way that this would not happen,
would be if the robot could get stuck in some section of the
map, which is not possible. That is, after spending some time
inside a region, the potentials of unvisited regions will fall,
attracting the robot toward them. The longer it takes to visit a
cell the stronger its attractive force. However, we must point
out that in sparse environments the thinning algorithm will
not cover all free regions seen by the sensors. Thus, in this
cases we recommend the additional use of partial thinning
relaxations [16], to ensure that the robot will stay close to
walls, delaying the entrance on featureless regions.
Fig. 2 shows an example of our integrated exploration
method in a simulated environment. The robot is shown
in red, its trajectory in blue, and the obstacles in brown.
Observe that the zone just behind the robot is the lightest
area of the map, along with the surroundings of the walls
(in black), because their potentials are practically equal to 1.
The darkest area of the map is the top right zone between
the two inner walls, which was visited only in the beginning
of the experiment – the elder cells claiming for a visit.
An important advantage of our strategy in relation to the
BVP exploration described in Section III, is that our potential
ﬁeld can always be computed using only a small window
around the robot. In our approach, the potential of the cells
surrounding the robot is primarily affected by the nearby
center cells and obstacles. Thus, there is no need to compute
the potentials in the whole map, making this computation
independent of the size of the environment. In contrast, the
potentials in the traditional BVP exploration are affected by
3696
Fig. 2. Example of the potential ﬁeld based on the time of visit. The dark
areas are the regions visited the longest time ago.
cells that can be very far in the environment. For example,
when the robot is almost ﬁnishing the exploration and only
a distant small portion of the map remains unexplored, the
potential can take a very long time to converge and start
attracting the robot.
V. DYNAMIC ACTIVATION OF PREFERENCES
As discussed in Section III, the potential ﬁeld inside a
region can be distorted by inserting a preference parameter
in the potential computation. This distortion can be seen as
a form of control over the robot navigation in situations of
indecision. For instance, bifurcations zones with openings
of similar potentials cause unpredictability in the robot
movement.
In the integrated exploration problem, the robot has to
return to regions already visited, aiming to improve its
localization. This is the idea behind the active loop closure
behavior, as studied by Stachniss et al. [2]. Our approach
deals with loop closures by setting dynamic preferences in
the front of the robot. We divide the front of the robot (half
semi-circular region in front of the robot) in two (left side
and right side), and apply different preferences on each side.
If we put a positive preference on the left and a negative
preference on the right, the potential ﬁeld beneath the region
will be distorted to the left. The contrary will happen if we
invert the preferences.
If we maintain the preference ﬁxed for turning to a
side during a loop closure, whenever the robot reaches a
bifurcation with balanced potentials, it will choose to turn
to this side. However, if the potential outside the loop is
signiﬁcantly lower than the potential inside the loop, the
robot will resume the exploration outside the loop. As a
result, there is an emergent balance in our approach between
the loop closure behavior and the exploratory behavior. This
balance is controlled implicitly by the values of preferences
and boundary conditions.
That said, using always the same preferences conﬁguration
is not the better strategy, because depending on the choice
of the preferences, the robot might even end up avoiding
loop closures. So, we adapt the preferences using a simple
algorithm based on the brief history of turns performed by
the robot. When the robot sensors detect a large opening to
the left or to the right (i.e. a sudden increase in the range
of the corresponding laser readings), the method tries to set
the preferences to the side of the observed opening. Yet, to
avoid changing them too quickly, the preference side is only
changed when two subsequent decisions are the same.
VI. EXPERIMENTS
We evaluate our method comparing it with the frontier-
based BVP exploration in real and simulated environments.
Both strategies were combined with a Rao-Blackwellized
Particle Filter SLAM algorithm, with laser range-ﬁnder and
occupancy grids [17].
The experiments in the real environment were performed
in a building ﬂoor of 72m13m containing two loops (with
90m and 60m of length). Fig. 3 presents the maps obtained
by both methods using 300 particles in SLAM. We can see
in Fig. 3(a) that the frontier-based BVP exploration did not
close the two loops separately, spoiling SLAM quality. This
happened because the small passageway at the center of the
map is too narrow to attract the robot when there are other
near unexplored frontiers. We also observe in the ending of
the frontier-based exploration that the robot trajectory was
not stable. This is a result of the increase in the BVP’s
convergence time associated to the map growth. The slow
update of the potential ﬁeld generates abrupt changes in the
potential gradient that makes the robot oscillate. We could
stop the robot and wait for the full convergence of BVP
to avoid the trajectory oscillation, unfortunately, this would
render the method useless for real-time applications. Fig. 3(b)
shows the resulting map of the time-driven approach. We
see that the robot separately closed both loops, due to its
potential distortion strategy, and also presented a smooth
trajectory, due to the local window computation of the
potential ﬁeld.
Start
End
(a) Frontier-based BVP
Start
End
(b) Time-Driven BVP
Fig. 3. Experiments in a real environment. The robot trajectory is shown
in blue.
The simulated experiments were performed in two dif-
ferent scenarios, adding uncertainty to the sensors measure-
ments. The ﬁrst scenario is composed of three adjacent loops
(with 72m of length each), while the second is composed
of nested loops (the larger has 88m of length). For each
3697
scenario, we chose 10 different starting positions to perform
the test for each method. In all simulated experiments, we
used 200 SLAM particles.
Fig. 4 presents the best visual results obtained in the
adjacent loops scenario. The trajectories of the remaining
particles at the end of the process are shown in blue, while
the correct trajectory of the robot is shown in magenta.
Fig. 4(a) depicts the best map generated by the frontier-
based BVP exploration. The robot does not try to revisit
known places, in fact, it spends too much time in the external
long corridors guided to unexplored areas. As a result, its
localization only gets worse with time. Later, when the robot
ﬁnally returns to visited areas, there is not enough diversity
to recover the quality of the map. Another issue is that
the robot trajectory oscillates, like in the real environment
experiment. As the goal changes abruptly to a distant region,
the potential can take too long to converge. Fig. 4(b) shows
the best map generated by our approach. As expected, the
robot tends to close loop by loop, which is good to improve
its localization. The estimated trajectory in the SLAM still
deviates slightly from the exact one, but the consistency of
the map is maintained.
(a) Frontier-based BVP
(b) Time-Driven BVP
Fig. 4. Experiments in the adjacent loops scenario. The trajectories of
the particles are shown in blue, while the correct trajectory of the robot is
shown in magenta.
Fig. 5 shows the best results obtained in the nested loops
scenario. As shown in Fig. 5(a), the frontier-based strategy
does not proved to be suitable for the situation. The problem
is that one badly consolidated loop can directly damage the
closure of subsequent ones. In Fig. 5(b), we show the map
obtained with our method, and, once again, the result is
visually better.
Besides the visual analysis, we also compare the methods
by the error in the robot position. At each iteration of the
process, the particle ﬁlter error (PFErr) is computed by the
mean distance between each particle position and the real
robot position (which is known in a simulated experiment).
The ﬁnal mean error PFErr is the mean of PFErr over
(a) Frontier-based BVP (b) Time-Driven BVP
Fig. 5. Experiments in the nested loops scenario. The trajectories of the
particles are shown in blue, while the correct trajectory of the robot is shown
in magenta.
all iterations.
Table I shows the results for the adjacent loops scenario.
The number of exploration steps was just slightly larger using
our method (1:24%). Thus, apparently, our loop closure be-
havior does not slow down the exploration process too much
in this environment. Regarding the mean error measures, our
approach presented a gain of 83:06%. Applying a statistical
sign test [18] to the results of 10 runs, we prove that the mean
error measures obtained by our method in this environment
are signiﬁcantly larger than the ones obtained by the frontier
based BVP (p-value=0:001).
Frontier-based BVP Time-driven BVP diff
    %
Steps 1947 319.80 1971.1 186.67 +1.24
PFErr 2.3489 1.1733 0.3979 0.1335 -83.06
TABLE I
RESULTS OF THE EXPERIMENTS IN THE ADJACENT LOOPS SCENARIO.
Table II shows the results for the nested loops scenario.
This time, the number of exploration steps was smaller
with our approach than with BVP ( 1:41%), nonetheless,
the difference was not signiﬁcant. The mean errors are
also smaller using the proposed method (56:98% smaller).
After running the sign test, we observe that our approach
performed signiﬁcantly better in this environment than the
frontier-based BVP exploration (p-value=0:001).
Frontier-based BVP Time-driven BVP diff
    %
Steps 1627.4 254.21 1604.4 290.76 -1.41
PFErr 1.4732 1.2783 0.6338 0.1312 -56.98
TABLE II
RESULTS OF THE EXPERIMENTS IN THE NESTED LOOPS SCENARIO.
Finally, we present the histograms of the mean position
error in the adjacent loops and in the nested loops scenarios,
Fig. 6(a) and Fig. 6(b) respectively. We detect that in our
approach, most of the time – 97% in the ﬁrst scenario and
98% in the second – the error was smaller than 1m, while in
the frontier-based BVP approach these percentages are only
45% in the adjacent loops and 62% in the nested loops.
3698
 0
 5
 10
 15
 20
 25
 0 1 2 3 4 5 6 7
Frequency (%)
Particles Mean Error (m)
Frontier?based BVP
Time?driven BVP
(a) Adjacent loops scenario
 0
 5
 10
 15
 20
 25
 30
 35
 0 1 2 3 4 5 6 7
Frequency (%)
Particles Mean Error (m)
Frontier?based BVP
Time?driven BVP
(b) Nested loops scenario
Fig. 6. Histograms of the mean error for the simulated experiments.
VII. CONCLUSIONS
In this paper we propose a strategy that blends harmonic
ﬁelds and a modiﬁed V oronoi Skeleton of the known area.
These techniques are used to guide the robot to unexplored
regions or to the “oldest” explored area. Our key contri-
butions are: (i) a potential ﬁeld that never ceases to exist,
even if the whole map is known; (ii) a loop-closing behavior
that emerges naturally from the TDBVP equations; (iii) a
less expensive approach than traditional BVP-Exploration, as
the potential ﬁeld is computed only inside a local window,
instead of the whole map.
Experiments show that TDBVP presents statistically sig-
niﬁcant improvements in terms of localization errors in
simulated environments. We can also see in the results that
in simulated and real environments TDBVP presents better
map quality than the traditional “greedy” BVP-Exploration.
Moreover, our strategy does not demand substantial increase
in the exploration time. This is interesting, because the
traditional BVP tends to use the smallest path to the goal
and also to avoid revisits.
Our idea can be easily integrated to topological SLAM
strategies since most of them create a path as the robot
moves (be it a skeleton or a given sequence of positions).
Even though the exploration strategy is fairly simple and
effective, we believe that it is possible to gain more control
over the harmonic potential ﬁeld. We continue to explore new
ways to improve the integrated exploration strategy (e.g., by
incorporating utility and cost metrics.
We are planning to extend the algorithm to 3D envi-
ronments where approaches using BVP are still not real-
time. Finally. our algorithm is well suited for cluttered
environments. When we consider sparse environments there
are still some issues that need to be addressed. First, we
need to test the algorithm using partial relaxations. Second,
the robot may be traveling in a rail that suddenly disappears.
In this case, the robot will move to the next nearby rail.
However, it might not be in the local window. If that happens,
the robot can try to move towards the next unexplored region
inside the window. In the event that none of this conditions
are satisﬁed, we need to drive the robot towards the V oronoi
diagram, relying on its property of accessibility [19].
ACKNOWLEDGMENT
The authors thank the ﬁnancial funds provided by CNPq and
CAPES.
REFERENCES
[1] F. Amigoni, “Experimental evaluation of some exploration strategies
for mobile robots,” in Proc. of the IEEE Int. Conf. on Robotics and
Automation (ICRA), may 2008, pp. 2818–2823.
[2] C. Stachniss, D. Hahnel, and W. Burgard, “Exploration with active
loop-closing for fastslam,” in Proc. of the IEEE/RSJ Int. Conf. on
Intelligent Robots and Systems (IROS), 2004.
[3] C. Stachniss, G. Grisetti, and W. Burgard, “Information gain-based ex-
ploration using rao-blackwellized particle ﬁlters,” in Proc. of Robotics:
Science and Systems, Cambridge, USA, June 2005.
[4] E. Prestes, P. M. Engel, M. Trevisan, and M. A. P. Idiart, “Exploration
method using harmonic functions,” Robotics and Autonomous Systems,
vol. 40, no. 1, pp. 25–42, 2002.
[5] E. Prestes, M. Trevisan, M. A. P. Idiart, and P. M. Engel, “Bvp-
exploration: further improvements,” in Proc. of the IEEE/RSJ Int. Conf.
on Intelligent Robots and Systems (IROS), vol. 4, 2003, pp. 3239–3244.
[6] E. Prestes and P. M. Engel, “Exploration driven by local potential
distortions,” in Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots
and Systems (IROS), sept. 2011, pp. 1122–1127.
[7] B. Yamauchi, “A frontier-based approach for autonomous exploration,”
in Proc. of the IEEE Int. Symp. on Computational Intelligence in
Robotics and Automation, 1997.
[8] A. A. Makarenko, S. B. Williams, F. Bourgault, and H. F. Durrant-
Whyte, “An experiment in integrated exploration,” in Proc. of the
IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2002.
[9] R. Shade and P. Newman, “Choosing where to go: Complete 3d
exploration with stereo,” in Proc. of the IEEE Int. Conf. on Robotics
and Automation (ICRA), 2011, pp. 2806–2811.
[10] L. Freda, F. Loiudice, and G. Oriolo, “A randomized method for inte-
grated exploration,” in Proc. of the IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), oct. 2006, pp. 2457–2464.
[11] F. Amigoni and V . Caglioti, “An information-based exploration strat-
egy for environment mapping with mobile robots,” Robotics and
Autonomous Systems, vol. 58, no. 5, pp. 684–699, 2010.
[12] J. L. Blanco, J. A. Fern´ andez-Madrigal, and J. Gonzalez, “A novel
measure of uncertainty for mobile robot slam with rao-blackwellized
particle ﬁlters,” International Journal of Robotic Research, jan 2008.
[13] M. Juli´ a,
´
O. Reinoso, A. Gil, M. Ballesta, and L. Pay´ a, “A hybrid
solution to the multi-robot integrated exploration problem,” Eng.
Applications of Artiﬁcial Intel., vol. 23, no. 4, pp. 473–486, 2010.
[14] L. Carlone, J. Du, M. K. Ng, B. Bona, and M. Indri, “An application of
kullback-leibler divergence to active slam and exploration with particle
ﬁlters,” in Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and
Systems (IROS), oct. 2010, pp. 287–293.
[15] Z. Guo and R. W. Hall, “Parallel thinning with two-subiteration
algorithms,” Commun. ACM, vol. 32, no. 3, pp. 359–373, Mar. 1989.
[16] E. Prestes, M. Ritt, and G. Fuhr, “Improving monte carlo localization
in sparse environments using structural environment information,” in
Proc. of IEEE/RSJ Int. Conf. on Int. Robots and Systems (IROS), 2008.
[17] A. I. Eliazar and R. Parr, “Dp-slam 2.0,” in Proc. of the IEEE Int.
Conf. on Robotics and Automation (ICRA), 2004, pp. 1314–1320.
[18] W. J. Dixon and A. M. Mood, “The statistical sign test,” Journal of
the Amer. Statistical Association, vol. 41, no. 236, pp. 557–566, 1946.
[19] H. Choset and J. Burdick, “Sensor-based exploration: The hierarchical
generalized voronoi graph,” The International Journal of Robotics
Research, vol. 19, no. 2, pp. 96–125, 2000.
3699
