New Marker for Real-Time Industrial Robot Programming by
Motion Imitation
?
Marcos Ferreira
1
,Paulo Costa
1
, Lu« õs Rocha
1
, A. Paulo Moreira
1
, Norberto Pires
2
AbstractÑ This paper presents a new marker for
robot programming by demonstration through mo-
tion imitation. The device is based on high intensity
LEDs (light emission diodes) which are captured by
a pair of industrial cameras. Using stereoscopy, the
marker supplies 6-DoF (degrees of freedom) human
wrist tracking with both position and orientation
data. We propose a robust technique for camera and
stereo calibration which maps camera coordinates
directly into the desired robot frame, using a single
LED. The calibration and tracking procedures are
thoroughly described. The tests show that the marker
presents a new robust, accurate and intuitive method
for industrial robot programming. The system is able
to perform in real-time and requires only a single pair
of industrial cameras though more can be used for
improved e?ectiveness and accuracy.
I. INTRODUCTION
Industrial manipulators are the ultimate automation
tool. These machines deliver accuracy and repeatability
aiding industrial processes at becoming increasingly e?-
cient while reducing production costs. Yet, programming
industrial manipulators is still extremely time consuming
and usually require experienced and highly qualiÞed
workers. Overall this is not compatible with ßexible
setups neither with small companies budgets since both
qualiÞed programmers and reconÞgurations downtime
imply strong Þnancial e?orts.
Even though the former is a quite restrictive scenario,
manipulators are still strongly desired at production lines
due to a series of advantages over human work, e.g.,
the ability to work continuously while preserving output
quality, immunity to fatigue, distractions and hazardous
environments.
*This work was Þnanced by the ERDF-European Regional De-
velopment Fund through the COMPETE Programme, and by
National Funds through the FCT-Fundaü c÷ ao para a Ciö encia e a
Tecnologia (Portuguese Foundation for Science and Technology)
within project ?FCOMP-01-0124-FEDER-022701? This work has
also been supported by FCT through the project PTDC/EME-
CRO/114595/2009 ÓHigh-Level programming for industrial robotic
cells: capturing human body motionÒ . This work is also part of
the project ÓNORTE-07-0124-FEDER-000060Ò which is Þnanced
by the North Portugal Regional Operational Programme (ON.2 -
O Novo Norte), under the National Strategic Reference Framework
(NSRF), through the ERDF fund, and by national funds, through
FCT.
1
M. Ferreira et al. are with the Dept. of Electrical and Computer
Eng., Faculty of Eng. Ð Univ. of Porto, and with INESC TEC Ð
Institute for Systems and Computer Engineering of Porto, Portugal
fmarcos.ferreira,paco,lfr,amoreirag@fe.up.pt
2
N. Pires is with the Dept. of Mechanical Eng., Fac-
ulty of Science and Technology, Univ. of Coimbra, Portugal
jnp@robotics.dem.uc.pt
A. Proposed Solution and Aims
To overcome this reality, this work presents a method-
ology for fast industrial robot programming via human
demonstration by gesture. The main goal is to achieve a
new type of marker that enables the human to quickly
show the robot how to do a concrete task with abstrac-
tion of the programming language and even completely
avoiding the use of the teach pendant.
The focus of this paper is to describe a new 6-DoF
marker that is captured by a pair of industrial cameras.
The artiÞcial vision system captures images can accu-
rately tell us information of position and orientation.
The resulting set of points that describe the human path
are automatically transformed into a robot program. The
human uses his natural abilities and skills to accomplish
the demonstration process without needing further in-
struction on using new software packages, interfaces or
tools. Unlike similar researches, the proposed marker is
able to perform in real time and the required apparatus
is reduced and cheap.
B. Related Work
The universe of human-robot interaction is vast and
it is hard to cover all the current streams of research
in the area. With a shorter scope, on motion and task
demonstrations and industrial robot interface, there are
still a number of contributions to consider: CAD based
programming, such as in [1] and [2], where the user
interacts with a simulated environment in order to draw
the robot paths; gesture and voice recognition, [3] and
[4] , where the robot is pre-programmed and the gestures
are used to deploy actions. In more similar approaches
to the work of this contributions there is the vast work
of R. Dillman, using stereo vision and a data glove [5] for
tracking, and adding tactile sensors for grasp recognition
[6] and [7]. Another similar approach has been presented
by B. Hein, [8] and [9], where a new tracking marker
is also proposed, based on infra-red LEDs that can
be coupled to any work-tool. This paper focus on the
development of a new marker that uses visible light
LEDs; with a new method for synchronized acquisition,
the visible markers can perform faster than the infra-
reds (either passive or active) because ours are easily
distinguished using colour. The proposed solution also
proves to be accurate and, unlike most of the previous
solutions, cheap as it is employed with a minimum of just
2 standard industrial cameras.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 742
Fig. 1:Sincrovisiontimings: A timing diagram depict-
ing the synchronous image acquisition. Both cameras are
triggered at the same time. From that moment on they
are acquiring the scene for a very limited time (3ms). The
ultra bight LEDs are triggered at the same instant the
cameras were triggered too and remain lit only during
the camera exposure period.
II. Motion Imitation Framework
A. Stereoscopy using the sincrovision technique
The sincrovision concept was developed and patented
[10] in the University of Porto - Faculty of Engineering. It
implements a system of 3D acquisition based on stereo-
scopic vision synchronised with high intensity luminous
markers. The key idea is to turn on the markers as soon
as the cameras start acquiring image and turn them o?
after the camera exposure time has expired. Figure 1
shows a timing diagram of the system.
The high intensity lights will be very bright on the
images whilst the background noisy data will have no
time to be acquired by the camera. At the same time,
blinking the markers for a short time makes it possible
to stare at them; keeping them always on would cause eye
damage. To ensure that unwanted light is not captured,
the lenses aperture is reduced to minimal. This setup
makes it possible to triangulate the markers positions
in space in a robust way, independently of lighting con-
ditions in the scene and ignoring most of the common
noise sources in artiÞcial vision applications. Figure 2
shows a typical image captured by the cameras using this
synchronous feature: on the left, the scene is captured
using standard camera aperture and exposition time; on
the right, the sincrovision e?ect over the same scene.
Multiple View Geometry [11] holds a complete guide for
camera models, the stereoscopy principles and camera
calibration, which are the necessary basis for retrieving
3D measures from a pair of images.
B. 6-DoF Marker
The major concerns for the development of this new
tracking tool were to achieve a device that provides an ac-
curate measure of the pose of the human hand/tool while
maintaining costs low, reduced processing times and low
impact on the process and on the human movements.
Using thesincrovision technique, this marker was built
from a set of high intensity LEDs.
(a)
(b)
Fig. 2: (a)Standardacquisition: The scene is captured
with no timings constraints. The exposure and lense
aperture make the image too dirt to be processed. The
background lighting obfuscates the marker. There are
many noise sources, mainly the ßuorescent lamp. (b)
Synchronous acquisition: The lenses aperture are re-
duced to a minimal. Exposure is very low (around 3ms).
The background noise is eliminated and the markers
shine robustly in the image. Under these conditions,
image processing and clustering is straightforward.
While one LED is enough to track position (3D), in
order to capture the other three degrees of freedom from
orientation at least three non-collinear LEDs are needed.
Nevertheless, such a scarce number of lights would fail
to provide a complete freedom of movements to the
end-user: all of those individual markers should have
to visible at all times on both cameras otherwise pose
estimation would fail due to occlusions. Increasing the
number of cameras around the working area can Þght
back this problem but at a greater Þnancial cost. On
this line of reasoning, the proposed marker is based on
20 visible-light (RGB Ð red, green, blue) LEDs. These are
distributed in a special manner, based on the shape of
an icosahedron Ñ see Fig. 3 Ñ , as it showed to provide
an interesting set of properties that aid in constructive
and algorithmic aspects, described below.
1) Detection and Stereo Matching: Tracking the
marker starts from a pair of synchronized images. The
sincrovision technique (recall the section II-A and Fig.
2) provides very clean shots of the marker. The detection
is accomplished by means of a global threshold which is
very fast and still robust due to the acquisition e?ect.
Each cluster (single LED) is then evaluated according to
743
(a) (b)
(c) (d)
Fig. 3: (a) Polyhedron 3D view: A 3D representation
of an icosahedron. (b) CAD: the CAD model of the
designed marker. (c) Construction Details: details on
the developed marker; cabling, electronics and the LEDs
are visible. (d) Lights ON: real marker with LEDs in
ON state.
its colour.
Using the principles of stereoscopy, each LED turns
into a 3D measure given that it is visible in both cameras.
The stereo correspondence problem is easily solved Ñ
in real time Ñ using the LED colour and the epipolar
constraint as depicted in Fig. 4: once a cluster of a
given colour is found in one image, the stereoscopy laws
deÞne a line (epipolar line) on the other image where
the matching cluster must lie (provided the cameras have
been properly calibrated). Searching along the epipolar
allows Þnding a cluster of the same colour thus making
possible to retrieve a 3D measure from the pair.
2) Position Estimation: One of the advantages of the
icosahedron shaped marker relates to the position of the
LEDs. Placing them of the centre of each face enables
that there are always some visible LEDs on both cameras
for whichever movement is made by the human holding
the marker (unless he steps in-between the marker and
the cameras). Also, when the LEDs are positioned on
the faces, they lie on a sphere shell that touches every
face centre. Due to this property, the 3-DoF related to
the marker translation are computed taking advantage
of this spherical positioning of the LEDs. Since the 3D
coordinates of the matched clusters are already available,
the world points are used to estimate the sphere shell in
which they lie. From the sphere equation
(x
i
?x
c
)
2
+ (y
i
?y
c
)
2
+ (z
i
?z
c
)
2
=r
2
(1)
we use algebraic Þtting [12] which solves the sphere Þt-
ting problem using least squares (the alternative method
(a)
(b)
G
R
B
GÕ
RÕ
BÕ
(c)
Fig. 4: (a) Camera 1 Image with clusters: an image
from camera 1 with a set of clusters. (b) Camera 2
Image with Epipolar Lines: the red, green and blue
lines (not straight due to barrel distortion) are epipolar
lines from some clusters in image (a). (c) Zoom on
Epipolar Lines and Cluster Correspondence: the
highlighted and labelled red, green and blue clusters Ñ
on the left, zoomed from (a) Ñ need a matching pair
in the other camera image so that 2D? 3D is possible.
Finding the correspondence is achieve by colour and the
geometric constraint given by the epipolar lines Ñ on the
right, zoomed from (b).
is the geometric Þtting, which requires an iterative min-
imization, [13]):
? =x
c
2
+y
c
2
+z
c
2
?r
2
(2)
Using Eq. (2) in (1) holds:
?
?
?
1 2x
1
2y
1
2z
1
.
.
.
.
.
.
.
.
.
.
.
.
1 2x
n
2y
n
2z
n
?
?
?
| {z }
A
?
?
?
?
??
x
c
y
c
z
c
?
?
?
?
| {z }
?
=
?
?
?
x
2
1
+y
2
1
+z
2
1
.
.
.
x
2
n
+y
2
n
+z
2
n
?
?
?
| {z }
b
(3)
Fig. 5 shows a 3D scene with the luminous markers
and the sphere shell in which they lie at.
The advantage of the fast least squares solution is that
it can be used a number of times for the same image pairs
744
Fig. 5: 3D Scene with LEDs and a Sphere: To
estimate the whole marker position each individual LEDs
contributes for a sphere Þtting algorithm.
to eliminate stereo ambiguities without compromising
the real time capability or minimization divergences: if
some points are phantom, i.e., result from a bad stereo
matching, using the algebraic Þtting recursively while
dropping some points ultimately comes up with the
correct measure. Knowing when an estimate is good or
not is done through the radius estimation: the marker
radius is a well known constructive measure and the bad
stereo matches produce a radius estimate di?erent from
the known value. When the radius approximates the real
value, the estimate is considered a good measure and the
marker position is known.
3) Orientation Estimation: After retrieving the trans-
lation vector, the missing 3-DoF with respect to angu-
lar displacement are computed from the known well-
matched 3D points.
Using again the advantages of the icosahedron, it is
possible to build a list of the LEDsÕ coordinates in the 3D
world. When the icosahedron face centres are connected,
we get the dual polyhedron, the dodecahedron. Each face
centre of the icosahedron is a vertex of the dodecahedron.
There is an orientation of the marker at which the LEDs
lie at the given positions:
s(±1,±1,±1)
s(0,±1/?,±?)
s(±1/?,±?,0)
s(±?,0,±1/?)
(4)
where s is a scale factor and ? =
 
1 +
Ã
5

2 is the so
called golden ratio (if a and b are in golden ration, and
a>b, then ? =
a+b
a
=
a
b
).
Moreover, the schlegel diagram of the dodecahedron
(a planar representation of the 3D solid where the sides
never cross) allows us to use a set of 5 di?erent colours
and distribute them around the marker in such a way
that, given 4 visible LEDs, it is possible to instanta-
neously know the marker rotation. Fig. 6 (a) shows the
coloured dodecahedron schlegel where a ÒYÓ is high-
lighted; in (b), the actual detection of the ÒYÓ in a
captured image, enabling us to know which part of the
marker is being seen. With 5 di?erent colours there are 20
unique ÒYÓs, as many as there are in the dodecahedron.
Unique ÓYÓ
With 5 colours there are
20 unique sets
(a)
(b)
Fig. 6: (a) Coloured Dodecahedron Graph: The
schlegel diagram provides an intuitive way to view the
distribution of LEDs around the dodecahedron. Using
5 colours allows a quick identiÞcation of the marker
orientation as soon as a Y is completely visible in the
images. (b) Y Detection: Detection of a complete Y in
a real image capturing the luminous marker.
For the rotation estimation algorithm, the set of known
3D positions is designated byP , whereP
i
is a 3D vector
[x
i
,y
i
,z
i
]
T
with the coordinates of dodecahedron vertex
i.P is considered the set of stand-by positions. From the
detected ÒYÓ, it is possible to know which LED is which,
for all visible LEDs in the images. After retrieving the
3D measures from the stereo analysis, these positions are
stored in the matrix Q, the set of measured positions.
The nth vector in Q stores the current world position
of the nth stand-by LED in P Ñ Q and P are said to
be paired. To Þnd the marker orientation, we Þnd the
rotation from points Q
i
to points P
i
.
The kabsch algorithm [14] provides a mean to solve
this problem. This method Þnds the rotation matrix that
optimally describes (in a root-mean-squared-error sense)
the rotation from two paired 3D point lists:
1) P and Q must have origin-centred vectors so the
Þrst step is subtracting both sets their respective
centroid.
2) Compute the covariance matrix A deÞned as: A =
P
T
Q
3) Compute the singular value decomposition of A:
A =USV
T
745
Fig. 7: (a) Two Camera Arrangement for Stere-
oscopy: The Þgure shows a cabinet that holds both
cameras. These are separated by 700 mm and slightly
rotated towards each other.
and the optimal rotation matrix W
?
comes from :
d =sign(det(VU
T
));
W
?
=V
?
?
1 0 0
0 1 0
0 0 d
?
?
U
T
(5)
(The auxiliary parameterd is used to insure a right
handed coordinated system.)
At this point we have the full (6-DoF) characterization
of the movement of the marker, with both position (3-
DoF) and orientation (3-DoF). Also note that using only
5 colours has an added advantage on the detection stage:
ror instance, using HSV colour space analysis, the 5
colours can be chosen to have hue values of 0, 60, 120, 240
and 300 as it was used in the examples presented in this
paper. This way colour misclassiÞcation is minimized.
III. TESTS & RESULTS
The system performance was studied using a single
pair of industrial cameras (Fig. 7) and a marker with 50
mm radius.
The cameras are both from Imaging Source, 1024?768
CCD sensor with USB 2.0 connectivity. The cameras
and the marker are synchronously triggered using a
microprocessor.
Table I shows how the pose estimation algorithms
perform: the fail rate is below 0.2%. For this test the
marker was attached to an industrial spray painting gun
and also to an adapted tool with industrial suction cups
for metal sheets bending operations. Fig. 8 shows both
of these industrial tools: in (a) the marker is attacked to
the spray painting gun so that it does not interfere with
the painter movements; in (b), two markers are used in a
single tool Ñ this tool is used to manipulate metal sheets,
and since large sheets can easily occlude one marker, two
are used at each end of the tool to ensure a successful
tracking during the whole demonstration. The workspace
volume for testing was 1000?1000?400 mm.
(a) (b)
Fig. 8: Marker attached to an industrial spray painting
gun and to industrial suction cups (one marker at each
side).
(a)
y
1,200 1,300 1,400 1,500 1,600 1,700 1,800 1,900 2,000 ?200
0
200
400
450
500
550
600
650
700
750
Real Spray Painting Path (dimensions in mm)
(b)
Fig. 9: (a) Baking tray sample, used in our tests (b) spray
coating tracked trajectory from the part shown in (a)
In order to Þnd the marker precision in this workspace,
the luminous device was attached to the industrial robot
end-e?ector. Then, the robot was set to move in a 100
mm grid, i.e., moving to a point then stopping for
measuring the marker pose, until the whole workspace
was covered. At each stop, the coordinates of the posi-
tionM
i
and orientation (quaternion)q
M
i
were read from
the robot controller. The corresponding measured pose
from the stereo is
 
÷
M,q
÷
M

. Table II summarizes the
marker pose precision for the 2 camera arrangement. The
maximum errors are found near the edges of the images
(mostly due to barrel distortion); increasing the number
of cameras and their resolution can e?ectively enhance
these results.
Finally, the real time capability of this system is
stated on Table III. The setup was used in an industrial
environment, in spray coating processes. Fig. 9 (a&b)
shows a sample tracked path using the proposed marker
(b), captured during the spray coating of a baking tray,
as shown in (a). The output was validated by the painters
Ñ check the motion mimic in the video attached to this
contribution.
IV. CONCLUSIONS
A new marker was proposed, which enables an human
operator to program an industrial manipulator by show-
ing the required movements. Tests, in a real industrial
scenario, show that the marker performs in real-time and
746
TABLE I: Evaluation of the position and orienta-
tion estimation: The success and failure rate for each
algorithm is presented for a total of 50000 successfully
paired image frames. The complete pose estimation per-
formance is held at the last sub-table.
Stereo Frames: 50000 pairs
Position Estimation
Success Failure
50000 [100%] 0 [0%]
Orientation Estimation
Success Failure
49919 [> 99.8%] 81 [< 0.2%]
Complete Pose Estimation
Success Failure
> 99.8% < 0.2%
average/per demonstration (1500 frames)
< 3 fails
TABLE II: Marker pose error: Marker position error
measured as the euclidean distance from the expected
positionM to the estimated
÷
M. For orientation, the ab-
solute error is the shortest path (angle) between the two
orientations which is computed from the angle between
the two corresponding quaternions, q
M
and q
ø
M
.
Marker Position Error: E =


M?
÷
M


2
, and Orientation
Error: ? = 2cos
?1
q
?1
M
q
÷
M
?
( all distances in mm, angles in degrees
?
)
E ?
Mean Error 3.8 1.7
Max Error 8.9 6.2
Std. Deviation 2.7 2.1
?
possible implementation for the shortest angle between 2
quaternions
TABLE III:Processingtimesforeverystageofthe
motion demonstration: The tests were run on a core
i7 @2.8GHz , under a Fedora 16 installation.
Routine Time (ms)
Online
Image Processing (Debayer
and Clustering)
2.2
2-Camera Cluster Match-
ing and 3D Retrieval
0.5
Pose Estimation 0.6
Total < 6
O?ine
Robot Program Genera-
tion
250
Communication and Up-
load
1000
Total < 1300
has a low fail rate (below 0.2%). Furthermore, using only
two cameras, a satisfactory precision was achieved for a
spray painting application. The operators were able to
program the robot with complete abstraction of the pro-
gramming language and the robot was ready to perform
as soon as the demonstration was over. The proposed
system greatly reduces programming downtime, and it
enables non-programmers to successfully develop robot
programs without a single line of code.
ACKNOWLEDGMENT
Marcos Ferreira acknowledges FCT Ð Fundaü c÷ ao
para a Ciö encia e a Tecnologia for his PhD grant:
SFRH/BD/60221/2009.
References
[1] H. C., W. S., N. Xi, M. Song, and Y. Chen, ÒAutomated robot
trajectory planning for spray painting of free-form surfaces
in automotive manufacturing,Ó in Robotics and Automation,
2002. Proceedings. ICRA Õ02. IEEE International Conference
on, vol. 1, 2002, pp. 450Ð455 vol.1.
[2] P. Neto, N. Mendes, R. Arajo, J. N. Pires, and A. P. Moreira,
ÒHigh-level robot programming based on cad: Dealing with
unpredictable environments,Ó Industrial Robot, vol. 39, no. 3,
pp. 294Ð303, 2012.
[3] P. Kumar, J. Verma, and S. Prasad, ÒHand data glove: A
wearable real-time device for human-computer interaction,Ó
Hand, vol. 43, 2012.
[4] J. N. Pires, ÒRobot-by-voice: Experiments on commanding an
industrial robot using the human voice,Ó Industrial Robot, an
International Journal, vol. 32, 2005.
[5] R. Dillmann, O. Rogalla, M. Ehrenmann, R. Zollner, and
M. Bordegoni, ÒLearning robot behaviour and skills based
on human demonstration and advice: the machine learning
paradigm,Ó in Robotics Research-International Symposium,
vol. 9, 2000, pp. 229Ð238.
[6] R. Zollner, O. Rogalla, and R. Dillmann, ÒIntegration of
tactile sensors in a programming by demonstration system,Ó
in Robotics and Automation, 2001. Proceedings 2001 ICRA.
IEEEInternationalConferenceon, vol. 3, 2001, pp. 2578Ð2583
vol.3.
[7] R. Zollner, O. Rogalla, R. Dillmann, and M. Zollner, ÒUn-
derstanding users intention: programming Þne manipulation
tasks by demonstration,Ó in Intelligent Robots and Systems,
2002. IEEE/RSJ International Conference on, vol. 2, 2002,
pp. 1114Ð1119 vol.2.
[8] B. Hein, M. Hensel, and H. Wo?rn, ÒIntuitive and model-
based on-line programming of industrial robots: A modular on-
line programming environment,Ó in Robotics and Automation,
2008. ICRA 2008. IEEE International Conference on, 2008,
pp. 3952Ð3957.
[9] B. Hein and H. Worn, ÒIntuitive and model-based on-line
programming of industrial robots: New input devices,Ó in
Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ
International Conference on, 2009, pp. 3064Ð3069.
[10] P. Malheiros, P. Costa, and A. P. Moreira, ÒRobust 3d mo-
tion capture and object positioning system using light emit-
ting markers synchronized with stereoscopic camera system,Ó
UPIN NPat.77/ Pat. 41, Int. Patent PCT/IB2009/007186.
[11] R. Hartley and A. Zisserman, Multiple View Geometry in
ComputerVision, 2nd ed. Cambridge University Press, ISBN:
0521540518, 2004.
[12] V. Pratt, ÒDirect least-squares Þtting of algebraic surfaces,Ó
SIGGRAPHComput.Graph., vol. 21, no. 4, pp. 145Ð152, Aug.
1987.
[13] G. Lukcs, A. D. Marshall, and R. R. Martin, ÒGeometric
least-squares Þtting of spheres, cylinders, cones and tori,Ó
Tech. Rep., 1997.
[14] W. Kabsch, ÒA solution for the best rotation to relate two sets
of vectors,Ó Acta Crystallographica Section A, vol. 32, no. 5,
pp. 922Ð923, Sept. 1976.
747
