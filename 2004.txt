Adaptive Image-based Visual Servoing of Wheeled Mobile Robots with
Fixed Camera Conﬁguration
Xinwu Liang, Hesheng Wang and Weidong Chen
Abstract—In this paper, we will study the uncalibrated
vision-based positioning problem of wheeled mobile robots by
using a ceiling-mounted camera. A new image-based visual
servoing scheme will be proposed, which can cope with the
unknown intrinsic and extrinsic parameters of the camera and
the uncertain distance parameter of the feature point from
geometric center of the mobile robot. The presented approach
is developed via extending the depth-independent interaction
matrix framework for robot manipulators to mobile robots
such that the nonlinear dependence on unknown parameters
can be removed from the image-Jacobian matrix and then,
we can linearly parameterize uncertain parameters in the
closed-loop system. In this way, an estimation scheme for the
online updating of uncertain parameters can be developed very
efﬁciently. To show that image errors can be guaranteed to be
asymptotically convergent, stability analysis will be carried out
by using Lyapunov theory. To validate the performance of the
presented approach, simulation and experimental results will
also be provided.
Index Terms—Adaptive visual servoing, depth-independent
image Jacobian, ﬁxed-camera, wheeled mobile robot.
I. INTRODUCTION
Visual servoing is a key approach that adopts vision
feedback from camera systems for the efﬁcient control of
robotic systems, which can be mainly classiﬁed into image-
based, position-based and hybrid visual servoing approaches.
Visualservoingapproacheswereoriginallyproposedtosolve
the motion control of 6-DOFs robotic manipulators, where
the motion constraints were usually not taken into account.
In contrast, a wheeled mobile robot is usually suffered
from nonholonomic constraints. It is well known that no
continuous time-invariant state feedback control scheme can
be designed to stabilize a nonholonomic system to a single
equilibrium. Hence, the motion control problem of wheeled
mobile robots can be more challenging compared with that
of robot manipulators. To achieve high performance of non-
holonomic systems, various strategies have been presented in
This work was supported in part by Specialized Research Fund for
the Doctoral Program of Higher Education of China (20100073120020,
20100073110018), in part by Shanghai Municipal Natural Science Foun-
dation (11ZR1418400), in part by China Postdoctoral Science Foundation
(2012M511095), in part by Shanghai Postdoctoral Science Foundation
(12R21414200), in part by the Special Financial Grant from the China Post-
doctoral Science Foundation (2013T60448), in part by the China Domestic
Research Project for the International Thermonuclear Experimental Reactor
(ITER) (2012GB102008), in part by International Cooperation Project of
Science and Technology Department, China (2011DFA11780), in part by
the Natural Science Foundation of China (61105095, 61203361, 61175088,
61221003), and in part by the National High Technology Research and
Development of China (2012AA041403).
The authors are with the Department of Automation, Shanghai Jiao
Tong University, and Key Laboratory of System Control and Information
Processing, Ministry of Education of China, Shanghai 200240, China.
xinwu113@163.com;wanghesheng@sjtu.edu.cn;wdchen@sjtu.edu.cn
recentyears.Itisnotedthatthesemethodswerealldeveloped
by assuming that the robot states must be exactly available.
Due to the uncertain kinematics and the slippage of the
wheels, however, such an assumption is usually not satisﬁed
in most applications.
To handle uncertainties in real environments and improve
the control performance of mobile robot systems, applying
the sensor information directly into controller designs is an
alternative important strategy. Such a strategy is known as
the sensor-based control of mobile robots. Up to now, lots of
works have been done in the research area of vision-based
control of mobile robots. Similar to robot manipulators, for
visual servoing of mobile robots, there are also two basic
conﬁgurations to perform a visual servoing task, i.e., eye-in-
hand and ﬁxed-camera conﬁgurations.
So far, various visual servoing strategies have been pre-
sented for wheeled mobile robots. In [1], a visual servoing
scheme was presented for the nonholonomic wheeled mobile
robots, where a pan and tilt unit was added onboard to
increase the camera DOFs. To guarantee that the landmark
is always kept in the camera ﬁeld of view, a novel visual
servoing strategy was developed to efﬁciently control non-
holonomic vehicles in [2]. It is noted that all these methods
are belong to position-based visual servoing approaches, and
hence,metricalinformationisrequiredtoobtainallthestates
for feedback control such that the mobile robot is guaranteed
to be asymptotically convergent to its desired pose.
To eliminate dependence on prior 3D information of the
scene, a new vision-based control scheme was proposed
in [3] to solve the positioning problem of wheeled mobile
robots. Considering its good properties, epipolar geometry
was adopted as a useful tool to effectively solve the vision-
based control issue of wheeled mobile robots. A new two-
step vision-based control method was developed in [4] with-
outanypriorinformationaboutthe3Dscenegeometry.Since
wecannotdirectlycontrolthedistanceofthecurrentposition
from the desired position, the short baseline degeneracy
problem may occur in strategies based on epipolar geometry.
To solve this degeneracy problem, a direct vision-based
controlapproachwasproposedin[5]byintroducingavirtual
target and using essential matrix. However, the epipolar
geometry will become ill-conditioned when all feature points
are coplanar.
The planar homography strategies can overcome problems
of strategies based on epipolar geometry. These strategies
can be called 2.5D visual servoing. In [6], a homography-
based vision-based control scheme was proposed to guaran-
tee asymptotical convergence of the pose of nonholonomic
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6199
mobile robots. To compensate for the constant unmeasurable
depth parameter, an adaptive vision-based controller was
developed in [7] to handle the pose control problem of
mobile robots. A homography-based visual tracking scheme
was presented in [8] to make a nonholonomic mobile robot
track a desired trajectory. As we know, in most homography-
based strategies, homography estimation and decomposition
are two necessary processes, and in the general case, two
possible solutions will be found through the decomposition
process. To avoid ambiguity issues of homography decom-
position, the input-output linearization technique was used
to develop a vision-based controller [9]. To overcome the
oversensitivity problem of the strategy in [9] to various
disturbances in the vision system, a two-level vision-based
control method was developed in [10]. To completely re-
move both the homography estimation and decomposition
processes, a 2.5D visual servoing approach was presented in
[11] for the stabilization of mobile robots.
The previously mentioned approaches are all designed
for the eye-in-hand conﬁguration. Lots of studies have also
been focused on the visual servoing problem of wheeled
mobile robots for the ﬁxed-camera conﬁguration. A robust
two-step technique was proposed in [12], where the lack
of depth information and precise visual parameters can be
compensated for efﬁciently. To handle both the unknown
camera orientation and scaling parameters, a robust stabi-
lization controller was presented in [13]. To cope with the
vision-based tracking issue of nonholonomic mobile robots,
a robust visual tracking controller was designed in [14]. A
Jacobian estimation scheme based on the dynamic quasi-
Newtonstrategywaspresentedin[15]foruncalibratedvisual
servoingofmobilerobots,wherethesimultaneousnavigation
and obstacle avoidance problem was solved through opti-
mizationofpotentialﬁeld-basedobjectivefunction.Allthese
methods are developed only by considering the mobile robot
kinematics.Consideringtheimportanceofthenonholonomic
mobile robot dynamics to its control performance, a sliding-
mode scheme was proposed in [16] to visually stabilize
mobile robots with uncertain dynamics. In [17], a dynamic
controller was developed for the position/orientation tracking
of nonholonomic mobile robots. To apply these approaches,
the depth of the imaging sensor from the planar ground is
required to be invariant during the visual servoing process,
i.e., the camera plane must be parallel to the plane of motion
of mobile robots. Hence, their applicability can be very
limited.
In the current work, an adaptive image-based visual ser-
voing method is presented for wheeled mobile robots with
ﬁxed-camera conﬁguration. The main novelty and one of
the main contributions in this paper is that, the controller
design in our scheme does not impose any conditions on
the depth information, i.e., time-varying depth information
is allowed and the camera image plane and the plane of
motion of mobile robots are not required to be parallel.
Furthermore, in the proposed scheme, exact knowledge of
the camera parameters is not necessary. Via compensating
for the uncertain time-varying depth parameter, the proposed
w
o
w
z
w
x
w
y
r
y
r
x
r
o
P
c
z
c
x
c
y
c
o
d
q
Fig. 1. Wheeled mobile robots with ceiling-mounted camera
approach can regulate the wheeled mobile robot toward its
desired position. Asymptotical convergence of the closed-
loop dynamics is proved by using Lyapunov-theory. Simu-
lation and experimental results are also provided to validate
our scheme.
II. PROBLEM FORMULATION
The mobile robot system for the case of ﬁxed-camera
conﬁgurationisillustratedinFig.1.Inthissetup,wesuppose
that a pinhole camera is attached overhead to observe the
robot motion but the motion plane and the camera image
plane are not necessary to be parallel to each other. To
execute the control task, we assume that in the period of
visual servoing, the camera can always observe the feature
point marked on board of the robot, which is tracked by the
image processing system. In addition, we suppose that the
location of the mass center of the mobile robot is the same
as that of its geometric center. In this system, we deﬁne
three related coordinate frames, i.e., the inertial frame W,
the camera frame C and the body-ﬁxed frame R, where the
relationship of these three coordinate frames can be found
in Fig. 1. As we know, the inertial frame W will be taken
as the body-ﬁxed frame R when the mobile robot is at its
initial position. The homogeneous transformation from C
to W is denoted by a 4? 4 matrix T
c
w
, which represents
the constant camera extrinsic parameters. The homogeneous
transformation between W and R is denoted by a 4? 4
matrixT
w
r
, which depends on the pose of the mobile robot.
In the case of planar motion, the mobile robot position can
be represented byx(t) = (x(t),y(t))
?
, and its orientation is
denoted as ?(t), which represents the angle between x-axis
positive direction of W and the robot forward direction.
The nonholonomic constraint for mobile robots is given
by
˙ x(t)sin?(t)? ˙ y(t)cos?(t) = 0. (1)
The nonholonomic kinematics of mobile robots is written as
˙ x(t)=?cos?(t)
˙ y(t)=?sin?(t) (2)
˙
?(t)=?
where ? and ? respectively represent the forward and rota-
tional velocities of the mobile robot.
6200
Generally speaking, the mobile robot positionx(t) can be
derived via encoders of the motors or other sensors. In com-
plex environments, however, obtaining this information can
be very difﬁcult. To solve this problem, visual information
can be considered as an effective solution. In our case, a
black circular mark on board of the robot, which is denoted
by P, is used to facilitate the detection and task speciﬁcation
of the robot through the ceiling-mounted camera system.
In this way, image coordinates of P can be used for the
localization of the mobile robot and to deﬁne a desired robot
position. In contrast, the orientation angle ?(t) of the mobile
robot can be easily obtained via angle sensors and will be
used directly in the vision-based controller. P is located at
the x-axis of frame R, and let d denote the distance of P
from the origin of frame R. Then, x
p
(t) = (x
p
(t),y
p
(t))
?
,
the position of P on the motion plane with respect to frame
W, is given by
x
p
(t) =
[
x
p
(t)
y
p
(t)
]
=
[
x(t)+dcos?(t)
y(t)+dsin?(t)
]
. (3)
Differentiating (3) and then substituting (2), we have
˙ x
p
(t) =
[
cos?(t) ?dsin?(t)
sin?(t) dcos?(t)
][
?
?
]
. (4)
The homogeneous coordinates of P with respect to W can
be given by x
w
p
(t) = (x
?
p
(t),0,1)
?
, whose homogeneous
coordinates with respect to frame C can be expressed as
x
c
p
(t) =T
c
w
x
w
p
(t) =
[
R
c
w
t
c
w
0
1?3
1
]
x
w
p
(t) (5)
where R
c
w
and t
c
w
are respectively the rotational and trans-
lational components of T
c
w
.
By applying the pinhole camera model, image coordinates
of P y
p
(t) = (u
p
(t),v
p
(t))
?
can be written as
[
y
p
(t)
1
]
=
1
z
c
p
(t)
?x
c
p
(t) (6)
where ? ? ?
3?4
only depends on intrinsic parameters of
the camera ? = [A 0
3?1
], with A ? ?
3?3
given in [18],
and z
c
p
(t) represents the depth of P expressed in the camera
frame. Substituting (5) into (6) and using x
w
p
(t) yields
[
y
p
(t)
1
]
=
1
z
c
p
(t)
A
[
r
1
r
2
t
c
w
]
[
x
p
(t)
1
]
(7)
where R
c
w
= [r
1
r
2
r
3
]. Let H = A[r
1
r
2
t
c
w
], which is
actually the homography between the camera image plane
and the motion plane and is determined by uncertain camera
parameters, (7) can be simpliﬁed as
[
y
p
(t)
1
]
=
1
z
c
p
(t)
H
[
x
p
(t)
1
]
. (8)
Based on the notation of H, (8) can be further rewritten as
y
p
(t) =
1
z
c
p
(t)
[
h
?
1
h
?
2
][
x
p
(t)
1
]
(9)
where h
?
i
represents the ith row of H. Similarly, the depth
z
c
p
(t) can be expressed as
z
c
p
(t) =h
?
3
[
x
p
(t)
1
]
. (10)
To further simplify the developments, the sub-matrix con-
sisting of the ﬁrst two columns of H is denoted by
¯
H.
Differentiating (10) and then substituting (4), we have
˙ z
c
p
(t)=
¯
h
?
3
[
cos?(t) ?dsin?(t)
sin?(t) dcos?(t)
][
?
?
]
,b
?
p
(t)
[
?
?
]
(11)
where
¯
h
?
i
represents the ith row of
¯
H andb
?
p
(t) is a 1?2
row vector which depends on the camera parameters and d.
Differentiating (9) and then substituting (11) arrives at
˙ y
p
(t)=
1
z
c
p
(t)
[
¯
h
?
1
?u
p
(t)
¯
h
?
3
¯
h
?
2
?v
p
(t)
¯
h
?
3
][
cos?(t) ?dsin?(t)
sin?(t) dcos?(t)
][
?
?
]
,
1
z
c
p
(t)
Q
p
(t)
[
?
?
]
(12)
where Q
p
(t) ? ?
2?2
is known as the depth-independent
image Jacobian, which depends on camera parameters and
d.
Note that Q
p
(t) and b
?
p
(t) have the following property:
Property 1: For any 2?1 vector ?, the termsQ
p
(t)? and
b
?
p
(t)? can be linearly parameterized as
Q
p
(t)? =N
q
(y
p
(t),?(t),?)? b
?
p
(t)? =n(?(t),?)?
(13)
where N
q
(y
p
(t),?(t),?) ? ?
2?14
and n(?(t),?) ?
?
1?14
are regressor matrices which can be
calculated without the use of unknown pa-
rameters and ? = (h
11
,h
12
,h
13
,h
21
,h
22
,h
23
,
h
31
,h
32
,h
11
d,h
12
d,h
21
d,h
22
d,h
31
d,h
32
d)
?
? ?
14?1
,
with h
ij
being the element in the ith row and jth column
of H.
The problem to be addressed can be described as follows.
Problem: Given the desired image coordinates of P,
design a vision-based controller to provide kinematics-based
control inputs v = (?,?)
?
for the mobile robot such that
P can be driven asymptotically toward its desired image
location without exact knowledge of camera parameters and
d.
III. ADAPTIVE IMAGE-BASED VISUAL SERVOING
In this section, we propose an adaptive kinematics-based
visual servoing scheme for wheeled mobile robots.
Let y
pd
be the desired constant image coordinates of P.
Subtracting the desired position from the current one, we
have
∆y
p
(t) =y
p
(t)?y
pd
. (14)
Assume that we have no exact knowledge of the camera
parameters and the parameter d and let ˆ ?(t) denote the
estimated value of ?. For convenience, we introduce
D
p
(t) =Q
p
(t)+
1
2
∆y
p
(t)b
?
p
(t). (15)
6201
By substituting ˆ ?(t), the estimated matrix
ˆ
D
p
(t) can be cal-
culated from (15). Then, we design the following controller
to drive the robot toward its desired position
v =?
ˆ
D
?
p
(t)K
p
∆y
p
(t) (16)
where K
p
??
2?2
is a symmetric positive-deﬁnite matrix.
Based on Property 1, D
p
(t)v can be expressed as
D
p
(t)v =N
(
y
p
(t),?(t),v
)
? (17)
where N
(
y
p
(t),?(t),v
)
? ?
2?14
denotes the regressor
matrix and can be calculated without the knowledge of ?.
It is not difﬁcult to derive that
N
(
·
)
=N
q
(y
p
(t),?(t),v)+
1
2
∆y
p
(t)n(?(t),v). (18)
To update ˆ ?(t), we propose the following adaptive law
˙
ˆ ?(t) =?
?1
N
?
(y
p
(t),?(t),v)K
p
∆y
p
(t) (19)
where ? is a symmetric positive-deﬁnite matrix.
Now, it is time to state the main result in the following.
Theorem 1: Assume that P is always in the ﬁeld of view
of the camera in the period of task execution, the control law
(16) and the estimation law (19) can guarantee asymptotical
convergence of image errors of P in the following way
lim
t?∞
ˆ
D
?
p
(t)K
p
∆y
p
(t) =0. (20)
That is to say, the mobile robot can be ensured to arrive at its
desired position despite the lack of exact camera parameters
and exact position of P.
Proof: Introduce the following positive function:
V(t) =
1
2
(
z
c
p
(t)∆y
?
p
(t)K
p
∆y
p
(t)+∆?
?
(t)?∆?(t)
)
(21)
where ∆?(t) = ˆ ?(t)? ?. By noting that ∆˙ y
p
(t) = ˙ y
p
(t)
and ∆˙ ?(t) =
˙
ˆ ?(t), differentiating (21) and then substituting
(11) and (12), we can obtain
˙
V(t) = ∆y
?
p
(t)K
p
D
p
(t)v+∆?
?
(t)?
˙
ˆ ?(t) (22)
where the deﬁnition given in (15) has been used. Adding and
subtracting the term ∆y
?
p
(t)K
p
ˆ
D
p
(t)v into (22) leads to
˙
V(t) = ∆y
?
p
(t)K
p
ˆ
D
p
(t)v+∆?
?
(t)?
˙
ˆ ?(t)
? ∆y
?
p
(t)K
p
N
(
y
p
(t),?(t),v
)
∆?(t), (23)
where (17) has been used. Substituting the controller (16)
and the adaptive law (19) into (23), we can easily obtain
˙
V(t) =?∆y
?
p
(t)K
p
ˆ
D
p
(t)
ˆ
D
?
p
(t)K
p
∆y
p
(t) (24)
from which
˙
V(t)≤ 0 can be easily concluded, and we know
that the mobile robot system is guaranteed to be stable by
applyingthedesignedvisualservoingcontrollerandadaptive
law. Then, from LaSalle’s Invariance Principle, we can arrive
at the conclusion of (20). 
Remark 1: Since Cartesian position x(t) is not required
and hence, in real environments, the robot can be driven to
its desired location more precisely. On the other hand, ?(t)
can be measured by angle sensors with high precision and
positioning performance will not be affected much.
Remark 2: In order to make the proposed scheme applica-
ble to the environments without angle sensors and to further
extenditsapplicability,itisagoodideatoremovethedepen-
dence on ?(t) from the scheme to make its implementation
easier, and this is one of our main objectives in the future.
Remark 3: It is noted that, (20) can only show that
(?,?) ? 0. If
ˆ
D
p
(t) can be guaranteed to be always
nonsingular, we can directly derive that ∆y
p
(t) ? 0 from
(20). As demonstrated by the simulation and experimental
results, image errors are actually convergent to zeros, which
conﬁrms the conclusion that ∆y
p
(t) ? 0. Though we
have never encountered, it is possible that, in some cases,
ˆ
D
p
(t) happens to be singular during task execution and
then, the conclusion that ∆y
p
(t) ? 0 cannot be derived.
To theoretically guarantee ∆y
p
(t) ? 0 in a strict manner,
in the future, we will investigate improved schemes to make
ˆ
D
p
(t) always nonsingular.
IV. SIMULATION RESULTS
In this section, simulation results based on a two-wheeled
mobile robot will be provided to show the control perfor-
mance of the presented approach. In this simulation setup,
we use a ceiling-mounted camera to observe the motion of
the mobile robot. Intrinsic parameters of the camera are
given by k
u
= k
v
= 1800 pixels/m, u
0
= 280 pixels,
v
0
= 250 pixels and f = 0.035 m, where (k
u
,k
v
) represent
the scaling factors, (u
0
,v
0
) is the principal point, and f is
the focal length. We can directly obtainA from the intrinsic
parameters. To accomplish the control task, a black circular
mark is attached on board of the mobile robot. The distance
parameter d between the geometric center of the mark and
that of the mobile robot is given by d = 0.2 m.
Wesupposethattheconstanthomogeneoustransformation
matrixT
w
c
between frames C and W (which is the same as
the initial body-ﬁxed frame of the mobile robot) is given by
T
w
c
= Rt(x,π??
1
)Rt(y,?
2
)Rt(z,?
3
)Tr(a
1
,a
2
,?a
3
)
where Rt(?,?) represents a rotation matrix about ?-axis by
angle ?, Tr(a
1
,a
2
,?a
3
) is a translation matrix, and ?
1
, ?
2
,
?
3
, a
1
, a
2
and a
3
are all positive constants. Then, extrinsic
parameters of the camera can be derived asT
c
w
= (T
w
c
)
?1
.
Inthissetup,theceiling-mountedcameraisnotparalleltothe
robot motion plane, which means that the depth parameter
is time-varying. Hence, previous vision-based controllers
cannotbeappliedinthiscase,butourschemecanhandlethis
problemeffectively.Inthissimulation,thevalues?
1
= π/18
rad, ?
2
= ?
3
= π/4 rad, a
1
= a
2
= 1 m and a
3
= 3 m are
used,fromwhichwecaneasilycalculatethecameraextrinsic
parameters T
c
w
. From T
c
w
, we can respectively obtain its
rotational and translational components R
c
w
= [r
1
r
2
r
3
]
andt
c
w
. Then, the planar homographyH between the image
plane and the robot motion plane can be computed by
H =A[r
1
r
2
t
c
w
].
In the simulation, we set ˆ ?
1
(0) = ˆ ?
2
(0) = ˆ ?
3
(0) = 0
rad, ˆ a
1
(0) = ˆ a
2
(0) = 3 m, ˆ a
3
(0) = 8 m,
ˆ
f(0) = 0.35
6202
?500 ?400 ?300 ?200 ?100 0 100 200 300
?200
?100
0
100
200
u (pixel)
(a)
v (pixel)
 
 
real trajectory
initial position
desired position
0 20 40 60 80 100
?300
?200
?100
0
100
200
300
400
500
600
time(sec)
(b)
Position Errors(pixel)
 
 
? u
? v
Fig. 2. Simulation results: Trajectory and position errors of the feature
point on the image plane. (a) Image trajectory. (b) Image errors.
0 20 40 60 80 100
?2
?1.5
?1
?0.5
0
0.5
1
1.5
2
time(sec)
Control Inputs (m/s & rad/s)
 
 
?
?
Fig. 3. Simulation results: Controller inputs (;!).
m, ˆ u
0
(0) = ˆ v
0
(0) = 200 pixels,
ˆ
k
u
(0) =
ˆ
k
v
(0) = 1000
pixels/m to calculate the initial estimation of the planar
homography matrix
ˆ
H(0). In addition, the initial estimation
of the distance parameter is given by
ˆ
d(0) = 0.8 m. Then,
we can easily obtain the initial value ˆ ?(0) by using both
ˆ
H(0) and
ˆ
d(0).
Assumedthatinitialstatesofthemobilerobotaregivenby
x(0) = (0,0)
?
and ?(0) = 0. The desired image position of
P is acquired when the mobile robot is at its desired position
x
pd
= (?2,7)
?
m, from which the desired image position
canbecalculatedasy
pd
= (?263.6081,?225.5603)
?
pixels
and will be used in the feedback law. The simulation results
with the control gainsK
p
= 5?10
?5
I
2
and?
?1
= 10
?5
I
14
are given in Fig. 2 and Fig. 3. It can be clearly seen that
convergenceoftheimageerrorstozerosisindeedguaranteed
even without knowing the camera parameters and d.
Remark 4: Due to the page limitation, the simulation
results only show evaluation of the proposed scheme for one
test with noiseless data. To further test its robustness and
show its superiority, much more simulations should be done
by taking simulated camera noises into consideration, and
this is one of our main objectives in the future.
V. EXPERIMENTAL RESULTS
To further validate the performance of the proposed
scheme, in this section, we will provide experimental results
based on a wheeled mobile robot. The experimental system
setupisgiveninFig.4.Inthissetup,awide-anglenetworked
camera is ﬁxed to the ceiling to track the robot motion,
where the angle between the optical-axis and the vertical
line is about 30 degrees. The robot motion can be detected
by observing a particular mark attached at the mobile robot,
which consists of ﬁve colored circular blobs. This particular
mark is designed in such a way that the robot motion can
be robustly tracked even in light-varying conditions. The
Thelaptop
computer
Thewheeled
mobilerobot
Thefeature
pointP
Theon-board
particularmark
Thewide-angle
networkedcamera
Thewireless
router
Thewired
router
Fig. 4. Experimental system setup.
geometriccenterofthebiggestcircularblobisselectedasthe
feature point P. The captured images are transmitted to the
host computer via wireless network, where the feature point
P is tracked continuously and then its image coordinates
are provided to the laptop computer, where the proposed
visual servoing algorithm is carried out. Once the controller
inputs are generated by the proposed algorithm executed
on the laptop computer, they will be sent to the embedded
system on-board the mobile robot via a USB port. After
receiving commands from the laptop computer, the low-level
controller implemented in the embedded system will output
the corresponding driving signals for the wheels to drive the
mobile robot toward its destination. The previous processes
form a feedback loop and execute repeatedly at a frequency
of about 10 Hz until the mobile robot has arrived at its goal
position with enough precision. The wide-angle camera has
beencalibratedwithouttakingintoaccountthenonlinearlens
distortion effects. But in the calculation of
ˆ
H(0), we only
usetheapproximateintrinsicparameters,anddirectlyusethe
same initial values ˆ ?
1
(0), ˆ ?
2
(0), ˆ ?
3
(0), ˆ a
1
(0), ˆ a
2
(0) and
ˆ a
3
(0) given in the simulation without calibrating the camera
extrinsic parameters. The distance of P from the geometric
centerofthemobilerobotisapproximately0.4m,while
ˆ
d(0)
in the simulation is used in the experiment. By combining
ˆ
H(0) and
ˆ
d(0), the initial estimation ˆ ?(0) can be obtained
and will be updated by the proposed adaptive law.
In the implementation, the control gains are given by
K
p
= diag(0.003,0.003) and ?
?1
= 10
?6
I
14
. Satisﬁed
experimental results are given in Fig. 5 and Fig. 6. From
Fig. 5, we can see that, even in the presence of large camera
lens distortion effects, the proposed scheme guarantees that
the mobile robot can be driven to its target position. In other
words, the image errors are asymptotically convergent to
zeros, which further conﬁrms the validity of our approach.
Remark 5: Though the mobile robot moves on a planar
ground, the feature depth is still time varying since in the
6203
700 800 900 1000 1100 1200 1300 1400
900
1000
1100
1200
1300
u (pixel)
(a)
v (pixel)
 
 
real trajectory
initial position
desired position
0 50 100 150
0
200
400
600
800
time(sec)
(b)
Position Errors (pixel)
 
 
? u
? v
Fig. 5. Experimental results: Trajectory and position errors of the feature
point on the image plane. (a) Image trajectory. (b) Image errors.
0 50 100 150
?100
?50
0
50
100
150
200
time(sec)
Control Inputs (mm/s & degree/s)
 
 
?
?
Fig. 6. Experimental results: Controller inputs (;!).
experiment, the camera is in a general position with respect
to the ground (the camera image plane is not parallel to
the ground plane). The variation level of the depth will
depend on the angle ? between the camera optical axis
and the vertical line, and ? is about 30 degrees in the
current experimental results. The variation of the depth will
become larger when a bigger ? is used. To further test
the performance of the proposed scheme, in the future, we
will carry out more experiments for different ? in a more
systematic way.
Remark 6: As we can see from the attached video of
the experiment, we obtain the desired image position of
the mobile robot by placing it at the target location. This
is known as the “teach-by-showing” approach. Actually,
the proposed scheme can work if a big black cross is
selected as the target location while the mobile robot is
never placed there before. In the current work, we just used
another different Human Machine Interface (HMI) design
methodology.
VI. CONCLUSION
In this paper, we presented an adaptive image-based
kinematics-level visual servoing approach to visually guide
the wheeled mobile robots with ceiling-mounted camera,
which can efﬁciently deal with both the unknown camera
parameters and the unknown position of feature points. The
proposed scheme can handle the positioning control problem
of mobile robots and can allow the depth parameters of
featurepointstobetimevarying.Imageerrorswereshownto
be asymptotically convergent to zeros by theoretical analysis
so that the mobile robot can be driven to its desired position.
Simulation results based on a two-wheeled mobile robot
were also provided to show the satisfying performance of
the proposed method. To further validate the performance
of the proposed scheme, experimental tests on real robotic
systems were also provided. It should be pointed out that we
only consider the adaptive vision-based positioning problem
without considering the orientation. In the future, we will
extend the proposed scheme to solve the stabilization prob-
lem of simultaneous control of position and orientation for
wheeled mobile robots.
REFERENCES
[1] Y.Masuutani,M.Mikawa,N.Maru,andF.Miyazaki,“Visualservoing
for nonholonomic mobile robots,” in Proc. IEEE/RSJ International
Conference on Intelligent Robots and Systems,Munich,Germany,Sep.
1994, pp. 1133–1140.
[2] N. R. Gans and S. A. Hutchinson, “A stable vision-based control
scheme for nonholonomic vehicles to keep a landmark in the ﬁeld
of view,” in Proc. IEEE International Conference on Robotics and
Automation, Roma, Italy, Apr. 2007, pp. 2196–2201.
[3] F. Conticelli, B. Allotta, and P. K. Khosla, “Image-based visual
servoing of nonholonomic mobile robots,” in Proc. IEEE International
Conference on Decision and Control, Phoenix, Arizona, USA, Dec.
1999, pp. 3496–3501.
[4] G. L. Mariottini, G. Oriolo, and D. Prattichizzo, “Image-based visual
servoing for nonholonomic mobile robots using epipolar geometry,”
IEEE Trans. Robot., vol. 23, no. 1, pp. 87–100, Feb. 2007.
[5] G. Lopez-Nicolas, C. Sagues, and J. J. Guerrero, “Parking with the
essential matrix without short baseline degeneracies,” in Proc. IEEE
International Conference on Robotics and Automation, Kobe, Japan,
May 2009, pp. 1098–1103.
[6] Y. Fang, D. M. Dawson, W. E. Dixon, and M. S. deQueiroz,
“Homography-based visual servoing of wheeled mobile robots,” in
Proc. IEEE International Conference on Decision and Control, Las
Vegas, Nevada, USA, Dec. 2002, pp. 2866–2871.
[7] Y. Fang, W. E. Dixon, D. M. Dawson, and P. Chawda, “Homography-
based visual servo regulation of mobile robots,” IEEE Transactions on
Systems, Man, and Cybernetics–Part B: Cybernetics, vol. 35, no. 5,
pp. 1041–1050, Oct. 2005.
[8] J. Chen, W. E. Dixon, D. M. Dawson, and M. McIntyre,
“Homography-basedvisualservotrackingcontrol ofawheeledmobile
robot,” IEEE Trans. Robot., vol. 22, no. 2, pp. 407–416, Apr. 2006.
[9] G. Lopez-Nicolas, S. Bhattacharya, J. J. Guerrero, C. Sagues, and
S. Hutchinson, “Switched homography-based visual control of differ-
ential drive vehicles with ﬁeld-of-view constraints,” in Proc. IEEE
International Conference on Robotics and Automation, Rome, Italy,
Apr. 2007, pp. 4238–4244.
[10] Y. Fang, X. Liu, and X. Zhang, “Adaptive active visual servoing
of nonholonomic mobile robots,” IEEE Transactions on Industrial
Electronics, vol. 59, no. 1, pp. 486–497, Jan. 2012.
[11] X. Zhang, Y. Fang, and X. Liu, “Motion-estimation-based visual
servoingofnonholonomicmobilerobots,” IEEE Trans. Robot.,vol.27,
no. 6, pp. 1167–1175, Dec. 2011.
[12] C. Wang, W. Niu, Q. Li, and J. Jia, “Visual servoing based regulation
of nonholonomic mobile robots with uncalibrated monocular camera,”
in Proc. IEEE International Conference on Control and Automation,
Guangzhou, China, May 2007, pp. 214–219.
[13] C. Wang, “Visual servoing feedback based robust regulation of
nonholonomic wheeled mobile robots,” in Proc. IEEE International
Conference on Robotics and Automation, Shanghai, China, May 2011,
pp. 6174–6179.
[14] Q. Li, C. Wang, and W. Niu, “Tracking of nonholonomic control
systems based on visual servoing feedback,” in Proc. Chinese Control
Conference, Zhangjiajie, Hunan, China, Jul. 2007, pp. 459–463.
[15] J. A. Piepmeier, “Uncalibrated vision-based mobile robot obstacle
avoidance,” in Proc. the 33rd Southeastern Symposium on System
Theory, Athens, OH, Mar. 2001, pp. 251–255.
[16] F. Yang and C. Wang, “Adaptive stabilization for uncertain nonholo-
nomic dynamic mobile robots based on visual servoing feedback,”
Acta Automatica Sinica, vol. 37, no. 7, pp. 857–864, Jul. 2011.
[17] W. E. Dixon, D. M. Dawson, E. Zergeroglu, and A. Behal, “Adaptive
tracking control of a wheeled mobile robot via an uncalibrated camera
system,” IEEE Transactions on Systems, Man, and Cybernetics–Part
B: Cybernetics, vol. 31, no. 3, pp. 341–352, Jun. 2001.
[18] E. Malis, F. Chaumette, and S. Boudet, “2d 1/2 visual servoing,” IEEE
Trans. Robot. Autom., vol. 15, no. 2, pp. 234–246, Apr. 1999.
6204
