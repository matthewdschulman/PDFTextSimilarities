Detection, Localization and Picking Up of Coil Springs from a Pile
Keitaro Ono
1
, Takuya Ogawa
1
, Yusuke Maeda
1
,
Shigeki Nakatani
2
, Go Nagayasu
2
, Ryo Shimizu
2
and Noritaka Ouchi
2
Abstract—Picking of parts loaded in bulk is an industrial
need. Thus bin-picking systems for various objects have ever
been studied by various ways. However, it is difﬁcult to
recognizecoilspringsrandomlyplacedinapilebyconventional
machine vision techniques because of their shape characteris-
tics.In thispaper, wepropose a methodof recognition andpose
estimation of coil springs. This method uses their highlights
made by illumination for their recognition and pose estimation
withstereovision.Weimplementedthismethodasabin-picking
system with an industrialrobot. Bin-pickingof coil springs was
successfully demonstrated on the system. Position errors were
less than 2 mm. The average success rate for a coil spring in
the part box was 94% when multiple retrials of picking were
allowed. This rate could be improved by implementation of
collision avoidance.
I. INTRODUCTION
Part feeders are widely used to supply parts loaded in bulk
to production processes. However, they have many problems
such as vibration, noise, damage and stops due to part
jamming. Moreover, part feeders specialize in speciﬁc parts
and require specialized jigs. Thus, bin-picking of various
parts loaded in bulk is industrial need.
Bin-picking systems for various objects have ever been
studied by various ways. Rahardja and Kosaka [1] proposed
a bin-picking algorithm for complex objects. They used
simple object features such as circles to recognize objects
and estimate their pose. They conducted pose-estimation
experiments with randomly cluttered alternator covers and
demonstrated that the method is sufﬁcient to be extended
for actual grasping. Kirkegaard and Moeslund [2] applied
Harmonic Shape Contexts features, which are invariant to
translation, scale, and 3D rotation, to object localization.
They matched these features from 3D data and CAD models
to estimate object pose. They succeeded in correct pose esti-
mation in presence of occlusion.Oh et al. [3] proposeda bin-
picking system using several kinds of structured light. In this
system, they estimated the pose of an object with geometric
primitive information such as planes, columns, spheres and
cones. They conﬁrmed that an industrial robot could pick up
randomly piled bolts. Domae et al. [4] demonstrated bin-
picking of 13 kinds of parts including a coil spring. In
this method, a depth map was segmented based on edges
and appropriate segments to grasp objects were determined
automatically.Shroffet al. [5] proposeda bin-pickingsystem
of detection and pose estimation using specular highlights.
They used a multi-ﬂash camera (MFC) to extract specular
features and succeeded in bin-picking of screws. Liu et al.
1
Yokohama National University
2
NHK SPRING CO., LTD.
[6] proposeda fast directional chamfermatching(FDCM) al-
gorithm,which uses line-segmentapproximationsof edges, a
three-dimensional distance transform and directional integral
images. They succeeded in bin-picking of several kinds of
objects. Nieuwenhuisen et al. [7] proposed a framework to
grasp objects composed of shape primitives like cylinders
and spheres. They generated shape compositions from CAD
models and performed sub-graph matching with the primi-
tives in scenes to detect and localize objects of interest. They
demonstrated picking up an object consisting of a cylinder
and two spheres from a transport box.
Coil springs are fundamental mechanical parts, but they
are not dealt with in most of previous studies. Coil springs
have the following shape characteristics:
• They have a succession of identical shapes.
• They have a complicated outline.
• They are see-throughdue to the openingsin their shape.
• They have few planar sections.
Thus, coil springs do not have positive features for typical
model-based image processing and therefore conventional
bin-picking methods are not applicable.
Now, the purposeof thisstudy is developinga reliable bin-
picking system for coil springs. Our proposed method uses
highlights on coil springs to detect them. Then we localize
them with stereo vision for bin-picking.
II. RECOGNITION OF COIL SPRINGS
II-A Extraction and discrimination of highlights
We focus on highlights which are made on coil springs
by illumination. Highlights are usually avoided in machine
vision but they are useful for recognition of coil springs.
We binarize the image captured by a CCD camera (Fig.
1(a)) and the highlights (Fig. 1(b)) can be extracted. We
classify them into side highlights (Fig. 1(c)) and end-face
highlights (Fig. 1(d)) based on their areas as described in
Fig. 2. The insides of end-face highlights are daubed with
white. Hereafter, we process binarized images except for
image matching explained in Section III-C.
(a) source image (b) binary image (c) side highlights (d) end-face high-
lights
Fig. 1. Highlight Extraction
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3477
Centroid of Highlight
Centroid of Group
(a) centroid of
group
Direction of Group
(b) direction
of group
Fig. 5. Parameters of Group
tlg lg
x x = Transformed
Right Image Left Image
l
x
r
x
l
y
r
y
lg
x
rg
x
| |
rg tlg
x x ?
Fig. 6. Distance between Centroids of
Left and Right Groups
c) If there is a chosen right group,
i) Check the following conditions for the left group
and the chosen right group:
Condition 2-1 Numbers of highlights are nearly
equal.
Condition 2-2 Two y coordinatesof the centroids
of the groups are nearly equal.
ii) If condition 2-1 and 2-2 are satisﬁed, the left
group corresponds to the chosen right group.
iii) If condition 2-1 or 2-2 are not satisﬁed, check
correspondence of each highlight in the highlight
groups as described in Section III-B.
d) If there is not the chosen right group, there is no
correspondence.
4) Repeat 3) for all groups recognized in the left image.
The centroid of a highlight group is the average of centroids
of highlights in the group (Fig. 5(a)). The direction of
the group is that of the approximation line of centroids of
highlights in the group (Fig. 5(b)). The distance between the
centroids of the groups is deﬁned by the following equation:
g
dis
= |x
tlg
?x
rg
|, (4)
where x
rg
is x coordinate of the centroid of the group in
the right image (Fig. 6); x
tlg
is deﬁned by the following
equation:
x
tlg
= x
lg
?fT/(L
m
P
s
), (5)
where x
lg
is x coordinate of the centroid of the group in the
left image, f is the focal length, T is the baseline length,
L
m
is the median of the possible depth of springs from the
cameras and P
s
is the pixel size of the cameras.
In regard to end-face highlights, we ﬁnd correspondence
of them if the area of the highlight and y coordinate of the
centerof the ﬁtted ellipse in the left imageare nearlyequalto
those in the right image, and the distance between the centers
of the ﬁtted ellipses is small. The corresponding points are
the centers of the ﬁtted ellipses.
III-B Checking correspondence between highlights in
groups
We can ﬁnd correspondence between highlight groups
onlyif recognizedhighlightsin theleft and rightimageshave
one-to-one correspondence like Fig. 7(a) using the algorithm
explained in Section III-A. If not one-to-one like Fig. 7(b),
we cannot ﬁnd group correspondence because the condition
2-1 or 2-2 are not satisﬁed. Thus, if the condition 2-1 or 2-2
Left Right
Highlight Centroid of Group
(a) one-to-one
Left Right
Highlight Centroid of Group
(b) not one-to-one
Fig. 7. Recognized Highlights for a Spring
Left Right
Centroid of Group
Fig. 8. Highlight Cor-
respondence
are not satisﬁed, we check correspondencefor each highlight
in highlight groups like Fig. 8. Now the corresponding point
of the highlight group is not the centroid of the group,
but that of corresponding highlights in the group only. The
algorithm of checking correspondence is as follows:
1) Choose one highlight from the left highlight group (a
left highlight).
a) Check the followingconditionsbetween the left high-
light and highlights of the chosen right group:
Condition 3-1 The distance between the centroids of
the possibly corresponded highlights
is smaller than a threshold.
Condition 3-2 Two y coordinates of the centroids of
the highlights are nearly equal.
b) The left and right highlights for which condition 3-1
and 3-2 are satisﬁed and the distance between their
centroids is the smallest correspond.
2) Repeat 1) for all highlights included in the left group.
3) Include only corresponding highlights in the groups.
III-C Checking correspondence between highlights in
groups by image matching
If the direction of the group is nearly horizontal in camera
images, we cannot ﬁnd correct correspondence as shown in
Fig. 9 using only binary highlight information, on which
the algorithms explained in Section III-A and III-B depend.
Thus, we checkcorrespondencebetween highlightsby image
matching with not binary but source images in this case. The
algorithm of checking correspondencebetween highlights by
image matching is as follows (Fig. 10):
1) Consider a neighbor region of the left group in the left
image as a template.
2) Consider a broader neighbor region of the right group
in the right image as a search area.
3) Find a matched area in the search area by image
matching with the template.
4) Choose one highlight from the left group (a left high-
light).
a) Deﬁne x coordinate of the centroid of the left high-
light as d
l
.
b) Choose one highlight from the right group (a right
highlight).
i) Deﬁne x coordinate of the centroid of the right
highlight as d
r
.
ii) If |d
l
? d
r
| is smaller than a threshold, the left
and right highlights correspond.
3479
(a) grouping result (left) (b) grouping result (right)
(c) correspondence result w/o
image matching (left)
(d) correspondence result w/o
image matching (right)
(e) correspondence result w/ im-
age matching (left)
(f) correspondence result w/ im-
age matching (right)
Fig. 14. An Example of Recognition Results
(a) grouping re-
sult (left)
(b) grouping re-
sult (right)
(c) found correspondence (left and
right)
Fig. 15. Correspondence between Non-horizontal Highlight Groups
V-B Experiments of correspondence detection
Fig. 14 is an example of results of correspondence de-
tection. Left images of Fig. 14 are identical to Fig. 4.
Corresponded springs in the left image and the right image
are shown in the same color. Fig. 15 is a magniﬁcation of
a speciﬁc coil spring in Fig. 14. Although the recognized
highlights in the left image did not have one-to-one corre-
spondencewith those in the right image, correspondencewas
found correctly. Fig. 16 is also a magniﬁcation of a speciﬁc
coil spring in Fig. 14. When the image matching algorithm
was applied, correspondence between highlights in groups
whosedirectionswerenearlyhorizontalinthecameraimages
was found correctly.
V-C Experiments of pose estimation
We placed a single spring like Fig. 17 and estimated its
position andorientation.Table III shows the averageabsolute
(a) incorrect correspondence w/o im-
age matching (left and right)
(b) found correspondence w/ image
matching (left and right)
Fig. 16. Correspondence between Horizontal Highlight Groups
(a) ?=90
?
(b) ?=0 (c) 0<?< 90
?
Fig. 17. Source Images of Left Camera
error between the true and estimated values.
When ? = 0 [deg], the error of orientation is about
15 [deg] mainly due to the sensibility of cos
?1
. Although
it seems large, it is acceptable for bin-picking.
VI. CONSTRUCTION OF A BIN-PICKING SYSTEM
VI-A Picking strategy
In our bin-picking system, outer side of the coil spring
is grasped by two ﬁngers of a gripper. The gripper is
horizontally positioned at the centroid of the highlight group
if ? =0 (Fig. 18(a)), at the higher end of the group if
0<?< 90
?
(Fig. 18(b)), and at the center of the ellipse of
the end-face highlight if ?=90
?
(Fig. 18(c)), respectively.
Basically the highest coil spring is picked up ﬁrst. How-
ever, when its picking fails, we skip it and try the second
highest one.
VI-B Experimental setup
Fig. 19 shows our experimental setup. We used a manipu-
lator, Mitsubishi Electric CorporationRV-1A with an electric
gripper, in addition to the two CCD cameras and the spot
light used in Environment A.
TABLE III
ESTIMATIONRESULTS
Average Error
?Hspr [mm] ?? [deg]
?=90
?
1.3 —
?=0 0.7 15
0<?< 90
?
0.8 2
Centroid of Group
Coil Spring
Finger
(a) ?=0
Centroid of Group End Highlight
Coil Spring
Finger
(b) 0<?< 90
?
Coil Spring
Finger
(c) ?=90
?
Fig. 18. Grasping Approach
3481
CCD Cameras
Manipulator
LED Spot Light
Electric Gripper
Reference Plane
(a) front view
CCD Cameras
LED Spot Light
Manipulator
(b) back view
Fig. 19. Experimental Environment
Fig. 20. Bin-picking
VI-C Experimental results
First, one coil spring was placed as shown in Fig. 17. The
isolated coil spring was successfully picked up in all the
placements. The accuracy of the estimation of its horizontal
position was under 1 [mm].
Next, forty-ﬁve coil springs were loaded in a part box and
picked up. Five experiments of bin-picking were conducted.
Fig. 20 shows some scenes of bin-picking. Fig. 21 shows
the ﬁrst and the last correspondence result of a bin-picking
experiment. At last, two coil springs could not be recognized
because of halation of the bottom of the bin. The accom-
panying video shows another bin-picking experiment. The
time for recognition of coil springs and pose estimation was
about 1.7[s]. Fig. 22 shows the number of picking trials and
the number of picked coil springs in each experiment. The
average success rate for each picking trial was 77% in ﬁve
experiments, and the average success rate for a coil spring
in the part box was 94% when multiple retrials of picking
were allowed. The main reason for the failure in picking was
collisions between the ﬁngers of the gripper and the part
box, and those between the ﬁngers and other coil springs.
When coil springs are in the edge of the part box or upright
coil springs are lined up like Fig. 23, bin-picking may fail
due to our naive implementation in which collisions are not
considered. In order to solve this problem, we need to check
collisions before picking and choose collision-free grasping
paths.
VII. CONCLUSIONS
In this study, we developed a method to recognize coil
springs with their highlights and estimate their position
and orientation by stereo vision. Horizontal and vertical
accuracy of estimation was under 1 [mm] and under 2 [mm],
respectively. Calculation time was about 1.7[s],whichis
shortenoughtocompleteduringpick-and-placeofa previous
(a) the ﬁrst correspondence re-
sult (left)
(b) the ﬁrst correspondence re-
sult (right)
(c) the last correspondence re-
sult (left)
(d) the last correspondence re-
sult (right)
Fig. 21. An Example of Correspondence Results
54 54
52 53
61
42 44 43
40
42
0
10
20
30
40
50
60
70
12345
Number of Picking Trials Number of Picked Coil Springs
Total Number of Springs
45
Fig. 22. Picking Results in Five Experi-
ments
Finger
Edge of Bin
(a)
Finger
(b)
Fig. 23. Possible Collisions
coil spring. We also developed a bin-picking system using
this method. The average success rate of each picking trial
for randomly placed coil springs was 77%, and that of a
coil spring when multiple retrials were allowed was 94%.
The main reason for picking failure was collisions between
the gripper and obstacles, and therefore collision avoidance
would improve the reliability of bin-picking signiﬁcantly.
REFERENCES
[1] K. Rahardja and A. Kosaka: “Vision-based Bin-Picking: Recognition
and Localization of Multiple Complex Objects using Simple Visual
Cues,” Proc. of 1996 IEEE/RSJ Int. Conf. on Intelligent Robots and
Systems, vol. 3, pp. 1448–1457, 1996.
[2] J. Kirkegaard and T. B. Moeslund: “Bin-Picking based on Harmonic
Shape Contexts and Graph-Based Matching,” Proc. of 18th Int. Conf.
on Pattern Recognition, vol. 2, pp. 581–584, 2006.
[3] J.-K. Oh et al.: “Development of Structured Light based Bin Picking
System Using Primitive Models,” Proc. of 2009 IEEE Int. Symp. on
Assembly and Manufacturing, pp. 46–51, 2009.
[4] Y. Domae et al.: Proc. of the 29th Annu. Conf. of Robotics Soc. of
Japan, RSJ2011AC3B2-1, 2011. (in Japanese)
[5] N. Shroff et al.: “Finding Needle In A Specular Haystack,” Proc. 2011
IEEE Int. Conf. on Robotics and Automation, pp. 5963–5970, 2011.
[6] M.-Y. Liu et al.: “Fast object localization and pose estimation in heavy
clutter for robotic bin picking,” Int. J. of Robotics Research, vol. 31,
no. 8, pp. 951–973, 2012.
[7] M. Nieuwenhuisen et al.: “Shape-Primitive Based Object Recognition
and Grasping,” Proc. of 7th German Conf. on Robotics, 2012.
[8] R. Mukundan and K. R. Ramakrishnan: “Moment functions in image
analysis: theory and applications,” World Scientiﬁc, 1998.
3482
