Deciding on Optimal Assistance Policies in Haptic Shared Control Tasks
Javier Corredor
1
, Jorge Sofrony
1
and Angelika Peer
2
Abstract— This paper presents a haptic assistant that en-
hances task performance and human-machine interaction via
a gain-scheduled impedance controller. The assistance strategy
proposed builds on decision-making studies and models ﬁrst
proposed in the ﬁeld of cognitive science and combines these
models with a gain-scheduled impedance control technique in
order to enhance human machine interaction in a tracking
task with environmental uncertainties. This paper explores the
Drift-Diffusion Model as decision making model and proposes
an adaptive impedance control strategy that enhances both,
task performance and human-machine interaction.
I. INTRODUCTION
Bilateral teleoperation systems have gained importance in
application ﬁelds like medical robotics, micro/macro manip-
ulation, manipulation of robots in hazardous environments
as well as remote control of unmanned vehicles, space and
mobile robots [1]. Haptic teleoperation systems come with
challenges in terms of human-machine interaction (HMI)
and closed-loop control. Although the main underlying con-
trol challenges (i.e. nonlinearities and uncertainties in the
robot manipulator model, time-delays in the communication
channel, varying environment and human impedances) have
been widely addressed in literature (see [2] for an overview),
the interest in HMI has grown in recent years [3]–[5].
Nonetheless, most of the developments in this ﬁeld have
focussed on modeling the human intention, with human-
machine cooperation receiving a considerable amount of
attention recently [6], [7].
Cooperative control schemes require humans and robots
to collaborate as peers, implying a change from a strategy
where the human is a supervisor, to another, where the robot
is allowed to make its own decisions and to collaborate
with the operator. Hence, it is necessary to consider an
interaction strategy where the operator’s and robot’s authority
are adjustable over the task.
Haptic assistants implementing shared control and chang-
ing authority have been applied with techniques ranging
from a constant level of assistance [8], to assistances with
switching [9], linear [10] and nonlinear [11] adapting mecha-
nisms. Nonetheless, these controllers tend to have scheduling
dynamics that may be not very intuitive for the operator
1
Javier Corredor and Jorge Sofrony are with the Faculty
Engineering, Universidad Nacional de Colombia, fjcorredorc,
jsofronyeg@unal.edu.co
2
Angelika Peer is with the Institute of Automatic Control Engineering,
Technische Universit¨ at M¨ unchen, angelika.peer@tum.de
This work is supported in part by the EU project MOBOT within the
7th Framework Programme of the European Union, contract number ICT-
2013-600796 as well as the Institute of Advanced Studies of the Technische
Universit¨ at M¨ unchen. The doctoral studies of the ﬁrst author and his visit
to Technische Universit¨ at M¨ unchen are supported by the National PhD
Support Program, Instituto Colombiano para el desarrollo de la Ciencia
y la Tecnolog´ ıa (COLCIENCIAS), Colombia.
and hence, they may present poor performance or unwanted
behavior in unknown environments. In order to produce a
scheduling/switching strategy that results naturally intuitive
to the operator, [12] ﬁrst proposed to incorporate a Decision-
Making Model to aid the human operator make suitable
choices in a cooperative robotic-human foraging task; in this
paper we propose to use the Drift-Diffusion (DD) model
as the main scheduling operator in a haptic human-robot
collaboration task, and presume that this will result in an
intuitive assistance strategy that may enhance collaboration.
DD models describe the decision making mechanism
in humans when confronted with a Two-Alternative Force
Choice (TAFC) task; the model is constructed based on
ﬁndings of behavioral studies performed in the ﬁeld of
cognitive science [13]. The model describes the decision
making mechanism presented by humans as a process in
which based on past decisions predictions about the success
of future decisions are made, and the decision criteria is con-
tinuously adjusted in order to maximize the reward obtained
throughout the task.
The paper is organized as follows: First we present the
general control structure and discuss the gain scheduled
admittance control strategy. Then, we present the decision
making model and propose a suitable scheduling strategy in
the context of a TAFC decision making problem. This is
followed by some experimental results. The paper closes by
giving some concluding remarks.
II. PROBLEM STATEMENT
In teleoperation it is desirable for the human to maintain
control over the task, for example, when an obstacle or any
other environment uncertainty, unknown to the assistance
(but known to the operator) is present. On the other hand,
when the assistance has full knowledge of the environment,
we wish to give more authority to the remote robot in order to
enhance task performance. In fact, if the task is static and is
developed in a structured environment, the robot can handle
the execution autonomously. We consider the problem where
the environment can be unstructured or partially structured
(i.e. composed of clear tracking objectives and unmodeled
obstacles), making it difﬁcult or even impossible to precisely
model the entire task a priori.
Both objectives (e.g. trajectory tracking and obstacle
avoidance) are in contradiction since good task performance
requires tight control, while unrestricted movements require a
less aggressive control strategy. This trade-off was explored
in our previous work [14], and we found that in case of
a continuous, multi-criteria assistance policy, unclear (or
unnatural) haptic feedback signals may be sent to the human,
hence the meaning of these signals may not be intuitive for
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2679
the operator. We also found that a switching policy between
high and low haptic assistance based on a predeﬁned and
time-invariant decision criterion, allows to achieve a good
trade-off between task performance and agreement of human
and haptic assistant.
In the literature, most assistance mechanisms move be-
tween levels of assistance that are thought to improve some
metrics involved in the task [15], [14], hence an inherent
difﬁculty is to establish suitable metrics and scheduling
strategies such that the haptic feedback results in a natural
assistance for the operator [14]. In this paper we propose to
consider the allocation of control authority as a decision mak-
ing process in a TAFC problem setup, where our choices are
whether to assist the human to improve task performance or
interaction performance. In the proposed assistance scheme,
we use ideas borrowed from behavioral studies performed in
the ﬁeld of cognitive science to adapt the level of assistance
(authority) by varying the stiffness of the haptic device.
III. CONTROL SYSTEM ARCHITECTURE
The teleoperation setup presented in this paper is standard
and considers two robot manipulators: the master (haptic)
device and the slave device. We assume that an exact
linearization control scheme is present at the master and
slave sides, thus the plant considered in Fig. 1 is assumed
to be linear. This (linear) plant is later augmented with
a PD (internal loop) controller in order to give nominal
position tracking dynamics. If we want to imprint some type
of “reference” dynamics to the system (for example if we
want to vary the system’s apparent stiffness), an additional
block must be introduced. The additional ﬁlterY implements
the desired dynamics and outputs a reference position (see
Fig. 1). This position-based admittance controller will be
used later as the main mechanism to vary the level of
assistance perceived by the human operator.
Stability of the overall teleoperation architecture may be
concluded from the following two steps: the ﬁrst step is
concerned with showing that the master side is stable and
strictly passive (SP) for any (bounded) variation of the
scheduling parameter suggested, hence we have no restric-
tions on the velocity of the scheduling parameter (only
the mild assumption of boundedness); the second step uses
the previous fact to conclude that under mild and general
assumptions, the teleoperation system is stable since we have
the feedback interconnection of two SP components (see
[16]). In general we assume that the robot manipulators at the
master and slave sides have an exact model, that interaction
with the environment is strictly passive (SP) and that the
communication channel has no time delays, hence we are
only concerned with the former step.
A. Variable gain admittance control
Note that the master (or slave) device may be modeled as
a mass-damper system [17], [18]. If we assume that the low-
level position controller has fast and well damped dynamics,
then a position-based admittance controller may be used to
imprint some reference dynamics to the master device and
PD
G
R
Gext
-
-
f
ext
X
Z
Gain Scheduling
-
Fig. 1. Position-based admittance control: varying admittance (Y ), G
R
haptic device and Gext operator dynamics.
give the human the sensation that the system is more or
less stiff or damped. By varying either the stiffness (k) or
damping (b) it is possible to change the system dynamics,
which can translate into a more or less assistive control
strategy; hereafter we will assume that only k is varied.
Since the internal control loop is considered to be SP, the
interconnection of the reference admittance and this internal
loop is SP if Y is SP. Nonetheless, since the reference
admittance is a parameter varying system, we must show
this system is in fact SP. We will now restrict ourselves to
the analysis of the dynamics of Y .
The state space model for the admittance controller has as
states the position (x
1
) and the velocity (x
2
). The relating
state space equations are:

_ x
1
(t)
_ x
2
(t)

=

0 1
 
k(t)
m
 
b
m

x
1
(t)
x
2
(t)

+

0
1
m

u(t); (1)
y(t) =

1 0


x
1
(t)
x
2
(t)

: (2)
First we must ensure that the time-varying ﬁlter, as made
explicit by the term k(t) in the A-matrix in (1), is strictly
passive for any bounded variation k(t). Assume that the
parameter variation is bounded, i.e. k(t) 2 [k
1
;k
2
], and
deﬁneA
1
,A
2
as the A-matrix of the state-space system when
k(t) =k
1
andk(t) =k
2
, respectively. The system may now
be described as an afﬁne linear parameter varying system (a-
LPV), which can be described by its convex polytopic set.
This set is spanned by its vertices computed at the upper and
lower limit of the uncertain parameter k,
_ x x x =
m
X
i=1

i
A
i
;
m
X
i=1

i
= 1; (3)
where m = 2. For a-LPV systems, stability of the au-
tonomous system (3) may be guaranteed if there exists a
Common Quadratic Lyapunov Function (CQLF) to all the
subsystems such that
A
T
i
P +PA
i
< 0 8i: (4)
If this is the case, stability of (3) is guaranteed under any
variation of the parameter [19] (i.e. the velocity of k(t) is
not restricted). If m = 2 in (3), the existence of a CQLF
is guaranteed under certain conditions as detailed in the
following theorem.
Theorem 1: [20], [21]: Given two Hurwitz matrices A
1
,
A
2
2R
nn
such that rank(A2 A1) = 1, a necessary and
sufﬁcient condition for the existence of a CQLF is that the
matrix product (A
1
A
2
) has no negative real eigenvalues.
2680
Notice that in the haptic assistance scenario, the difference
(A
1
  A
2
) is always rank = 1 by construction. The
eigenvalues for (A
1
A
2
) are,

1;2
= 
k
1
m +k
2
m b
2

p

2m
2
with (5)
 =b
4
 2b
2
k
1
m 2b
2
k
2
m+k
1
2
m
2
 2 k
1
k
2
m
2
+k
2
2
m
2
:
The existence of a CQLF for (4) is guaranteed if:
< 0 k
1
m +k
2
m b
2
< 0: (6)
Finally, we may conclude that the interconnection presented
in Fig. 1 is stable if conditions (6) are satisﬁed.
B. Teleoperation control scheme
Consider a tracking task where the desired path is known
to the haptic assistant.
DMM
PD Master
Human
Y PD
Slave
Env.
-
-
-
-
x x xm
f f fs
f f fm
GVFs
x x xs
Assistance
x x x
dm
Y
?
Y
k
f f f
?
f f f
k
Fig. 2. Scheme of the decision making based assistance, the assistance is
composed by the decision making model (DMM) and the guidance virtual
ﬁxture (GVFs). The control scheme is a variable admittance (Y ) and aPD
controller
The force measured at the tip of the master can be
decomposed asf f f
m
=f f f
k
+f f f
?
, wheref f f
k
;f f f
?
are the forces
in the parallel and perpendicular direction of the desired path,
respectively. Without any loss of generality, we will assume
that a time-varying admittance block will only be present
in the perpendicular direction, since we want to help the
operator to stay on the path and not to move along it. The
admittance in the parallel direction (Y
k
) is given by
f f f
k
=m x x x
k
+b
k
_ x x x
k
; (7)
where m is the mass and b
k
the damping coefﬁcient for
the parallel admittance. The admittance in the perpendicular
direction (Y
?
) is given by
f f f
?
=m x x x
?
+b
?
_ x x x
?
+k
?
x x x
?
; (8)
where b
?
and k
?
are the damping coefﬁcient and the
stiffness, respectively. The desired position of the master is
then given by
x x x
dm
=x x x
k
+x x x
?
: (9)
Note that the assistance must make a decision about the
stiffness (k
?
) that is suitable to improve the task or the
interaction performance, hence the decision making model
presented in the next section will adapt the level of assistance
(see Y
?
in Fig. 2).
IV. DECISION MAKING MODELS
In cognitive science it was found that humans make deci-
sions based on previous rewards obtained on past decisions,
and that they estimate future rewards in order to maximize
their total intake (see e.g. [22]). In this paper, we take
inspiration from this ﬁnding to implement a similar behavior
to our haptic assistance, where the reward may be calculated
based on some measure of the performance, including the
dynamics of the interaction between the human and the
assistance. In this paper we present a haptic assistance
scheme where the reward is calculated based on the tracking
error as task performance measure (T
P
), and the interaction
forces as interaction performance measure (I
P
).
In cognitive science, the two-alternative forced-choice
(TAFC) task was proposed as an experiment to explore
human decision making mechanisms [13]. In this task the
human is confronted with a two alternative, sequential choice
problem. The alternatives in the TAFC task are intended to
induce dynamics where the human has to exploit available or
new resources. Assume that only two choices are available
(i.e. namely T and I), and that the reward r(t) is given by
r(t) =
(
r
I
(I
P
) if z(t) =I;
r
T
(T
P
) if z(t) =T;
(10)
where z(t) 2 [T;I] is the decision made at time t. The
functionsr
T
andr
I
represent the reward calculated for each
choice, where T
P
and I
P
are the established performance
measures, and:
r
I
=k
I
I
P
+I
P0
r
T
=k
T
T
P
+T
P0
; (11)
wherek
T
andk
I
are the ratio of the change (slope),I
P0
and
T
P0
are the reward at the lowest and highest performance,
respectively. Due to their contradicting nature, both choices
may be represented as linear functions with contradicting
slopes as depicted in Fig. 3 (see [14]).
Now it is necessary to ﬁnd a method that allows the
appropriate selection of z(t) such that the total intake is
maximized. For this purpose, the Drift Diffusion Model (DD)
proposed in cognitive science literature will be used.
A. Drift Diffusion Model
The DD model is a reinforcement learning model, in
which evidence in favor of one choice is integrated until
a predetermined threshold is reached; the soft-max model is
used as the DD model strategy such that the probability of
Reward
Normalized Performances
r
T
r
I
0 0.2 0.4 0.6 0.8 1
0.25
0.50
0.75
1.00
T
P0
I
P0
Fig. 3. The matching shoulder task with linear reward schedules
2681
choosing I over T (or vice versa) is given by
p
I
(t + 1) =
1
1 + exp
 (w
I
 w
T
)
; (12)
where  is the slope of the sigmoidal function, and w
I
and
w
T
are the evidence in favor of (I) and (T ) respectively.
Remark 1: Larger  represents a high slope in the sig-
moidal function (12), this means that there is more certainty
in the decision making process, hence the commutation
between gains is faster.
In order to take performance information into account,
an update rule for the evidence in favor of each choice is
proposed as
w
z
(t + 1) =w
z
(t) +(r
z
(t) w
z
(t));
w
z
(t + 1) =r
z
(t);
(13)
where z2 [I;T ] represents the decision just made, r
z
(t) is
the reward of decision z, 2 [0; 1] is the learning rate and
the symbol denotes the “not” operator. This is inspired by
the reinforcement rule presented in [22], where the evidence
for the decision just made (w
z
) is accumulated after each
decision and the weight is updated based on the difference
between the actual reward r
z
and the expected reward w
z
.
The evidence of the option that is not chosen (z) is updated
without any learning rule, just with the previous reward
obtained. The parameter, in thez choice, scales the reward
to maintain no preference over any option when interaction
commences.
Remark 2: The accumulated evidence can be seen as
the memory of the model of previous rewards. Putting more
weight on memory means that the model takes into account
history more when making a decision. The parameter 
represents the memory of the DDM. If  is low, the model
takes into account history more, this is, memory (and vice-
versa). The decision making rule proposed is given by
z(t + 1) =
(
I if p
I
> 0:5
T otherwise:
(14)
V. DDM-BASED HAPTIC ASSISTANCE
The setup proposed consists of tracking a predeﬁned path
combined with unmodeled obstacles that the operator must
evade (see Fig. 4).
Desired Path
Master Side Slave Side
Display
Obstacles
~ x
k
~ x
?
Fig. 4. Teleoperation task
Performance measures are calculated in the perpendicular
direction (see Section III-B ) of the path over an observation
window of N samples. The tracking performance is mea-
sured via the position error between the desired path and the
current position of the master; the normalized version of the
tracking performance measure is given by
T
P;N
=
mean
N
(kek)
max(kek)
;
where T
P;N
is the normalization of the task performance
and e is the position error, where max(kek) is calculated
for each decision window.
Internal forces occur if two partners push or pull in
different directions [14] and may be used as an interaction
measure. These forces are wasted effort from a physical point
of view because they do not contribute to the movement
of the object, but still provide important information on the
haptic interaction and negotiation strategy [7]. The (haptic)
interaction performance is deﬁned as the agreement measure-
ment between the operator and the master device. A measure
of these internal forces is deﬁned as in [7]
f
i
=
8
>
<
>
:
f
?
if sign(f
?
)6=sign(f
a?
)^kf
?
kkf
a?
k
 f
a
?
if sign(f
?
)6=sign(f
a
?
)^kf
?
k>kf
a
?
k
0 if otherwise
;
(15)
where f
i
represents the internal forces, f
?
is the force
exerted by the human and f
a
is the force exerted by the
assistance. All forces are measured in the perpendicular
direction of the desired path. The interaction performance
is ﬁnally deﬁned by
I
P;N
= 1 f
i;N
; f
i;N
=
mean
N
(kf
i
k)
max(kf
i
k)
; (16)
whereI
P;N
is the interaction performance and is referred to
as agreement measure;f
i;N
is the normalized version of the
internal forces. The level of the haptic assistance depends on
the probability that a certain choice improves the total system
reward. Deﬁne the scheduling parameter function 2 [0 1]
as
(p
I
) =
1
2
+
1
2
tanh

p
I
 
'

(17)
where  and ' are user deﬁned parameters;  is the switch
point and' is the smoothing level. The function maps the
probability (p
I
) to the level of haptic assistance via a linear
homotopy given by
k = k
low
+ (1 ) k
high
(18)
wherek is the assistance level,k
high
andk
low
are the max-
imum and minimum values of the assistance, respectively.
VI. EXPERIMENTAL RESULTS
The data presented was recorded in a real teleoperation
system consisting of two admittance-type haptic feedback
devices with 4-DOF (see [23] for details). The human
operator was asked to move along a predeﬁned path known
to the haptic assistant, but had to avoid unmodeled obstacles
(see Fig. 4). The parameters for the target dynamics are:
m
?
= 5; b
?
= 200; k1
?
= 1000; k2
?
= 100:
2682
Conditions (6) are satisﬁed, and the eigenvalues of (A1A2)
are 
1;2
= [2:9 1377:1], therefore stability is guaranteed
for (bounded) parameter variations. To observe the effect
of changing some user deﬁned parameters, different values
of the parameters  and matching shoulder structure are
considered. The  parameter is selected to be constant in
all the experiments to reduce the number of parameters to
be analyzed simultaneously.
When the operator moves off the path, the position error
(T
P;N
) increases and the agreement (I
P;N
) decreases (cf.
Perf. in Fig. 5 and 6). Marked time windows indicate phases
in which the operator moved off the path.
In general, the-parameter adjusts the degree of certainty
to improve a certain performance index. In the experiment
conducted, having large certainty (high probability) to im-
prove interaction performance, means that small movements
outside the path are associated with a fast change in the
levels of assistance, (see Fig. 5 at 5-7 and 9-11 sec.). This
behavior may resemble the proposed strategy in [24], which
assist the operator when it lies in a given region and no
assistance is provided outside this region. Even though this
type of fast changing dynamics can be desirable when the
operator is required to maintain control of the robot in light
of sudden changes in the remote environment, quick changes
in the level of assistance are felt as unnatural by the operator,
hence decreasing the perceived comfort. On the other hand,
if  is gradually decreased to lower values (see Fig. 6), the
operator does not experiment sudden changes in the level
of assistance, and the interaction may be perceived as more
natural.
Different scheduling dynamics may also be achieved by
changing the reward structure to assist one objective with
more preference. For example in telesurgery, it might be
necessary to restrict the movement of the operator to a given
path. In other tasks path tracking may be less important,
for example, when it is necessary that the operator avoids
k
?
0 2 4 6 8 10 12 14 16
100
500
1000
p
I
0 2 4 6 8 10 12 14 16
0
0.5
1
w
0 2 4 6 8 10 12 14 16
-1
0
1
Perf.
t [s]
0 2 4 6 8 10 12 14 16
0
0.5
1
I
P;N
T
P;N
Fig. 5. Drift-Diffusion model-based assistance response in the perpendic-
ular direction of the desired path, for  = 0:7 and  = 100
k
?
0 5 10 15 20
100
500
1000
p
I
0 5 10 15 20
0
0.5
1
w
0 5 10 15 20
-1
0
1
Perf.
t [s]
0 5 10 15 20
0
0.5
1
I
P;N
T
P;N
Fig. 6. Drift-Diffusion model response in the perpendicular direction of
the desired path, for  = 0:7 and  = 10
obstacles. These situations may be modeled by the reward
structures (r
I
;r
T
). The parametersk
T
;k
I
;T
P0
andI
P0
(see
(11)) adjust the preference to assist the operator for the task
or interaction.
The preference to assist the operator predominantly in
tracking may be achieved by considering k
T
= 1;k
I
=
0:5;T
P0
= 1;I
P0
= 0. The maximum reward for interaction
is half of the one set in the previous experiment, giving more
importance to task performance rather than to interaction per-
formance. Likewise to weight the interaction performance,
the reward for the task performance can be reduced; for
example, considering k
T
= 0:5;k
I
= 1;T
P0
= 0:5 and
I
P0
= 0.
k
?
0 1 2 3 4 5
100
500
1000
p
I
0 1 2 3 4 5
0
0.5
1
w
0 1 2 3 4 5
-1
0
1
Perf.
t [s]
0 1 2 3 4 5
0
0.5
1
I
P;N
T
P;N
Fig. 7. Response for the DDM-based assistance with task preference (T
P
),
k
T
= 1;k
I
= 0:5;c
T
= 1 and c
I
= 0
When task performance is preferred over interaction per-
formance, the evidence w to choose a stiffness that im-
proves interaction performance is low, therefore the probabil-
2683
k
?
0 1 2 3 4 5
100
500
1000
p
I
0 1 2 3 4 5
0
0.5
1
w
0 1 2 3 4 5
-1
0
1
Perf.
t [s]
0 1 2 3 4 5
0
0.5
1
I
P;N
T
P;N
Fig. 8. Response for the DDM-based assistance with interaction preference
(I
P
), k
T
= 0:5;k
I
= 1;c
T
= 0:5 and c
I
= 0
ityp
I
is generally low (Fig. 7). In contrast, when interaction
is preferred over task performance, the evidence w to assist
in the interaction is generally high, so the probability to assist
the operator to improve the interaction is also generally high
(Fig. 8).
When the task is preferred and the user moves on the path,
the agreement and the tracking error accumulate evidence
to assist in task performance. On the other hand, when the
user moves off the path (mark in Fig. 7), the evidence to
assist in improving interaction performance increases and the
probability p
I
also increases slowly. The user feels a high
level of assistance when he/she moves on the path and the
stiffness slowly decreases when moving off the path, but
generally the stiffness level remains quite high in favor of
improving task performance.
When interaction is preferred and the user moves on
the path, the agreement and the tracking error accumulate
evidence to assist in improving interaction performance. On
the other hand, when the user moves off the path (mark
in Fig. 8), the agreement decreases and the probability p
I
reduces because the evidence w is accumulated at slow
rate. The user feels a low stiffness when he/she moves on
the path while the stiffness slightly increases when the user
moves off the path, but generally the stiffness level remains
quite low in favor of improving interaction performance.
VII. CONCLUSIONS
Our results showed that the DD model is a suitable
scheduling strategy for haptic shared control, where the
decision making process can be inﬂuenced via the parameters
of the reward functions. The DD model consists of two parts:
one that models the dynamics of the decision-making process
(soft-max rule) and another that models different dynamics
of the interaction between the assistance and the operator.
Cao et.al. [12] have proposed adapting the slope of the
reward structures online in the DD model to assist the
operator in making optimal decisions. Future work will target
adapting the level of assistance based on an online adaptation
of the slope of the matching shoulder structure depending on
new incoming information on the task.
REFERENCES
[1] G. Niemeyer, C. Preusche, and G. Hirzinger, “Telerobotics,” in
Springer Handbook of Robotics, B. Siciliano and O. Khatib, Eds.
Springer, 2008, pp. 741–757.
[2] P. F. Hokayem and M. W. Spong, “Bilateral teleoperation: An histor-
ical survey,” Automatica, vol. 42, no. 12, pp. 2035–2057, Dec. 2006.
[3] R. Aracil, M. Buss, S. Cobos, M. Ferre, S. Hirche, M. Kuschel, and
A. Peer, “The human role in telerobotics,” Advances in Telerobotics,
2007.
[4] D. Powell and M. O’Malley, “The task-dependent efﬁcacy of shared-
control haptic guidance paradigms,” IEEE Transactions on Haptics,
vol. 5, no. 3, pp. 208–219, 2012.
[5] A. Kheddar, “Human-robot haptic joint actions is an equal control-
sharing approach possible?” 2011, pp. 268–273.
[6] J.-M. Hoc, “From human-machine interaction to human-machine
cooperation,” Ergonomics, vol. 43, no. 7, pp. 833–843, 2000, PMID:
10929820.
[7] R. Groten, “Haptic human-robot collaboration: How to learn from
human dyads,” Ph.D. dissertation, Technische Universit¨ at M¨ unchen,
2011.
[8] H. Boessenkool, D. Abbink, C. J. M. Heemskerk, and F. C. T. Van der
Helm, “Haptic shared control improves tele-operated task performance
towards performance in direct control,” 2011, pp. 433–438.
[9] Y . Li, V . Patoglu, and M. K. . O’Malley, “Negative efﬁcacy of ﬁxed
gain error reducing shared control for training in virtual environments,”
ACM Transactions on Applied Perception, vol. 6, pp. 1–21, 2009.
[10] P. Evrard and A. Kheddar, “Homotopy switching model for dyad
haptic interaction in physical collaborative tasks,” 2009, pp. 45 –50.
[11] H. Yu, M. Spenko, and S. Dubowsky, “An adaptive shared control
system for an intelligent mobility aid for the elderly,” in Autonomous
Robots, vol. 15, 2003, pp. 53–66.
[12] M. Cao, A. Stewart, and N. Leonard, “Integrating human and robot
decision-making dynamics with feedback: Models and convergence
analysis,” in Decision and Control, 2008. CDC 2008. 47th IEEE
Conference on, Dec. 2008, pp. 1127 –1132.
[13] P. Montague and G. S. Berns, “Neural economics and the biological
substrates of valuation,” Neuron, vol. 36, no. 2, pp. 265–284, 2002.
[14] C. Passenberg, A. Glaser, and A. Peer, “Exploring the design space
of haptic assistants: the assistance policy module,” IEEE Transactions
on Haptics, vol. Early Access Online, 2013.
[15] S. Oguz, A. Kucukyilmaz, T. Sezgin, and C. Basdogan, “Haptic nego-
tiation and role exchange for collaboration in virtual environments,”
in Haptics Symposium, 2010 IEEE, Mar. 2010, pp. 371 –378.
[16] H. Khalil, Nonlinear Systems. New Jersey: Prentice Hall, 1996.
[17] Y . Yokokohji and T. Yoshikawa, “Bilateral control of master-slave ma-
nipulators for ideal kinesthetic coupling-formulation and experiment,”
in , 1992 IEEE International Conference on Robotics and Automation,
1992. Proceedings. IEEE, May 1992, pp. 849–858 vol.1.
[18] A. Peer and M. Buss, “Robust stability analysis of a bilateral tele-
operation system using the parameter space approach.” IEEE, Sept.
2008, pp. 2350–2356.
[19] S. Boyd, L. Ghaoui, E. Feron, and V . Balakrishnan, Linear Matrix
Inequalities in System and Control Theory. SIAM, 1994, vol. 1.
[20] H. Lin and P. J. Antsaklis, “Stability and stabilizability of switched
linear systems: A survey of recent results,” IEEE Transactions on
Automatic Control, vol. 54, no. 2, pp. 308–322, Feb. 2009.
[21] R. N. Shorten, O. Mason, F. O’Cairbre, and P. Curran, “A unifying
framework for the siso circle criterion and other quadratic stability
criteria,” Int. Journal of Control, vol. 77, no. 1, pp. 1–8, 2004.
[22] R. Bogacz, S. M. McClure, J. Li, J. D. Cohen, and P. R. Montague,
“Short-term memory traces for action bias in human reinforcement
learning,” Brain Research, vol. 1153, pp. 111–121, 2007.
[23] M. Buss and G. Schmidt, “Control problems in multimodal telepres-
ence systems.” London: Springer-Verlag, 1999, pp. 65–101.
[24] A. Bettini, P. Marayong, S. Lang, A. Okamura, and G. Hager,
“Vision-assisted control for manipulation using virtual ﬁxtures,” IEEE
Transactions on Robotics, vol. 20, no. 6, pp. 953–966, Dec. 2004.
2684
