Active Scene Recognition for Programming by Demonstration using
Next-Best-View Estimates from Hierarchical Implicit Shape Models
Pascal Meißner, Reno Reckling, Valerij Wittenbeck, Sven R. Schmidt-Rohr and R¨ udiger Dillmann
Abstract—We present an approach that combines passive
scene understanding with object search in order to recognize
scenes in indoor environments that cannot be perceived from
a single point of view. Passive scene recognition is performed
using Implicit Shape Models based on spatial relations between
objects. ISMs, a variant of the Generalized Hough Transform,
are extended to describe scenes as sets of objects with relations
lying between them. Relations are expressed as six-degree-of-
freedom (DoF) relative object poses. They are extracted from
sensor recordings of human demonstrations of actions usually
taking place in the corresponding scene. In a scene ISMs solely
represent relations of n objects towards a common reference.
Violations of other relations are not detectable. To overcome
this limitation, we extend our scene model, using hierarchical
agglomerative clustering, to a binary tree consisting of ISMs.
Active scene recognition aims to simultaneously detect present
scenesandlookforobjectsthesescenesconsistof.Forapivoting
stereo camera rig, we achieve this by performing recognition
with ISMs in an object search loop using next-best-view (NBV)
estimates. A criterion, on which we greedily choose views the
rig shall adopt next, is the conﬁdence to detect objects in
them. In each step during the search, conﬁdences on potential
positions of objects, not found yet, are calculated based on the
best available scene hypothesis. This is done by reversing the
principleofISMsandusingspatialrelationstopredictpotential
object positions starting from the objects already detected.
I. INTRODUCTION
Programming by Demonstration (PbD) is a robot learning
approach that aims to acquire knowledge about everyday
actions from demonstrations performed by humans and
recorded by sensors. For a robot to employ this knowledge
autonomously,ithastobeabletoadaptittovariouscontexts.
From this fact follows that robots need capabilities to assess
their environment. A crucial aspect are the geometrical
scenes in which a robot operates. Characteristics of indoor
environments are best described by the objects they contain,
as Quattoni et al. [1] argue for two-dimensional images. In
manipulation scenarios, not only object occurrences, but also
spatial relations, being described by relative object poses in
six DoF, are indispensable to characterize scenes.
Due to their spatial extent and present clutter, in general
we cannot assume that scenes are perceivable from a single
point of view. Therefore we present an approach that recog-
nizes scenes based on detected objects and searches further
objects, not found yet, to improve the scene estimation
gotten beforehand. The large search space of viewpoints and
sensor orientations, coming into question to detect objects in
scenes,as wellastravelcosts comingwithactions andobject
P. Meißner, R. Reckling, V. Wittenbeck, S. R. Schmidt-Rohr and R.
Dillmann are with Institute of Anthropomatics, Karlsruhe Institute of
Technology, 76131 Karlsruhe, Germany. pascal.meissner@kit.edu
detector runtimes, make uninformed search infeasible in this
setting. Models of scenes, based on spatial relations, provide
a mean to restrict that search space. Object search, guided by
minimizing costs and maximizing expectation of detecting
objects, may choose actions looking multiple search steps
ahead. We preferred a next-best-view approach looking one
step ahead following arguments in [2]. It states that in
object search, frequent measurements affect available search
policies so much, that constant replanning is necessary.
In the following, we present an active scene recognition
system that deals with the issues, discussed so far. In Sec.
III to V, we describe a passive scene recognition approach
that assumes input data is given. In Sec. VI to VII we
introduce an search approach giving up this assumption and
performing the presented passive scene recognition in a loop.
Thereforepassivescenerecognitionneedstoruninrealtime.
We learn our scene models following the principle of PbD,
but we differ from learning manipulations in that we only
record object conﬁgurations before and after actions take
place and not in-between. Therefore occlusions during object
manipulation have no impact on our learning methodology.
II. RELATED WORK
Scene understanding, a computer vision problem, is tack-
ledintwoways:Algorithmseitherrelyonresultsfromobject
detection or they infer scene information directly from image
data without referring to intermediate concepts. Methods
corresponding to the latter paradigm employ global image
descriptors as the gist descriptor [3] or use local descriptors,
calculated on speciﬁc image regions [4] that are determined
beforehand. Such approaches work well for outdoor scenes,
but deliver poor performance on indoor scenes.
Scene understanding, based on objects, is more concerned
withmodelingcorrelationsbetweenidentitiesofobjects,than
capturing details of spatial relations between them. One of
the few relation-oriented approaches is [5] that maps relative
object poses on symbolic qualitative relations, deﬁned by
hand. Symbolic relations generalize well, but lose details in
thetopologiesofobjectrelationsasneededinourapplication
area. In contrast, numerous methods deal with learning spa-
tial relations in the ﬁeld of part-based object recognition. Su-
pervised learning of object conﬁgurations is performed with
Constellation Models in [6], where positions are represented
usingnormaldistributions,ignoringobjectorientations.Asin
[7], where unsupervised learning of scenes takes place using
Bayesian non-parametric models, parametric modeling limits
expressiveness concerning spatial relations. A method that is
able to represent the complex relations encountered in our
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5585
Fig. 1. Cups rotate around a static object in ﬁxed relations towards each other. Center-left: Recorded trajectories & relative object poses towards center,
resulting from learnt non-hier. ISM (arrows). Center: ISM fully detects scene despite relation between angled red & blue cup violated. Center-Right:
Heuristic (Sec. V-A) detects relation between red & blue cup (yellow arrows). Others not due to high noise. Right: Relation violation found by hier. ISM.
scenario, are Implicit Shape Models (ISMs) [8]. Being non-
parametric, they model relative object positions in any type
of complexity. Orientation information is neglected.
Another ﬁeld in computer vision is view planning which
deals with optimizing information gain in capturing image
resp. range data by autonomously adapting sensor parame-
ters. Most approaches in this area focus on choosing view
points and only look one step ahead instead of considering
entire sequences of actions. These assumptions lead to a
research topic known as next-best-view problem. Recon-
structing object surfaces is a popular application for next-
best-viewoptimization.In[9],viewpointsforamovinglaser
scanner are chosen based on already measured data in order
to optimize information gain in terms of sensing unknown
space. On a larger scale, [10] deals with the similar task of
scanning an initially unknown environment with a mobile
robot. Both approaches derive actions without considering
prior knowledge like scene models.
A less common task in view planning is three-dimensional
object search. In [11], objects are being looked for based on
prior knowledge. Both detection likelihoods and action costs
are taken into account to choose next-best navigation and
camera orientation actions. Instead of searching conﬁgura-
tionsofobjectsasinourapplication,itislimitedtosearching
a single object. [2] deals with a similar problem, but con-
siders action sequences as well as uncertainties in decision-
making. A Partially Observable Markov Decision Process
(POMDP) is used to handle that additional complexity in
order to enable a robot to ﬁnd objects in cluttered scenarios.
Object locations are modeled as six DoF poses, but in a
parametric manner and based on absolute poses instead of
spatial relations. To be able to cope with the high runtimes of
a POMDP, important simpliﬁcations in the planning model,
e.g. regarding the size of the action space, are made.
III. SCENE, OBJECTS AND DATA ACQUISITION
We deﬁne a scene S=({o},{R}) as a set of spatial
relations {R} between pairs of objects o
j
and o
k
, including
the objects as well. Relations R consist of sets of relative
object poses {T
jk
} that are represented as transformation
matrices T 2 R
4x4
. Matrices T capture all six DoF in
space and include positions p2 R
3
. Models of scenes are
learnt from demonstrations during which trajectories J(o)=
(E(o,1),...,E(o,m)) are extracted from sensor data for each
object o in S. Trajectories J(o) are made up of object
state estimates E(o,t) for every time step t. E(o,t) contain
absolute poses T that are acquired using object localizers on
stereo images [12], otherwise they are empty. In this setting,
we regard recognizing a scene S at pose T
F
as equal to
estimating to which degree an object conﬁguration {E(o,t)},
given at time step t, corresponds to a model we learnt for S.
IV. IMPLICIT SHAPE MODEL FOR PASSIVE
SCENE RECOGNITION ON SPATIAL RELATIONS
A. Scene model learning
We extend Implicit Shape Models to represent spatial
relations between n objects o in a scene S as introduced
in [13]. As an ISM represents a scene S as a star topology
of relations {T
oF
} towards a common reference, a reference
object o
F
with pose T
F
is chosen among objects o. Learning
such an ISM for sceneS is accomplished by inserting entries
into a common table. An entry is added for each estimate
E(o,t)intrajectoriesJ(o)thatbelongtoobjectsoofsceneS.
Besides identiﬁers for scene and object, each entry contains
two relative poses: T
Fo
(t) transforms from the coordinate
frame of object o into the frame of reference object o
F
and
T
oF
(t)representsatransformationfromreferenceo
F
towards
object o. Some training data is shown in Fig. 2 on the left.
To express how much recognition of scene S depends on
having detected object o, we add beliefs b
S
(o) for every o to
our scene model. Beliefs b
S
(o) are initialized with an equal
distribution on {o} or using the number of entries for o and
S divided by the number of all entries for S .
B. Passive scene recognition
For a given set of input objects {i} that are detected at
poses {T}, scene recognition with ISMs, being learnt for
scenes {S},takesplaceasavotingprocess.Eachinputobject
i casts votes v where it expects references of scenes S based
on its pose T. A vote v contains an estimation from object
i for the absolute pose T
F
 T·T
oF
of scene reference o
F
as well as the pose T
Fo
of object i relative to reference o
F
.
Both T
oF
and T
Fo
result from an entry in the ISM table that
matches sceneS and object i. Despitea votev contains poses
in six DoF, it is casted on position p
F
of reference o
F
, i.e.
in position spaceR
3
, to reduce time and memory that voting
requires. To be able to cumulate votes, we subdivideR
3
into
a voxel grid accumulator B
S
for every scene S.
After voting is ﬁnished, an exhaustive search on accumu-
lator B
S
is performed to detect hypotheses about existing
instances of scene S. During this process, a greedy search
considering six DoF poses is executed on every grid element
5586
Fig. 2. Left: Scene in which blue cup is moved from table to frame. Center-left: Arrow-shaped relative object poses in ISM training data. Center: First
search step - Best scene hypothesis for detected cup points to table and predicts positions of other objects. Center-right: Second search step - After two
objects detected, remaining position votes projected on PTU sphere with quads colored according to counters & NBV in purple. Right: Recognition result.
B
S
(X,Y,Z) individually. Among the objects for which votes
are present in grid element B
S
(X,Y,Z), it seeks a set {i}
S
of maximum size, in which an arbitrary object i2{ i}
S
permits to predict absolute poses T
0
p
=T
F
·T
0
Fo
of all other
objects i
0
2{ i}
S
sufﬁciently well. T
F
results from a vote
of i in B
S
(X,Y,Z) for the pose of the reference. Beyond
acquiring conﬁdence about nothing but the existence of a
scene S through voting, this approach enables us identify
which objects cause a speciﬁc recognition result. Largest
input object sets {i}
S
are rated by aggregating beliefs b
S
(i)
of all objects i2{ i}
S
. In case, object set {i}
S
is rated as
sufﬁciently good, it is returned as a recognized instance I
S
of scene S with reference pose T
F
und its rating j(I
S
).
V. IMPLICIT SHAPE MODEL HIERARCHY
BY AGGLOMERATIVE CLUSTERING
A. Scene model learning
Up to this point, Implicit Shape Models describe scenes in
terms of spatial relations of n objects {o} towards a common
reference o
F
. In Fig. 1, a scene is depicted in which non-
reference objects o2{ o}\o
F
are distinctly related to each
other. As such relations are not considered by the ISM, their
violation is not detected, leading to false positive scene de-
tections. Our solution [13] to this problem is dividing object
set {o} into subsets based on spatial relations detected in
{o}. ISMs are trained on each subset separately. Hierarchical
agglomerative clustering is used to build a binary tree of
ISMs {m}. Leafs {o}
L
in tree {m} stand for real objects
for which trajectories J(o) were recorded, while inner nodes
{o}\{o}
L
represent the references {o
F
} of the ISMs in tree
{m}. ISMs m2{ m} pass their results as input to adjacent
ISMs m
0
2{ m} on the next lower level in {m} except for
ISM m
R
. Reference o
F
of m is handled as a regular object in
m
0
. As the reference of ISM m
R
is the root o
R
of tree {m},
it returns the actual scene recognition results.
The linkage criteria used in our clustering approach are
heuristics {H} that rate relations between pairs of objects
o
j
,o
k
2{ o}. To cope with the problem shown in Fig.
1 on the left, we developed the heuristic H
C
(o
j
,o
k
). It
analyzes temporal continuity in the direction of two types
of vectors: p
jk
(t) connecting positions of objects o
j
and
o
k
as well as q
jk
(t) which is their relative orientation. For
a pair of trajectories J(o
j
) and J(o
k
), both of length l,
discontinuities u are determined by repeatedly comparing
angles \ (p
jk
(t),p
jk
(t +x)) resp. \ (q
jk
(t),q
jk
(t +x)) be-
tween the same vectors, valid at different time steps, to a
given threshold. Beyond representing distributions of values
of p
jk
(t) and q
jk
(t), H
C
captures how both change over time
by counting discontinuities. For p
jk
(t), H
C
is deﬁned as:
n 1
for t 1...l do
if E(o
j
,t)6= / 0 in J(o
j
)^ E(o
k
,t)6= / 0 in J(o
k
) then
p
jk
(n) T
j
(t)
  1
·p
k
(t) and n n+1
if n>e ·l then
x 1 and u 0
for i 1...n  1 do
while i+x n^ \ (p
jk
(i),p
jk
(i+x))<d do
x x+1
if i+x n then
u u+1
i i+x, and x 1
return 1  u
/l
Agglomerative clustering uniﬁes pairs of trajectories
J(o
j
), J(o
k
) to clusters m until all heuristics H return ratings
beneath threshold e. The remaining trajectories J(o) are
subsumed to root ISM m
R
. This process functions as follows.
(H
M
,o
M
,q
M
) argmax
(H,o,q)2 ({H},{o},{o})
H(o,q)
while H
M
(o
M
,q
M
)>e do
Learn ISM m with J(o
M
), J(q
M
). o
F
taken among o
M
, q
M
{J(o)} { J(o)}\(J(o
M
)[ J(q
M
))
{J(o)} { J(o)}[ J(o
F
)
(H
M
,o
M
,q
M
) argmax
(H,o,q)2 ({H},{o},{o})
H(o,q)
{m} { m}[ m
Learn root ISM m
R
with {J(o)}
{m} { m}[ m
R
B. Passive scene recognition
In the ﬁrst step of scene recognition with a tree of ISMs
{m}, inputs objects {i}, given at poses {T}, cast votes in
every ISM m, where they match a leaf node {o}
L
. In tree
{m}, beliefs b
S
(o) of leaf nodes {o}
L
are set to 1, while a
belief b
S
(o) of 0 is assigned to every inner node {o}\{o}
L
.
5587
Fig. 3. Left two images: Objects on a common plate used in Sec. VIII-B and object trajectories resp. relative object poses while rotating plate during
training. Else: PTU observing scene used in Sec. VIII-C with objects distributed across laboratory. Learning data for both scenes in setup is on the right.
After a ﬁrst recognition run in every ISM m as described
in Sec. IV-B, returned scene references o
F
may only be
expected from those ISMs m that contain leafs {o}
L
. Inner
nodes {o}\{o}
L
do not participate in voting yet, leading
to missing or incomplete recognition results. The ratings of
those recognition results are used to update beliefs b
S
(o
F
) in
the tree and references o
F
are added to the object set {i}=
{i}[{ o
F
}. We repeat the process, described until this point,
to iteratively generate new reference objects o
F
, thereby
increasing potential input to ISMs in tree {m} and beliefs
b
S
(o
F
) for present scene references. We stop propagating
beliefs b
S
(o) from the leafs {o}
L
to the root o
R
as soon
as b
S
(o) converge in tree {m}. Then we extract recognition
results for scenes S from ISM m
R
whose reference is root
o
R
of {m}. An example is depicted in Fig. 2 on the right.
VI. QUADGRID AND VIEWS
Suppose a sensor setup able to localize objects is mounted
on a motorized Pan-Tilt Unit (PTU). As a simpliﬁcation we
assume that this setup is located at the center of a frame
rotating in two degrees of freedom r and t . In this case,
we can model all possible views V(u,v) of this setup on the
environmentaslyingonaspherearounditasdepictedinFig.
2. Ignoring its radius, points on this sphere can be described
by angles q and f , which we assume to be identical to the
angle positions r and t of the PTU. As the workspace of
the PTU is limited, only a subset [r min
,r max
]? [t min
,t max
]
of the possible views V(u,v) can be reached. We subdivide
this subset into a two-dimensional grid Q=[  m,m]? [  n,n]
similar to [11] in order to accumulate votes v on positions
p2 R
3
in it and call it quadgrid. We assign to each quad
at position (u,v)2 Q a counter c(u,v) for votes v that fall
into it after having been projected on the sphere. For each
quad at (u,v), we deﬁne a ﬂag e(u,v,o) telling whether we
searched for object o2{ o} in it. We deﬁne views V(u,v)=
[  a+u,u+a]? [  b+v,v+b] at positions (u,v)2 Q (V =
[  a,a]? [  b,b] ) as being aligned to the quadgrid.
VII. RELATION-BASED OBJECT SEARCH
FOR ACTIVE SCENE RECOGNITION
A. Object Search
Until this point, we assumed that a ﬁxed input set of
objects {i} is given for recognizing a set of scenes {S}
that are made up of objects {o}, with ISM tree {m}. An
alternative to acquire localization results actively is spiral
search [14], an uninformed search strategy for points in a
plane. Starting at a given point (u
0
,v
0
), it traverses the plane
of quadgrid Q on a spiral track until the point, being looked
for, is found. With no objects given in advance, we use this
strategy on quadgrid Q to ﬁnd at least one object i that is
mandatory for our active scene recognition approach to start
searching based on spatial relations. We assume that the ISM
tree {m} is learnt as described in previous sections but with
the sensor setup being guided by hand to capture scenes
across different viewpoints during learning. Based on the set
{o}
A
?{ o} of already detected objects, we use the passive
scene recognition approach presented in this paper to predict
possible positions of objects {o}
P
?{ o}, not detected yet.
Thissystemrunsinaloop,inwhichviewsV aresuccessively
being switched until all objects o2{ o} are found or no
promising hypotheses for potential object locations remain.
GiventheviewV
C
,towhichthesensorsetupcurrentlypoints,
the body of this loop is deﬁned as follows.
Perform object localization for o2{ o}
P
on view V
C
.
if we found an object o2{ o}
P
then
{o}
A
 { o}
A
[ o, {o}
P
 { o}
P
\o and {I
S
} /0
{I
S
} resultsofhierarchicalISMrecognitionon {S}& {o}
A
if {o}
P
= /0 then
for all S2{ S} do
Add to results: argmax
I
S
2{ I
S
}
j(I
S
) using rating j()
Quit loop.
for all (u,v)2 Q do
c(u,v) 0 and e(u,v,o) true
I
max
 argmax
{I
S
|I
S
2{ I
S
}^ S2{ S}}
j(I
S
)
calcPositionHypotheses(I
max
,T
max
,1)
for all (u,v)2 V
C
do
c(u,v) 0 and8 o2{ o}
P
:e(u,v,o) true
Estimate next-best-view V
N
with rating r(V
N
) based on V
C
while r(V
N
)<d do
if |{I
S
}|>1 then
{I
S
} { I
S
}\I
max
calcPositionHypotheses() on I
max
, recalculated on {I
S
}
Estimate V
N
with rating r(V
N
) based on V
C
else
for all S2{ S} do
Add to results: argmax
I
S
2{ I
S
}
j(I
S
)
Quit loop.
Move PTU to V
N
.
As soon as a previously not found object is detected
and scene recognition with hierarchical ISMs is run to
5588
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
Fig. 4. Search steps on quadgrid and recognition results for rotating-scene scenario. Quadgrid shown as two-dimensional grid with a resolution of ﬁve
degrees. Current view of sensors shown as a red box, while next-best-view drawn in green. Hypotheses on positions used for NBV shown in turquoise.
update our hypotheses about present scenes, we greedily
choose the scene hypothesis I
max
, currently rated best and
of any considered scene S2{ S}. Assuming that I
max
is
valid, we use a voting scheme to calculate the view V
N
,
the sensor setup shall be pointed to, next. The ﬁrst step of
this process generates votes where to expect objects, in the
method calcPositionHypotheses(), after which V
N
is chosen
by interpreting casted votes in a next-best-view estimation
step. This process is depicted in Fig. 2. To prevent (u,v)2 Q being searched multiple times for the same objects o,
we mark it as explored using e(u,v,o). While objects are
searched using hypothesis I
max
, we invalidate counters of
object position hypotheses c(u,v) as we traverse views V.
When hypothesis I
max
delivers no more promising views V
N
to move to, we switch to the next-best hypotheses, possibly
for another scene S
0
that is being looked for as well.
B. Voting on Object Position Hypotheses
for all o2{ o} of m do
if o2{ o}
P
then
Extract all ISM table entries matching m and o
for all matching table entries do
Extract T
Fo
from entry
T T
F
·T
Fo
with T
F
given for m
if T? T
F
then
w w
F
· number of matching table entries
if o2{ o}
L
then
Get (u,v)2 Q of p from T (Projection on sphere)
if e(u,v,o)= false then
c(u,v) c(u,v)+w
else
calcPositionHypotheses(o,T,w)
Quit loop.
else
if o2{ o}
L
then
Get (u,v)2 Q of p from T (Projection on sphere)
if e(u,v,o)= false then
c(u,v) c(u,v)+w
F
else
calcPositionHypotheses(o,T,w
F
)
Given reference object o
F
of scene m with its pose T
F
and a weight w
F
, hypotheses about absolute positions p of
objects o2{ o}
P
are calculated in calcPositionHypotheses()
by using entries in the ISM table learnt for m. By using
poses T
Fo
of object o relative to reference o
F
, we reverse
the principle used in ISMs for voting on poses of scene
references. Once p2 R
3
is estimated, we project in onto the
sphere and determine into which quad at position (u,v)2 Q,
it falls. Counter c(u,v) is incremented accordingly. Both is
shown in Fig. 2 in the center. As we use hierarchical ISMs,
such statements only apply to leaf nodes {o}
L
. If object o is
an internal node, we pass every hypothetical pose T having
beenestimatedforototheISMinthenext-higherlevelinthe
tree {m} that uses o as reference. Positions p are calculated
for every combination of pose votes on the path from root
node o
R
to leaf {o}
L
. To reduce computations, we introduce
weights w that permit to vote once with a weight instead of
calculating redundant votes. This is important since half of
the nodes in {m} vote on references being identical to them.
C. Next-best-view estimation
Given the current view V
C
at position (u,v)
C
2 Q and
Q ﬁlled with votes for objects o2{ o}
P
, we estimate the
next view V
N
to move to in a greedy manner using a
sliding window approach. This is done by maximizing a
reward r(u,v) that consists of a cost term r
c
(u,v) and r
d
(u,v)
which expresses the conﬁdence to ﬁnd any objects o2{ o}
P
in a view located in (u,v). Costs r
c
(u,v) are deﬁned as
normalized absolute distances on each joint of a kinematic
chain similar to [2], but deﬁned for the PTU. r
d
(u,v) is
acquired by summing all object positions counts in the
window on quadgrid Q that a view located at (u,v) deﬁnes.
r
Q
 max(1, Â (u,v)2 Q
c(u,v))
for u   m+a...m  a do
for v   n+b...n  b do
r
c
(u,v) Â (i,j)2 V
c(u+i,v+ j) and r
c
(u,v) 
rc(u,v)
/rQ
r
d
(u,v) 0.5·
|u
C
  u|
2(m  a)+1
+0.5·
|v
C
  v|
2(n  b)+1
r(u,v) a
1
·r
c
(u,v)+a
2
·(1  r
d
(u,v))
return (u,v)
N
 argmax
(u,v)2 [  m+a,m  a]? [  n+b,n  b]
r(u,v)
VIII. EXPERIMENTS AND RESULTS
A. Experimental setups
Exemplary runs of the presented active scene recognition
are analyzed for three hierarchical scene models. Data from
demonstrations, out of which these models are learnt, is
visible in Fig. 3. The size of view V during object search
5589
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
-60
-40
-20
 0
 20
 40
 60
-60
-40
-20
 0
 20
 40
 60
Tilt Angle [deg]
Pan Angle [deg]
Object Positions in PTU Space
Found Objects
Object Hypotheses
Fig. 5. Search steps on quadgrid and recognition results for scenario with two scenes. On the right: Relations in ISM recognition results shown as lines,
that turn from green to red as ratings of recognition results decrease. They meet at a reference shown as green sphere. Lines from reference object o
F
to
ISM reference are invisible as both are equal. Red arrows stand for ISM references being passed as objects to other ISMs that reside in lower tree levels.
is set to half of the viewing angle of the sensor setup
since object localization quality decreases considerably at
the borders of the processed images. On the two left images,
a conﬁguration of four objects is shown which all stand on
a plate on a table. On the right, two different conﬁgurations
reside in the same setting, having one object, a ketchup
bottle,incommon.ExperimentsarerunonaPCwitha“Core
i5 750” and 4 GB RAM. Objects are localized using [12].
B. Recognizing scenes indepedent of their emplacement
The experiments, conducted in this section, illustrate how
activescenerecognitionbasedonspatialrelationsisindepen-
dent of absolute poses of detected objects. Over the object
search process, it is shown how passive scene recognition
with hierarchical ISMs deals with incomplete input data.
During learning, all objects on the plate are ﬁrst moved in
proximityoftheirﬁnallocationsontheplate,beforetheplate
is rotated about 180° on the table. All objects are subsumed
to a common cluster, since heuristic H
C
detects no speciﬁc
relations. The trained ISM table contains 472 entries.
We performed two object search runs with the scene being
rotated about 180° in-between. In both cases the sensor setup
initially points at the pair of objects being on the left. Both
scenarios are shown in Fig. 4. The ﬁrst run, to which the
three pictures on the left of Fig. 4 belong, reaches two
additional views, before the scene is detected completely by
our hierarchical ISM as shown in the middle of Fig. 4. In
the initial view, shown in red on the left, we ﬁnd both cup
and plate. The conﬁdence of the scene hypothesis I
S
rated
best, lies at j(I
S
)=0.5. The NBV V
N
calculated based on
this hypothesis, is shown in green. Compared to other views,
containing votes for potential object positions, its distance
to the current view V
C
is high, but its conﬁdence reward
r
d
(u,v)= 0.69 compensates. NBV V
N
contains hypotheses
for both ketchup bottle and measuring both being looked
for. As visible next to the right, unexpectedly only ketchup
is localized in this view. Based on an updated best scene
recognition result I
S
with a conﬁdence of j(I
S
)=0.75, the
measuring cup is searched in a NBV V
N
with conﬁdence
rewardr
d
(u,v)=0.69,solelyprocessingvotesforthisobject.
It is found there and the scene is entirely detected. During
this run, NBVs are calculated two times with an average
runtime of 39.08 ms. Estimating hypotheses of potential
object positions is done twice and takes in average 4.22 ms.
Passive scene recognition takes 0.87 ms.
The second search run takes only one view in addition to
the initial. Votes as depicted in the second image from the
right in Fig. 4 differ from the ﬁrst run, since other objects,
i.e. measuring cup and ketchup bottle are found in the initial
view. As position hypotheses are more spread in that run, not
the conﬁdence reward r
d
(u,v)= 0.24, but low travel costs
r
c
(u,v)=0.05 are decisive for choosing NBV V
N
. In NBV
V
N
, we ﬁnd both missing objects and the scene is entirely
detected as shown on the right in Fig. 4. The runtime for
NBV estimation is 38.30 ms, position hypothesis estimation
takes 2.92 ms and ISMs 0.24 ms. Despite objects being
switched in front of the camera, active scene recognition
moves in similar directions when choosing a NBV. This
shows processing spatial relations allows to adapt knowledge
about object poses in contrast to using absolute object poses.
C. Recognizing scenes independent of their number
In this experiment with two scenes present at once, objects
are not moved during training. As continuity in H
C
only
dependsondetectionnoise,objectsbeingclosertoeachother
are subsumed to clusters. For example in one scene, which
we call B, the ketchup bottle and a plate are subsumed. The
reference of the cluster is subsumed with a cup. We denote
the other scene as A. There are 68 votes in the ISM table.
Starting with a view on a vitalis cornﬂakes box shown on
the bottom right in the right picture in Fig. 5, three additional
views are necessary to detect both scenes completely. Using
a detection result for vitalis, our best scene recognition
result I
A
has a conﬁdence of j(I
A
)= 0.33 and belongs to
scene A. Since all other objects of A did not move during
training the scene model, views on them are rated equally
by rewards r
d
(u,v). This situation is shown on the left in
Fig. 5. Therefore NBV V
N
is chosen based on travel costs
r
c
(u,v), moving the sensor setup to the ketchup bottle. Due
to the working area of the pan angle being larger, travel costs
to it are lower than to the Smacks box. Unexpected for the
active scene recognition, an additional plate is found in view
V
N
. As both ketchup bottle and plate belong to scene B, our
best hypotheses I
B
receives a conﬁdence of j(I
B
)= 0.66.
5590
!"#
!$#
!%#
&#
&%#
&$#
&"#
!"#
!$#
!%#
&#
&%#
&$#
&"#
'()*&+,-).&/0.-1
23,&+,-).&/0.-1
456.7*&289(*(8,9&(,&2':&;<37.
=8>,0&456.7*9
!"#
!$#
!%#
&#
&%#
&$#
&"#
!"#
!$#
!%#
&#
&%#
&$#
&"#
'()*&+,-).&/0.-1
23,&+,-).&/0.-1
456.7*&289(*(8,9&(,&2':&;<37.
=8>,0&456.7*9
456.7*&?@<8*A.9.9
Fig. 6. All views visited by spiral search on the left and by relation-based
object search on the right. Their order is illustrated by blue arrows.
For Scene A, a recognition result I
A
with j(I
A
)= 0.66 is
provided. The hypothesis for scene B is selected at random,
since both ratings are equal. Now searching for scene B
instead of scene A, the NBV V
N
points towards votes for
the cup as shown in the second plot from the left in Fig.
5. As this object is found and a recognition result I
B
for
scene B with rating j(I
B
)=1 is available, all hypotheses for
scene B are excluded from further processing and scene A is
considered again. In the second ﬁgure from the right, a NBV
is depicted, illustrating that the view changes from the cup to
the smacks box. Afterwards both scenes are fully detected.
In this run, NBV estimation is executed four times with an
averageruntimeof46.39ms.Positionhypothesescalculation
takes 1.08 ms and hierarchical ISM detection 0.24 ms.
D. Efﬁciency of Scene Recognition
We ran the presented active scene recognition approach
against a conjunction of passive ISM recognition and spiral
search, starting both at viewV(0,0) in the scenario described
in the previous section. We intend to compare object search
based on spatial relations, after an initialization with spiral
search, to uninformed search by itself. As shown in Fig. 6,
relation-based search took 6 additional views to ﬁnd both
scenes, while spiral search took 26. Movement costs of
relation-based search were 59° in pan direction and 86° in
tilt, while spiral search took 202° in pan and 182° in tilt.
These were the signiﬁcant runtime factors during search.
We analyzed runtimes for passive scene recognition with
hierarchical and non-hierarchical ISMs. As shown in Fig.
7, we measured runtimes for different sizes of input object
sets. Both systems run at most 10 ms with six input objects.
Increasing learning data set size does almost not affect non-
hierarchical ISMs, while hierarchical ISMs run with linear
time on the lengths of recorded object trajectories. A similar
observation holds for the number of input objects.
IX. CONCLUSIONS AND FUTURE WORKS
An approach has been presented that searches for objects
based on spatial relations in order to improve its estimation
of scenes, present in its environment. Scenes are modeled as
conﬁgurations of objects in terms of six DoF interrelation-
shipsusingImplicitShapeModels.Modelsareacquiredfrom
Fig. 7. Passive scene recognition runtimes for 2 to 6 input objects resp.
hier. (C) or non-hier. (NC) ISMs shown for different trajectory lengths.
demonstrations by humans. To overcome limitations of ISMs
in modeling relations in scenes, we generate hierarchies of
ISMs using agglomerative clustering. Object search is done
by successive estimation of next-best-views based on votes
for potential positions of not-found objects in a quadgrid.
Voting is realized as a reversion of scene detection with
ISMs. Starting with the objects already detected, object
positions are calculated using object relations, starting from
the location of the scene hypothesis rated best at this point.
Experiments show that scenes can be found independent of
their emplacement or their number using a real-time capa-
ble passive scene recognition. Object search using spatial
relations outperforms uninformed search like spiral search.
Future work includes processing votes on hypothetical object
positions separately for each object in order to model the
inﬂuence of object identities on next-best-view estimation.
REFERENCES
[1] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in Com-
puter Vision and Pattern Recognition, 2009.
[2] R. Eidenberger, T. Grundmann, M. Schneider, W. Feiten, M. Fiegert,
G. v. Wichert, and G. Lawitzky, “Scene analysis for service robots,” in
Towards Service Robots for Everyday Environments. Springer, 2012.
[3] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
representation of the spatial envelope,” Int. Journal of CV, 2001.
[4] A. Borji and L. Itti, “Scene classiﬁcation with a sparse set of salient
regions,” in Int. Conf. on Robotics and Automation, 2011.
[5] T. Southey and J. Little, “3d spatial relationships for improving object
detection,” in Int. Conf. on Robotics and Automation, 2013.
[6] A. Ranganathan and F. Dellaert, “Semantic modeling of places using
objects,” in Robotics: Science and Systems Conference, 2007.
[7] D. Joho, G. Tipaldi, N. Engelhard, C. Stachniss, and W. Burgard,
“Nonparametric bayesian models for unsupervised scene analysis and
reconstruction,” in Robotics: Science and Systems Conference, 2012.
[8] B. Leibe, A. Leonardis, and B. Schiele, “Robust object detection with
interleavedcategorizationandsegmentation,”Int.JournalofCV,2008.
[9] S. Kriegel, C. Rink, T. Bodenmuller, A. Narr, M. Suppa, and
G. Hirzinger, “Next-best-scan planning for autonomous 3d modeling,”
in Int. Conf. on Intelligent Robots and Systems, 2012.
[10] C. Potthast and G. S. Sukhatme, “A probabilistic framework for next
best view estimation in a cluttered environment,” in Int. Conf. on
Intelligent Robots and Systems, 2011.
[11] Y. Ye and J. K. Tsotsos, “Sensor planning for 3d object search,”
Computer Vision and Image Understanding, 1999.
[12] P. Azad, T. Asfour, and R. Dillmann, “Stereo-based 6d object local-
ization for grasping with humanoid robot systems,” in Int. Conf. on
Intelligent Robots and Systems, 2007.
[13] P. Meißner, R. Reckling, R. J¨ akel, S. R. Schmidt-Rohr, and R. Dill-
mann, “Recognizing scenes with hierarchical implicit shape models
based on spatial object relations for programming by demonstration,”
in Int. Conf. on Advanced Robotics, 2013.
[14] E. Langetepe, “On the optimality of spiral search,” in ACM-SIAM
Symposium on Discrete Algorithms, 2010.
5591
