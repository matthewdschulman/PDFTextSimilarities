Rapid Vision-based Shape and Motion Analysis System
for Fast-ﬂowing Cells in a Microchannel
Qingyi Gu, Tadayoshi Aoyama, Takeshi Takaki, and Idaku Ishii
Abstract— This paper proposes a novel method for simulta-
neous cell shape and motion analysis in rapid microchannel
ﬂows based on a multi-object feature extraction algorithm with
a frame-straddling high-speed vision platform. This system can
synchronize two camera inputs that share the same view with
only a very small sub-microsecond time delay. Real-time video
processing is performed using the hardware logic by extracting
the moment features of multiple cells at 2000 fps or more, which
are obtained from the two camera inputs, and their frame-
straddling time can be adjusted from 0 to 0.5 ms in 9.9 ns steps.
After setting the frame-straddling time within a certain range
to avoid large image displacements between the two camera
inputs, the frame-straddling high-speed vision platform can
perform simultaneous shape and motion analysis of cells in fast
microchannel ﬂows of 1 m/s or greater. The results of real-time
experiments conducted to analyze the deformabilities, velocities,
and shapes of fast-ﬂowing sea urchin egg cells in straight and
L-type microchannels veriﬁed the efﬁcacy of our vision-based
cell analysis system.
I. INTRODUCTION
Recent progress in micrometer-scale techniques has facil-
itated the analysis of cells using a lab-on-a-chip (LOC) [1].
LOC is a high-throughput device that integrates laboratory
functions on a single chip measuring a few square cen-
timeters or less, which contains a microﬂuidic network of
microchannels. Several non-vision-based cell analysis sys-
tems have been developed to extract the shapes and motions
of cells in LOCs [2]. However, the detectable geometrical
properties of cells have been limited by several technical
factors, such as poor spatial resolution and the difﬁculty of
obtaining quantitative measurements at such small scales.
To overcome these constraints, vision-based cell analysis
systems have been proposed for analyzing the shapes and
motions of cells in microchannel ﬂows, such as red blood
cells (RBCs) and cancer cells.
Real-time vision-based shape analysis systems for cells
in microchannels are being developed for many biological
applications [3], [4]. However, the frame rates of most of
these systems are limited by conventional video signals
(NTSC: 30 fps, PAL: 25 fps), which are not sufﬁciently
fast to allow the observation of cells ﬂowing in microchan-
nels. Ofﬂine high-speed cameras have recently been used
for vision-based cell analysis in microchannel ﬂows [5],
[6]. However, the measurement speed of cells ﬂowing in
microchannels is often limited by the frame intervals of the
ofﬂine high-speed cameras, because the apparent speed of
the microchannel ﬂow increases in microscopic views as the
magniﬁcation ratio becomes larger. To improve the speed
that can be measured without increasing the frame rate of a
Q. Gu, T. Aoyama, T. Takaki, and I. Ishii are with the Department of
System Cybernetics, Hiroshima University, 1-4-1, Kagamiyama, Higashi-
Hiroshima, Hiroshima 739-8527, Japan Email: gu@robotics.hiroshima-
u.ac.jp
vision system, a frame-straddling technique that used a pair
of images with a microsecond time delay has been applied
to an online particle image velocimetry (PIV) system [7]. If
this frame-straddling function could be applied to a real-time
multi-object feature extraction system, it would signiﬁcantly
improve the upper limit of the measurement speed for online
cell shape and motion analysis in microchannels.
Recently, ﬁeld-programmable gate array (FPGA)-based
high-speed vision platforms have been developed for hard-
ware implementations of various types of image processing
algorithms. Gu et al. developed a multi-object feature extrac-
tion system for rapid shape-based blob processing, which
could extract multiple objects and their shape features in
real time using 512512 images at 2000 fps [8]. The use
of a real-time function to measure the shapes and speeds
of multiple objects could accelerate microchannel-based cell
analysis of the shape, motion, stiffness, and other image-
based parameters, thereby facilitating the automated mass
production of cells.
In the present study, we propose a real-time cell shape
and motion analysis system for inspecting fast-ﬂowing cells
in microchannels, which is based on a frame-straddling high-
speed vision platform. The system uses two camera inputs
and a frame-straddling function. It has a time delay that
ranges from 0 to 0.5 ms in 9.9 ns steps, which can avoid large
image displacements between the two camera inputs when
observing fast-ﬂowing cells. Thus, our system facilitates cell
shape inspection in microchannel ﬂows at 1 m/s or greater
based on real-time video processing with multi-object feature
extraction using 512256 images at 4000 fps, or 512512
images at 2000 fps, where the images are obtained from two
camera inputs and there is only a very small time delay.
II. CELL SHAPE AND MOTION ANALYSIS SYSTEM
BASED ON FRAME-STRADDLING HIGH-SPEED VISION
A. Concept
We introduce the concept of frame-straddling multi-object
feature extraction for measuring the shapes and speeds of
cells in microchannel ﬂows simultaneously. Figure 1 shows
an outline of the system used for microchannel-based cell
shape and motion analysis. Given a frame-straddling high-
speed vision system that can synchronize two camera inputs
of the same view ﬁeld with a time delay, which can obtain
the positions, sizes, and other features of multiple objects in
images at a high frame rate, the time delay can be adjusted
in a suitable range for speciﬁc processes to calculate the
speeds of cells ﬂowing in microchannels, thereby avoiding
large image displacements between the two camera inputs.
Compared with a single-camera system, our frame-straddling
concept can expand the measurable range of speeds for fast-
ﬂowing cells in microchannels to 1 m/s or more, without
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5848
Fig. 1. The frame-straddling multi-object feature extraction concept.
(a) System overview
(b) Dual-camera housing
(c) IDP Express board
Fig. 2. System overview.
increasing the frame rate and the computational cost of the
vision system. The shape deformation of fast-ﬂowing cells in
microchannels can also be quantiﬁed in real time to facilitate
the long-term automated sorting of cells in LOCs.
B. System Conﬁguration
We propose a fast vision-based shape and motion analysis
system for fast-ﬂowing cells in microchannels based on a
hardware implementation of a cell-based multi-object feature
extraction algorithm [8] using a dual-camera high-speed
vision platform, as shown in Fig. 2(a). Our system comprises
a dual-camera high-speed vision platform (IDP Express) [9]
with a frame-straddling function for two camera inputs, a
microscope (BX51RT; Olympus Co., Tokyo, Japan), a metal-
halide light source (PCS-MH375RC; Optical Garden Co.,
Tokyo, Japan), microﬂuidic chips with microchannels that
measure hundreds of micrometers in width, and an electric
syringe pump (KDS200; KD Scientiﬁc Inc., Holliston, MA,
USA). The camera housing shown in Fig. 2(b), where the
two camera heads of the IDP Express system are arranged
so they can capture a common view via a prism, is mounted
on top of the camera port of the BX51RT microscope. In
the present study, the microchannels on the microﬂuidic
chips were observed using a 10 objective lens, where the
measured area was 512512m and one pixel corresponded
to 1.0 m in the microscopic view of 512512 pixels on a
10 m-pixel-pitch image sensor.
The IDP Express board has two camera inputs, an FPGA
for camera I/O and PCI-e bus controls, a user-speciﬁed
FPGA (Xilinx XC3S5000) for the hardware implementation
of the algorithms, and several peripheral I/O circuits. Fig-
ure 2(c) shows an overview of the board. The details of
the image processing path and transfer in the IDP Express
board were described in a previous study [9]. In the present
study, the input images from two camera heads are processed
simultaneously at 2000 fps or more by implementing the
cell-based multi-object feature extraction algorithm [8] in
the hardware logic of the user-speciﬁed FPGA. In addition,
the IDP Express board is customized for dual-camera frame-
straddling by improving the hardware logic of the time
delay control between the two camera heads. The time delay
between the two camera heads is controlled simultaneously
via the PC from 0 to 0.5 ms in 9.9-ns steps. The input images
of the two camera heads and the processed results on the
IDP Express board are memory-mapped via eight-lane PCI-
e buses at 2000 fps or more onto the allocated memory in
the PC with an ASUSTek P6T7 mainboard, Intel Core i7
975 3.33 GHz CPU, 6 GB memory, 4PCI-e 2.016 buses
I/F, and the Windows XP Professional 32-bit OS. Arbitrary
processing can be programmed to allow real-time execution
on the PC.
C. Algorithms
To facilitate the real-time shape and motion analysis ofL
cells ﬂowing in microchannels, we implemented the follow-
ing algorithms for a pair of frame-straddledMN images:
(1) ofﬂine pre-processing for dual-camera calibration; (2)
feature extraction from multiple cells in microchannels; and
(3) motion estimation based on cell tracking using a pair of
frame-straddled images.
(1) Pre-processing for dual-camera calibration
a) Calculation of the brightness ratio
To adjust the brightness difference between the two camera
inputs, the brightness ratioR
12
is determined by calculating
the total image brightness as follows:
R
12
=
X
x;y
2
I
R
(x;y)
X
x;y
1
I
R
(x;y)
; (1)
where
1
I
R
(x;y) and
2
I
R
(x;y) are the reference images at
pixel (x;y) in camera heads 1 and 2, respectively, when a
non-saturated bright scene is captured with no time delay
between the two camera inputs.
b) Calculation of the afﬁne parameters
To adjust the geometric differences between the two
camera inputs, the image coordinates of camera head 1 are
calibrated based on those of camera head 2 using an afﬁne
transformation as follows:

x
0
y
0

=

a
11
a
12
a
21
a
22

x
y

+

b
1
b
2

: (2)
where (x
0
;y
0
) are the image coordinates of camera head 1
after calibration, while a
11
;a
12
;a
21
;a
22
;b
1
, and b
2
are the
afﬁne parameters, which indicate the geometric relationships
between the image coordinates of camera heads 1 and 2.
These afﬁne parameters are obtained using images captured
when a 30-m-pitch micro-chess pattern is observed with
no time delay between the two camera inputs, in the same
manner as the widely used camera calibration method [10].
c) Assignment of microchannel regions
5849
To avoid confusing the cell regions with the microchannel
walls during image binarization, the following one-bit mask
images
i
C(x;y) (i = 1; 2) are preassigned as the inner
regions of the microchannels that need to be observed as
the two camera inputs:
i
C(x;y) =

1 ((x;y) belongs to inner region)
0 (otherwise)
; (3)
where the assignments are made when the microchannels
are observed in the same conditions used in the online
experiments, except there are no cells in the microchannels.
(2) Feature extraction from multiple cells
a) Binarization of input images
The input image of the camera headi (= 1; 2) captured at
timet,
i
I(x;y;t), is converted into a binary image
i
B(x;y;t)
using its corresponding mask image
i
C(x;y), as follows:
i
B(x;y;t) =

i
C(x;y) (
i
I(x;y;t)
i

B
)
0 (otherwise)
; (4)
where the threshold
i

B
is determined using a speciﬁc value

B
and the brightness ratio R
12
is as follows:
1

B
=
B
;
2

B
=R
12

B
: (5)
b) Calculation of subregion-based moment features
The zeroth-, ﬁrst-, and second-order moment features are
calculated for M
0
N
0
subregions  
ab
(a = 0;;M
0
 1;b =
0;;N
0
 1) with mn pixels in
i
B(x;y;t) as follows:
i
M
pq
( 
ab
;t) =
a(m+1) 1
X
x=am
b(n+1) 1
X
y=bn
x
p
y
q

i
B(x;y;t); (6)
where M = mM
0
and N = nN
0
; and p and q are non-
negative integers that satisfy p +q 2.  
ab
is expressed as
follows:
 
ab
=f(x;y)j (am+s;bn+t); 0s<m; 0t<ng: (7)
c) Labeling of connected components with feature calcula-
tions
To obtain the moment features of multiple objects in
an image, the connected components labeling process is
accelerated using the cell-based labeling algorithm with the
moment features for M
0
N
0
subregions,
i
M
pq
( 
ab
;t) [11].
The computational complexity and memory consumption
of the labeling process can be reduced by the order of
O(M
0
N
0
), i.e., 1=mn of the pixel-level complexity of the or-
derO(MN). For labeled objects
i
O
l
(t) (l = 0: ;L 1) in
i
B(x;y;t), the label-domain moment features M
pq
(
i
O
l
(t))
(p+q 2) are accumulated sequentially, at the same time as
the scanning of a ﬂag map
i
P
ab
(t) withM
0
N
0
subregions.
i
P
ab
(t) is deﬁned by checking
i
M
00
( 
ab
;t) with a threshold

P
, as follows:
i
P
ab
(t) =

1 (
i
M
00
( 
ab
;t)
P
)
0 (otherwise)
; (8)
where M
pq
(
i
O
l
(t)) are deﬁned as follows:
M
pq
(
i
O
l
(t)) =
X
(x;y)2
i
O
l
(t)
x
p
y
q

i
B(x;y;t): (9)
The detailed processes used to calculate the label-domain
moment features in an image are given in [8] and [11].
Using the zeroth-, ﬁrst-, and second-order label-domain
moment features, the sizes S(
i
O
l
(t)), centroid positions
X(
i
O
l
(t)), and eccentricities E(
i
O
l
(t)) are calculated for
L labeled objects
i
O
l
(t) in
i
B(x;y;t), as follows:
S(
i
O
l
(t)) = M
00
(
i
O
l
(t)); (10)
X(
i
O
l
(t)) = (X(
i
O
l
(t));Y (
i
O
l
(t)))
=

M
10
(
i
O
l
(t))
M
00
(
i
O
l
(t))
;
M
01
(
i
O
l
(t))
M
00
(
i
O
l
(t))

; (11)
E(
i
O
l
(t)) =

+
(
i
O
l
(t)) 
 
(
i
O
l
(t))

+
(
i
O
l
(t)) +
 
(
i
O
l
(t))
; (12)
where e
20
(
i
O
l
(t)), e
11
(
i
O
l
(t)), and e
02
(
i
O
l
(t)) are the
second-order moment features about the centroid position
for
i
O
l
(t). The eccentricity E(
i
O
l
(t)) is expressed using
the major axis 
+
(
i
O
l
(t)) and the minor axis 
 
(
i
O
l
(t)),
where
i
O
l
(t) is approximated as an ellipse, which indicates
the degree of deviation from a true circle. 
+
(
i
O
l
(t)),

 
(
i
O
l
(t)), e
20
(
i
O
l
(t)), e
11
(
i
O
l
(t)), and e
02
(
i
O
l
(t)) are
calculated as follows:
e
20
(
i
O
l
(t)) =
M
20
(
i
O
l
(t))
M
00
(
i
O
l
(t))
  (X(
i
O
l
(t)))
2
; (13)
e
11
(
i
O
l
(t)) =
M
11
(
i
O
l
(t))
M
00
(
i
O
l
(t))
 X(
i
O
l
(t))Y (
i
O
l
(t)); (14)
e
02
(
i
O
l
(t)) =
M
02
(
i
O
l
(t))
M
00
(
i
O
l
(t))
  (Y (
i
O
l
(t)))
2
; (15)

+
(
i
O
l
(t)) = e
20
+e
02
+
p
4(e
11
)
2
+ (e
20
 e
02
)
2
; (16)

 
(
i
O
l
(t)) = e
20
+e
02
 
p
4(e
11
)
2
+ (e
20
 e
02
)
2
: (17)
These shape parameters are calculated for L labeled ob-
jects
i
O
l
in
i
B(x;y;t) (i = 1; 2) using both camera inputs.
(3) Cell tracking for motion estimation
a) Cell sorting in the ﬂow direction
When the sizes of the labeled objects exceed a speciﬁc
threshold, 
S
, the labeled objects are sorted in the upstream
direction of the microchannel, as follows:
i
~
O
u
(t) = sort(
i
F(t);u) (u=1;;U(t)); (18)
i
F(t) =f
i
O
l
(t) :S(
i
O
l
(t))>
S
(l=1;;L)g; (19)
where
i
~
O
u
(t) is the u-th farthest object from the upstream
end of the microchannel in an image, which belongs to a set
i
F(t). Its centroid position
i
X(
i
~
O
u
) satisﬁes the following
condition:
d(
i
~
O
u1
(t))d(
i
~
O
u2
(t)) when u
1
u
2
; (20)
d(
i
~
O
u
(t)) =jX(
i
~
O
u
(t)) X
U
j; (21)
where X
U
= (X
U
;Y
U
) are the x;y coordinates of the
upstream end of the microchannel in an image.
b) Velocity calculation using frame-straddled images
To match the sorted objects as cell regions in the two
camera inputs, the displacement V
uu
0(t) between
1
~
O
u
0(t 
) and
2
~
O
u
(t) is calculated as follows:
V
uu
0(t) =X(
2
~
O
u
(t)) 
^
X(
1
~
O
u
0(t ))
(u
0
=1;;U(t );u=1;;U(t)); (22)
5850
where  is the frame-straddling time difference between the
two camera inputs. In this case, V
uu
0(t) is expressed as the
image coordinates of camera head 2 and the centroid position
of
1
~
O
u
0(t ) is calibrated using the initial afﬁne parameters
as follows:
^
X(
1
O
u
0(t ))=

a
11
a
12
a
21
a
22

X(
1
O
u
0(t ))+

b
1
b
2

: (23)
Assuming that is so small that the cell displacements are
minimal between the two frame-straddled images, the u-th
cell region
2
~
O
u
(t) in the view of camera head 2 corresponds
to its nearest cell region
1
~
O
u
0
M
(u)
(t ) in the view of camera
head 1 if their displacement is less than a speciﬁc threshold

V
, where its object numberu
0
M
(u) is determined as follows.
u
0
M
(u)=
(
arg min
u
0
jV
uu
0(t)j (minjV
uu
0(t)j<
V
)
; (otherwise)
; (24)
The velocity of theu-th cell regionv(
2
~
O
u
(t)) in the view
of camera head 2 is calculated as the displacement magniﬁed
by the reciprocal of the frame-straddling time as follows:
v(
2
~
O
u
(t)) =
1

V
uu
0
M
(u)
(t): (25)
c) Tracking identiﬁed cells
In general, the cells ﬂowing in microchannels are observed
in many frames of a single camera input, because the
observation time depends on their speeds. To avoid the
incorrect counting of the same cells as different cells, all
of the cell regions
2
~
O
u
(t) (u = 1; ;U(t)) in the view
of camera head 2 at current time t are tracked by matching
them with those at the previous timet t. t is the frame
interval of the input images for camera head 2. Assuming
that the cells ﬂow unidirectionally in the microchannels and
move further from the upstream end as time passes, their
identiﬁcation numbers are assigned in ascending order ofl
0
,
i.e., in order of their distance from the upstream end.
If there are cell regions at time t  t in the upstream
region from the u-th cell region
2
~
O
u
(t) at time t that do
not correspond with the cell regions
2
~
O
1
(t), ,
2
~
O
u 1
(t)
at time t,
2
~
O
u
(t) corresponds to the farthest cell region
from the upstream end of the cell regions at time t  t in
the upstream region. For u = 1; ;U(t), its identiﬁcation
number Q(
2
~
O
u
(t)) is assigned as follows:
Q(
2
~
O
u
(t)) =Q(
2
~
O
u
0
T
(u)
(t  t)) (u = 1; ;U(t)); (26)
u
0
T
(u) = minu
0
; (27)
s.t. d(
2
~
O
u
(t))d(
2
~
O
u
0(t  t)) (28)
and u
0
T
(u  1)<u
0
U(t  t); (29)
where the order of the cell regions at time t is preserved
in the order of their corresponding objects at time t  t;
u
0
T
(u
1
) > u
0
T
(u
2
) when u
1
> u
2
. Initially, u
0
T
(u) is set to
zero when u = 1.
Otherwise,
2
~
O
u
(t) is regarded as a new cell region in the
image and its identiﬁcation number Q(
2
~
O
u
(t)) is assigned
a new number, which is the minimum number of those that
have not been assigned to any cell regions.
After the tracking process, the moment-based shape fea-
tures and velocities described in Eqs (10)–(12) and (25)
Fig. 3. Schematic showing the data ﬂow.
are assigned to all of the identiﬁed cell regions, and these
features facilitate the analysis of the motion and shapes of
the cells ﬂowing in the microchannels.
D. Implementation of the Hardware Logic and its Speciﬁca-
tion
The process used to extract the features from multiple
cells, which has a computational complexity of O(MN),
was implemented in the hardware logic of the user-speciﬁed
FPGA on the IDP Express board. Figure 3 shows a schematic
of the data ﬂow of the multi-object moment feature extraction
module implementation, which was designed as an extended
multi-object feature extraction circuit [8] to facilitate the
rapid analysis of cells ﬂowing in microchannels. The sys-
tem comprises a binarization submodule, a subregion-based
features calculation submodule, a connected components
labeling submodule, and a data selector for the FIFO output.
The input images obtained from camera heads 1 and 2 are
scanned in parallel with a time delay  to allow frame-
straddling. The input images from each camera head are
scanned in units of four pixels from the upper left to the
lower right usingX andY address signals with a 151.2 MHz
clock. In the present study, the two circuit modules shown
in Fig. 3 were implemented to allow the parallel execution
of multi-object moment feature extraction based on the two
camera inputs. The delay time when calculating the label-
domain moment features of 512 labeled objects is 25 clocks
(one clock = 6.6 ns) after raster-scanning all of the pixels in
an input image, while the delay time when outputting them
to the external PC is one frame.
III. EXPERIMENTS
To verify the performance of our vision-based cell analysis
system, we conducted two experiments using sea urchin
egg cells that ﬂowed in straight and L-type microchannels,
which were fabricated from polydimethylsiloxane and had
a rectangular cross-section that measured 200 m wide and
100m deep. In the experiments, water containing sea urchin
egg cells (Hemicentrotus pulcherrimus) that measured 80–
100m in diameter was injected into the microchannel using
an electric syringe pump. After dual-camera calibration, the
afﬁne parameters were set to a
11
= 1.000, a
12
= 0.001, a
21
= 0.001, a
22
= 0.999, b
1
= 2.66, and b
2
= 0.19. The
5851
camera head 1 camera head 2 camera head 1 camera head 2
(a) input images (b) mask images
Fig. 4. Input and mask images used for the straight microchannel.
TABLE I
TIME DURATIONS AND NUMBER OF CELLS COUNTED FOR SEA URCHIN
EGG CELLS AT DIFFERENT FLOW RATES.
ﬂow rate [l/min] 125 250 500 1000 2000
time duration [s] 96.4 61.6 58.8 32.4 24.7
Fig. 5. Speeds and eccentricities of sea urchin egg cells at different ﬂow
rates in a straight microchannel.
exposure times of the two camera heads were 6.25 s. The
threshold used to determine active subregions was set to

P
= 4, the threshold used to reject small labeled objects
was set to 
S
= 7000, and the threshold used to determine
corresponding objects in two frame-straddled images was set
to 
V
= 80.
A. Sea urchin egg cells ﬂowing in a straight microchannel
The experiment used sea urchin egg cells spawned on the
previous day, which ﬂowed from left to right in a straight
microchannel, where the upstream end of the microchannel
was set toX
U
= (0, 128). The brightness ratio was set toR
12
= 0.83. The mask images of the two camera inputs,
1
C(x;y)
and
2
C(x;y), were prestored, as shown in Fig. 4. The
threshold for binarization was set to
B
= 69. The 512256
input images and label-domain features were recorded in real
time at 2000 fps for data logging whereas the label-domain
features of 512256 input images were extracted at 4000 fps.
The sea urchin egg cells were subjected to different ﬂow
speed and were observed to quantify the deformation of
their shapes with rapid microchannel ﬂow rates. The frame-
straddling times were set to  = 240, 240, 80, 40, and
20 s, when the ﬂow rates were 125, 250, 500, 1000,
and 2000l/min, respectively. Measurements were acquired
continuously during each trial with different ﬂow rates,
except when the number of cells counted exceeded 800.
Table I shows the time durations and the number of cells
counted in the trials at different ﬂow rates.
Figure 5 shows the averages and standard deviations for
the speeds and eccentricities of the egg cells at ﬂow rates of
125, 250, 500, 1000, and 2000l/min. The average speed of
the egg cells increased by 0.12, 0.28, 0.60, 0.97, and 1.70 m/s
as the ﬂow rate increased to 125, 250, 500, 1000, and 2000
l/min, respectively. This demonstrated that our system could
observe sea urchin egg cells with rapid ﬂow rates of 1 m/s or
more in the microchannels, which corresponded to an image
Fig. 6. Eccentricity histograms for sea urchin egg cells at different ﬂow
rates in a straight microchannel.
Fig. 7. Snapshots of sea urchin egg cells at different ﬂow rates in a straight
microchannel.
0.2310 s 0.2315 s 0.2320 s
binary 1
binary 2
result
Fig. 8. Binarized images, x;y positions, and velocities of a tracked sea
urchin egg cell ﬂowing in a straight microchannel at 1000 l/min.
displacement of 110
6
pixels or more per second in the
microscopic view.
Figure 6 shows the relative eccentricity histograms for the
egg cells ﬂowing at different speeds, where the bin size is
0.05. Figure 7 shows snapshots of the egg cells. The average
eccentricities of the egg cells increased by 0.13, 0.16, 0.18,
0.25, and 0.34 as the ﬂow rate increased to 125, 250, 500,
1000, and 2000l/min, respectively. This trend corresponded
to the degree of sea urchin egg cell shape deformation shown
in Fig. 7, where the egg cells became more deformed as the
ﬂow rate increased.
Figure 8 shows three binarized image sequences obtained
from the two camera heads, their x;y positions, and the
velocity of the tracked egg cell, which were taken at intervals
of 0.5 ms for t = 0:2310–0:2320 s. In the ﬁgure, the
x;y positions of the same egg cell are shown at different
times based on the two camera inputs with the major axes
and minor axes, where the egg cell was approximated as
an ellipse in the image coordinates from camera head 2.
In Fig. 8, the egg cell was only observed in one or two
frames in the image sequence at 2000 fps, which made it
difﬁcult to estimate its velocity from a single camera view.
However, the egg cell was observed without large image
displacements between the two camera views by setting their
frame-straddling time to 40 s, i.e., 40 s is 0.08 times
0.5 ms, which was the frame interval for the 2000 fps video.
This demonstrated that our system could measure the motion
and shapes of the cells in the microchannel simultaneously
using two frame-straddled camera inputs with a small time
delay, even when the cells ﬂowed too rapidly to be tracked
in a single camera view.
5852
camera head 1 camera head 2 camera head 1 camera head 2
(a) input images (b) mask images
Fig. 9. Input and mask images for an L-type microchannel.
0.248 s 0.249 s 0.250 s 0.251 s 0.252 s
binary 1
binary 2
result
Fig. 10. Binarized images, x;y positions, velocities, and contours of a
tracked sea urchin egg cell ﬂowing in an L-type microchannel at 250l/min.
strobe image
Fig. 11. Eccentricities and velocities of sea urchin eggs ﬂowing in an
L-type microchannel at 250 l/m.
B. Sea urchin egg cells ﬂowing in an L-type microchannel
In this experiment, one-day-old sea urchin egg cells ﬂowed
from the left bottom to the right top of an L-type mi-
crochannel, where the upstream end of the microchannel
was set to X
U
= (0, 511). The brightness ratio was set
to R
12
= 0.83. The mask images used for the two camera
inputs,
1
C(x;y) and
2
C(x;y), were prestored, as shown
in Fig. 9. The threshold for binarization was set to 
B
=
60. The 512512 input images and label-domain features
were recorded in real time at 1000 fps to allow data logging
whereas the label-domain features of 512512 input images
were extracted at 2000 fps.
Sea urchin egg cells were observed ﬂowing in a L-type
microchanel at 250 l/min to quantify the deformation of
their shapes in irregular microchannel, where the frame-
straddling time was set to = 250s. Figure 10 shows ﬁve
binarized image sequences from the two camera heads, with
thex;y positions, velocities, and contours of the tracked egg
cell, which were taken at intervals of 0.5 ms fort = 0:2480–
0:2520 s. Figure 11 shows the changes in the eccentricity
and velocity of the egg cell in the L-type microchannel.
The velocities detected by our system were lower when the
cell was close to the corner of the L-type micro-channel.
By contrast, the eccentricity of the cell increased when the
cell was close to the corner of the L-type micro-channel.
The contours of cells could also be extracted at 1000 fps
by selecting small regions of interest (ROIs) in extracted
bounding boxes of cells, which were extracted by hardware
circuits.
The experimental results showed that the shape and
movement of fast-ﬂowing cells in an irregular microchannel
could be quantiﬁed in real-time using our high-speed frame-
straddling vision system.
IV. CONCLUSION
In this study, we developed a rapid vision-based cell
analysis system that quantiﬁed the shape and motions of
cells simultaneously while ﬂowing at 1 m/s or more in
microchannels. Our system could extract the locations and
shape features of multiple cells in two frame-straddled im-
ages in real time at 2000 fps or more by implementing multi-
object moment feature extraction circuit modules on a dual-
camera high-speed vision platform. This system obtained the
velocities by setting a very small time delay between the
two frame-straddled camera inputs, if their speed meant that
they were too fast to track in a single-camera view. The
experimental results showed that the shape deformations of
sea urchin egg cells could also be quantiﬁed simultaneously,
and this veriﬁed the performance of our system. Based
on these results, we plan to improve our vision-based cell
analysis system further so it can be used to inspect smaller
cells, such as RBCs, in rapid microchannel ﬂows. We also
plan to develop an LOC-based cell-sorting system to manip-
ulate fast-ﬂowing cells in microchannels using simultaneous
visual feedback control based on their shapes, motions, and
other properties, as another step toward the automated mass
production of good quality cells.
REFERENCES
[1] E. Oosterbroek and A. Van den Berg, Lab-on-a-chip: Miniaturized sys-
tems for (bio)chemical analysis and synthesis, Amsterdam: Elsevier,
2003.
[2] Q.-Q. Ji, G.-S. Du, M. J. van Uden, Q. Fang, and J. M. den Toonder,
“Microﬂuidic cytometer based on dual photodiode detection for cell
size and deformability analysis,” Talanta, vol. 111, pp. 178–182, 2013.
[3] Z. Tong, E. M. Balzer, M. R. Dallas, W.-C. Hung, K. J. Stebe, and K.
Konstantopoulos, “Chemotaxis of cell populations through conﬁned
spaces at single-cell resolution,” PLoS one, vol. 7, no. 1, pp. e29211,
2012.
[4] M. L. Heuz´ e, O. Collin, E. Terriac, A.-M. Lennon-Dum´ enil, and M.
Piel, “Cell migration in conﬁnement: A micro-channel-based assay,”
Cell Migration, vol. 769, pp. 415–434, 2011.
[5] A. M. Forsyth, J. Wan, W. D. Ristenpart, and H. A. Stone, “The dy-
namic behavior of chemically tiffenedred blood cells in microchannel
ﬂows,” Microvasc. Res., vol. 80, no. 1, pp. 37–43, 2010.
[6] D. R. Gossett, H. T. K. Tse, S. A. Lee, Y . Ying, A. G. Lindgren, O. O.
Yang, J. Rao, A. T. Clark, and D. Di Carlo, “Hydrodynamic stretching
of single cells for large population mechanical phenotyping,” Proc.
Natl. Acad. Sci., vol. 109, no. 20, pp. 7630–7635, 2012.
[7] M. Kobatake, T. Takaki, and I. Ishii, “A real-time micro-PIV system
using frame-straddling high-speed vision,” in Proc. IEEE Int. Conf.
Robot. Automat., 2012, pp. 397–402.
[8] Q. Gu, T. Takaki, and I. Ishii, “Fast FPGA-based multiobject feature
extraction,” IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 1,
pp. 30–45, 2013.
[9] I. Ishii, T. Tatebe, Q. Gu, Y . Moriue, T. Takaki, and T. Tajima, “2000
fps real-time vision system with high-frame-rate video recording,” in
Proc. IEEE Int. Conf. Robot. Automat., 2010, pp. 1536–1541.
[10] Z. Zhang, “Flexible camera calibration by viewing a plane from
unknown orientations,” in Proc. IEEE Int. Conf. Comput. Vis., vol.
1, 1999, pp. 666–673.
[11] Q. Gu, T. Takaki, and I. Ishii, “A fast multi-object extraction algorithm
based on cell-based connected components labeling,” IEICE Trans.
Inform. Syst., vol. E95-D, no. 2, pp. 636–645, 2012.
5853
