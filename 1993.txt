Combining learned controllers to achieve new goals based on linearly
solvable MDPs
Eiji Uchibe and Kenji Doya
Abstract—Learning complicated behaviors usually involves
intensive manual tuning and expensive computational opti-
mization because we have to solve a nonlinear Hamilton-
Jacobi-Bellman (HJB) equation. Recently, Todorov proposed
a class of the so-called Linearly solvable Markov Decision
Process (LMDP) which converts a nonlinear HJB equation
to a linear differential equation. Linearity of the simpliﬁed
HJB equation allows us to apply superposition to derive a new
composite controller from a set of learned primitive controllers.
However, his method was a model-based approach and it was
not evaluated in a real domain. This study proposes a model-
free method which is similar to the Least Squares Temporal
Difference (LSTD) learning. In this method, the exponentially
transformed cost function can be regarded as the discount
factor in LSTD. Our proposed method is applied to learning
walking behaviors with the quadruped robot to evaluate in real
robot experiments. The goal of each primitive task is to go to
the speciﬁc target position in the environment and that of the
composite task is to approach arbitrary region represented by
the primitives’ target positions. Experimental results show that
the composite policy can be used as a good initial policy for
the new task.
I. INTRODUCTION
When we want to design an autonomous robot that can act
optimallyinitsenvironment,therobotshouldsolvenonlinear
optimization problems in continuous state and action spaces.
If a precise model of the environment is available, then both
optimal control [1] and model-based reinforcement learning
[2] give a computational framework to ﬁnd an optimal
control policy which minimizes cumulative costs (or maxi-
mizes cumulative rewards). If not, model-free reinforcement
learningisapplicable.Inrecentyears,reinforcementlearning
algorithmshavebeenappliedtoawiderangeofneuroscience
data [3] and model-based approaches have been receiving
attention among researchers who are interested in decision
making [4], [5].
However, a drawback is the difﬁculty to ﬁnd an opti-
mal policy for continuous states and actions. In particular,
learning time can become unacceptable long if the model
is unknown in advance. There exist several ways to reduce
learningtime,andweareinterestedinthereuseofpreviously
learned policies in order to ﬁnd or design a new optimal
policy. If the robot has enough learned policies, no additional
learning is required to design the new task by combining
learned policies as primitives.
*This work was supported by JSPS KAKENHI Grant Number 24500249
and Grant-in-Aid for Scientiﬁc Research on Innovative Areas: Prediction
and Decision Making (24120527).
The authors are with the Neural Computation Unit, Okinawa Institute
of Science and Technology Graduate University, 1919-1 Tancha, Onna-son,
Okinawa 904-0495, Japan fuchibe,doyag@oist.jp
Recently, a new framework of linearly solvable Markov
decision process (LMDP) has been proposed, in which a
nonlinear Hamilton-Jacobi-Bellman (HJB) equation for con-
tinuous systems or Bellman equation for discrete systems is
convertedintoalinearequationundercertainassumptionson
the action cost and the effect action on the state dynamics
[6], [7]. In this approach, an exponentially transformed state
value function is deﬁned as a desirability function and it is
derived from the linearized Bellman’s equation by solving
an eigenvalue problem [8] or an eigenfunction problem [9],
[10]. One of the beneﬁts is its compositionality. Linearity of
the Bellman equation enables deriving an optimal policy for
acompositetaskfrompreviouslylearnedoptimalpoliciesfor
basic tasks by linear weighting by the desirability functions
[11], [12]. Although the goal of the composite task should be
represented by a linear combination of those of the primitive
tasks, there is no need to learn the composite task from
scratch. However, their methods assume the environmental
dynamics is known and they have so far been tested only
in simulation. In this study, we test the applicability of the
compositionality theory to real robot control.
Sincetheenvironmentaldynamicsisoftenunknowninreal
robot applications, a model free approach is required. We
proposed the method which integrates model learning with
the LMDP framework [13], but we found that the learned
desirability function is biased if the estimated model is not
accurate. To overcome this problem, this paper proposes a
model-freereinforcementlearningofthedesirabilityfunction
and it is combined with the model learning. The proposed
learning algorithm is based on the least-squares reinforce-
ment learning algorithm [14], [15] and it can be regarded
as the learning method with state-dependent discount factor.
In addition, the composite policy derived from the compo-
sitionality theory is used as an initial policy to learn the
corresponding composite task because the assumptions are
sometimes violated.
We test the proposed method in the navigation task using
the quadruped robot called the Spring Dog. The goal of the
primitive task is to approach the position speciﬁed by the
landmark while the desired position of the composite task
is represented by the set of positions used in the primitive
tasks.Wecomparethefollowingfourmethods:(1)composite
optimal policy derived from the compositionality theory, (2)
optimal policy with additional learning from the composite
policy, (3) optimal policy learned from scratch, and (4)
weighted sum of policies designed in the value function
space. Although the composite policy is a weighted sum
of primitive policies designed in the desirability function
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5252
space, our experimental results show that the performance
of the proposed method is comparable to that of the optimal
policy learned from scratch. Furthermore, it is shown that
the additional learning from the composite policy learns
faster than learning from scratch while the composite policy
designed in the value function space is not a good policy. It
suggests that the compositionality is very useful in the real
robot experiment and model free learning is promising.
II. COMPOSITIONALITY THEORY
To begin with, we introduce the basics of the composition-
ality theory of the LMDP [11], [12] brieﬂy. The basic idea is
to apply the principle of superposition of linear differential
equations subject to some boundary conditions.
A. Basic idea
Suppose that there exist N learned optimal policies u
i
(i = 1,...,N) for a set of speciﬁc N primitive tasks shown
in Fig. 1 (a). Here, the primitive task is characterized by the
terminalconditiong
i
.Itmeansthatu
i
derivedfromthevalue
function v
i
trained with g
i
. When a new composite task is
created from primitive tasks, our goal is to learn an optimal
policy but to compute it represented by
u
?
(x,t) =
N
∑
i=1
m
i
(x,t)u
i
(x,t), (1)
where x and t denote the state and time, respectively.
m
i
(x,t) is a mixing weight, and the problem is how we
design or optimize it without additional learning. Since the
value function is the solution of the nonlinear HJB equation,
a linear sum of value functions is not optimal even if the
terminal condition of the composite task is given by the sum
of that of primitive tasks.
For example, Doya et al. propose a multiple module based
reinforcement learning [16] in which the mixing weight is
computed from the prediction error of the primitive module.
Although their method does not require additional learning
for mixing, the combined policy is not necessarily optimal.
Thomas and Barto propose a framework based on policy
gradient reinforcement learning that learns constant mixing
weights as well as optimal policies of primitive tasks [17].
Their method can ﬁnd a local optimal policy in principle,
but the learning mixing weights is always necessary even if
the goal of the composite task is slightly modiﬁed from one
of primitive tasks.
Recently, the compositionality theory is applied to this
problem by Todorov [11], in which the optimal mixing
weights are designed without additional learning for the
composite task in the framework of linearly solvable Markov
decision process (LMDP). Under some conditions, the non-
linear HJB equation is transformed into the linear differential
equation with respect to a so-called desirability function
z, shown in Fig. 1 (b). This linearity allows us to apply
superposition of desirability functions. In other words, the
weighted sum of the desirability functions is the solution of
linearized HJB equation with the weighted sum of terminal
conditions.
primitive task N
g
N
v
N
, u
N
nonlinear HJB linearized HJB
w.r.t. the desirability function w.r.t. the value function
primitive task 1
g
1
v
1
, u
1
composite task
g’
v’ , u’
primitive task N
g
N
z
N
, u
N
primitive task 1
g
1
z
1
, u
1
composite task
g’
z’ , u’
(a) (b)
Fig. 1. Basic idea of compositionality. (a) value function. v: value
function,u: optimal policy, g: boundary condition. (b) desirability function.
z: desirability function. See the main text.
B. Markov decision process
We give a brief introduction of Markov Decision Process
(MDP) for a continuous domain [1], [18]. LetX  R
Nx
and U  R
Nu
be the continuous state and continuous
action spaces, where N
x
and N
u
are the dimensionality
of the spaces, respectively. At time t, the robot observes
the environmental current state x(t) 2 X and executes
action u(t)2U. Consequently, the environment makes a
state transition according to the following continuous-time
stochastic differential equation,
dx =a(x)dt+B(x)(udt+?d?), (2)
where?2R
Nu
and ? denote Brownian noise and a scaling
parameter for the noise, respectively. a(x) describes the
passive dynamics of the system while B(x) represents the
input-gain matrix. Note that Eq. (2) is generally nonlinear
with respect to the state x but linear with respect to the
actionu.
It is convenient to represent Eq. (2) in discrete time. By
discretizingthetimeaxiswithsteph,weobtainthefollowing
transition probability,
p
u
k
(x
k+1
jx
k
) =N(x
k+1
jµ(x
k
,u
k
)+x
k
,h(x)), (3)
whereN(x j µ,) denotes a Gaussian distribution with
meanµ and covariance matrix, and
µ(x,u) = h(a(x)+B(x)u), (4)
(x) = ?
2
B(x)
?
B(x), (5)
where µ(x,u) can be regarded as a deterministic state
transition function. Note that x
k
and u
k
represent the state
and action at time step k, and they correspond to x(hk)
and u(hk) in continuous time, respectively. It should be
noted that a state transition probability is deﬁned as an
uncontrolled probability when no control is applied (u =0),
and otherwise, it is called a controlled probability.
A control policy or controller u = π(x) is deﬁned as a
mapping from the state x to the action u. The goal of the
i-th primitive task is formulated as a ﬁnite horizon problem
because the boundary conditions of a differential equation
play an important role. The expected cumulative cost, which
is known as a cost-to-go or value function, is given by
v
π
i
(x,t) =E
[
∫
Tg
t
c(x(?),π(x(?)))d? +g
i
(x(T
g
))
]
,
5253
where c(x,u) and g
i
(x) respectively denote the immediate
and terminal cost. T
g
is a given ﬁxed evaluation time. It
should be noted that c(x,u) and T
g
are the same among
all the primitive tasks, and therefore the primitive task is
characterized by g
i
. We shall return to this point later. The
optimal value function is the minimal expected cumulative
cost deﬁned by
v
?
i
(x,t) = min
π
v
π
i
(x,t).
It is known that the optimal value function satisﬁes the
following Hamilton-Jacobi-Bellman’s (HJB) equation [6],
[18]
 
∂v
?
i
(x,t)
∂t
= min
u
(
c(x,u)+L
(u)
[v
i
]
)
, (6)
v
?
i
(x,T
g
) = g
i
(x), (7)
whereL
(u)
is the second-order linear differential operator
deﬁned by
L
(u)
[v
i
] = (a(x)+B(x)u)
?
∂v
i
∂x
+
1
2
tr
(
BB
?
∂
2
v
i
∂x
2
)
.
Since Eq. (6) is nonlinear with respect to the value function,
it is difﬁcult to solve the optimal value function in general
even though the model parameters a(x) and B(x) are
known in advance.
C. Linearly solvable MDP
Next, we show how the nonlinear HJB equation (6) can
be made linear. Suppose that the cost function is given by
c(x,u) = q(x)+
1
2?
2
u
?
u,
where q(x) denotes a non-negative state dependent cost
function. It should be noted that c(x,u) is nonlinear with
respect to x but quadratic with respect to u. In this case,
it is possible to minimize the right hand side of Eq. (6)
with respect tou analytically, and Eq. (6) becomes linear as
follows:
 
∂z
i
(x,t)
∂t
=L
(0)
[z
i
] q(x)z
i
(x,t), (8)
z
i
(x,T
g
) = exp( g
i
(x)), (9)
where z
i
(x) is the desirability function transformed by
z
i
(x,t) = exp( v
?
i
(x,t)). (10)
Eq. (8) plays an important role for compositionality because
superpositionofthedesirabilityfunctionsisconsideredusing
this equation. Therefore, the immediate cost function must
be shared not to change the equation among primitive tasks.
Hereafter Eq. (8) is called the linearized HJB equation and
its counterpart in discrete time is the linearized Bellman
equation given by
z
i
(x,t) = exp( hq(x))G[z
i
](x,t) (11)
z
i
(x,T
g
) = exp( g
i
(x
g
)). (12)
TheoperatorG shownontherighthandsideofthelinearized
Bellman’s equation (11) is the integral operator given by
G[?](x) =
∫
p
0
(x
?
jx)?(x
?
)dx
?
,
where p
0
is the uncontrolled probability when no control
is applied (u = 0) in Eq. (3). See [6] for more details. It
should be noted that Eqs (8) and (11) are always satisﬁed
by the trivial solution (z
i
(x) 0 for all x) if no boundary
conditions (9) and (12) are taken into account.
D. Control policy
In the LMDP framework, the optimal control policy of the
i-th primitive task is given by
p
u

(x
?
jx) =
p
0
(xjx)z(x,t)
G[z](x,t)
.
Speciﬁcally, if the dynamics are represented in the form
of the stochastic differential equation (2), then the optimal
control policy is represented by
u
?
i
(x,t) = ?
2
B(x)
?
∂lnz
i
(x,t)
∂x
. (13)
We assume that the terminal cost of the composite task
g
?
(x) is given by the weighted sum
exp( g
?
(x)) =
N
∑
i=1
w
i
exp( g
i
(x)) (14)
in the nonlinear transformed space, where w
i
is a mixing
weight of terminal conditions. If the set of terminal costs
fg
i
(x)g
N
i=1
is sufﬁciently rich, we can represent an arbitrary
cost. If a desired g
?
(x) is given,fw
i
g
N
i=1
can be optimized
by the least-squares method. Because of linearity, the desir-
ability function of the composite task is also given by
z
?
(x,t) =
N
∑
i=1
w
i
z
i
(x,t). (15)
As a result, the optimal policy of the composite task is
derived as follows:
u
??
(x,t) = ?
2
B(x)
?
∂lnz
?
(x,t)
∂x
=
N
∑
i=1
m
i
(x,t)u
?
i
(x,t),
(16)
where the mixing weight of the policy is deﬁned by
m
i
(x,t) =
w
i
z
i
(x,t)
∑
j
w
j
z
j
(x,t)
. (17)
We call Eq. (16) the composite optimal policy designed in
the desirability function space.
We can also create a composite policy designed in the
value function space if the value function is approximated
by
v
?
(x,t) =
N
∑
i=1
w
i
v
i
(x,t). (18)
It is possible to derive the policy from v
?
, but it should be
noted that linearity does not hold in the value function space.
5254
III. LEARNING PARAMETERS
This section describes how the optimal policy (13) of
the primitive task is learned from samples. Speciﬁcally, we
introduce a model free learning of the desirability function
z(x,t) and a simple learning of the input-gain matrixB.
A. Model free learning of the desirability
In our previous work [13], we proposed a model-based
framework in which the desirability function was computed
from the estimated dynamics. In this paper, we propose
a novel model-free framework of learning the desirability
function based on the least squares reinforcement learning
algorithms [14], [15]. We assume that the desirability func-
tion is approximated by the following linear approximator
z
i
(x,t;w,?) =
Nz
∑
j=1
w
ij
?(x,t;?
j
) =w
?
i
?(x,t;?),
(19)
where w
ij
andw
i
are a learning weight and its vector repre-
sentationofthei-thprimitivetask,?(x,t;?
j
)isabasisfunc-
tion parameterized by ?
j
, and ?(x,t;?) is the vector con-
sisting of basis functions [?(x,t;?
1
), ..., ?(x,t;?
Nz
)]
?
.
Let us denote the robot’s experience in discrete time
by d = (x
0
,...,x
Nstep
) where hN
step
= T
g
. From the
linearized Bellman’s equation (11) and the boundary con-
dition (12), the temporal difference error of state transition
to non-terminal and terminal states are given by
?
i,k+1
= (?
k
 exp( q(x
k
))?
k+1
)
?
w
i
,
?
i,Nstep
= exp( g
i
(x
Nsteps
)) ?
?
Nsteps
w
i
,
where ?
k
= ?(x
k
,hk). According to the LSTD [14], the
update ofw
i
at the end of episode is calculated by
∆w
i
=
Nsteps?1
∑
k=0
?
k
?
i,k+1
=Aw
i
 b
i
,
where?
Nsteps+1
=0 andA andb
i
are computed recursively
by
A A+
Nstep
∑
k=0
?
k
(?
k
 exp( q(x
k
))?
k+1
)
?
, (20)
b
i
 b
i
+?
Nstep
exp( g
i
(x
Nstep
)). (21)
Instead of using a stochastic gradient method with ?w
i
,
the least-squares method solves w
i
= A
?1
b
i
directly. As
compared with the standard LSTD, exp( q(x
k
)) in Eq. (20)
corresponds to the discount factor and it can be regarded as a
state-dependent discount factor. In additionA is independent
of the primitive tasks while b
i
is dependent on them and it
is updated at the end of episode.
B. Learning model parameters
In the LMDP framework, the system dynamics (2) is
assumed to be known in advance. When it is unknown,
estimating the dynamics is required from samples collected
by the passive dynamics. Although we propose the model-
free learning of a desirability function in Section III-A, the
Fig. 2. Quadruped robot, Spring Dog
Jacobian matrix B(x) is required to compute the optimal
policy (13). This section describes how to learnB(x) from
samples.
Many methods exist which can estimate the system dy-
namics [19], [20], and we adopt a simple least squares
method to estimate µ(x) with basis functions. Speciﬁcally,
we estimate a deterministic state transition (4). It should
be noted that the scale parameter of noise ? is generally
unknown, but it is determined by the experimenters here
since it can be regarded as the parameter that controls
exploration of the environment.
Let us suppose that the deterministic state transition
µ(x,u)isapproximatedbythelinearfunctionwithN
?
basis
functions ?
i
(x,u),
µ(x,u;W) =W
?
?(x,u). (22)
where W is a weight matrix and ?(x,u) is a vector con-
sisting of basis functions. Suppose that the training samples
fx
1
,u
1
,...,x
Ns
,u
Ns
,x
Ns+1
g are obtained by the passive
dynamics. The objective function of model learning is given
by the following sum-of-squares error function,
E =
1
2
∑
k=1


∆x
k
 W
?
?(x
k
,u
k
)


2
,
where ∆x
k
=x
k+1
 x
k
. Setting ∂E/∂W =0 yields
W = (?
?
?)
?1
?
?
∆X,
where ∆X is the matrix whose a row vector consisted of
state transition in each sample ∆x
k
and? is also the matrix
whose a column vector consisted of the basis functions in
each sample?(x
k
,u
k
). The detail is as follow,
∆X =
[
∆x
1
 ∆x
Ns
]
?
,
? =
[
?(x
1
,u
1
)  ?(x
Ns
,u
Ns
)
]
.
See our previous study of model learning [13] for more
details.
5255
IV. EXPERIMENTS
A. Locomotion Task
We conduct a control task of a quadruped robot called
the “Spring Dog” to evaluate the proposed model free
method and the compositionality of the LMDP in the real
environment. Fig. 2 shows a hardware of the Spring Dog.
The Spring Dog is endowed with a variety of sensory inputs,
including a USB camera, touch sensors, foot sole sensors,
gyros, and accelerometer. The Spring Dog is a quadruped
robot and each leg has one active hip joint and one passive
kneejointwhileitsheadhastwodegreesoffreedom(DOFs):
pan and tilt joints. As a whole the Spring Dog has six DoFs.
The programs are coded in C++ on top of a standard linux
operating system.
Fig. 3 shows the experimental ﬁeld, in which three land-
marks with LED are located in front of the Spring Dog.
The task of the robot is to walk from the ﬁxed starting
position to the goal position determined by the landmarks.
Speciﬁcally, the Spring Dog learns to approach one of
landmarks in the primitive task. Since there exist three
landmarks, three primitive tasks should be optimized by
the LMDP framework. Blue, green, and red curves shown
in Fig. 4 represent the exponentially transformed terminal
cost function exp( g
i
(x)) of primitive tasks, projected to
the x position in the global coordinate system and it is
maximum when the Spring Dog arrives at the speciﬁed
landmark while q(x) = 0.95 for all x. The composite task
is go to the position between the blue and green landmarks
and its corresponding terminal cost function is shown in
black in Fig. 4. This terminal cost is created by setting
w
1
= w
2
= 0.86,w
3
= 0 in Eq. (14). It should be noted
that the shape of the terminal cost of the composite task
(black line) is different from those of the component tasks
because it is unable to represent a complicated function by
a weighted sum of three functions as shown in Eq. (14).
Fig. 5 shows the control architecture. The state vector x
consists of six components,
x =
[
?
?
hip
,p
?
]
?
,
where?
hip
2R
4
is the current angles of the hip joints, and
p = [x,y]
?
represents the position of the Spring Dog in the
global coordinate system shown in Fig. 3. The camera head
modulecomputespfromcapturedimagesoftheenvironment
and generates the desired angle of the pan and tilt joints. On
the other hand, the desired angle of the hip joint?
hip,d
2R
4
is determined by
?
hip,d
=u+?
CPG
, (23)
where u and ?
CPG
2 R
4
denote the action from the
reinforcement learning module and the output from the CPG,
respectively. The role of the CPG is to generate a stable
movement as a base controller and it helps the robot to
collectgoodexperienceforlearning.Theequationsofmotion
with the CPG control is interpreted as the uncontrolled
probability. The modiﬁed Hopf oscillator [21] is applied in
this implementation.
Fig. 3. The Spring Dog and three landmarks in the experimental ﬁeld.
The top right image represents the view from the Spring Dog. The global
coordinate system is also shown in this ﬁgure.
0 20 40 60 80 100 120
0
0.2
0.4
0.6
0.8
1
x [cm]
exp(?g
i
(x)), exp(?g’(x))
 
 
exp(?g
1
(x))
exp(?g
2
(x))
exp(?g
3
(x))
exp(?g’(x))
Fig. 4. Three terminal cost functions of primitive tasks (blue, green, and
red) and that of the composite task (black).
CPG
RL
camera head
Spring Dog
?
hip,d
?
head,d
?
head
x,y
?
hip
?
CPG
u
Fig. 5. Control architecture.
In order to represent the desirability function, we adopt an
unnormalized Gaussian function given by
?(x,t;?
i
) = exp
(
 
1
2
(x c
i
(t))
?
S
i
(x c
i
(t))
)
,
where?
i
=fc
i
,S
i
g andc
i
andS
i
denote a center position
and a precision matrix of thei-th basis function, respectively.
On the other hand, the linear basis function used in our
previous study [13] to approximate µ(x,u). In this case,
B(x) can be represented a constant matrix.
B. Experimental results
IttookaboutthreehoursfortheSpringDogtolearnappro-
priatebehaviorineachprimitivetask.Toseetheperformance
5256
0 50 100 150 200
0
20
40
60
80
100
episode
distance to the landmark [cm]
 
 
primitive 1
primitive 2
primitive 3
Fig. 6. Learning curves of primitive tasks. The error bars represent the
standard deviation of 10 experimental runs.
0 20 40 60 80 100 120
120
140
160
180
200
220
240
x position [cm]
y position [cm]
 
 
primitive 1
primitive 2
primitive 3
landmark 1
landmark 2
landmark 3
Fig. 7. Positions at the end of episodes generated by one learned policy
of the primitive tasks in a typical experiment.
of the behaviors during learning we measured the distance
between the landmark and the position of the Spring Dog at
the end of episodes. Fig. 6 shows the experimental results
of learned primitive tasks, in which error bars represent the
standard deviation of 10 runs. Since the landmark 2 (blue)
was located in front of the Spring Dog shown in Fig. 3,
the basic controller ?
CPG
in Eq. (23) was close to the
optimal controller, and the performance at the early stage
of learning was better than those in other primitive tasks.
Fig. 7 shows the ﬁnal positions controlled by the learned
primitive behaviors. Each circle represents the position of
the landmark. To evaluate the learned primitive behaviors,
we conducted 10 evaluation runs for each primitive task.
The colored crosses show the ﬁnal position of the Spring
Dog at the end of episode in a typical experiment. It is
conﬁrmed that the proposed model free learning explained
in Section III-A could learn good behaviors even though the
dynamics of the Spring Dog was not well approximated by
the form of Eq.(2).
Next, the composite optimal policy was designed from
three primitive policies using Eq. (16). To evaluate the com-
positionality theory in the real environment, we compared
the following policies:
1) composite policyu
?
z
: the composite policy computed
by Eq. (16).
2) composite policy with additional learning u
??
z
: the
policy learned with the composite policy u
?
z
. That is,
the base controller?
CPG
in Eq. (23) was replaced by
the composite controlleru
?
z
.
3) optimal policy learned from scratch u
?
z
: the policy
learned with the base controller?
CPG
.
4) value-weighted policy u
?
v
: the composite policy de-
signed in the value function space. The value function
is created by a weighted sum of the value functions
of primitive tasks by Eq. (18) and the weights were
optimized by an exhaustive search.
Note that the same terminal condition g
?
was used to train
u
??
z
andu
?
z
inthecompositetask.Theoreticallyspeaking,the
composite policy u
?
z
should be optimal with respect to the
terminal cost function shown in Fig. 4 and it is identical to
the optimal policyu
?
z
. However, it was not necessarily true
because there exist approximation errors in the desirability
function in practice and the dynamics of the Spring Dog was
not completely expressed by Eq.(2).
Fig. 8 compares the learning curves of the composite
policy with additional learning u
?
z
and the optimal policy
learned from scratch u
?
. As we expected, the composite
policy u
?
z
was not optimal in the composite task, and the
performance of u
??
z
was improved by additional learning.
The learning curve of u
?
z
was similar to those of the
primitive tasks shown in Fig.6. The two policies eventually
obtained the same performance, but the composite policy
with additional learning learned much faster than the optimal
policy learned from scratch. It suggests that the composite
policy u
?
z
was a good approximation of the optimal policy
as compared with the pre-deﬁned base controller?
CPG
.
Fig. 9 compares the ﬁnal positions in the composite task.
Cyan crosses represent the positions generated by u
?
z
and
they can be regarded as the results of the optimal policy
in the composite task. The ﬁnal positions generated by u
?
z
are shown as black circles and its distribution was slightly
different from that of the positions generated by u
?
z
. The
ﬁnal positions generated byu
??
z
are shown as black crosses
in Fig. 9 and they were distributed in the similar areas of
u
?
z
. On the other hand, the ﬁnal positions obtained by the
value-weighted policyu
?
v
were quite different from those of
u
?
z
.
V. CONCLUSION
This paper evaluates how the learned optimal policies are
combined to generate a new optimal policy based on the
compositionality theory of the LMDP framework. One of
contributions of this paper is to propose the model free
learning algorithm based on the least squares reinforcement
learning method for approximating the desirability function.
Although the composite policy is slightly different from the
optimal one due to the violation of the assumptions, our
experimental results suggest that the composite policy is a
5257
0 50 100 150 200
0
20
40
60
80
100
episode
distance to the landmark [cm]
 
 
learned from scratch
composite (learned)
Fig. 8. Learning curves of composite tasks. The black line shows the result
ofu
??
when the composite controller was used as the base controller while
the magenta shows the result of u
?
when the base controller used in the
primitive tasks were used in Eq.(23). The error bars represent the standard
deviation of 10 experimental runs.
0 20 40 60 80 100 120
120
140
160
180
200
220
240
x position [cm]
y position [cm]
 
 
landmark 1
landmark 2
landmark 3
composite (learned)
composite (initial)
learned from scratch
value weighted
Fig.9. Comparisonamongthecompositepolicy,theoptimalpolicylearned
from the composite cost function, and the composite policy designed in the
value function space.
good approximation of the optimal policy and the optimal
policycanbelearnedmuchfasterthanthecasewithoutusing
the composite policy. To our best knowledge, this is the ﬁrst
time that the compositionality in LMDP is demonstrated to
be successful in real robot experiments.
Historically, the basis idea of linearization of the HJB
equation using logarithmic transformation has been shown in
the book written by Fleming and Soner and its connection
to risk sensitive control has been discussed in the ﬁeld of
control theory [18]. Their study has been receiving attention
recently in the ﬁeld of robotics and machine learning ﬁelds
[22] because there exist a number of interesting properties in
the linearized Bellman equation [6]. This paper focused on
thedesirabilityfunctionapproach,butthecompositionalityin
the path integral approach [23], [24] is also promising. In the
path integral approach, the linearized Bellman is computed
along paths starting from given initial states using sampling
methods. The path integral approach has been successfully
appliedtolearningofstochasticpoliciesforrobotswithlarge
degrees of freedom [25], [26], [27], and it is best suited for
optimization around stereotyped motion trajectories. How-
ever,anadditionallearningisneededwhenanewinitialstate
or a new goal state is given. Two approaches are closely
related and new theoretical ﬁndings are reported [22], but
there are some differences in practice.
Further empirical and theoretical analysis is required to
show the efﬁciency of the proposed model-free learning of
the desirability function. In particular, we are interested in
the relation to the state-dependent discount factor in MDP
[28], [29]. Next, we plan to extend the proposed method to
construct the set of primitive tasks incrementally. Since the
terminal cost is approximated by Eq.(14), the approximation
error can be used as a criterion to add a new primitive task.
REFERENCES
[1] E.Todorov. Optimalcontroltheory. InK.Doyaetal.,editors,Bayesian
Brain: Probabilistic Approaches to Neural Coding, chapter 12, pages
269–298. MIT Press, 2006.
[2] R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT
Press/Bradford Books, 1998.
[3] Y. Niv. Reinforcement learning in the brain. Journal of Mathematical
Psychology, 53(3):139–154, June 2009.
[4] N. D. Daw, S. J. Gershman, B. Seymour, P. Dayan, and R. J. Dolan.
Model-based inﬂuences on humans’ choices and striatal prediction
errors. Neuron, 69(6):1204–1215, March 2011.
[5] B. B. Doll, D. A. Simon, and N. D. Daw. The ubiquity of model-based
reinforcementlearning. CurrentOpinioninNeurobiology,22(6):1075–
1081, September 2012.
[6] E. Todorov. Efﬁcient computation of optimal actions. Proceedings of
the National Academy of Sciences of the United States of America,
106(28):11478–83, July 2009.
[7] K. Doya. How can we learn efﬁciently to act optimally and ﬂexibly?
Proceedings of the National Academy of Sciences of the United States
of America, 106(28):11429–30, July 2009.
[8] E.Todorov. Linearly-solvableMarkovdecisionproblems. In Advances
inNeuralInformationProcessingSystems19,pages1369–1376.2007.
[9] E. Todorov. Eigenfunction approximation methods for linearly-
solvable optimal control problems. In Proc. of the 2nd IEEE Sympo-
siumonAdaptiveDynamicProgrammingandReinforcementLearning,
pages 161–168, 2009.
[10] M. Zhong and E. Todorov. Aggregation Methods for Lineary-solvable
Markov Decision Process. In Proc. of the World Congress of the
International Federation of Automatic Control, 2011.
[11] E. Todorov. Compositionality of optimal control laws. In Advances in
Neural Information Processing Systems 22, pages 1856–1864. 2009.
[12] M. da Silva, F. Durand, and J. Popovi´ c. Linear Bellman combination
for control of character animation. ACM Transactions on Graphics,
28(3), July 2009.
[13] K. Kinjo, E. Uchibe, and K. Doya. Evaluation of linearly solvable
Markov decision process with dynamic model learning in a mobile
robot navigation task. Frontiers in Neurorobotics, 7(7), 2013.
[14] J. A. Boyan. Technical Update: Least-Squares Temporal Difference
Learning. Machine Learning, 49(2/3):233–246, 2002.
[15] M. G. Lagoudakis and R. Parr. Least-Squares Policy Iteration. Journal
of Machine Learning Research, 4:1107–1149, 2003.
[16] K. Doya, K. Samejima, K. Katagiri, and M. Kawato. Multiple model-
based reinforcement learning. Neural Computation, 14(6):1347–1369,
June 2002.
[17] P. S. Thomas and A. G. Barto. Motor primitive discovery. In Proc.
of IEEE International Conference on Development and Learning and
Epigenetic Robotics. IEEE, November 2012.
[18] W. H. Fleming and H. M. Soner. Controlled Markov Processes and
Viscosity Solutions. Springer, 2 edition, 2006.
[19] D. Nguyen-Tuong and J. Peters. Model learning for robot control: a
survey. Cognitive Processing, 12(4):319–40, November 2011.
[20] O. Sigaud, C. Sala¨ un, and V. Padois. On-line regression algorithms
for learning mechanical models of robots: A survey. Robotics and
Autonomous Systems, 59(12):1115–1129, December 2011.
5258
[21] L. Righetti and A. J. Ijspeert. Pattern generators with sensory
feedback for the control of quadruped locomotion. In Proc. of IEEE
InternationalConferenceonRoboticsandAutomation,pages819–824.
IEEE, May 2008.
[22] E. A. Theodorou and E. Todorov. Relative entropy and free energy
dualities: Connections to Path Integral and KL control. In Proc. of the
51st IEEE Conference on Decision and Control, pages 1466–1473.
IEEE, December 2012.
[23] H. Kappen. Linear Theory for Control of Nonlinear Stochastic
Systems. Physical Review Letters, 95(20), November 2005.
[24] H. J. Kappen. Path integrals and symmetry breaking for optimal con-
trol theory. Journal of Statistical Mechanics: Theory and Experiment,
2005(11):P11011–P11011, November 2005.
[25] E. Theodorou, J. Buchli, and S. Schaal. A Generalized Path Integral
Control Approach to Reinforcement Learning. Journal of Machine
Learning Research, 11:3137–3181, 2010.
[26] F. Stulp and O. Sigaud. Path Integral Policy Improvement with Co-
variance Matrix Adaptation. In Proc. of the 10th European Workshop
on Reinforcement Learning (EWRL 2012), 2012.
[27] N. Sugimoto and J. Morimoto. Phase-dependent trajectory optimiza-
tion for periodic movement using path integral reinforcement learning.
In Proc. of the 21st Annual Conference of the Japanese Neural
Network Society, 2011.
[28] N. Yoshida, E. Uchibe, and K. Doya. Reinforcement learning with
state-dependent discount factor. In Proc. of IEEE Joint International
Conference on Development and Learning and Epigenetic Robotics,
pages 1–6. IEEE, August 2013.
[29] Q. Wei and X. Guo. Markov decision processes with state-dependent
discount factors and unbounded rewards/costs. Operations Research
Letters, 39(5):369–374, July 2011.
5259
