Sample Path Sharing in Simulation-Based Policy Improvement
Di Wu
1
, Qing-Shan Jia
1
, and Chun-Hung Chen
2
AbstractÑSimulation-based policy improvement (SBPI) has
been widely used to improve given base policies through
simulation. The basic idea of SBPI is to estimate all the Q-
factors for a given state using simulation, and then select
the action that achieves the minimal cost. It is therefore of
great importance to efÞciently use the given budget in order
to select the best action with high probability. Different from
existing budget allocation algorithms that estimate Q-factors
by independent simulation, we share the sample paths to
improve the probability of correctly selecting the best action.
Our method can be combined with equal allocation, Successive
Rejects, and optimal computing budget allocation to enhance
theirprobabilitiesofcorrectselectionaswellastoachievebetter
policies in SBPI. Such improvement depends on the overlap
in reachable states under different actions. Numerical results
show that with such overlap, combining our method with equal
allocation, Successive Rejects and optimal computing budget
allocation produces higher probability of selection as well as
better policies in SBPI.
keywordsÑ Discrete event dynamic system, simulation-based
optimization, optimal computing budget allocation.
I. INTRODUCTION
Simulation-based policy improvement (SBPI) has been
widely used in practice to improve given base policies
through simulation. It is an important approximate solution
methodology to approach large-scale Markov decision pro-
cess. SBPI improves the current policy by iteration in an
online fashion. For a known current state, a Q-factor is used
to measure the cost of taking an action in the current stage,
followed by the optimal policy in the future stages. Since
the optimal policy is not known in advance, the Q-factor is
usuallyestimatedthroughsimulation.Basedontheestimates,
the action that achieves the minimum is selected, which
completes the decision-making for the current stage. When
the system evolves to the next stage, similar procedures can
be followed. Due to the randomness in the system dynamics,
a large number of sample paths are usually required to
estimate the Q-factors. Therefore, it is of great practical
interest to study how to best utilize the given computing
budget so that the best action can be selected with high
probability.
*This work was supported in part by the National Science Foundation
of China under grants (Nos. 61174072, 61222302, 90924001, 91224008,
and U1301254), the National 111 International Collaboration Project (No.
B06002), and the TNList Funding for Cross Disciplinary Research.
1
Di Wuand Qing-Shan Jia are with Center for Intelligent and Networked
Systems (CFINS), Department of Automation, TNLIST, Tsinghua Univer-
sity, Beijing 100084, China. Email: woody10074026@gmail.com
jiaqs@tsinghua.edu.cn
2
Chun-Hung Chen is with Department of Systems Engineering &
Operations Research, George Mason University, 4400 University Drive, MS
4A6, Fairfax, VA 22030, USA. He is also with National Taiwan University.
Email: cchen9@gmu.edu
A brief literature review on the existing studies of the
computing budget allocation in SBPI will be given in section
II. In those work independent simulation of Q-factors for
different actions are usually used. In this paper we consider
how to share the sample paths to improve the probability
of correct selection. We make the following contributions.
First, we develop an algorithm to aggregate the sample paths
according to the states that are visited in the simulation.
Second, we analyze the performance of this algorithm and
show under which cases the algorithm is effective. Third,
through numerical experiments we demonstrate that our al-
gorithmcanbeeasilycombinedwithothercomputingbudget
allocation procedures such as equal allocation, Successive
Rejects, and optimal computing budget allocation (OCBA).
We also discuss the limitation of our algorithm.
The rest of this paper is organized as follows. We brießy
review the related works in section II, and mathematically
formulatetheprobleminsectionIII.Ourmethodispresented
in section IV. Numerical results can be found in section V,
and a brief conclusion is given in section VI.
II. RELATED WORKS
Markov decision process (MDP) has provided a general
framework for modeling many problems in control, decision
making, and optimization [1]. It is well known that exact
solution methods such as the traditional policy iteration and
value iteration suffer from the curse of dimensionality. Many
methods have been developed and can be classiÞed into two
groups. In the Þrst group, the methods explore the system
structure to solve the problem exactly and fast, such as
state aggregation [2], [3], time aggregation [4], [5], and
action elimination [6], [7]. In the second group, the methods
solve the problem approximately, such as neuro-dynamic
programming [8], reinforcement learning [9], approximate
dynamic programming [10], event-based optimization [11]Ð
[14], and receding horizon approach [15].
Rollout is a simulation-based policy improvement (SBPI)
method that was originally developed to tackle deterministic
optimization problems [16] and later extended to stochastic
optimization problems [17]. SBPI improves given base poli-
cies through simulation, and has been successfully applied to
many problems including the quiz problems, wireless sensor
network [18], water source management [19], and engine
maintenance problems [20]. Because simulation could be
very time-consuming and a large number of replications is
usually used in SBPI, it is of great practical interest to seek
a method that efÞciently uses the sample paths so that the
best action is selected with high probability.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3291
There have been many studies on computing budget al-
location in simulation-based optimization, such as ranking
and selection (R&S) including both the indifference-zone
formulation [21] and the subset selection formulation [22],
[23],multi-comparisonprocedure[24],hitandrun[25],[26],
COMPASS [27], and nested partitions [28], just to name
a few. Excellent reviews are available in [29]Ð[31]. In this
paper, we combine our method with two of the existing
allocation algorithms, namely Successive Rejects [32] and
OCBA [33]Ð[36].
We develop an algorithm to aggregate the sample paths
according to the states that are visited in the simulation. This
introduces correlation among the estimation of Q-factors.
By combining our algorithm with Successive Rejects and
OCBA, we achieve higher probability of correct selection
for a single stage, and better policy in the long run. This
will be demonstrated by numerical results in section IV.
III. PROBLEM FORMULATION
ConsiderastationaryandÞnite-horizonMDPwithdiscrete
state spaceS and discrete action spaceA. The objective is to
minimize the overall cost within T stages. Assume we have
a base policy. SBPI then uses this base policy to improve the
decision making in each stage. We provide more details for
a single stage in the following.
At stage k, the state s
k
becomes available. Let A =
{1;:::;n} be the set of feasible actions. The Q-factor for
each action a?A is deÞned as
Q(s
k
;a)= c(s
k
;a)+
·
s
?
?S
P(s
?
|s
k
;a)v(s
?
); (1)
where c(s
k
;a) is the deterministic immediate cost if action
a is taken at state s
k
, P(s
?
|s
k
;a) is the one-step transition
probability to s
?
if action a is taken at s
k
, and v(s
?
) is
the value function that represents the future cost under the
optimal policy, which is deÞned as
v(s
?
)= E
?
?
?
?
?
?
?
T
·
t=k+1
c(s
t
;
?
(s
t
))





s
k+1
= s
?
?
?
?
?
?
?
?
; (2)
where 
?
denotes the optimal policy. The action to be taken
under policy 
?
at state s
k
is

?
(s
k
)= argmin
a?A
Q(s
k
;a): (3)
In practice, however, the optimal policy 
?
is not known
in advance (otherwise we would have already solved the
problem and implemented policy 
?
) and we usually replace
it with a given base policy 
b
in Eq. (1). In this way, we
obtain the following equations
ö
Q(s
k
;a)= c(s
k
;a)+
·
s
?
?S
P(s
?
|s
k
;a)ö v(s
?
); (4)
where
ö v(s
?
)= E
?
?
?
?
?
?
?
T
·
t=k+1
c(s
t
;
b
(s
t
))





s
k+1
= s
?
?
?
?
?
?
?
?
: (5)
Let 
PI
denote the policy that is generated by using SBPI
and the base policy 
b
. Then we have

PI
(s
k
)= argmin
a?A
ö
Q(s
k
;a): (6)
Note that
ö
Q in Eq. (4) is an expectation, which can only be
accurately evaluated by inÞnite number of replications, i.e.,
ö
Q(s
k
;a)= lim
N
a
?°
1
N
a
N
a
·
i=1
?
?
?
?
?
?
?
c(s
k
;a)+
T
·
t=k+1
c(s
t
;
b
(s
t
)|
i
)
?
?
?
?
?
?
?
; (7)
where 
i
representstherandomnessintheithsimulation.This
may be approximated by a Þnite number of replications, i.e.,
÷
Q(s
k
;a)=
1
N
a
N
a
·
l=1
?
?
?
?
?
?
?
c(s
k
;a)+
T
·
t=k+1
c(s
t
;
b
(s
t
)|
l
)
?
?
?
?
?
?
?
; (8)
for a = 1;:::;n. Then action that is observed as the best
action is
a
1
= arg min
a=1;:::;n
÷
Q(s
k
;a): (9)
Note that a
1
may not minimize
ö
Q(s
k
;a). So we deÞne the
probability of correct selection (PCS) as
PCS = Pr
{
ö
Q(s
k
;a
1
)²
ö
Q(s
k
;a);a?A;a, a
1
}
: (10)
The problem we consider here is how to maximize PCS
under a Þxed given computing budget, i.e.,
max
N
i
;i=1;:::;n
PCS; s.t.
n
·
i=1
N
i
= N; (11)
where N is the total available computing budget.
IV. SAMPLE PATH SHARING
Conventionally, all the sample paths generated by the
existing computing budget allocation algorithms are directly
averaged to obtain estimates of Q-factors according to Eq.
(8). In this section, we present an algorithm to aggregate
the sample paths to generate better estimates of Q-factors
and therefore achieves a higher PCS. Our method can be
combined with any allocation strategy.
A. Sample path generation
There have been many studies on how to efÞciently
allocateaÞxedbudgettodifferentsolutioncandidates.Inthis
paper we consider two of them. The Þrst one is Successive
Rejects (SR), which was developed to handle the multi-
armed bandit problem. In our settings, the Þxed number
of budget corresponds to Òthe number of pullsÓ, and the
best action corresponds to Òthe best armÓ. SR eliminates
the action that is observed as the worst after each round
of simulation. The budget allocated to each action at the end
of the kth round can be calculated by
n(k)=
?
?
?
?
?
?
?
?
?
1
log(N)
N?n
n+1?k
?
?
?
?
?
?
?
?
?
?k= 1;:::;n?1; (12)
where n is the number of feasible actions and log(N)= 0:5+
·
n
k=2
1=k. Note that SR can be implemented ofßine because
it does not utilize any new information during simulation.
3292
OCBA was developed to address simulation-based opti-
mization problems in general. Its basic idea is to allocate
the computing budget so that the best solution candidate is
separated from the rest of the solution candidates with the
highest probability. A systematic introduction to OCBA is
now available in [37].
B. Estimation of reachable states
Note that when different actions are taken at s
k
, the same
state may be visited in the next stage in different sample
paths. We deÞne set of reachable states in the next stage if
action a is taken at state s as
R
a
(s)={s
?
|s
?
? S;P(s
?
|s;a) > 0}: (13)
Then the set of reachable states under different actions is
R(s)=?
a?A
R
a
(s); (14)
and Eq. (4) can be rewritten as
ö
Q(s
k
;a)= c(s
k
;a)+
·
s
?
?R(s
k
)
P(s
?
|s
k
;a)ö v(s
?
): (15)
As a result, for each s
?
? R(s
k
), we can aggregate the
sample paths that share the same s
?
to obtain a more accurate
estimate of ö v(s
?
) and use Eq. (15) to estimate
ö
Q. In practice,
we may not know R(s
k
) in advance and it must be estimated
from the given N sample paths. We denote the estimate as
÷
R
N
(s
k
), which is a subset of R(s
k
), and our method can then
be applied to
÷
R
N
(s
k
).
C. Estimation of one-step transition probabilities
In practice we may not know the one-step transition
probabilities. This can be addressed in two ways. First,
when single-step simulation is possible, we can perform
extra one-step simulation to estimate the one-step transition
probabilities. It should not be time-consuming because only
one-step simulation is required in each replication. Second,
when the states that are visted in the next stage can be stored
in the simulation, we can estimate the transition probability
directly from the existing sample paths. We focus on this
second way in the following discussion. Denote m(s
?
;a) as
the number of sample paths with s
k+1
= s
?
when action a is
taken at s
k
. The estimate of P(s
?
|s
k
;a) is
÷
P(s
?
|s
k
;a)=
m(s
?
;a)
·
s?R(s
k
)
m(s;a)
: (16)
D. Estimation of value function
Assume that we can store the states that are visited in the
nextstageduringallthesimulation. Inthiscase,eachsample
path that starts from the same state s
k+1
= s
?
and follows
the same base policy 
b
in the rest of the stages provides
an estimate of v(s
?
). The average of all such estimates then
provide an estimate of v(s
?
). Now we have
÷
Q(s
k
;a)= c(s
k
;a)+
·
s
?
?
÷
R
N
(s
k
)
÷
P(s
?
|s
k
;a)÷ v(s
?
): (17)
TheaboveproceduresaresummarizedinAlgorithm1,where
÷ v
i
(s
?
) is the estimate of v(s
?
) in the ith sample path, and B(s
?
)
is the sum of ÷ v
i
(s
?
).
Algorithm 1 Sample path sharing
1: Input: N sample paths starting from s
k
= s.
2: Step 0. Set
ö
R(s)=?;m=?;B=?.
3: Step 1.
for i= 1;:::;N do
Get the s
k+1
= s
?
and action a for the ith sample path
if s
?
<
ö
R(s)
ö
R(s)=
ö
R(s)
?
s
?
;
m(s
?
;a)= 0;
B(s
?
)= 0:
end if
m(s
?
;a)= m(s
?
;a)+1;
B(s
?
)= B(s
?
)+ ÷ v
i
(s
?
):
end for
4: Step 2.
Estimate P(s
?
|s;a) using Eq. (16).
5: Step 3.
for each s
?
?
ö
R(s) do
÷ v(s
?
)= B(s
?
)=
·
a?A
m(s
?
;a):
end for
6: Step 4. Calculate
÷
Q(s;a) using Eq. (17).
7: Output: a
1
= argmin
a=1;:::;n
÷
Q(s;a)
E. Discussion
In this subsection, we brießy discuss why and when our
method may achieve higher PCS. First we need to justify the
use of Eq. (4) rather than Eq. (8) in estimating Q-factors.
The estimation error of
ö
Q contains two parts, namely the
estimation error of P and the estimation error of ö v. In Eq.
(8) the two terms P and ö v are estimated based on the sample
paths under the same action a. In Eq. (4) the two terms
are estimated using all the sample paths that are obtained
through the simulation under all the actions. When there are
overlap among the sample paths under different actions, Eq.
(4) leads to smaller sample variance in P and ö v than that
in Eq. (8). In particular, the sample-path sharing may have
good performance under the following two conditions.
Condition 1. |R(s)|? N. Because one and only one state
in R(s) is visited in each sample path. When |R(s)| is much
smaller than N, this implies that most states in R(s) will
be visited by multiple times. This leads to more accurate
estimation of P and ö v.
Condition 2. |R(s;a) ? R(s;a
?
)| Å |R(s)| for any a;a
?
?
A. This means that there are a lot of overlap between
the reachable states under different actions. This condition
suggests that more sample paths can be aggregated to get
better estimates of P and ö v.
Note that when the above two conditions are not satisÞed,
our algorithm simply recovers the naive estimation of
ö
Q as
in Eq. (8). In order to see this, consider the extreme case in
which R(s;a)?R(s;a
?
) =? for any a;a
?
? A. This means
that the sample paths under different actions reach different
states in the next stage. So there is not any sample paths to
share. This is also demonstrated by numerical results in the
next section.
3293
V. NUMERICAL RESULTS
We demonstrate the performance of our methods by
conducting three experiments. In the Þrst experiment, we
compare the PCS of 6 algorithms (EA, SR, OCBA, EA-
Sharing,SR-SharingandOCBA-Sharing)ona10-stateMDP.
In the second experiment, we slightly modify the transi-
tion probabilities in the Þrst experiment and re-examine
the performance of our method. In the third experiment,
we compare the policies obtained from 4 algorithms (EA,
OCBA, EA-Sharing and OCBA-Sharing) based on a Þnite-
state controllable random walk.
In the Þrst two experiments, we compare the performance
of the following methods.
Method 1: Equal Allocation (EA). EA equally allocates the
computing budget among different actions.
Method 2: Successive Rejects (SR). SR successively elimi-
nates the action that is observed as the worst in each iteration
and eventually outputs the action that is observed as the best.
Method 3: OCBA. OCBA sequentially allocates the comput-
ing budget among different actions.
Method 4: EA-Sharing (EA-S). EA-S equally allocates the
computingbudgetamongdifferentactioncandidatesandthen
uses Algorithm 1 to aggregate the sample paths.
Method 5: SR-Sharing (SR-S). SR-S uses SR to allocate the
computing budget among the action candidates and then uses
Algorithm 1 to aggregate the sample paths.
Method 6: OCBA-Sharing (OCBA-S). OCBA-S uses OCBA
toallocatethecomputingbudgetamongtheactioncandidates
and then uses Algorithm 1 to aggregate the sample paths.
A. Experiment 1
Consider an MDP with 10 states as that is shown in
Fig. 1, where the curves indicate reachable states and the
numbers beside the curves represent the one-step transition
probabilities. Note that red curves stand for the probabilities
that can be controlled by taking different actions. There are
Þve actions available in stage 1, i.e., A = {1;:::;5}. One
can verify that action 1 is the best. The cost function is
c(s;a)= 0;s , 1, and c(1;a)= 1, and we want to minimize
the total cost over T = 100 stages. The base policy always
picks action 1 at state 1. Suppose the initial state is 1. We
use the six methods to allocate the computing budget when
N = 100;200;:::;1000. Their performance are estimated by
10000 replications. The parameters for OCBA is n
0
= 10 and
Æ= 10. When the exact values of the transition probabilities
are used in the methods, the results are shown in Fig. 2.
When the transition probabilities are estimated, the results
are shown in Fig. 3. We make the following remarks.
Remark 1. In Fig. 2, our algorithm signiÞcantly improves
the PCS. This is demonstrated by comparing EA with EA-
S, SR with SR-S, and OCBA with OCBA-S. Actually after
sharing the sample paths, the PCS of different methods are
close to each other.
Remark 2. In Fig. 3, due to the estimation error of the
transition probabilities, these PCSs are smaller than those in
Fig. 2. But our algorithm still improves the PCS and this
advantage increases with N.
1 2 3 10
 
(1 )/8 !
(1 )/8 !
0.5 0.5
0.5
0.5 0.5
0.5
0.5 0.5
Fig. 1. A Markov chain with 10 states.
100 200 300 400 500 600 700 800 900 1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
N
PCS
EA
SR
OCBA
EA-S
SR-S
OCBA-S
Fig. 2. PCS based on accurate transition probabilities in experiment 1.
B. Experiment 2
In the second experiment, we follow the same settings as
that in the Þrst experiment except for some slight modiÞca-
tions in the transition probabilities. The one-step transition
probability matrix now is
P(s
?
= 2i?1|s= 1;a= i) = 0:5;
P(s
?
= 2i|s= 1;a= i) = 0:5; (18)
P(s
?
|s= 1;a= i) = 0;otherwise:
Inthiscase,onlytwostatescanbereachedwhenanaction
is taken. Furthermore, the reachable states under different
actions are completely disjoint, namely R(1;a)?R(1;a
?
)=?
for a;a
?
? A and a , a
?
. In other words, when different
actions are taken, the state will always transit to different
states,andthereisnooverlapbetweendifferentsamplepaths
in terms of s
k+1
. One can verify that action 3 is the best
action. The results are summarized in Fig. 4. We make the
following remarks.
Remark 3. Fig. 4 shows that when there is not any overlap
in the reachable states under different actions, our algorithm
neither improves nor degrades PCS. It simply recovers the
case where the sample paths are not aggregated at all.
C. Experiment 3
In the third experiment, a Þnite-state controllable random
walk is used to evaluate the policies that are obtained from
4 methods (EA, OCBA, EA-S and OCBA-S). To be more
speciÞc, let S={?10;?9;:::;10} be the state space. If s=
3294
100 200 300 400 500 600 700 800 900 1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
N
PCS
EA
SR
OCBA
EA-S
SR-S
OCBA-S
Fig. 3. PCS based on estimated transition probabilities in experiment 1.
100 200 300 400 500 600 700 800 900 1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
N
PCS
EA
SR
OCBA
EA-S
SR-S
OCBA-S
Fig. 4. PCS based on estimated transition probabilities in experiment 2.
10 (or -10), the system will jump to state 9 (or -9) for sure in
the next step. For all the other states, there are three available
actions A={?1;0;1}. We have for i?S and |i| < 10,
Pr{s
?
= i+1|s= i;a= 0} = 0:5;
Pr{s
?
= i+1|s= i;a=?1} = 0:2;
Pr{s
?
= i+1|s= i;a= 1} = 0:8:
In other words, actions -1 and 1 tend to move the system
to the left and the right, respectively, while action 0 does
not have any preference but just pushing the system away
from the current state. The cost function is c(s
k
;a) = |s
k
|.
Suppose the state is at 0 in the beginning. The base policy
picks action 0 regardless of the current state. We want to
minimize the total cost over T = 100 stages. Intuitively, a
ÒsensibleÓ policy should always take action 1 for negative
states and take action -1 for positive states. This encourages
the system to stay at state 0 with high probability and leads
to a low total cost in the long run.
During each iteration, every visited state from -9 to 9 will
be given a budget of 100 (no action can be taken at state
-10 or 10). We estimate the performance of the 4 policies by
1000 replications. The policies are plotted in Fig. 5, where
the horizontal axis is the state space and the vertical axis is
the action space. The estimated total cost of the policies that
-10 -5 0 5 10
-1
0
1
state space
action space
EA
-10 -5 0 5 10
-1
0
1
state space
action space
OCBA
-10 -5 0 5 10
-1
0
1
state space
action space
EA-S
-10 -5 0 5 10
-1
0
1
state space
action space
OCBA-S
Fig. 5. Policies obtained from the 4 methods in experiment 3.
are obtained by these methods are summarized in Table I.
We make the following remarks.
TABLE I
PERFORMANCES UNDER THE 4 POLICIES IN EXPERIMENT 3.
EA OCBA EA-S OCBA-S
Total Cost 186±67 188±88 156±19 159±42
Remark 4. Table I shows that the policies that are obtained
from using EA-S and OCBA-S achieve smaller cost than EA
and OCBA. This is consistent with Fig. 5 in which both EA-
S and OCBA-S tend to attract the system around state 0.
Remark 5. The variances of the total costs of the policies
that are obtained by EA-S and OCBA-S are also smaller than
that of EA and OCBA, respectively.
Remark 6. Note that the set of reachable states from a
common state under different actions are identical. So both
aforementioned conditions are satisÞed in this problem. This
explains the good performance of our algorithm.
VI. CONCLUSION
In this paper, we consider the problem of computing
budget allocation in simulation-based policy improvement.
Different from existing methods that estimate Q-factors by
independent simulation under different action candidates, we
develop a sample path sharing procedure to aggregate the
sample paths according to the states that are visited in the
next stage. Our method can be easily combined with other
computing budget allocation procedures. Numerical results
showthatourmethodcanbecombinedwithequalallocation,
Successive Rejects, and OCBA to improve the probability
of correct selection in a single stage, and to output better
policies. We plan to improve our method to consider the
estimation error of the transition probabilities when these
values are unknown. Note that a special issue on event-based
control and optimization is upcoming [38], which includes
seven interesting papers covering various recent progress in
thisarea[39]Ð[45]. It isa futureworktoapply ourmethod to
event-based optimization [39]. Also, one can aggregate the
3295
sample paths according to the states that have been visited
in each stage and simulation as long as there are enough
memory. This may lead to a higher PCS and a better policy.
REFERENCES
[1] M. L. Puterman, Markov Decision Processes: Discrete Stochastic
Dynamic Programming. New York, NY: John Wiley and Sons, Inc.,
1994.
[2] Z. Ren and B. H. Krogh, ÒState aggregation in markov decision
processes,Ó in Proceedings of the 41st IEEE Conference on Decision
and Control, Dec. 2002, pp. 3819Ð3824.
[3] Q.-S. Jia, ÒOn state aggregation to approximate complex value func-
tions in large-scale markov decision processes,Ó IEEE Transactions on
Automatic Control, vol. 56, no. 2, pp. 333Ð344, 2011.
[4] X. R. Cao, Z. Y. Ren, S. Bhatnagar, M. Fu, and S. Marcus, ÒA
time aggregation approach to markov decision processes,Ó Automatica,
vol. 38, no. 6, pp. 929Ð943, 2002.
[5] T. Sun, Q. C. Zhao, and P. B. Luh, ÒIncremental value iteration for
time aggregated markov decision processes,Ó IEEE Transactions on
Automatic Control, vol. 52, pp. 2177Ð2182, 2007.
[6] L. Xia, Q. Zhao, and Q.-S. Jia, ÒA structure property of optimal
policies for maintenance problems with safety-critical components,Ó
IEEE Transactions on Automation Science and Engineering, vol. 5,
no. 3, pp. 519Ð531, 2008.
[7] Q.-S. Jia, ÒA structural property of optimal policies for multi-
component maintenance problems,Ó IEEE Transactions on Automation
Science and Engineering, vol. 7, no. 3, pp. 677Ð680, 2010.
[8] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming.
Belmont, MA: Athena ScientiÞc, 1996.
[9] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-
tion. Cambridge, MA: MIT Press, 1998.
[10] W. B. Powell, Approximate Dynamic Programming: Solving the Curse
of Dimensionality. Wiley-Interscience, 2007.
[11] X. R. Cao, ÒA basic formula for online policy gradient algorithms,Ó
IEEE Transactions on Automatic Control, vol. 50, no. 5, pp. 696Ð699,
May 2005.
[12] ÑÑ, Stochastic Learning and Optimization: A Sensitivity-Based Ap-
proach. New York, NY: Springer, 2007.
[13] Q.-S. Jia, ÒOn solving optimal policies for Þnite-stage event-based
optimization,ÓIEEETransactionsonAutomaticControl,vol.56,no.9,
pp. 2195Ð2200, 2011.
[14] ÑÑ, ÒOn solving event-based optimization with average reward over
inÞnite stages,Ó IEEE Transactions on Automatic Control, vol. 56,
no. 12, pp. 2912Ð2917, Dec. 2011.
[15] H. S. Chang and S. I. Marcus, ÒApproximate receding horizon ap-
proach for markov decision processes: Average reward case,Ó Journal
of Mathematical Analysis and Applications, vol. 286, no. 2, pp. 636Ð
651, 2003.
[16] D. P. Bertsekas, J. N. Tsitsiklis, and C. Wu, ÒRollout algorithms for
combinatorial optimization,Ó Heuristics, vol. 3, pp. 245Ð262, 1997.
[17] D. P. Bertsekas and D. A. Casta÷ non, ÒRollout algorithms for stochastic
scheduling problems,Ó Journal of Heuristics, vol. 5, pp. 89Ð108, 1999.
[18] Q.-S. Jia, ÒA rollout method for Þnite-stage event-based decision
processes,Ó in Proceedings of the 2010 Workshop on Discrete Event
Systems, Berlin, Germany, 2010, pp. 257Ð262, aug. 30 - Sept. 1.
[19] Y. Zhao, X. Chen, Q.-S. Jia, X. Guan, S. Zhang, and Y. Jiang,
ÒLong-termschedulingforcascadedhydroenergysystemswithannual
water consumption and release constraints,Ó IEEE Transactions on
AutomationScienceandEngineering,vol.7,no.4,pp.969Ð976,2010.
[20] Q.-S. Jia, ÒEfÞcient computing budget allocation for simulation-based
policy improvement,Ó IEEE Transactions on Automation Science and
Engineering, vol. 9, no. 2, pp. 342Ð352, Apr. 2012.
[21] R. E. Bechhofer, ÒA single-sample multiple decision procedure for
ranking means of normal populations with known variances,Ó The
Annals of Mathematical Statistics, vol. 25, pp. 16Ð39, 1954.
[22] S. S. Gupta, ÒOn a decision rule for a problem in ranking means,Ó
Ph.D. dissertation, University of North Carolina, Chapel Hill, NC,
1956.
[23] ÑÑ, ÒOn some multiple decision (ranking and selection) rules,Ó
Technometrics, vol. 7, pp. 225Ð245, 1965.
[24] C. W. Dunnett, ÒA multiple comparison procedure for comparing
several treatments with a control,Ó Journal of the American Statistical
Association, vol. 50, no. 272, pp. 1096Ð1121, Dec. 1955.
[25] R. L. Smith, ÒEfÞcient monte carlo procedures for generating points
uniformly distributed over bounded region,Ó Operations Research,
vol. 32, pp. 1296Ð1308, 1984.
[26] Z.B. Zabinsky,R. L. Smith, J. F.McDonald, H. E. Romeijn, andD. E.
Kaufman, ÒImproving hit-and-run for global optimization,Ó Journal of
Global Optimization, vol. 3, no. 2, pp. 171Ð192, 1993.
[27] L. J. Hong and B. L. Nelson, ÒDiscrete optimization via simulation
using compass,Ó Operations Research, vol. 54, no. 1, pp. 115Ð129,
2006.
[28] L. Shi and S. Olafsson, ÒNested partitions method for global optimiza-
tion,Ó Operations Research, vol. 48, no. 3, 2000.
[29] R. E. Bechhofer, T. J. Santner, and D. Goldsman, Design and Anal-
ysis of Experiments for Statistical Selection, Screening and Multiple
Comparisons. New York, NY: Wiley, 1995.
[30] J. R. Swisher, S. H. Jacobson, and E. Y¬ ucesan, ÒDiscrete-event simu-
lation optimization using ranking, selection, and multiple comparison
procedures: A survey,Ó ACM Transactions on Modeling and Computer
Simulation, vol. 13, pp. 134Ð154, 2003.
[31] S.-H. Kim and B. L. Nelson, ÒSelecting the best system: Theory and
methods,Ó in Proceedings of the 2003 Winter Simulation Conference,
S.Chick,P.J.S« anchez,D.Ferrin,andD.J.Morrice,Eds. Piscataway,
NJ: IEEE, 2003, pp. 101Ð112.
[32] J.-Y. Audibert, S. Bubeck et al., ÒBest arm identiÞcation in multi-
armed bandits,Ó COLT 2010-Proceedings, 2010.
[33] C.-H. Chen, ÒA lower bound for the correct subset-selection proba-
bility and its application to discrete event system simulations,Ó IEEE
Transactions on Automatic Control, vol. 41, pp. 1227Ð1231, 1996.
[34] H.-C. Chen, C.-H. Chen, and E. Y¬ ucesan, ÒComputing efforts allo-
cation for ordinal optimization and discrete event simulation,Ó IEEE
Transactions on Automatic Control, vol. 45, no. 5, pp. 960Ð964, May
2000.
[35] C.-H. Chen, J. Lin, E. Y¬ ucesan, and S. E. Chick, ÒSimulation budget
allocationforfurtherenhancingtheefÞciencyofordinaloptimization,Ó
Discrete Event Dynamic Systems - Theory and Applications, vol. 10,
pp. 251Ð270, 2000.
[36] C.-H. Chen and E. Y¬ ucesan, ÒAn alternative simulation budget al-
location scheme for efÞcient simulation,Ó International Journal of
Simulation and Process Modeling, vol. 1, pp. 49Ð57, 2005.
[37] C.-H. Chen and L. H. Lee, Stochastic Simulation Optimization: An
Optimal Computing Budget Allocation. Singapore: World ScientiÞc,
2010.
[38] Q.-S. Jia and K. H. Johansson, ÒGuest editorial: Event-
based control and optimization,Ó Discrete Event Dynamic
Systems - Theory and Applications, 2014. [Online]. Available:
http://dx.doi.org/10.1007/s10626-014-0181-y
[39] L. Xia, Q.-S. Jia, and X.-R. Cao, ÒA tutorial on event-based
optimization - a new optimization framework,Ó Discrete Event
Dynamic Systems - Theory and Applications, 2014. [Online].
Available: http://dx.doi.org/10.1007/s10626-013-0170-6
[40] L. Xia, ÒEvent-based optimization of admission control in
open queueing networks,Ó Discrete Event Dynamic Systems
- Theory and Applications, 2014. [Online]. Available:
http://dx.doi.org/10.1007/s10626-013-0167-1
[41] L. Li and M. Lemmon, ÒWeakly coupled event triggered output
feedback system in wireless networked control systems,Ó Discrete
Event Dynamic Systems - Theory and Applications, 2014. [Online].
Available: http://dx.doi.org/10.1007/s10626-013-0165-3
[42] Y. Sun and X. Wang, ÒStabilizing bit-rates in networked control
systems with decentralized event-triggered communication,Ó Discrete
Event Dynamic Systems - Theory and Applications, 2014. [Online].
Available: http://dx.doi.org/10.1007/s10626-013-0169-z
[43] M. C. F. Donkers, P. Tabuada, and W. P. M. H. Heemels, ÒMinimum
attention control for linear systems - a linear programming approach,Ó
Discrete Event Dynamic Systems - Theory and Applications, 2014.
[Online]. Available: http://dx.doi.org/10.1007/s10626-012-0155-x
[44] G. A. Kiener, D. Lehmann, and K. H. Johansson, ÒActuator saturation
and anti-windup compensation in event-triggered control,Ó Discrete
Event Dynamic Systems - Theory and Applications, 2014. [Online].
Available: http://dx.doi.org/10.1007/s10626-012-0151-1
[45] A. Molin and S. Hirche, ÒA bi-level approach for the design of
event-triggered control systems over a shared network,Ó Discrete
Event Dynamic Systems - Theory and Applications, 2014. [Online].
Available: http://dx.doi.org/10.1007/s10626-012-0156-9
3296
