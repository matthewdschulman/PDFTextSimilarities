  
? 
 Abstract— Autism Spectrum Disorders (ASD) impact 1 in 88 
children in the United States. The cost of ASD intervention is 
tremendous with huge individual and social consequences. In 
recent years, robotic systems have been introduced with 
considerable success for ASD intervention because of their 
potential to engage children with ASD. In this work, we present 
a novel closed-loop autonomous robotic system for imitation 
skill learning for ASD intervention. Children with ASD show 
powerful impairment in imitation, which has been associated 
with a host of neurodevelopmental and learning challenges over 
time. The presented robotic system offers dynamic, adaptive 
and autonomous interaction for learning of imitation skills with 
real-time performance evaluation and feedback. The system has 
been tested in a user study with young children with ASD and 
typically developing (TD) control sample. Further, the 
performance of the system was compared with that of a human 
therapist in the user study. The results demonstrate that the 
developed robotic system is well-tolerated by the target 
population, engaged the children with ASD more than a human 
therapist, and produced performances that were relatively 
better than that of a human therapist.    
I. INTRODUCTION 
Autism Spectrum  Disorders (ASD) are characterized by 
difficulties in social communication as well as repetitive and 
atypical patterns of behavior [1]. According to the report by 
the Centers for Disease Control and Prevention (CDC), an 
estimated 1 in 88 children and an estimated 1 out of 54 boys 
in the United States have ASD[2].  ASD is associated with 
enormous individual, familial, and social cost across the 
lifespan [3]. The cumulative ASD literature suggests earlier 
and more intensive behavioral interventions are efficacious 
for many children [4].  However, many families and service 
systems struggle to provide intensive and comprehensive 
evidence-based early intervention due to extreme resource 
limitations [5, 6]. As such, there is an urgent need for more 
efficacious treatments whose realistic application will yield 
more substantial impact on the neurodevelopmental 
trajectories of young children with ASD within resource 
strained environments. 
Robotic technology appears particularly promising for 
potential application to ASD intervention [7-14]. In this 
 
Zhi Zheng, Eric M. Young, Amy Swanson, Zachary Warren, and Nilanjan 
Sarkar are with Vanderbilt University, Nashville, TN, USA, email: 
firstname.lastname@vanderbilt.edu. Shuvajit Das is with University of 
Michigan, Ann Arbor, MI, USA, email: shudas@umich.edu. 
This study was supported in part by the National Science Foundation 
under Grants 0967170 and 1264462,the National Institute of Health under 
Grant 1R01MH091102-01A1, a grant from the Vanderbilt Kennedy Center 
(Hobbs Grant), the Marino Autism Research Institute, and a Vanderbilt 
University Innovation and Discovery in Engineering and Science (IDEAS) 
grant. Work also includes core support from NICHD (P30HD15052) and 
NCATS (UL1TR000445-06). 
 
work, we design and test a new co-robotic intervention 
platform and environment specifically designed to accelerate 
improvements in imitation skills. Children with ASD show 
powerful impairments in imitation with such deficits 
associated with a host of neurodevelopmental and learning 
challenges over time[15].  As such, a number of researchers 
propose imitation skills as critical targets for intervention 
aimed at ameliorating core features of ASD and as a 
methodology for enhanced learning outcomes across many 
other skill domains.  
Attempts to teach imitation from both highly structured 
[16, 17], as well as naturalistic responsive imitation platforms 
[18], have demonstrated the capacity for within treatment and 
generalized improvements in terms of both basic imitation as 
well as collateral social communication skills.  Unfortunately, 
these protocols have often relied on high intensity 
interventions delivered by limited expert providers across 
substantial periods of time.  In this paper, we design and test 
the value of a closed-loop robotic interaction system that could 
potentially impact similar core areas of deficit via intensive 
responsive reinforcement and specific intrinsically meaningful 
reinforcement of imitated communicative gestures.   
Several research groups have studied robot-assisted 
imitation learning for children with autism. Robins et al.[19] 
used a remotely-operated humanoid robotic doll to 
investigate imitation learning skills in children with autism. 
Kozima et al.[20] developed a creature-like robot “Keepon” 
for ASD intervention in younger children. Duquette et al. [21] 
developed a mobile robot “Tito” and  Ferrari et al. [22] built a 
non-humanoid robot “IROMEC” for imitation learning for 
children with ASD. While these and other important studies 
have established the potential benefit of robot-assisted 
imitation for ASD intervention, there exists significant 
opportunities for further contributions. In particular, most of 
these robotic systems are either remotely operated or 
open-loop systems and thus are not capable of autonomous 
adaptation to address intervention need.  The gesture analysis 
portion of these tasks has generally been performed via 
offline video coding of experimental data.  In this context, 
Fujimoto et al.[23] developed a robotic system which could 
mimic and evaluate child’s motion in real-time. However, the 
participants had to wear a sensorized long-sleeved T-shirt, 
and they reported that some children did not tolerate it. 
Feil-Seifer et al.[24], and Greczek et al.[25] introduced 
Graded Cueing to guide the robot intervention on imitation 
study for school age children with autism. In this work, 
however, we are more interested in robot intervention for 
young children of 3-5 years’ of age.  
The first contribution of the present work is to develop a 
robot-mediated learning system for imitation intervention in 
Autonomous Robot-mediated Imitation Learning for Children with 
Autism 
Zhi Zheng, Student Member, IEEE, Shuvajit Das, Eric M. Young, Amy Swanson,  
Zachary Warren, and Nilanjan Sarkar, Senior Member, IEEE 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 2707
  
young children with ASD that provides: i) dynamic, adaptive 
and autonomous interaction without the use of a Wizard of Oz 
human control system; ii) a non-invasive set-up for the 
participating child; iii) real-time performance evaluation and 
feedback; and iv) realized capability for providing detailed 
performance comparisons between robot and human 
therapist.  The second contribution is to conduct a user 
study of the system with young children with ASD and 
typically developing (TD) control sample and present 
comparative results between a robot and a human 
administrator.  
The rest of the paper is organized is as follows. Section II 
describes the system architecture, system components, 
gesture recognition and user interface. Section III presents the 
details of a user study that evaluates the system. The results 
and implications are discussed in Section IV. 
II. SYSTEM ARCHITECTURE 
The robot-mediated imitation skill learning system (Fig.1) 
consists of a humanoid robot, NAO [26],  and a Microsoft 
Kinect [27]  motion sensing module, both of which are 
integrated with a supervisory controller (SC) for real-time 
closed-loop interaction. The Kinect system is placed in front 
of the participant for gesture recognition. The SC in this case 
first instructs the robot to show a target gesture to the child.   
Once the gesture is completed, the child is asked by the robot 
to imitate the gesture. The SC continuously monitors the 
Kinect and determines whether the child’s imitative attempt 
falls within a movement band deemed as ‘sufficient’.   This 
‘sufficiency’ is modifiable to scaffold learning toward more 
optimal or complete performance of the target gesture.  Based 
on the performance, the SC may instruct the robot either to 
move towards another gesture or aid the child with 
reinforcement components and approximations of the gestures 
within their motor movements. The SC continues this 
procedure in a closed-loop manner for a specified duration of 
time and collects data to evaluate the efficacy of the trials 
towards optimal performance of specific gestures. The SC also 
instructs the robot to give appropriate rewards and 
encouragement to the child. A Graphical User Interface (GUI) 
is designed to administer the tasks, modify task parameters 
when needed, and record data from experiments.  
Supervisory Controller
Graphical User Interface
Parameter 
Control
 Procedure 
Selection
Skeleton 
Tracking Display
Robot 
Speech
Motion
Microsoft Kinect
Skeleton Tracking
Head Pose Estimation
Components 
communication
Procedure 
Control
Gesture 
Recognition
Participant 
Performance
Head Pose 
Gesture
 
Figure 1.  System architecture 
A. The Humanoid Robot 
Use of a humanoid robot affords for approximation of 
highly relevant and meaningful human behaviors with 
ultimate hope that such approximation will enhance 
generalization [19]. We choose NAO, a medium child-sized 
humanoid robot (58 cm, 4.3 kg) with 25 degrees of freedom 
(DOF) for our architecture. We explicitly acknowledge that 
NAO is not without limitations, including: it cannot control its 
eye gaze relative to its head and thus head turn is 
approximated as gaze, and it has other software and system 
limits as manufactured that necessitate augmentation for more 
flexible use.  Despite these limitations, we choose NAO 
because of ease of use, sufficient expressive power for 
intervention tasks, and above all its open architecture that 
allows relatively easy pathways for custom software 
development and integration with other devices.   
B.  Motion Sensing 
We used the Microsoft Kinect and its SDK for robust 
skeleton tracking to get joints positions in the Kinect frame 
(Fig.2 (a)). In this study, we used joint coordinates of right and 
left arm wrist, elbow, and shoulder. Skeleton data were 
processed using a Holt double exponential smoothing filter to 
avoid glitch and jitter.  
Kinect 3D face tracking functions fit a 3D convex mesh on 
participant’s head and output X, Y, Z direction rotation angle 
in Kinect frame.  The head pose was used for estimating the 
participant’s attention on the robot and the human therapist.  
Participants were seated facing the Kinect. The distance 
between participant’s head and the Kinect was approximately 
1.5m; and the distance between the participant and the robot or 
human therapist was about 2m. 
C. Gesture Selection, Mapping and Recognition 
Four gestural movements were chosen for this study: 1) 
raising one hand; 2) raising two hands; 3) waving; and 4) 
reaching arms out to the side. These four gestures were 
intentionally selected due to the low motor skill requirements 
they presented to participating children (e.g., to ensure focus 
on imitation skills rather than gross motor abilities) as well as 
to avoid motor limitations of the humanoid robot. 
To map a participant’s gesture, each frame’s skeleton data 
(30 frames per second) were transferred to the robot and 
displayed by the robot in real-time in robot mirroring a child’s 
gesture. Joints detected by Kinect were mapped to the same 
robot joints. When a joint position was out of robot’s 
workspace, the corresponding robot joint was moved to the 
nearest boundary of its work space.  
This study was designed for 3-5 years’ old children. 
Neither TD children nor children with ASD in this age range 
can, in general, repeat a specific gesture multiple times in a 
precise manner. Therefore, to get enough training data for 
statistical model-based gesture recognition methods was not 
attempted. Besides, those methods may not be appropriate for 
providing qualitative feedback regarding the quality of 
imitated gestures, which is what we want during intervention.  
As a result, we designed a model-free rule-based gesture 
recognition method that can provide fast and accurate 
recognition and qualitative performance feedback to the 
participant.  In order to capture a gesture that can vary in 
2708
  
speed among participants, we defined 5 sliding time windows 
(updated at every frame) ranging from 1s to 5s so that any 
gesture completed within 5s would be captured.  
A correct gesture is defined by a sequence of trajectory 
constrains (TC) under precondition (PC). PC describes the 
basic regional constraints of the gesture and the basic joint 
positional relations. Therefore, the gesture recognition has two 
steps: precondition estimation and trajectory grading. A 
correct gesture is { | }
gesture gesture
gesture TC ture s PC ? ? ? , 
where s is arm joints coordinates set. 
Fig.2 (b)-(d) show participant’s views considering 
movements of the right arm as an example. The solid lined 
arm indicates the current arm position; the dash lined arm 
indicates previous arm position, and dash lined light color arm 
indicates final arm position.  The figures only show angles and 
distances for the right arm for clarity.  Table I lists all the 
angles and distances in Kinect frame required for gesture 
definition and recognition. 
Y-axis
    
                    (a). Kinect frame                   (b). Gesture front view                     
    
                (c). Gesture right side view           (d). Gesture top view 
Figure 2.  Kinect frame and participant’s gesture view 
1 ang
T
,
2 ang
T
, 
3 ang
T
, 
1 up
T
, 
2 up
T
, T
x dis ?
 were chosen to be 
?
,
?
, /2 ? , 20cm, 10cm, 20cm, respectively. These values can 
be adjusted through the GUI to make the task easier or more 
difficult.  A gesture is graded positive if the skeleton 
trajectory satisfies PC, otherwise it is graded as 0. For 
gestures raising one hand and waving, a correct gesture can be 
accomplished by either a right or a left hand. For gestures 
raising two hands and reach arms out, both hands have to 
satisfy all the TCs. For gestures with one hand, if the gesture 
satisfies the first n TCs, it is graded as 10/ number of n TC ? ; 
for gesture with two hands, the grades for two hands are 
averaged as the final score. The correct gesture got a score of 
10. 
1) Waving 
?
1 2 3
{ 1 , 2 , , 3 }
Wave
ang ang y y ang
PC
A T A T W S A T
?
? ? ? ? ? ? ?
???????????
1
1
ang
AT ?? implies the gesture is started from raising the 
arm from a lower position; 
2
2
ang
AT ?? indicates wave is in 
front of the body instead of side; 
yy
WS ? indicates wrist is 
higher than shoulder;  
3
3
ang
AT ?? means the forearm remains 
relatively parallel to the XY plane.  
TABLE I.  GESTURE VARIABLES 
Symbol Definition 
SW 
Vector pointing from shoulder to wrist 
EW 
Vector pointing from elbow to wrist 
y
W 
Y coordinate of wrist joint 
y
E 
Y coordinate of elbow joint  
y
S 
Y coordinate of shoulder joint  
1 A ? 
Angle between SW and negative Y 
axis 
2 A ? 
Angle between SW and YZ plane, 
when SW in negative z direction 
(arm pointing forward) 
3 A ? 
Angle between EW and XY plane 
4 A ? 
Angle between SW and positive X 
axis for right arm, angle between SW
and negative X axis for left arm. 
5 A ? 
Angle between SW and XY plane, 
SW with positive X direction for right 
arm, and with negative X direction for 
left arm. 
WES ?
 
Angle between upper arm and forearm 
X
T 
Threshold for angle or distance X 
D X direction movement 
()
xf
relation 
Relation in parentheses should be held 
for x consecutive frames  
 
10 10
7
45
{( ) & Upward Movement>T , (E ) , 
(Only One Hand) , D>T in one direction,
D>T in both direction, & }
Wave
y y f up y y f
f dis
dis Wave Wave
TC
W E S
TC TC
?
??
 (2) 
Upward Movement>T
up
means hand upward movement is 
larger than T
up
, 
45
&
Wave Wave
TC TC means the fourth and fifth 
condition in 
Wave
TC are satisfied at the same time. Those 
constraints indicate a wave should last for a reasonable 
amount of time, with only one hand, and will be side by side 
long enough with a curve shape. 
2) Raising one hand and raising two hands 
( ) 3 2
{ 1 , 2 }
RaiseHand s ang ang
PC A T A T ? ? ? ? ?           (3) 
2709
  
()
10 1
10
10
10
10
{( ) & Upward Movement> ,
  (E & ) ,
2
3
  ( & ( 1) ) ,
46
  (Only One Hand (one hand raise gesture)) , 
  (D<T ) }
RaiseHand s
y y f up
y y f
f
f
x dis f
TC
W E T
S WES
WES A
?
??
?
?
?
?
? ? ?
? ? ?? ?
(4) 
Those constraints indicates only one hand is raised high for 
one hand raise gesture. For two hands raise gesture, both 
hands are raised high until the arms are straight out vertically. 
In both gestures, hand(s) should be held for a while. 
3) Reaching arms out to the side 
Re 3 3
2
{ 1 , 4 ,
5 , . 0,
( . 0 (right arm) or . 0 (left arm))}
achArmsOut ang ang
ang
PC A T A T
A T SW y
SW x SW x
? ? ? ? ?
? ? ?
??
    (5)  
32
45
ang ang
A T A T ? ? ? ? indicates the arm go toward 
XY plane and stretch out  approximately along X axis. The 
very last condition indicates the arms should start from a lower 
position. 
Re
1 10 2
2 10 3 10
{( 1 ) & Upward Movement> ,
  ( 4 ) ,( ) }
achRamsOut
ang f up
ang f ang f
TR
A T T
A T WES T
?
??
? ? ? ?
     (6) 
The above equation means arms should be raised from 
lower positions and from the side of the body and then 
stretched out evenly side-wise.   
In order to validate the gesture recognition algorithm, we 
performed a small pilot study.  Seven adults and 3 TD children 
participated in an experiment where each participant sat facing 
the Kinect at a distance approximately 1.5m away from it. 
Each participant made each of the 4 gestures 10 times. They 
were instructed to slightly shift their front facing postures 
between gesturing to create a naturalistic condition. As the 
participants made the gestures, the gesture recognition 
algorithm classified them into one of the four categories or a 
“not recognized” category. These recognition results were 
compared with the subjective ratings of a therapist. The 
gesture recognition algorithm was remarkably accurate (> 
98%). In the few cases where it failed, it was mostly due to the 
failing of Kinect to track the participant primarily due to shift 
in postures.  
III. USER STUDY 
Fig.3 illustrates the experiment room set-up. An 
assessment room and observation room were divided by a 
sound-proof wall and a one-way mirror for observation.  The 
assessment room consisted of a participant chair, Microsoft 
Kinect, a seat for robot/human therapist, and a projector. In 
human therapist trials, a projector displayed instructions on 
the wall behind the participant’s chair. The observation room 
allowed the system operator to initiate and observe the session 
via a centralized controller and monitoring station.  
Mcrosoft Kinect 
for Windows
Participant 
Chair
Projector
Robot/
Human 
therapist 
Chair
Centralized 
Controller
Monitoring 
station
One 
Direction 
Window & 
Projection 
Wall
 
Figure 3.  Experiment room setup 
A. Participants 
Five children with ASD (age in years m = 3.93, SD = 0.68) 
and five typically developing children (age, m = 3.88, SD = 
0.39) participated in the study.  All children in the ASD group 
had received a clinical diagnosis of ASD based on 
DSM-IV-TR criteria from a licensed psychologist, met the 
spectrum cut-off of the autism diagnostic observation 
schedule (ADOS) and had existing data regarding cognitive 
abilities in the registry.  Parents of children in the ASD and 
non-ASD group completed both the Social Communication 
Questionnaire (SCQ) and the Social Responsiveness Scale 
(SRS) to index current ASD symptoms (see Table II.) 
TABLE II.  PARTICIPANT GROUP CHARACTERISTCS 
 
ADOS 
Raw 
Score 
ADOS 
Severity 
Score 
SRS-2 
Raw 
Score 
SRS-2 T 
score 
SCQ 
Lifetime 
Total 
Score 
IQ Age 
ASD_M 20.00 7.80 97.80 78.20 17.00 70.60 3.93 
ASD_SD 7.41 2.05 31.96 13.90 7.25 26.89 0.68 
TD-M NA NA 28.40 48.60 3.80 NA 3.88 
TD-SD NA NA 11.91 5.41 2.39 NA 0.39 
 
B. Task and Protocol 
Each child participated in two human-administered sub- 
sessions and two robot-administered sub-sessions. Each 
sub-session tested two gestures. All four gestures were 
exhaustively tested in a randomized order. We compared 
participant’s performance between the robot sessions and the 
human sessions for: 1) imitation of gesture for each trial; and 
2) participant’s attention toward robot and human therapist.  
Prior to the demonstration of each gesture, the robot and 
human therapist initiated a robot/human mirroring children 
gesture part with the verbal prompt “Let’s play!  I will copy 
you!” This part lasted for 15s to get the participant involved in 
the “game”.  Next, as shown in Fig. 4, is the “child copy 
robot/human” part. Participant performance and attention data 
were collected in this part. 
In Trial1, the robot or human therapist gave the verbal 
prompt “Okay! Now you copy me. Look at what I am doing!”, 
demonstrated the gesture twice, and provided the verbal 
prompt “You do it!”  Gesture recognition was initiated 
immediately upon gesture demonstration and ended 5 seconds 
following the second full demonstration of the gesture.  If the 
child correctly imitated the gesture, the system recorded full 
2710
  
score, and provided praise. If the child did not correctly 
imitate the gesture, the system provided feedback on the 
approximation if applicable, and recorded the best score that 
the participant got. For example, for wave gesture, if the 
participant just raised and held the hand, then the robot will 
give feedback “wave side by side!”. Trial2A was the same as 
gesture Trial1 if correct gesture detected in Trial1; Or Trial2A 
gave a second prompt of the gesture, followed by the verbal 
prompt “You do it!”, and  was set up as a gesture + mirroring 
trial. Here robot mirrored participant’s motion during his/her 
response to help the participant understand what he/she was 
doing.  
Trial1 (Gesture Only)
Trial2A (Gesture Only)
Trial2A (Gesture + 
Mirroring)
Trial2B (Gesture + 2 sec 
respond)
Gesture
Correct
Gesture
Correct
Start
End
Rewards
Y N
Y
N
Rewards
 
Figure 4.  Flow of “child copy robot/human” part 
If the child demonstrated the correct gesture in Trial2A, a 
reward was provided and the session was finished. If 
unsuccessful, Trial2B was provided with another gesture 
prompt and the participant was given an extra 2 seconds 
following the robot gesture to respond. All time constraints 
discussed above could be adjusted via the GUI to make the 
tasks easier or harder. 
In the human therapist administered sub-sessions, the 
human replicated robot-administered trials. The flow controls 
were completed by the system centralized controller and 
human therapist followed instructions projected on the wall 
behind the participant.  Therefore, the human therapist could 
fully match the timing and presentation of the 
robot-administered trials, while still maintaining eye contact 
with the child seated between her and the projected 
instructions.   
Head pose is a coarse indicator of people’s attention via 
approximation of eye gaze. We used a 85.77 cm? 102.42 cm 
box around the robot and the upper body of human therapist as 
the target attention region (robot’s height is similar to human 
therapist’s upper body height).  Here we assumed that the 
participant’s head orientated to this box represented his 
attention on the robot or the human therapist. 
C. Results 
We present two sets of results from the user study to show 
the effectiveness of the presented robotic system. First, we 
analyzed direct attention towards the administrator between 
the robotic and human led trials. This is an important indicator 
since attention to the administrator is a marker for eventual 
learning and success within intervention paradigms.   Table III 
shows a group-wise comparison between ASD and TD.  
TABLE III.  ATTENTION STATISTICAL RESULTS 
Group 
Robot session Human session 
Attention on 
target time(s) 
Total 
session 
time(s) 
Ratio 
(%) 
Attention on 
target time(s) 
Total 
session 
time(s) 
Ratio 
(%) 
ASD 
M 59.24 96.48 60 41.73 98.53 42 
SD 26.94 26.91 21 24.76 28.32 20 
TD 
M 70.14 101.16 70 47.77 81.23 70 
SD 26.52 29.35 18 15.80 33.32 30 
 
The ASD group spent 18% more time attending to the robot 
as compared to the human therapist across trials, while TD 
group paid equal attention to robot and human therapist. Also 
note that the ASD group required similar amount of time to 
complete the tasks within trials across robot and human 
therapists, while TD group spent more time to complete the 
robot trials. However, note that   two-sided Wilcoxon rank 
sum test results showed that there was no statistically 
significant difference between attention paid to the robot and 
human administrator for either group (p = 0.05 for ASD and p 
= 0.71 for TD) although the ASD group was much closer to 
having a statistical difference.  
Next we examined the actual demonstrated imitation skills 
between the human and robot administrator conditions.  In 
order to evaluate participant’s performance, every gesture was 
scored along a scale from 0 to 10 based on the components of 
the target skill demonstrated allowing us to evaluate not just 
binary success (e.g., correct/incorrect), but partial success and 
approximations towards the desired target imitative skill.  
Table IV shows the group performance between ASD and TD. 
As expected, given impairments in imitation representing a 
core symptom of ASD, results indicate that TD children were 
more successful than ASD children imitating the target 
gestures across both conditions and did not demonstrate 
differences in performance between the robot and human 
conditions.  Importantly, within the ASD group, children were 
far more successful imitating the target gestures during the 
robot session than in the human session. On an individual level, 
3 out of the 5 children indicated better performance in the 
robot condition with the other two children demonstrating 
essentially equivalent baseline floor performance across trials.  
Again we noticed no statistically significant difference 
between performance in robot session and human session (p = 
0.24 for ASD and p = 0.54 for TD) although the ASD group 
was closer to having a statistical difference.    
TABLE IV.  PERFORMANCE STATISTICAL RESULTS 
 
Robot session Human session 
Participant Mean STD Mean STD 
ASD 39.62 35.23 21.86 17.37 
TD 47.49 24.98 48.03 35.42 
2711
  
IV. DISCUSSION AND CONCLUSION 
We have presented a novel autonomous closed-loop 
robotic system for imitation learning for ASD intervention. 
The system is capable of administering gestures to the 
children, assessing the imitated gestures, and providing 
feedback, all in real-time. We have performed a user study to 
evaluate the efficacy of the system with an ASD and a control 
TD group. The results show that the presented robotic system 
was more engaging than a human therapist to the ASD group, 
and they performed better with the robot. Although no 
statistical significance was observed in the results, we believe 
that with more participants, we may observe such a difference. 
However, more experiments are needed to assess the benefit 
of such robotic systems conclusively. 
Movement in this research direction introduces the 
possibility of technological intervention tools that are not 
simple response systems, but systems that are capable of 
sophisticated adaptations. Systems capable of such adaptation 
may ultimately be used to promote meaningful change related 
to the complex and important social communication 
impairments of the disorder itself. We do not propose this 
technology as a replacement for existing necessary 
comprehensive behavioral intervention and care for young 
children with ASD.  Rather, our platform represents a move 
toward realistic deployment of technology capable of 
accelerating and priming a child for learning in key areas of 
deficit across these very same intervention environments. 
With careful design, robot-led imitation intervention systems 
have great potential to draw children with ASD into 
interactions and effectively (and enjoyably) teach children 
new skills. 
REFERENCES 
[1] Diagnostic and Statistical Manual of Mental Disorders: Quick 
reference to the Diagnostic Criteria from DSM-IV-TR, Fourth ed. 
Washington D.C.: American Psychiatric Association, 2000. 
[2] "Autism Spectrum Disorders Prevalence Rate," Autism Speaks and 
Center for Disease Control (CDC)2011. 
[3] M. L. Ganz, "The lifetime distribution of the incremental societal costs 
of autism," Archives of Pediatrics and Adolescent Medicine, Am Med 
Assoc, vol. 161, pp. 343-349, 2007. 
[4] Z. Warren, M. L. McPheeters, N. Sathe, J. H. Foss-Feig, A. Glasser, 
and J. Veenstra-VanderWeele, "A systematic review of early intensive 
intervention for autism spectrum disorders," Pediatrics, vol. 127, pp. 
e1303-e1311, 2011. 
[5] Z. Warren, A. Vehorn, E. Dohrmann, C. Newsom, and J. L. Taylor, 
"Brief report: Service implementation and maternal distress 
surrounding evaluation recommendations for young children diagnosed 
with autism," Autism, 2012. 
[6] M. Al-Qabandi, J. W. Gorter, and P. Rosenbaum, "Early autism 
detection: Are we ready for routine screening?," Pediatrics, vol. 128, 
pp. e211-e217, 2011. 
[7] S. Srinivasan and A. Bhat, "The Effect of Robot-Child Interactions on 
Social Attention and Verbalization Patterns of Typically Developing 
Children and Children with Autism between 4 and 8 Years," Autism, 
vol. 3, p. 2, 2013. 
[8] R. Dorsey and A. M. Howard, "Examining the Effects of 
Technology-Based Learning on Children with Autism: A Case Study," 
presented at the 11th IEEE International Conference on Advanced 
Learning Technologies 2011. 
[9] J. Diehl, Schmitt, L., Villano, M., and Crowell, C., "The clinical use of 
robots for individuals with Autism Spectrum Disorders: A critical 
review," Research in Autism Spectrum Disorders, vol. 6, pp. 249-262, 
2011. 
[10] B. Scassellati, H. Admoni, and M. Mataric, "Robots for use in autism 
research," Annual Review of Biomedical Engineering, vol. 14, pp. 
275-294, 2012. 
[11] E. T. Bekele, U. Lahiri, A. R. Swanson, J. A. Crittendon, Z. E. Warren, 
and N. Sarkar, "A step towards developing adaptive robot-mediated 
intervention architecture (ARIA) for children with autism," Neural 
Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 
21, pp. 289-299, 2013. 
[12] Z. Zheng, L. Zhang, E. Bekele, A. Swanson, J. A. Crittendon, Z. 
Warren, and N. Sarkar, "Impact of robot-mediated interaction system 
on joint attention skills for children with autism," in IEEE... 
International Conference on Rehabilitation Robotics:[proceedings], 
2013, pp. 1-8. 
[13] C. Liu, K. Conn, N. Sarkar, and W. Stone, "Online affect detection and 
robot behavior adaptation for intervention of children with autism," 
Robotics, IEEE Transactions on, vol. 24, pp. 883-896, 2008. 
[14] C. Liu, P. Rani, and N. Sarkar, "Human-robot interaction using 
affective cues," in Robot and Human Interactive Communication, 
2006. ROMAN 2006. The 15th IEEE International Symposium on, 
2006, pp. 285-290. 
[15] J. Williams, A. Whiten, and T. Singh, "A systematic review of action 
imitation in autistic spectrum disorder.  ," Journal of Autism and 
Developmental Disorders, vol. 34, pp. 285-299, 2004. 
[16] G. Dawson and A. Adams, "Imitation and social responsiveness in 
autistic children. ," Journal of Abnormal Child Psychology, vol. 12, pp. 
209-226, 1984. 
[17] B. Ingersoll, "The social role of imitation in autism: Implications for the 
treatment of imitation deficits. ," Infants & Young Children, vol. 21, pp. 
107-119, 2008. 
[18] B. Ingersoll, "Pilot Randomized Controlled Trial of Reciprocal 
Imitation Training for Teaching Elicited and Spontaneous Imitation to 
Children with Autism. ," Journal of Autism and Developmental 
Disorders, vol. 40, 2010. 
[19] B. Robins, K. Dautenhahn, R. Boekhorst, and A. Billard, "Robotic 
assistants in therapy and education of children with autism: can a small 
humanoid robot help encourage social interaction skills?," Universal 
Access in the Information Society, vol. 4, pp. 105-120, 2005. 
[20] H. Kozima, C. Nakagawa, and Y. Yasuda, "Children–robot interaction: 
a pilot study in autism therapy," Progress in Brain Research, vol. 164, 
pp. 385-400, 2007. 
[21] A. Duquette, Michaud, F., Mercier, H., "Exploring the use of a mobile 
robot as an imitation agent with children with low-functioning autism," 
Autonomous Robots, vol. 24, pp. 147-157, 2008. 
[22] E. Ferrari, B. Robins, and K. Dautenhahn, "Therapeutic and 
educational objectives in Robot Assisted Play for children with 
autism," in Robot and Human Interactive Communication, 2009. 
RO-MAN 2009. The 18th IEEE International Symposium on, 2009, pp. 
108-114. 
[23] I. Fujimoto, T. Matsumoto, P. R. S. De Silva, M. Kobayashi, and M. 
Higashi, "Study on an assistive robot for improving imitation skill of 
children with autism," in Social Robotics, ed: Springer, 2010, pp. 
232-242. 
[24] D. J. Feil-Seifer, "Data-Driven Interaction Methods for Socially 
Assistive Robotics: Validation With Children With Autism Spectrum 
Disorders," UNIVERSITY OF SOUTHERN CALIFORNIA, 2012. 
[25] J. Greczek, A. Atrash, and M. Matari?, "A Computational Model of 
Graded Cueing: Robots Encouraging Behavior Change," in HCI 
International 2013-Posters’ Extended Abstracts, ed: Springer, 2013, 
pp. 582-586. 
[26] Aldebaran Robotics,  
 http://www.aldebaran-robotics.com/en/ 
[27] Microsoft Kinect for Windows,  
 http://www.microsoft.com/en-us/kinectforwindows/ 
 
 
2712
