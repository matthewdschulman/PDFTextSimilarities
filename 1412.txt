 
 
? 
Abstract—An adaptive synergy controller is presented which 
allows a dexterous artificial hand to unscrew and screw an object 
using facial expressions derived from electromyogram (EMG) 
signals. In preliminary experiments, the finger joint motions of 
nine human test subjects were recorded as they unscrewed a 
bottle cap in multiple orientations of their hands with respect to 
the object. These data were used to develop a set of adaptive 
sinusoidal joint synergies to approximate the orientation-
dependent human motions, which were then implemented on a 
dexterous robotic manipulator via the proposed adaptive synergy 
controller. The controller is driven through a noninvasive 
interface which allows a single input to drive the bioinspired 
human motions using facial expressions. The adaptive synergy 
controller was evaluated by four able-bodied subjects who were 
able to unscrew and screw an instrumented object using the 
artificial hand in two orientations with a 100% success rate. 
 
Index Terms— Dexterous Hand, Electromyogram (EMG), and 
Grasp Synergy 
 
I. INTRODUCTION 
espite the advances in the mechanical dexterity [1, 2] and 
sensory feedback [3, 4] of robotic hands, disabled people 
often cannot fully make use of the increased functionality of 
these manipulators. This is usually due to the lack of an 
intuitive and robust interface between the human and the 
artificial hand. Recently, the use of noninvasive brain-machine 
interfaces (BMIs) using electroencephalogram (EEG) signals 
have been investigated as a  method of control for robotic 
systems [5]. One current problem with BMIs is that the 
number of independent inputs that can be extracted to control 
an artificial hand is limited. To help simplify the problem, the 
concept of grasp synergies has been explored, where a limited 
number of control inputs are used to specify the action of a 
larger number of joints [6, 7]. Therefore, grasp synergies are 
an important consideration to reduce the cognitive burden of 
disabled people to control a dexterous artificial hand. Proper 
 
Manuscript received September 15, 2013. This research supported in part 
by the National Science Foundation award #1317952. 
B. A. Kent is currently pursuing his PhD at University of Akron, Akron, 
OH 44325 USA (phone: 330-998-4122; e-mail: bak17@ zips.uakron.edu).  
Z. M. Kakish is currently a Master of Science student with the Mechanical 
Engineering Department at the University of Akron, Akron, OH 44325 USA 
(e-mail: zmk5@zips.uakron.edu) 
N. Karnati, has recently completed the Master of Science degree from 
Mechanical Department at the University of Akron, Akron, OH 44325 USA 
(e-mail: nk29@zips.uakron.edu). 
E. D. Engeberg has joint appointments with the Mechanical and 
Biomedical Engineering Departments at the University of Akron, Akron, OH 
44325 USA, (e-mail: engeberg@uakron.edu). 
use of grasp synergies can enable people to control very 
dexterous motions of artificial hands with a single input.  
Synergistic control strategies exist in various forms, i.e. 
postural [8], force [9], and temporal [10]. The latter of these 
three types, temporal, is particularly relevant to the current 
work. Underlying this approach is the idea that the CNS 
utilizes temporal coordination between muscle groups to 
achieve a given task [11]. This idea enables a viable solution 
to a serious problem for quadriplegic victims, which is that the 
information throughput to the artificial hand from the operator 
is quite small regardless of the control interface that is used. 
Often, BMIs for disabled people make use of the fact that 
the power in different frequency bands of EEG can increase or 
decrease before, during and after real and imagined hand 
movements; these are called event related synchronization 
(ERS) and event related desynchronization (ERD) [12, 13]. 
There are a number of problems associated with BMIs that 
use ERD and ERS to assess the intent of the operator. For 
example, they are susceptible to external visual stimuli, 
motion artifacts, and blinking, requiring advanced signal 
processing techniques to function [14]. The successful signal 
classification rates are often in the 70%-90% range and there 
is a great deal of inter-subject variability; some users cannot 
generate the necessary ERD and ERS signals to control BMIs 
[15]. The training time to become a proficient operator can 
also be substantial and the time delays in the BMIs required to 
adequately filter the low amplitude EEG signals can be 
lengthy. 
In the present work, an adaptive synergy controller is 
proposed and applied to a dexterous robotic hand to unscrew 
and screw objects in multiple orientations. The synergy 
controller is designed to require only a single input from the 
operator, which is obtained from EMG signals corresponding 
to the facial expressions of the subjects. This enables a high 
successful classification rate from the subjects and a very short 
training time required to learn to unscrew and screw objects 
with an artificial hand in different orientations. Thus, the 
prime application for this technique would be to assist 
quadriplegic victims in their daily tasks of life. The use of 
EMG also provides a robust and speedy classification system, 
which is a problem with BMIs that rely on EEG [15].  
To develop this synergy controller, the hand motions of 
nine human test subjects were recorded while unscrewing a 
bottle cap in multiple orientations during preliminary 
experiments. These motions were used to derive variable-
amplitude sinusoidal joint synergies to approximate the human 
motions depending upon the orientation between the hand and 
object (Fig. 1(a)) (Section III) [1]. From these, the proposed 
Adaptive Synergy Control of a Dexterous Artificial Hand to Rotate 
Objects in Multiple Orientations Via EMG Facial Recognition 
Benjamin A. Kent, Zahi M. Kakish, Nareen Karnati, and Erik D. Engeberg 
D 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 6719
 
 
             
                                 (a)                                                              (b) 
Fig. 2. (a) The CyberGlove II has 22 sensors to measure the motions of 
human hands. Axes of rotation are visualized as black arrows. Axes of 
rotation perpendicular to the page are designated by an (X). (b) The 
kinematic diagram of the thumb and first finger of the Shadow Hand.  
 
Fig. 1.  Top level block diagram of the Adaptive Synergy Controller. (a) In preliminary work, a set of orientation-dependent sinusoidal joint approximations 
were developed from observed human tendencies (b) The Adaptive Human Model controller utilizes the previously derived synergies and biological control 
signal E to determine the vector of desired sinusoidal joint angles (x D) for the Shadow Hand. (c) The human adaptive synergies were implemented on the C6M 
Dexterous Shadow Hand via a PID sliding mode controller. (d) A headset is used to create a biological control signal used to drive the screwing and unscrewing 
synergies based on the facial expressions of test subjects. (e) The proposed controller was evaluated by four able-bodied subjects who used the Shadow Hand to 
screw and unscrew an instrumented object in multiple orientations. 
Adaptive Synergy Controller was developed (Fig. 1(b)) and 
then implemented on the C6M Dexterous Shadow Hand 
(Section IV) via a robustly stable PID sliding mode controller 
(Fig. 1(c)). Operation of the Adaptive Synergy Controller is 
achieved via a non-invasive facial expression classifier using 
EMG signals measured bilaterally from the scalp (Fig. 1(d)). 
The current study expands upon previous work [1] by 
considering multiple orientations, and in addition investigates 
two separate methods of facial expression signal processing 
within the proposed adaptive control framework. Sections V 
and VI describe the experimental methods and results, as the 
timed task was repeated with the Shadow Hand using the 
adaptive synergy controller to unscrew and screw an 
instrumented object in multiple orientations (Fig. 1(e)).  
II. EQUIPMENT 
A. CyberGlove II 
The CyberGlove II (CyberGlove Systems, Inc.) records 
joint angle data of the human hand in real time. While the 
glove itself contains 22 sensors, only those associated with the 
first finger and thumb are used in the current work. The first 
finger of the CyberGlove contains four sensors, two of which 
record joint angle data for distal interphalangeal (DIP) and 
proximal interphalangeal (PIP) joints (FJ1a and FJ1b). The 
remaining two measure the extension/flexion (FJ2) and 
abduction/adduction (FJ3) of the metacarpophalangeal (MCP) 
joint. For the thumb, TJ1 and TJ2 measure the 
flexion/extension of the DIP and MCP, respectively. Sensors 
TJ3 and TJ4 of the CyberGlove respectively record the 
abduction and circumduction angles of the carpometacarpal 
(CMC) joint of the thumb (Fig. 2(a)). This joint convention 
was chosen to correlate with the kinematic model of the C6M 
Dexterous Shadow Hand (Fig. 2(b)). 
B. The C6M Dexterous Shadow Hand 
The C6M is a dexterous underactuated tendon-driven 
anthropomorphic robotic hand with 24 joints and 20 DOFs. 
However, in this paper only eight joints are used from the first 
finger and thumb (Fig. 2(b)). A system model for the Shadow 
Hand can be obtained through a summation of torques: 
 
              . 
 
The effective inertia (I   
   
), stiffness (   
   
) and 
damping (   
   
) are subject to change as the system 
establishes and loses contact with the environment. The 
angular position, velocity, and acceleration are x            . 
    
 
 are the torques supplied by the motors and    
 
 
are the unknown disturbance torques.The orientation of the 
Shadow Hand (?) was ascertained from a three axis 
accelerometer mounted on the back of the palm. 
III. HUMAN FINGER JOINT SYNERGIES 
A. Experimental Methods: Human Experiments 
In preliminary experiments, nine human test subjects were 
asked to unscrew a bottle cap with their thumb and first finger 
while wearing the CyberGlove II to record the motions (Fig. 
(1) 
6720
 
 
 
(a) 
 
(b) 
Fig. 3. (a) Recorded first finger joint angle data when the orientation angle 
? = 0rad. Presented data represents all nine test subjects. (b) Recorded 
thumb joint angle data in the ? = π/2rad orientation. 
 
Fig. 4. The PCA data from each test subject averaged together at five 
different values of ? shows that the orientation angle, ?, significantly affects 
the motor control strategy of the test subjects. Larger markers indicate the 
average percent contribution to the motion at each ? value, while the smaller 
markers with dashed lines denote the best-fit line approximation for the 
respective joint. The r-squared value for each linear approximation is given 
in the legend. 
1(a)). All subjects participated with informed consent in 
accordance with IRB protocol. Each test subject was first 
fitted with a brace to immobilize the wrist during testing. Prior 
to data acquisition, the forearm of each subject was secured in 
place and a bottle was placed at a measured orientation angle 
(?) relative to the wrist. Here, the orientation angle is defined 
as the angle between the axis of rotation of the bottle cap and 
the longitudinal axis of the forearm that runs from the elbow 
to the hand (Fig. 1(a)). Each subject was then asked to 
unscrew the bottle cap with their first finger and thumb while 
finger joint angle data were recorded. This procedure was 
performed five times by each participant as the orientation 
angle (?) between the bottle and the wrist was varied from 0 to 
π/2rad. Experimental data were recorded at orientation angles 
of 0, π/8, π/4, 3π/8, and π/2rad for each test subject. 
B. Results: Joint Space Analysis 
All joint angle data were first filtered and normalized with 
respect to time. Contour plots from a single trial for each 
subject were constructed for each joint while in the ? = 0rad 
(Fig. 3(a)) and ? = π/2rad (Fig. 3(b)) orientations. These plots 
show all joint angles for each test subject with respect to 
normalized time as the unscrewing task was performed. As 
observed for each trial, the relative amplitude of motion is 
comparable between subjects, and the frequency of motion for 
each joint remains largely constant for each trial. As with prior 
experiments, the joint angle motions of the test subjects 
strongly resemble sinusoids (Fig. 3) [16]. This sinusoidal trend 
was consistent with all the data for all values of ?. 
Next, a principal component analysis (PCA) was performed 
to determine the impact of each joint on the unscrewing 
motion with respect to the orientation angle, ?. A PCA on the 
thumb and first finger joints shows a very similar approach 
taken by each test subject, as in [16].  However, as the 
orientation of the bottle varied from 0 to π/2rad, the amount of 
contribution that each joint had upon the overall motion of the 
synergy varied substantially. The contribution of each joint 
(i.e. the percentage of variance in the data attributed to that 
joint in each trial) obtained by the PCA was averaged across 
all subjects at five different values of ? while using the thumb 
and first finger to unscrew the bottle cap (Fig. 4). Joints FJ1b, 
FJ2, and TJ1 contribute strongly to the motion, while the 
influence of joint TJ2 was less significant. Joint TJ3 (the 
abduction joint) of the thumb had a stronger impact upon the 
motion with large values of ? while FJ2, and FJ1b (the 
extension joints) were more influential with small values of ?. 
From Fig. 4, it can be seen that the percentage contribution 
of some joints exhibit a nearly linear relationship with varying 
values of ?. A best fit line was determined for the percentage 
contribution of each joint at each recorded orientation angle 
resulting in five data points per set, and the r-squared values 
were calculated (Fig. 4). A strong correlation is seen for joints 
FJ1b, TJ3, and TJ4. Lower r-squared values were obtained for 
joints TJ2 and FJ3, however these remain relatively constant 
for all ? values and do not contribute strongly to the variance 
in the motion. These results indicate that the test subjects used 
different motor control strategies to unscrew the object in 
different orientations (?) of the hand with respect to the bottle. 
From the PCA data, a set of sinusoidal trajectories were 
developed to approximate the recorded human motions. This 
was accomplished by approximating the motion of each joint 
as a single sine wave with an orientation-dependent amplitude: 
 
 
 
 
  
 
           
 
   
 
   . 
 
In (2), the desired sinusoidal approximation of joint k ( 
 
 
) 
is determined by appropriate choice of the amplitude ( 
 
   ), 
phase offset ( 
 
) and position offset ( 
 
   ). The relative 
speed of each approximation is determined by the frequency 
( ). For more information on this process, see [1]. To 
(2) 
6721
 
 
(5) 
(6) 
(7) 
(4) 
(3) 
 
Fig. 5. Joint Space Error Analysis between CyberGlove data and desired 
sinusoidal approximations (2) for the first finger and thumb. 
facilitate the versatile application of this adaptive human grasp 
strategy with a dexterous robotic hand, a set of linear 
equations were developed to scale the amplitudes of the 
sinusoids (A
k
) and offsets (b
k
) as a function of the orientation 
angle ? (TABLE I), described further in Section IV. The 
sinusoidal approximations produce fingertip trajectories in the 
Cartesian space that are periodic on 2π.  
Previous work has shown that the same set of sinusoidal 
trajectories can be used to produce both unscrewing and 
screwing motions at the fingertips [1]. This is achieved by 
reversing the direction of the time vector t in (2) used to drive 
the sinusoids. This produces the same Cartesian trajectories of 
the fingertips, but mirrored in time. An increasing time vector 
produces unscrewing motions, while a decreasing time vector 
produces screwing motions. This feature is used in the current 
work to impart both directions of motion to an object in 
multiple orientations. 
A joint space error analysis was performed to determine 
how well the approximated sinusoids represented the actual 
recorded human joint motions for the first finger. Two cycles 
of each trial, normalized with respect to time, were extracted 
from each trial for comparison with the sinusoidal 
approximations, and the absolute relative error was calculated 
(Fig. 5). Across all orientations for all joints, the average error 
in joint space remains less than 0.1rad. This suggests that the 
derived sinusoidal approximations (TABLE I) accurately reflect 
the differing manipulation strategies adopted by the human 
test subjects as the orientation angle ? varied. This adaptive 
sinusoidal synergy approximation was next applied to the 
Shadow Hand (see [1] for more details on this process). 
IV. SHADOW HAND ADAPTIVE SYNERGY CONTROLLER 
A. Adaptive Synergy Controller 
The desired position (x
D
) for the adaptive synergy controller 
is of the form 
 
 
 
            
 
where        
nxn
 is an adaptive diagonal matrix 
. 
      
 
 
     
   
   
 
   
 , 
and u    
n
 is a vector of sinusoids: 
 
           
 
           
 
  
T
. 
 
E is a scalar input based on measured EMG signals used to 
drive the adaptive synergy controller, and is explained in 
further detail subsequently.   is an arbitrary scalar value used 
to scale the frequency of the sinusoidal joint motions relative 
to the input, E. b    
n
 is a vector of joint angle offsets: 
 
       
 
      
 
    
T
. 
 
The phase shift,  
 
  and joint angle offset,  
 
     for any joint 
k, are determined from the observations of the human data and 
are included in TABLE I. The controller for the Shadow Hand 
can be visualized as in Fig. 1. The input to the adaptive 
synergy controller, E in (5), can be thought of as a “time” 
vector which temporally synchronizes the cyclic motions of 
the joints which are periodic on 2π. 
B. PID Sliding Mode Controller 
To facilitate sliding mode control, an error state vector is 
defined as    
 
  . The PID sliding mode control law for 
the Shadow Hand is written as  
 
 
 
        
 
      
 
   
 
   ,  
 
where  
 
   
nxn
 is the voltage input vector to the motors (Fig. 
1). C     
nxn
 is a diagonal matrix that is chosen as an upper 
bound estimate on the motor voltages required to overcome 
the torques applied to the joints of the Shadow Hand that are 
involved in the synergies. K
I
    
nxn
, K
P 
   
nxn
, and K
D
    
nxn
 
are the diagonal integral, proportional and derivative gain 
matrices, respectively. The sat function partially linearizes the 
control law to alleviate the chattering phenomenon that is 
common with mechanical systems that use a fully nonlinear 
control law associated with the signum function [17]. 
V. METHODS: SHADOW HAND 
A. Biological Signal Measurement 
EMG signals were recorded using the Emotiv Systems, Inc. 
(San Fransisco, USA) EPOC wireless headset. The EPOC 
headset contains 16 sensors for measuring EMG and EEG 
signals and has a 128Hz sampling frequency. The locations of 
the sensors correspond to the 10-20 system [14]. Using the 
EPOC headset, two separate methods of control with the 
recorded EMG signals were developed for use with the 
proposed Adaptive Synergy Controller. Both methods utilize 
the Emotiv EPOC software development kit (SDK), which 
recognizes different facial expressions using EMG signals. 
TABLE I: SINE WAVE SCALING PARAMETERS (RAD) 
Joint 
Phase Offset 
? k 
Position Offset  
b k(?) 
Amplitude 
A k(?) 
x F1 0.800 0.795 0.520 – 0.07? 
x F2 0.000 0.370 0.405 – 0.16? 
x F3 1.571 0.000 0.000 - 0.20? 
x 1 0.305 0.390 0.200 - 0.15? 
x 2 0.000 0.140 0.250 – 0.05? 
x 3 1.070 0.698+0.111? 0.050 + 0.20? 
x 4 -0.000 0.349+0.055? 0.000 
 
6722
 
 
 
Fig. 6. (a) The EPOC system converts registered facial expressions into a continuous, normalized output signal (Q) relative to the intensity of the expression. A 
threshold classifier is implemented on the output signal Q to convert to the signal to a tertiary on-off-on signal (Q N). This promotes smoother operation of the 
Adaptive Synergy Controller. The resulting signal Q N is then used to drive the two separate mapping methods (b) The Continuous mapping continuously 
increments or decrements the value of the input E (5) while Q N is nonzero. (c) The Interval mapping requires intermittent expression and relaxation of the face to 
properly execute the desired motions. This method provides more direct control but requires more cognitive effort to operate effectively.  
 
Fig. 7. (a) Pseudocode representation of the Continuous mapping method, 
which requires a sustained expression to produce rotational motion of an 
object. (b) Pseudocode representation of the Interval mapping method. The 
control logic considers the synergy in two halves, requiring a cyclic 
expression and release to drive the controller. 
The EPOC software outputs a normalized signal proportional 
to the intensity of each registered facial expression.  
To implement the proposed controller on the Shadow Hand, 
a LabVIEW VI was developed allowing the users’ facial 
expressions to read by the EPOC SDK as a means of control. 
For bilateral expressions, the sign of the output is determined 
by which side of the face it is recognized on (i.e., a left smirk 
produces a negative signal, while a right smirk produces a 
positive signal). This property allows the creation of a 
proportional bipolar signal,   (Fig. 6(a)), which was used to 
drive the Shadow Hand. This bipolar signal was passed 
through a threshold classifier to convert the proportional 
signal to a three state on-off-on output ( 
 
         ). The 
resulting signal was then used as the input to two separate 
mapping methods (Fig. 6(b,c)), which impart different types of 
control. In the current work, smiling is utilized in a bilateral 
manner to drive the Adaptive Synergy Controller. A smirk on 
the right side of the face produces unscrewing, while a smirk 
on the left side of the face produces screwing motions. 
B. Method One – Continuous Mapping 
The first mapping method considered herein is termed 
Continuous mapping. In this approach, the input E functions 
as a monotonically increasing or decreasing vector used to 
drive the sinusoidal joint approximations (3), (5). E is 
initialized to zero when the controller is started. While Q
N
 
remains zero, the ‘time’ vector E remains at its previous value. 
When an expression is recognized (Q
N
 = -1, 1), the input E is 
incremented or decremented according to the sign of the 
expression output Q
N
 (Fig. 6(b)). This technique is presented 
in the form of pseudocode in Fig. 7(a). The Continuous 
mapping method requires little cognitive effort to properly 
operate, but requires a constant input signal to produce 
rotation of the object. 
C. Method Two – Interval Mapping 
The second mapping method builds upon the first method. 
In this approach, the control input E is defined piecewise 
linearly in halves. For both screwing and unscrewing motions, 
the first half of the synergy (E = 0 ? ±π) is executed when the 
appropriate facial expression is recognized (Q
N
 = -1, 1). The 
second half of the synergy (E = ±π ? ±2π) is then performed 
autonomously once the expression is released and the face 
returns to neutral (Fig. 6(c)). Upon completion of the second 
half of the synergy, the control input E is reset to zero via a 
modulo operation. This relationship is described 
mathematically by the pseudocode representation in Fig. 7(b). 
The result of this mapping is that the operator rhythmically 
generates and releases the appropriate facial expression to 
produce the desired rotational motion of the object. This 
technique requires more cognitive effort from the operator, but 
does not require constant expression of the face to produce 
rotation. This offers more direct control to the operator and 
possibly reduces the effect of fatigue relative to the first 
method, which requires a sustained expression to operate. 
D. Experimental Methods – Human Evaluation 
Both mapping methods were evaluated by four able-bodied 
test subjects, who participated with informed consent in 
accordance with IRB protocol. Each participant underwent 
two separate testing sessions. Each participant was fitted with 
the EPOC headset, and the threshold values (?, Fig. 6) were 
tuned for each user. After calibration, the subjects underwent a 
five minute training period to familiarize themselves with the 
6723
 
 
 
Fig. 8. Example trial of the Shadow Hand under the Continuous mapping 
while in Orientation 1. A sustained expression increments or decrements the 
‘time’ vector E according the sign of the expression output Q N. In this 
orientation, abduction/adduction of the first finger (x F3) is not required to 
produce rotational motion. The flexion/extension joints of the finger (x F1, 
x F2) play a larger role in this orientation and are responsible for screwing and 
unscrewing the potentiometer.  
 
Fig. 9. Example trial of the Shadow Hand under the Interval mapping while 
in Orientation 2. Under this mapping scheme, the operator is required to 
intermittently express and relax the face to produce rotational motion of the 
object. Unlike Orientation 1, abduction/adduction of the first finger (x F3) is 
required to produce rotational motion of the potentiometer, while the 
flexion/extension joints (x F1, x F2) play a diminished role in the task. 
 
Fig. 10. Extracted photo sequence of the Shadow Hand testing while in 
Orientation 1 (top) and Orientation 2 (bottom). 
system. During the first testing session, the Shadow Hand was 
mounted in the ? = 0rad orientation (Orientation 1) (Fig. 1(e)), 
with a potentiometer mounted to a cylinder to measure the 
angular position (?) of rotation. Each participant underwent 
five trials with each mapping method, resulting in 10 trials per 
subject per session. Each trial consisted of screwing and 
unscrewing the potentiometer across its range of motion 
(~1.5πrad).  During the second session, the Shadow Hand was 
mounted in the ? = π/2rad orientation (Orientation 2) and the 
above process was repeated (Fig. 1(e)).  
After data acquisition, the time to complete the individual 
screwing and unscrewing motions were tabulated for each 
trial. The average completion time data for each subject were 
evaluated using a two factor ANOVA test for the screwing 
and unscrewing motions separately. The ANOVA test 
evaluated the influence of orientation (factor A) and mapping 
method (factor B) in the performance of the controller. 
VI. RESULTS: SHADOW HAND 
An individual trial from the Continuous and Interval 
mapping methods in varying orientations are presented in 
Figs. 8 and 9, respectively. As can be seen in Fig. 8, a 
sustained expression increments or decrements the input E, 
producing rotation of the potentiometer. In contrast, the 
Interval mapping method (Fig. 9) requires intermittent 
expression and relaxation to execute a full cycle of the 
sinusoids. The orientation-dependent nature of the task is also 
apparent via comparison of the recorded joint angles in the 
bottom graph of each figure. While in Orientation 1, rotation 
of the object is performed primarily by the flexion/extension 
joints, with finger abduction (x
F3
) playing a minor role (Fig. 8, 
bottom). This is reversed in Orientation 2, where finger 
abduction is required to produce rotational motion and the 
flexion/extension joints play a lesser role in the task (Fig. 9, 
bottom). An extracted photo sequence of the Shadow Hand 
performing the task in each orientation is presented in Fig. 10. 
The completion time results of the Adaptive Synergy 
Controller evaluation are presented in Fig. 11. Results show 
that the Continuous mapping produced faster completion times 
for both orientations and motions (screwing, unscrewing). On 
average, the participants completed the tasks (both screwing 
and unscrewing) in 10.28s and 9.72s under the Continuous 
mapping method while in Orientation 1 and 2, respectively. 
Completion rates for the Interval mapping were slower, with 
mean times of 28.47s and 20.04s in Orientations 1 and 2, 
respectively (Fig. 11). The test subjects were able to complete 
all trials successfully. 
The results of the performed ANOVA analyses are 
presented in TABLE II. The Continuous mapping was 
statistically faster than the Interval mapping method for both 
the unscrewing and screwing motions, indicating that the 
mapping method (factor B) proved a significant factor in 
completion time for both motions (p
B
 < 0.001). For the 
screwing motion, the orientation (factor A) was a significant 
factor in the performance of the controller, while the opposite 
was true for the unscrewing motion (TABLE II). This 
discrepancy may be caused by the compliance of the 
manipulator as the fingertip comes into contact with the 
surface of the potentiometer. In Orientation 1, the unscrewing 
and screwing motions require ‘pulling’ and ‘pushing’ motions 
during contact, respectively. This is not the case in Orientation 
2, where the nature of the contact is similar between motions 
(contact is first established via finger flexion, followed by 
rotation of the object via ab/adduction of the finger) (Fig. 10). 
6724
 
 
 
(a) 
 
(b) 
Fig. 11. Average completion times and standard deviations per subject with 
each mapping method (Continuous, Interval) for the screwing (SC) and 
unscrewing (US) motions. (a) In Orientation 1 (? = 0rad) and (b) Orientation 
2 (? = π/2rad). The Continuous mapping method produced faster times in 
general, but all trials were successfully completed.   
This is further supported by the significant interaction factor 
between orientation and mapping (p
A/B 
= 0.0103) in the 
screwing task, while the interaction factor did not prove 
significant for the unscrewing task (p
A/B 
= 0.4023) (TABLE II). 
VII. CONCLUSION 
An adaptive synergy controller was presented which uses 
EMG signals from facial expressions to unscrew and screw an 
object in multiple orientations with a dexterous robotic hand. 
Two separate facial expression mapping methods were 
considered, with the Continuous mapping producing 
statistically faster completion times. This bioinspired control 
architecture can also be adapted for use with other temporal 
synergies to generate other complex motions or complete 
additional tasks via a single input. Such an approach can be 
realized via other forms of parametric approximations of 
finger joint motions that are coordinated via a shared time 
vector [18]. This level of model-based adaptive control is very 
important to restore lost functionality due to the difficulty in 
obtaining many independent control channels from paralyzed 
individuals. As demonstrated in this paper, a single input can 
be used to perform a task in multiple orientations by using an 
adaptive synergy controller. EMG-based facial recognition 
was used as the input to this adaptive controller because it 
enabled all test subjects to complete each task 100% of time. 
This approach is more robust to motion artifacts such as 
blinking and requires less advanced processing techniques for 
reliable operation than EEG-based BMIs; however eventual 
fatigue of the facial muscles is a possible drawback. Future 
work involves development of a higher level controller, 
integrating other temporal synergies within a finite state 
machine-like architecture to enable the user to select between 
multiple tasks or functions to be performed. 
REFERENCES 
[1] N. Karnati, B. A. Kent, and E. D. Engeberg, "Bioinspired sinusoidal 
finger joint synergies for a dexterous robotic hand to screw and unscrew 
objects with different diameters," IEEE/ASME Trans Mechatron, vol. 
18, pp. 612-623, 2013. 
[2] A. Deshpande, Z. Xu, M. Weghe, B. Brown, J. Ko, L. Chang, D. 
Wilkinson, S. Bidic, and Y. Matsuoka, "Mechanisms of the anatomically 
correct testbed hand," IEEE/ASME Trans Mechatron, vol. 18, pp. 238-
250, 2011. 
[3] J. Fishel and G. Loeb, "Bayesian exploration for intelligent identification 
of textures," Frontiers in Neurorobotics, vol. 6, pp. 1-20, 2012. 
[4] K. Horch, S. Meek, T. Taylor, and D. Hutchinson, "Object 
discrimination with an artificial hand using electrical stimulation of 
peripheral tactile and proprioceptive pathways with intrafascicular 
electrodes," IEEE Trans Neural Sys and Rehab Eng, vol. 19, pp. 483-
489, 2011. 
[5] A. Ferreira, W. Celeste, F. Cheein, T. Bastos-Filho, M. Filho, and R. 
Carelli, "Human-machine interfaces based on emg and eeg applied to 
robotic systems," J NeuroEng and Rehab, vol. 5, pp. 1-15, 2008. 
[6] R. Vinjamuri, M. Sun, C. Chang, H. Lee, R. Sclabassi, and Z. Mao, 
"Dimensionality reduction in control and coordination of the human 
hand," IEEE Trans Biomed Eng, vol. 57, pp. 284-295, 2010. 
[7] J. Shim, M. Latash, and V. Zatsiorsky, "Prehension synergies: Trial-to-
trial variability and hierarchical organization of stable performance," 
Exp Brain Res, vol. 152, pp. 173-184, 2003. 
[8] M. Ciocarlie and P. Allen, "Hand posture subspaces for dexterous 
robotic grasping," Int J Robotics Res, vol. 28, pp. 851-867, 2009. 
[9] M. Santello and J. Soechting, "Force synergies for multifingered 
grasping," Exp Brain Res, vol. 133, pp. 457-467, 2000. 
[10] R. Vinjamuri, S. Mingui, C. Cheng-Chun, L. Heung-No, R. J. Sclabassi, 
and M. Zhi-Hong, "Temporal postural synergies of the hand in rapid 
grasping tasks," IEEE Trans Info Tech in Biomedicine, vol. 14, pp. 986-
994, 2010. 
[11] A. d'Avella, P. Saltiel, and E. Bizzi, "Combinations of muscle synergies 
in the construction of a natural motor behavior," Nature Neuroscience, 
vol. 6, pp. 300-308, 2003. 
[12] R. Salmelin and R. Hari, "Spatiotemporal characteristics of sensorimotor 
neuromagnetic rhythms related to thumb movement," Neuroscience, vol. 
60, pp. 537-550, 1994. 
[13] G. Pfurtscheller and C. Neuper, "Event-related synchronization of mu 
rhythm in the eeg over the cortical hand area in man," Neuroscience 
Letters, vol. 174, pp. 93-96, 1994. 
[14] S. Sanei and J. Chambers, Eeg signal processing. West Sussex, England: 
Wiley, 2007. 
[15] L. Bi, X. Fan, and Y. Liu, "Eeg-based brain-controlled mobile robots: A 
survey," IEEE Trans Human-Machine Sys, vol. 43, pp. 161-176, 2013. 
[16] N. Karnati, B. Kent, and E. Engeberg, "Backdrivable periodic finger 
joint synergies: Human observations applied to a dexterous robotic 
hand," in IEEE Int Conf Robotics and Biomimetics, Phuket Island, 
Thailand, 2011. 
[17] E. D. Engeberg, "A physiological basis for control of a prosthetic hand," 
Biomed Sig Proc and Control, vol. 8, pp. 6-15, 2013. 
[18] B. A. Kent, J. Lavery, and E. D. Engeberg, "Anthropomorphic control of 
a dexterous artificial hand via task dependent temporally synchronized 
synergies," J Bionic Eng, 2014, to be published. 
TABLE II: ANOVA ANALYSIS - P VALUES 
Motion 
Factor 
A B A/B 
Screwing 0.0034 < 0.0001 0.0103 
Unscrewing 0.4074 < 0.0001 0.4023 
 
6725
