A Semi-autonomous UAV Platform for Indoor Remote Operation
with Visual and Haptic Feedback
Paolo Stegagno, Massimo Basile, Heinrich H. Bülthoff, and Antonio Franchi
Abstract—Wepresentthedevelopmentofasemi-autonomous
quadrotor UAV platform for indoor teleoperation using RGB-
D technology as exceroceptive sensor. The platform integrates
IMU and Dense Visual Odometry pose estimation in order
to stabilize the UAV velocity and track the desired velocity
commanded by a remote operator though an haptic inter-
face. While being commanded, the quadrotor autonomously
performs a persistent pan-scanning of the surrounding area
in order to extend the intrinsically limited ﬁeld of view.
The RGB-D sensor is used also for collision-safe navigation
using a probabilistically updated local obstacle map. In the
operator visual feedback, pan-scanning movement is real time
compensatedbyanIMU-basedadaptiveﬁlteringalgorithmthat
lets the operator perform the drive experience in a oscillation-
free frame. An additional sensory channel for the operator is
providedby the haptic feedback, which is based on the obstacle
map and velocity tracking error in order to convey information
about the environment and quadrotor state. The effectiveness
oftheplatformisvalidatedbymeansofexperimentsperformed
without the aid of any external positioning system.
I. INTRODUCTION
Micro UAVs constitute the ideal platform for many robotic
task, such as exploration, mapping, and surveillance. The
unconstrained workspace and versatility allow to use them as
ﬂying sensors and actuators to reach and operate on places
that are out of the range of more classical ground mobile
robots. On the other hand many real-world tasks require
(because of their nature or due to governmental regulation)
that one or more humans participate to the mission in the
quality of either simple supervisors or skilled operators, e.g.,
in the case of search-and-rescue missions [1],
Recent works have investigated the role of haptic feedback
and the fact that it can be successfully used in order to
increase the operator situational awareness (see, e.g., [2] and
references therein) and therefore to have a positive impact on
the human decisions. For this reason haptic shared control of
UAVs represents an emerging topic attracting the attention
of many research groups in the recent years.
Concerning the single-UAV case, an extensive study has
been already done, with special regard on the theoretical
point of view. The authors of [2] have studied how to
properly design artiﬁcial force ﬁelds for the haptic cue when
bilaterally teleoperating a UAV, while [3] presented the de-
sign of an admittance control paradigm from the master side
P. Stegagno, M. Basile, H. H. Bülthoff and A. Franchi are with the
Max Planck Institute for Biological Cybernetics, Department of Human
Perception Cognition and Action, Spemannstraße 38, 72076 Tübingen,
Germany { paolo.stegagno, mbasile, hhb}@tuebingen.mpg.de.
A. Franchi is also with Centre National de la Recherche Scien-
tiﬁque (CNRS), Laboratoire d’Analyse et d’Architecture des Systèmes
(LAAS), 7 Avenue du Colonel Roche, 31077 Toulouse CEDEX 4, France.
antonio.franchi@laas.fr
with position feedback. Single-UAV teleoperation control
based on the port-Hamiltonian approach has been presented
by [4] and extended by [5]. In [6] a strategy to generate the
haptic feedback as a virtual force based on both telemetric
and optic ﬂow sensors is designed. A novel force feedback
user interface for mobile robotic vehicles with dynamics has
been shown by [7], and a novel force feedback algorithm
that allows the user to feel the texture of the environment
has been recently presented by [8]. Finally in [9], [10] the
authors have shown how single-robot haptic teleoperation
can be seamlessly integrated with complex path planning
techniques.
Concerning the, more challenging, multi-UAV case, [11],
[12],[13],[14]presentedanextensiveframeworktocontrola
group of UAVs that can be interfaced with multiple operators
by means of haptic devices, e.g., to control some generalized
velocity of the group formation, and to receive a feedback
that is informative of the tracking quality, the swarm status,
and properties of the surrounding environment, such as
presenceofobstaclesorwindgusts.In[15],theauthorsshow
how that framework can be applied to perform teleoperation
over intercontinental distances.
The majority of the works never addressed the problem in
arealworldscenario,eitheremployingsimulationorexternal
motion capture systems. Even though in [8] the obstacles are
detected through a laser scanner the state for control purpose
is still retrieved by an external camera system. Similarly,
in [12] on-board cameras are used to measure the relative
bearings,butthevelocitieswereobtainedthroughanexternal
motion capture system. At the best of our knowledge none
of the approaches dealing with haptic-teleoperation of UAVs
have been experimentally proven on a platform that uses
onboard sensors only.
The goal of this paper is therefore to present a UAV
platform designed for haptic teleoperation that can be easily
operated using velocity control in real unstructured scenarios
providing safety against obstacles and relying on onboard
sensor only, namely IMU and RGB-D measurements. In fact,
this essential sensor equipment, thanks to the presence of a
depth camera, is relatively richer with respect to the standard
IMU-camera integration setting.
Whilebeingcommanded,thequadrotorautonomouslyper-
forms several tasks to improve safety and ease of operation.
First, it extends the intrinsically limited ﬁeld of view
(FOV) of the exteroceptive sensor executing a persistent
pan-scanning of the local surrounding area using its yaw
degree of freedom. In addition, the camera sensor is used for
collision-safe navigation, enacting obstacle avoidance on a
local obstacle map attached to the quadrotor body frame, that
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 3862
Fig. 3: The haptic device and its frame of reference.
hence with Z
W
pointing in the opposite direction of the
gravity vector, and with Q : {O
Q
,X
Q
,Y
Q
,Z
Q
} a frame
attached to a representative point of the quadrotor (ideally
its center of mass), which conforms to the North-East-Down
(NED) convention as common in the aerospace ﬁeld. In
general, we will denote with
A
p
B
the position of the origin
of a frame B in another frame A and with R
A
B
? SO(3)
the rotation matrix expressing the orientation of the frame
B in A. Hence,
W
p
Q
? R
3
and R
W
Q
? SO(3) represent
the position and orientation ofQ inW, respectively. Finally,
denotewith?,?,? respectivelytheroll,pitchandyawangles
that represent the orientation of the quadrotor in W, i.e.,
such that R
W
Q
= R
x
(π)R
z
(?)R
y
(?)R
x
(?), where R
x
(·),
R
y
(·),R
z
(·) represent the canonical rotation matrices about
the axesX,Y, andZ respectively. Therefore, e.g.,R
x
(π) is
the rotation matrix matrix transforming the NED coordinates
in NWU.
In order to express the human commands we introduce
the (NED) horizontal frame H : {O
H
,X
H
,Y
H
,Z
H
} such
that O
H
? O
Q
and Z
H
??Z
W
. Then, the rotation matrix
between H and Q is R
H
Q
= R
y
(?)R
x
(?). Finally, we
consider the camera frame C :{O
C
,X
C
,Y
C
,Z
C
}. Since the
camera is rigidly attached to the quadrotor,
Q
p
C
andR
Q
C
are
constant extrinsic parameters.
A human operator interfaces with the system and remotely
operate the UAV through an omega.6 haptic device, shown
in Fig. 3, and a standard 22” screen. The device provides
a handle with six degrees of freedom (DOFs), three trans-
lational and three rotational, offering complete motion to a
3D rigid body. However, we have limited our system to use
only the three translational DOFs.
In order to express all omega.6-related quantities, we
deﬁne D : {O
D
,X
D
,Y
D
,Z
D
} as the inertial NED frame
of reference whose origin is located in the steady position
of the end effector, placed at the center of its Cartesian
workspace. The translational dynamics of the haptic device
can be modeled through the Lagrangian equation:
M(q)¨ q+C(q, ˙ q)˙ q =? +f (1)
where q = (q
x
q
y
q
z
)
T
?R
3
is the position of the handle in
D, M(q) ? R
3?3
is the positive-deﬁnite/symmetric inertia
matrix, C(q, ˙ q) ? R
3?3
is the Coriolis matrix, and ?, f
?R
3
are the control and human forces respectively.
III. REFERENCE VELOCITY GENERATION AND
EXECUTION
Apart from the attitude of the quadrotor, represented by?,?,
the position
W
p
Q
and the yaw? are not observable without
performing a full SLAM approach, since the robot has only
relative measurements of the environment and no absolute
positioning capabilities or compass. Hence, it is not possible
to regulate the motion on a speciﬁc desired position or yaw
orientation, and the UAV can only follow a reference velocity
vector and reference yaw rate. Clearly one could integrate
the measurements and the motion to obtain an odometry-
like estimation of the relative position and yaw from the
starting pose. However this method can be employed only
for a short time since it would unavoidably drift due to
the aforementioned observability issue. Another way would
be to implement a computationally expensive full SLAM
approach. However, the focus of this work is to develop a
basic, solid, and adaptable platform to perform indoor aerial
bilateral teleoperation in which the decision on high the level
motion strategies are delegated to the human operator.
Denote with
H
v
r
and
˙
?
r
the reference velocity vector
and yaw-rate, respectively. The velocity
H
v
r
is conveniently
expressed in the horizontal frameH in order to abstract from
the current attitude of the quadrotor. Note also, that driving
the UAV in the frame H instead of the frame B is much
more convenient for the human operator. These reference
quantities are provided to a ﬂight controller (referred to as
‘tracker’ in the following) that uses the estimated state of
the robot (computed as per Sec. IV) to regulate the tracking
error to zero. The tracker, described in [14], is a simple PID
controller with gravity compensation computing the required
thrust, roll and pitch angles that are needed to accelerate
as requested and therefore applying the torques that are
necessary to track tht desired angles. Note that the integral
term is not performed using the position of the UAV, as usual
when dealing with localization issues, being that information
unobservable. Hence, the integral term is computed simply
integrating the error of the velocity. Yaw rate is tracked in a
similar fashion using the yaw torque command.
Some steps are performed in order to generate
H
v
r
and
˙
?
r
on the basis of the commands of the human operator and
other autonomous actions. In fact, although the robot tries
to reproduce the velocity and yaw rate commanded by the
operator, it also carries on some tasks autonomously in order
to guarantee safety during normal operation and improve the
capability of the sensors, as described in the following.
The ﬁrst step to obtain
H
v
r
and
˙
?
r
is the computation of
the desired velocity
H
v
d
and yaw rate
˙
?
d
commanded by
the human operator by means of the haptic device. This is
done mapping the 3D haptic device position as follows:
H
v
d
=k
v
?
?
q
x
cos?
q
x
sin?
q
z
?
?
(2)
˙
?
d
=?k
 
q
y
(3)
where k
v
and k
 
are positive gains and ? is a parameter
expressing the direction of the desired forward motion of the
quadrotor on the horizontal plane ofH. The effect of (2-3) is
that whenever the operator pushes forward the end effector
of the haptic device, the robot will move on the horizontal
plane the direction ?, whenever the operator lifts the end
effector the UAV will increase its altitude (and vice versa),
3864
IV. STATE ESTIMATION
The scheme presented in Sec. III needs some quantities
that must be retrieved by the UAV onboard sensors. The
working principle of our estimation system is summarized
in Fig. 4, and uses both measurements of the IMU and of
thedepth-camera.Theﬁrstonesareusedinacomplementary
ﬁlter to compute estimates
ˆ
?,
ˆ
? of the roll and pitch angles
as described in [17], [18]. In addition, a low-pass ﬁlter
improves the angular velocity measurements
Q
¯ ?
Q
from the
gyros producing an estimate denoted by
Q
ˆ ?
Q
.
Once the attitude (i.e., roll and pitch) of the quadrotor is
known, the images from the depth camera are used to obtain
an estimate of the velocity of the quadrotor in the frameH.
At each time-stepk the images are used to feed the dvo
1
algorithm [19] which provides the estimates
C0
¯ p
C
k
,
¯
R
C0
C
k
of
the position
C0
p
C
k
and orientationR
C0
C
k
of the camera frame
C
k
at time-step k w.r.t. the camera frame C
0
at time-step 0.
Obviously, since dvo performs a visual odometry algorithm,
the estimates will eventually diverge from the true value and
cannot be used for a long time to obtain absolute position
and orientation measurements. Nevertheless, it is possible to
extract a noisy but non-drifting measurement of the velocity
C
k
v
C
k
, i.e., the velocity of the origin O
C
k
of the frame C
k
expressed in C
k
, through the equation:
C
k
v
C
k
=
R
C
k
C0
(
C0
p
C
k
?
C0
p
C
k 1
)
∆T
(9)
whereC
k?1
denotes the camera frame at timek?1 and ∆T
is the elapsed time between time-stepsk?1 andk. However,
since (9) corresponds to a ﬁrst order numerical derivation of
the position
C0
p
C
it would be considerably affected by noise.
For this reason, instead of (9), we use
C
k
ˆ v
C
k
=
ˆ
R
C
k
C0
(
C0
ˆ p
C
k
?
C0
ˆ p
C
k 1
)
∆T
(10)
where
C0
ˆ p
C
k
, and
ˆ
R
C0
C
k
are the 1?-ﬁltered [20] versions of
C0
¯ p
C
k
, and
¯
R
C0
C
k
respectively.
The velocity
C
v
C
of O
C
in C can be written as
C
v
C
=R
C
Q
Q
v
C
=R
C
Q
(
Q
v
Q
+
Q
?
Q
?
Q
p
C
). (11)
Therefore we compute an estimate of
Q
v
Q
at time-stepk as
Q
ˆ v
Q
=R
Q
C
C
k
ˆ v
C
k
?
Q
ˆ ?
Q
?
Q
p
C
(12)
Finally, given the estimates
ˆ
?,
ˆ
?, we obtain the sought
velocity in the H frame
H
ˆ v
Q
=
ˆ
R
H
Q
Q
ˆ v
Q
(13)
which is then used in the velocity tracker in order to follow
the velocity commanded by the operator and the obstacle
avoidance module.
1
https://github.com/tum-vision/dvo
V. LOCAL OBSTACLE MAPS
In order to further extend the FOV of the camera and
increase the safety of the ﬂights, the system builds and
propagate in time a local map of the obstacles. The obstacle
avoidance will be then performed on the estimates provided
by this module. Nevertheless, this module does not perform
a SLAM algorithm, since there is no localization purpose in
its operation and the produced obstacle map will be limited
both in time and space.
The working principle is that of a Bayesian ﬁlter. A mea-
surement step initializes and improves the current estimates,
while a time update estimate propagate current estimates in
time using the state of the UAV.
The main idea is to divide the world surrounding the UAV
in cells by a discretization of azimuth and zenith distance
angles. Whenever a measurement occurs (i.e., a set of closest
points is extracted by a frame from the camera as explained
in Sec. III), each measured point is assigned to the correct
cell based on its relative azimuth and zenith distance, and
overwrites any other point possibly present in that cell. This
is to obtain a reactive behavior of the estimates. More copies
of the same obstacle point are introduced in the cell, in order
to be used as particles.
The time update uses as input the estimated velocity
H
ˆ v
Q
and yaw rate
˙
? of the UAV which is used to propagate each
estimated obstacle points ˆ p
j
according to the following:
ˆ p
k+1
j
=R
T
kk+1
(ˆ p
k
j
?u) (14)
where ˆ p
k+1
j
is the new estimate,u = ∆T(
H
v
k+1
Q
?
H
v
k
Q
+
n
v
)/2 is the estimated linear displacement of the UAV
between time k and time k + 1, R
kk+1
= R
z
(?
kk+1
) is
the rotation matrix about the vertical axis due to the yaw
displacement ?
kk+1
= ∆T(
˙
?
k+1
?
˙
?
k
+n
 
)/2 between
time k and time k + 1, ∆T is the elapsed time between k
andk+1,andn
v
,n
 
aretwosamplesrandomlyextractedon
the model of the noise over the estimates of the velocity and
the yaw rate, both assumed to be gaussian random variables
with zero mean and known covariance.
Examples of obstacle maps can be observed in the middle
column of Figs. 9 and 10, which shows the image plane of
the onboard camera with reprojection of the the 3D obstacle
points (red dots). Note the particles in the right part of the
third row, middle column, Fig. 9, which are propagated and
not directly measured, since the obstacles are not in the FOV.
VI. VISUAL/HAPTIC FEEDBACK FOR THE OPERATOR
A. Haptic Feedback
The main focus of this work is to enable a human operator
to remotely control the UAV using only onboard sensors.
At this aim, great importance assumes the feedback that the
human receives from the UAV and his interface with the
system. Hence, we propose a dual haptic and visual feedback
in order to enable the operator to safely control the robot.
Thehapticfeedbackalgorithmresemblestheoneproposed
in [14] but with a special adaptation to our particular input
paradigm and pan-scanning scheme. The force feedback
3866
Fig. 10: Snapshots of a typical experiment in a real ofﬁce envi-
ronment. Each row refers to a different time instant. Left column:
global views of the environment. Middle column: onboard views
from the visualizer (red dots are detected obstacles). Right column:
haptic interface used by the human operator. For safety reasons, a
loose rope secures the UAV and is moved by a person following
the UAV motion.
VIII. DISCUSSION AND FUTURE WORK
In this paper we have presented a semi-autonomous UAV
platform that is used for indoor haptic teleoperation control
and is able to exploit only onboard sensors, thus being in-
dependent from any motion capture system. No assumptions
on the environment are needed, such, e.g., the presence of
planar surfaces or objects of known sizes. The limitations
of the sensor in terms of small FOV have been overcome
by exploiting a pan-scanning action that also improves the
detection of surrounding obstacles, estimated in a local
moving map through ﬁltering.
In the near future we are planning to install a single-board
Odroid-based PC, which will be able to handle the camera
images. Other improvements will consider different ﬁltering
strategies for the angular velocities and for the whole state.
Once the platform is complete, we plan to employ it to
perform teleoperation experiments over the internet, hence
introducing signiﬁcant delay on the commanded velocities.
REFERENCES
[1] R. Murphy, S. Tadokoro, D. Nardi, A. Jacoff, P. Fiorini, H. Choset,
and A. Erkmen, “Search and rescue robotics,” in Springer Handbook
of Robotics, B. Siciliano and O. Khatib, Eds. Springer, 2008, pp.
1151–1173.
[2] T. M. Lam, H. W. Boschloo, M. Mulder, and M. M. V. Paassen,
“Artiﬁcial force ﬁeld for haptic feedback in UAV teleoperation,” IEEE
Trans. on Systems, Man, & Cybernetics. Part A: Systems & Humans,
vol. 39, no. 6, pp. 1316–1330, 2009.
[3] F. Schill, X. Hou, and R. Mahony, “Admittance mode framework for
haptic teleoperation of hovering vehicles with unlimited workspace,”
in 2010 Australasian Conf. on Robotics & Automation, Brisbane,
Australia, December 2010.
[4] S. Stramigioli, R. Mahony, and P. Corke, “A novel approach to haptic
tele-operation of aerial robot vehicles,” in 2010 IEEE Int. Conf. on
Robotics and Automation, Anchorage, AK, May 2010, pp. 5302–5308.
[5] A. Y. Mersha, S. Stramigioli, and R. Carloni, “Switching-based
mapping and control for haptic teleoperation of aerial robots,” in 2012
IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, Vilamoura,
Portugal, Oct. 2012, pp. 2629–2634.
[6] H. Rifa, M. D. Hua, T. Hamel, and P. Morin, “Haptic-based bilateral
teleoperationofunderactuatedunmannedaerialvehicles,”in18thIFAC
World Congress, Milano, Italy, Aug. 2011, pp. 13782–13788.
[7] X. Hou, R. Mahony, and F. S. Schill, “Representation of vehicle
dynamics in haptic teleoperation of aerial robots,” in 2013 IEEE Int.
Conf. on Robotics and Automation, Karlsruhe, Germany, May 2013,
pp. 1477–1483.
[8] S. Omari, M. D. Hua, G. J. J. Ducard, and T. Hamel, “Bilateral haptic
teleoperation of VTOL UAVs,” in 2013 IEEE Int. Conf. on Robotics
and Automation, Karlsruhe, Germany, May 2013, pp. 2385–2391.
[9] C. Masone, A. Franchi, H. H. Bülthoff, and P. Robuffo Giordano,
“Interactive planning of persistent trajectories for human-assisted nav-
igation of mobile robots,” in 2012 IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems, Vilamoura, Portugal, Oct. 2012, pp. 2641–2648.
[10] C. Masone, P. Robuffo Giordano, H. H. Bülthoff, and A. Franchi,
“Semi-autonomous trajectory generation for mobile robots with inte-
gral haptic shared control,” in 2014 IEEE Int. Conf. on Robotics and
Automation, Hong Kong, China, May. 2014.
[11] A. Franchi, C. Secchi, M. Ryll, H. H. Bülthoff, and P. Robuffo Gior-
dano,“Sharedcontrol:Balancingautonomyandhumanassistancewith
a group of quadrotor UAVs,” IEEE Robotics & Automation Magazine,
Special Issue on Aerial Robotics and the Quadrotor Platform, vol. 19,
no. 3, pp. 57–68, 2012.
[12] A. Franchi, C. Masone, V. Grabe, M. Ryll, H. H. Bülthoff, and
P. Robuffo Giordano, “Modeling and control of UAV bearing-
formations with bilateral high-level steering,” The International Jour-
nal of Robotics Research, Special Issue on 3D Exploration, Mapping,
and Surveillance, vol. 31, no. 12, pp. 1504–1525, 2012.
[13] A. Franchi, C. Secchi, H. I. Son, H. H. Bülthoff, and P. Robuffo
Giordano, “Bilateral teleoperation of groups of mobile robots with
time-varying topology,” IEEE Trans. on Robotics, vol. 28, no. 5, pp.
1019–1033, 2012.
[14] D. J. Lee, A. Franchi, H. I. Son, H. H. Bülthoff, and P. Robuffo
Giordano, “Semi-autonomous haptic teleoperation control architecture
of multiple unmanned aerial vehicles,” IEEE/ASME Trans. on Mecha-
tronics, Focused Section on Aerospace Mechatronics, vol. 18, no. 4,
pp. 1334–1345, 2013.
[15] M. Riedel, A. Franchi, H. H. Bülthoff, P. Robuffo Giordano, and
H. I. Son, “Experiments on intercontinental haptic control of multiple
UAVs,” in 12th Int. Conf. on Intelligent Autonomous Systems, Jeju
Island, Korea, Jun. 2012, pp. 227–238.
[16] D. Scaramuzza, M. C. Achtelik, L. Doitsidis, F. Fraundorfer, E. B.
Kosmatopoulos, A. Martinelli, M. W. Achtelik, M. Chli, S. A.
Chatzichristoﬁs, L. Kneip, D. Gurdan, L. Heng, G. H. Lee, S. Lynen,
L. Meier, M. Pollefeys, A. Renzaglia, R. Siegwart, J. C. Stumpf,
P.Tanskanen,C.Troiani,andS.Weiss,“Vision-controlledmicroﬂying
robots: from system design to autonomous navigation and mapping in
GPS-denied environments,” Accepted to IEEE Robotics & Automation
Magazine, 2013.
[17] R. Mahony, T. Hamel, and J.-M. Pﬂimlin, “Nonlinear complementary
ﬁlters on the special orthogonal group,” IEEE Trans. on Automatic
Control, vol. 53, no. 5, pp. 1203–1218, 2008.
[18] P. Martin and E. Salaün, “The true role of accelerometer feedback
in quadrotor control,” in 2010 IEEE Int. Conf. on Robotics and
Automation, Anchorage, AK, May 2010, pp. 1623–1629.
[19] C. Kerl, J. Sturm, and D. Cremers, “Robust odometry estimation
for RGB-D cameras,” in 2013 IEEE Int. Conf. on Robotics and
Automation, Karlsruhe, Germany, May 2013, pp. 3748–3754.
[20] G. Casiez, N. Roussel, and D. Vogel, “1 euro ﬁlter: a simple speed-
based low-pass ﬁlter for noisy input in interactive system,” in SIGCHI
Conference on Human Factors in Computing Systems, Austin, Texas,
May 2012, pp. 2527–2530.
[21] V. Grabe, M. Riedel, H. H. Bülthoff, P. Robuffo Giordano, and
A. Franchi, “The TeleKyb framework for a modular and extendible
ROS-based quadrotor control,” in 6th European Conference on Mobile
Robots, Barcelona, Spain, Sep. 2013.
3869
