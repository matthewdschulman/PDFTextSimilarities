The Flying Hand: a Formation of UA Vs for
Cooperative Aerial Tele-Manipulation
Guido Gioioso
1;2
, Antonio Franchi
3
, Gionata Salvietti
2
, Stefano Scheggi
1
, Domenico Prattichizzo
1;2
Abstract— The ﬂying hand is a robotic hand consisting of
a swarm of UA Vs able to grasp an object where each UA V
contributes to the grasping task with a single contact point
at the tooltip. The swarm of robots is teleoperated by a
human hand whose ﬁngertip motions are tracked, e.g., using
an RGB-D camera. We solve the kinematic dissimilarity of
this unique master-slave system using a multi-layered approach
that includes: a hand interpreter that translates the ﬁngertip
motion in a desired motion for the object to be manipulated; a
mapping algorithm that transforms the desired object motions
into a suitable set of virtual points deviating from the planned
contact points; a compliant force control for the case of
quadrotor UA Vs that allows to use them as indirect 3D force
effectors. Visual feedback is also used as sensory substitution
technique to provide a hint on the internal forces exerted on
the object. We validate the approach with several human-in-
the-loop simulations including the full physical model of the
object, contact points and UA Vs.
I. INTRODUCTION
Aerial mobile manipulation is a growing area of robotics
research that claims to bring in the air the results obtained
on the terrain by the mobile manipulation community using
UGVs. Exploiting the ubiquity of low cost and easy to ﬂy
UA Vs, great steps have been made in a relatively short time.
Different approaches have been pursued to let a ﬂying robot
grasp and manipulate an object. A solution is to equip the
vehicles with an arm and a gripper or more sophisticated
end-effectors. In [1], for instance, a ﬁxed gripper enabling
grasping and object retrieval at high speeds is attached to
a quadrotor UA V . A similar goal has been then pursued
in [2] with an actuated appendage. In [3] a small–scaled
autonomous helicopter is equipped with a robotic arm.
Having on-board a hand-arm system can be efﬁcient in
speciﬁc tasks, but presents several drawbacks. The whole
UA V-arm system results in a higher weight requiring higher
energy and decreasing the efﬁciency and autonomy of the
system. This makes such equipped UA Vs difﬁcult to utilize
for other tasks such as exploration or inspection. Moreover
simple gripper cannot accomplish complex manipulation
tasks, while robotic hands present a limited workspace and
are still difﬁcult to realize and control.
A possible way to overcome all these issues could be to
grasp and carry objects using more than one UA V . This is a
1
Department of Information Engineering and Mathematical Sciences,
University of Siena, via Roma 56, 53100 Siena, Italy. fgioioso,
scheggi, prattichizzog@dii.unisi.it
2
Department of Advanced Robotics, Istituto Italiano di Tecnologia, via
Morego, 30, 16163 Genova, Italy gionata.salvietti@iit.it
3
Centre National de la Recherche Scientiﬁque (CNRS), Laboratoire
d’Analyse et d’Architecture des Syst` emes (LAAS), 7 Avenue du Colonel
Roche, 31077 Toulouse CEDEX 4, France and Max Planck Institute
for Biological Cybernetics, Spemanstr. 38, 72076, T¨ ubingen, Germany.
antonio.franchi@laas.fr
challenging problem due to the complexity of the task and
the limitations of aerial vehicles when compared to grounded
robots. In [4], a team of quadrotors are rigidly attached to a
payload in order to grasp and transport it. In [5] and in [6] the
problem of controlling multiple aerial robots manipulating
and transporting a payload via cables is presented. However,
the use of cables or others ﬁxed handles is possible only if
the load to be carried is accessible and it is possible to ﬁx
the cable on it in a speciﬁed position.
In this paper we aim at merging the two approaches
considering cooperative UA Vs grasping an object, where
each UA V makes a single contact with the object through a
rigidly attached tool. Practically, we can assume each UA V
acting as a ﬁnger of an N-ﬁngered hand and collaborating
with the other UA Vs to realize a complex and unique hand-
system that is able to grasp and move an object. Some
preliminary results in this promising direction have been
presented in [7] and in [8]. In [7], a dynamic model for
a single and double quadrotor UA V manipulating a cart on
a track is derived. In [8] a novel second order sliding mode
controller is used considering a N quadrotors system grasping
and manipulating an object. In this work we consider a
control design different from the sliding mode, since this last
typically suffers from undesired phenomena, e.g., chattering,
that might severely degrade the real-world performances.
The ﬁrst contribution of this paper is the formalization and
the study of the problem of grasping an object by a swarm of
N UA Vs establishing N contacts with the grasped object, one
contact per each UA V , and then cooperatively manipulating
it. In this scenario, the UA V team can be used as grasping
device only if necessary, keeping its versatility and efﬁciency.
For instance, the robot formation can be guided through a
narrow passage varying its shape [9] and then be used to
grasp and move an obstacle. Moreover, with this solution,
complex grasps can be achieved since contact points can be
arbitrarily placed on the object and because UA Vs can sense
disturbance forces without additional sensors.
The second contribution refers to teleoperation. Teleop-
eration of multiple aerial vehicles, and in general mobile
robots, is a recent promising trend in robotics, see, e.g., [9],
[10]. In those works the aerial robots were used mainly as
mobile sensors and were not allowed to physically interact
with the environment. In the framework presented in this
paper, the UA Vs physically interact with the grasped object
and are teleoperated by a human hand. This poses interesting
theoretical issues on how to relate the movements and the
forces exerted by the swarm of robots with an extremely
different structure such as the human hand [11]. In fact,
the free motion of the human hand must be able to control
both the motion of the grasped object and the forces used to
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 4335
establish the grasp.
We propose in this work a layer structured approach. The
ﬁrst layer concerns the tracking of the human hand; in the
second layer the features extracted from the human hand are
mapped into virtual attraction points; ﬁnally the last layer
lets the UA Vs tooltip implement a force stiffness control that
acts in a compliant way with respect to the error between the
contact and the attraction points.
The results described in this paper are intended as a
guideline for future real implementation where the role of
UA V can be played by quadrotors or by other devices. In
particular, we believe that this approach will be notably
suitable for micro-UA Vs, since the reduced dimension and
weight allows for better performances in terms of power
efﬁciency [12]. As a ﬁrst step in this direction we recently
validated the ability of the Force Control Layer to work in
reality in [13] where the behavior of a single UA V in contact
with an object is analyzed.
The paper is organized as it follows. In Sec. II the problem
modeling, the objectives and the the structure of the proposed
framework are presented. In Sec. III the multi-layer orga-
nization of the proposed framework is described in detail.
Section IV deals with the human-in-the-loop simulations
performed to validate the proposed approach. Finally in
Sec. V conclusions and future work are outlined.
II. MODELING, OBJECTIVES AND STRUCTURE OF THE
TELEOPERATION FRAMEWORK
Consider a rigid object, denoted by O, and a group of
N UA Vs, denoted byU
1
:::U
N
. The conﬁguration ofO is
deﬁned by the position p
o
2R
3
of the center of mass O
b
in an inertial frameW :fO
w
;~ x
w
;~ y
w
;~ z
w
g and by the rotation
matrix R
o
2 SO(3) deﬁning the orientation of a moving frame
B :fO
b
;~ x
b
;~ y
b
;~ z
b
g that is rigidly attached toO.
We assume that a hand-tracking system records in real-
time the motion of the ﬁngertips of a human operator (e.g.,
using the technique described in Sec. III-A). The ﬁrst goal
of our system can be then stated as
Goal 1. To use the hand/ﬁngertip motion to remotely manip-
ulateO in the sense of changing its location p
o
and attitude
R
o
(i.e., to perform translations and rotations) by exploiting
the action of the N UA Vs as if they were a remote ﬂying
hand.
To this aim, each UA V possesses a rigidly attached tool
whose tooltip acts as a ﬁnger to exert the needed interaction
forces with the object. In particular in this framework we
consider the N tooltips acting as N ﬁngertips of a ﬂying
robotic hand that performs, borrowing the terminology from
grasping literature, a precision grasp [14] The contact be-
tween the tooltip and the object is modeled as a contact
point with friction, also referred to as hard ﬁnger contact
model [15]. According to this model no torsional moments
can be exchanged at the contact point. Furthermore, the
contact force at the tooltip ofU
i
expressed inB and denoted
with l
i
2R
3
, must satisfy the friction constraints described
by the Coulomb model [15].
Denote by w2R
6
the resulting wrench, expressed inB,
that is applied toO by l
1
:::l
N
. The relation between w and
l
1
:::l
N
, depends on the geometry of the contact points and
is given by
w= Gl (1)
where l = (l
T
1
::: l
T
N
)
T
2R
3N
is the stacked vector of
contact forces and G2R
63N
is the grasp matrix [15].
Solving (1) for the contact forces, it is possible to distinguish
two contributions. The ﬁrst, called internal forces [16], does
not contribute to the motion ofO, i.e., l
int
2 kerG, and can
be obtained through the null-space projector
l
int
=(I  G
#
G)l; (2)
where G
#
is the Moore-Penrose pseudoinverse of G and I is
the identity matrix. These forces, and in general the nullspace
of G, play an important role in maintaining a stable grasp,
as detailed in [17].
The second component of l is deﬁned as
l
ext
=l l
int
;
i.e., the part of the force that produces the motion ofO [18].
It has been proven [19] that the choice of the internal forces
l
int
and the control of the object motion via l
ext
can be
considered independently in the control design.
In grasping the optimal choice of the contact points on
the object strongly affects the grasp properties. The optimal
choice of the contact points is known as grasp planning
[16] and responds to many criteria including the task to be
performed with the object.
We assume that a preliminary grasp planning phase de-
termines the contact points and internal forces to be applied
according to some criteria, e.g., the one presented in [20]
to guarantee force closure [15]. The grasp planner returns
N contact points, whose positions in B are denoted by
¯ p
1
::: ¯ p
N
and a set of nominal internal force, denoted by
¯
l =(
¯
l
1
:::
¯
l
N
)
T
expressed inB. The nominal internal forces
guarantee a stable grasp in the static case and with nominal
environment conditions. At the starting time t
0
the object is
static and each UA V has approached the object. Each tooltip
coincides with the corresponding contact point provided by
the grasp planner.
The second goal of our system is then related to the control
of the grasping force.
Goal 2. Let the human operator be able to change the
intensity of the internal forces, i.e., to choose the actual
internal forces in spanf
¯
lg.
By fulﬁlling Goal 2 the object can be, for example, grasped
or released depending on the operator desire. Notice that the
hard ﬁnger contact model assumptions is, on purpose, very
general since it does not assume that the robot is able to
dock to the object at the tooltip. In fact, the possibility of
docking assumes the presence of some particular conditions
that might not be fulﬁlled in many real-world scenarios,
e.g., the availability of an actuated gripper on the UA V ,
the presence of a suitable docking surface or handle on the
object, etc. Nevertheless, the proposed approach is still valid
if some docking mechanism is provided. In this particular
case Goal 2 looses its relevance and only Goal 1 must be
pursued.
4336
Human
Hand
Tracking 
Device
Hand 
Interpreter
Virtual Point
Mapping
UAV 1
Object
UAV N
Fingertips
UAV 1 Force Control
UAV N Force Control
˜ y
1
˜ y
N
F
1
,M
1
F
N
,M
N
Layer 1 Layer 2 Layer 3
  f
e
1
  f
e
N
  h
,? h
, 
h
p
h
,  r
h
Sunday, 15 September 13
Fig. 1: Schematic representation of the framework. After the approaching phase where the UA Vs autonomously get closer to the contact
points, the hand motion is captured by a tracking system and translation and rotation of the hand palm and the radial deformation are
computed (Layer 1). Virtual attraction points are then computed from the human interpreted commands in order to induce a suitable
contact force (Layer 2). Contact forces are exerted by the tooltips on the object using a stiffness control and allow the formation to grasp
and manipulate the object (Layer 3). In order to close the loop of the teleoperation, force feedback is visually provided to the human user.
Due to the kinematic dissimilarity between the number
of human ﬁngertips and the number of UA V tooltips, the
two goals cannot be achieved just mapping the ﬁnger action
one-to-one to the UA V commands. A suitable ‘translation’
is therefore needed, which also takes into account possible
actuation limits of the UA Vs.
In order to overcome all these issues and allow for a
transparent and ﬂexible application of the human motion
directives to the remote object we designed a framework
that is based on a 3-layer approach (refer to Fig. 1 for
a block-diagram representation). In the ﬁrst layer, called
Hand Interpreter and described in Sec. III-A, the human
hand/ﬁnger motion is abstracted in order to retrieve high level
motion commands, i.e., desired translational velocity, desired
rotation rates (roll-rate, pitch-rate, and yaw-rate), and desired
grasping force intensity.
In the second layer, called Virtual Point Mapping (de-
scribed in Sec. III-B) we translate these desired actions into
virtual ‘attraction’ points for the UA V tooltips. The role of
the virtual attraction points is to indirectly generate a set of
forces as in the indirect force control paradigm. In fact, the
third layer, called UAV Force Control, implements a force
stiffness control that is in charge of controlling the UA V
actuation in a way that the tooltip behaves as a non-linear
spring with a known stiffness map.
Notice that this approach is substantially different from a
pure abstraction-layer formation control approach. In fact,
the virtual points do not represent in this case a desired
trajectory for the UA Vs but only some minimum points for
the exerted forces, which actually generate the motion ofO.
III. COOPERATIVE AERIAL TELEMANIPULATION
FRAMEWORK
A. Hand Interpreter
Denote by H :fO
h
;~ x
h
;~ y
h
;~ z
h
g a moving frame that
is rigidly attached to the tracked hand, and by H
0
:
fO
0
h
;~ x
0
h
;~ y
0
h
;~ z
0
h
g a ﬁxed frame that coincides with H at t
0
.
A possible choice for H is to set O
h
at the centroid of
hand, the axis ~ x
h
passing to a given ﬁnger,~ z
h
orthogonal
to the palm, and ~ y
h
accordingly. Denote by p
h
2R
3
the
position of O
h
expressed in H
0
, and by R
h
2 SO(3) the
rotation matrix that expresses the orientation of H w.r.t.
H
0
. Furthermore, denote by (f
h
;q
h
;y
h
) the roll-pitch-yaw
(RPY) representation of such orientation. Finally, deﬁne with
r
h
the radius of the circle that best ﬁts the detected positions
of the ﬁngertips, and with Dr
h
the difference between the
current value of r
h
and its value at t
0
.
The hand parameters: p
h
, h
h
= (f
h
q
h
y
h
)
T
, and Dr
h
are
used to represent the conﬁguration of the hand, and are
provided as input to the Virtual Point Mapping Layer.
Many tracking devices and algorithms can be used to
record in real-time the motion of the ﬁngertips, e.g., data
gloves, haptic interfaces, and vision-based systems. In our
speciﬁc implementation we tracked the hand movements
using a RGB-D camera. Given the point cloud of the hand,
the palm and the ﬁngers can be detected via cluster analysis.
Then for each cluster, the centroid and the direction of
maximum variance are computed. The ﬁngertips position is
estimated from the centroid and direction of the ﬁnger point
clouds, considering that in our setup the ﬁngertips usually
represent the points of the cluster with smaller depth. Then,
a RANSAC-based plane ﬁtting is performed on the ﬁngertips.
The ﬁngertips and the ﬁnger point clouds are projected on
such plane and the related bounding circle, and its radius,
are evaluated.
At t
0
the hand should be wide open in order to cor-
rectly detect the ﬁnger clusters and the related ﬁngertips. In
the successive frames, if the estimated ﬁngertips are close
enough (up to a given threshold) to the old ones, then
the ﬁngertip positions are updated. This control allows us
to overcome cases in which one or more point clouds are
wrongly detected. In this case the point clouds of the ﬁngers
are not used in the computation of the bounding circle.
Moreover, if one or more ﬁngers are not detected, the pose
of the related ﬁngertips is updated accordingly to the overall
hand conﬁguration. Figure 2 shows the working principle
and result of the proposed hand tracking algorithm used to
retrieve the hand parameters.
B. Virtual Point Mapping
The Virtual Point Mapping layer generates the N virtual
attraction points on the basis of the human hand parameters
provided by the Hand Interpreter (Fig. 3). Denote by ˜ y
i
2R
3
the i-th virtual attraction point position expressed inW . At
t
0
˜ y
i
= p
o
+ R
o
¯ p
i
;
i.e., the virtual points coincide with the grasping points.
At the generic time instant t we set
˜ y
i
(t)= ˜ p(t)
|{z}
translation
+
˜
R(t) ¯ p
i
| {z }
rotation
+a
r
Dr
h
˜
R(t)
¯
l
i
| {z }
grasping
; (3)
4337
H
0
 = H 
H
0
  H  
p
h
(a)
H
0
 = H 
H
0
  
H  
? h
(b)
r
h0
r
h
(c)
Fig. 2: The hand tracking algorithm retrieves the point clouds
related to the palm (magenta) and the ﬁngers (green). Fingertips
are then processed in order to retrieve the human commands: (a)
translation; (b) rotation and; (c) radius (i.e., grasping force).
i = 1:::N, where ˜ p
h
2R
3
and
˜
R2 SO(3) are generated
through the following two independent dynamical systems
˙
˜ p=a
p
p
h
; ˜ p(t
0
)= p
o
(t
0
) (4)
˙
˜
R= S(w
h
)
˜
R;
˜
R(t
0
)= R
o
(t
0
) (5)
in which
w
h
=a
w
T( ˜ h)h
h
being ˜ h the RPY angles associated to
˜
R and T()2R
33
the Jacobian matrix mapping RPY angle rates to angular
velocity. The positive scalars a
p
;a
w
;a
r
represent suitable
scale factors.
The following facts should be noted about the algorithm:
 if p
h
and h
h
are identically zero and Dr
h
< 0 in (3)–(5)
then the virtual points at time t are a compressed version
of the contact points ¯ p
1
::: ¯ p
N
only in the direction of
the internal forces
¯
l
i
, the higher the absolute value
of Dr
h
the higher the compression. Given a certain
variation Dr
h
, the amount of variation of the internal
forces depends on a
r
and on the gains of the force
controller implemented on the robots;
 ifDr
h
is identically zero in (3) then the virtual attraction
points at time t are a roto-translated version of the
contact points and basically represent a desired position-
orientation forO. In particular, p
h
is proportional to the
translation velocity of the whole points (through (4)) and
h
h
to the their RPY rotational rates (through (5));
 in the generic case the two kind of actions, compression
and roto-translation, sum up. However note that while
compression is proportional to the hand radius, roto-
translation is, loosely speaking, proportional to the ‘inte-
gral’ of the hand displacement and rotation. This choice
is made in order to allow to precisely set the desired
internal forces with a position-position teleoperation
scheme for the hand radius and, at the same time, allow
for unlimited space reachability with a position-rate
scheme for the displacement and rotation.
Now, if we consider ˜ y
1
;:::; ˜ y
N
to be the attraction points
for the UA V tooltips and if the tooltips behave like decou-
pled linear spring-damper systems, under the action of the
UA V Force Control Layer, then the object will be moved
~ x
w
~ y
w
~ z
w
~ x
b
~ y
b
~ z
b
¯ p
i
˜ y
i
¯
  i
Sunday, 15 September 13
Fig. 3: The Virtual Point Mapping layer. The features extracted
from the human hand/ﬁngertip in the ﬁrst layer are mapped onto
attraction points for the UA V tooltips.
accordingly to the operator directives. This assumes that
compression commanded through Dr
h
is sufﬁcient to ensure
a stable grasp while moving, e.g., by exploiting the coulomb
friction. No issues of this sort are instead present if the UA V
can dock toO (see discussion after Goal 2).
Some practical adjustments have to be done to the theoret-
ical algorithm described so far. First of all the scale factors
a
p
;a
w
;a
r
have to be chosen carefully, in order to avoid that
the operator commands motions that are too abrupt for the
dynamic capabilities of the UA Vs, e.g., taking into account
the saturation of their motors. Second of all, being the roto-
translation rate commands integrated over time, it might be
wise to introduce a saturation that avoids the virtual points
to reach regions of the workspace that are not feasible for
the mechanical geometry of the UA Vs.
C. UAV Force Control
The proposed tele-operation architecture works with dif-
ferent possible UA Vs and implementations of the Force Con-
trol Layer. In order to show its practicability in a speciﬁc case
we consider in this section the case of typical underactuated
VTOL (Vertical Take-Off and Landing) vehicles, such as,
e.g., quadrotors.
1) Quadrotor with a Tool: A quadrotor UA Vs is a me-
chanical system with four propellers placed at the vertices
of a square and aligned with the normal of that square,
see Fig. 4. We assume that the quadrotor is endowed with
a rigidly attached tool whose tip position, expressed in
the inertial frame is denoted with y
i
. Following the hard
ﬁnger contact model (see Sec. II) the i-th quadrotor interacts
through the tooltip with the environment (i.e., the objectO in
our speciﬁc case) that generates an interaction force f
e
i
2R
3
expressed in the inertial frame. A complete formal model of
this setup can be found in [13].
2) Force Stiffness Control: In [13] we presented a control
law designed to allow the tooltip to exert a force on the
environment. We derived the analytical expression of the
force   f
e
i
exerted by the tooltip on the environment for
any given ˜ y
i
  y
i
. The found relation for typical values of
the mechanical and control parameters can be approximated
with a linear map (see [13] for more details), thus showing
that the tooltip behaves similarly to a linear spring. Therefore
the UA V tooltip, under the action of the proposed controller
is a compliant system whenever the tooltip is in contact with
the objectO. This justiﬁes the design, in Sec. III-B, of the
virtual points as attraction positions that indirectly generate
4338
y
i
˜ y
i f
e
i
~ x
w
~ y
w
~ z
w
Fig. 4: One of the quadrotor UA Vs equipped with the tool, and the
main symbols used in the modeling.
the contact forces, implementing the action commanded by
the human operator. Note that the same controller can be used
also to let y
i
track ˜ y
i
in contact-free motion. More details on
the single-UA V control law derivation, its stability analysis,
and experimental validation are given in [13].
IV. SIMULATIONS
We conducted several human-in-the-loop (HIL) simula-
tions in order to validate the theoretical framework described
so far. In the Hand Interpreter layer we use the Microsoft
Kinect RGB-D sensor that sends the depth image to the
hand tracker available in the Robotic Operating System
(ROS), which runs at an average frame rate of 15 frames
per second (fps). This ﬁrst layer communicates via UDP/IP
the ﬁnger positions to the Virtual Point Layer that runs in
a Matlab program. The UA V Force Control Layer controls
several quadrotor UA Vs interacting with a object that are
both physically simulated within a custom-made environment
based on Simulink. Visual information is then provided to
the operator by means of the Virtual Reality Toolbox. The
whole system runs in real time on a Quad Core i7 3:07Ghz
with 16GB of RAM and NVIDiA GForce gtx 180 graphic
card.
In order to validate the capabilities of our proposed
framework in a realistic scenario, we present here a complete
set of plots regarding a prototypical HIL simulation in which
the user is asked to remotely move a wooden cubic box
between a start and a target location separated by a wall, as
depicted in Fig. 5. In the proposed task, the object has to be
lifted and rotated during the transport.
The box mass is 1:7 Kg and its edges are 2 m long. The
coordinates of the position and the RPY angles describing
the starting conﬁguration of the object in the world frame
are (0;0;1) m and (0;0;0) deg., respectively. The position
coordinates and RPY angles of the target position are
(10;0;1)
T
m and (0;0;45) deg. The wall is located between
the start and target position and its height is 2 m.
Four quadrotors with mass 1 Kg are used to manipulate
the object, each one being in charge of pushing one of the
four vertical faces of the box. A 4-th order low-pass ﬁlter has
been employed when mapping the human commands to the
quadrotor reference signals. From one side this ﬁlter allows
to cut off the noise introduced by the RGB-D sensor and the
hand tracking algorithm. From the other side such a ﬁlter
allows to have reference signals that are smooth enough in
order to be tracked by a quadrotor UA V .
(a)
(b)
(c)
Fig. 5: A human-in-the-loop simulation. (a) The operator task is to
bring a wooden cage on the other side of a wall. (b) visual feedback
(transparency level) informs the operator about the strength of the
internal forces; (c) a human operator performs the task: grasp the
box and move it to the red target. The target is rotated and placed
beyond the wall.
0 1 2 3 4 5 6 7 8
0
2
4
6
8
10
time [s]
Object position [m]
0 1 2 3 4 5 6 7 8
?80
?60
?40
?20
0
20
40
60
80
time [s]
Object rotation [deg]
Fig. 6: Object conﬁguration. Left: position (blue! x, green! y,
red! z). Right: Euler angles representing its orientation (blue!
roll, green! pitch, red! yaw).
In Fig. 6 we show the conﬁguration of the box subject to
the teleoperation action. The y coordinate remains constantly
at 0 m, while the x coordinate goes monotonically from 0 m
to 10 and the z coordinate ﬁrst goes monotonically from 1 m
to about 8 m (when x' 5 m) and then goes back to 0 m,
showing that the box reaches its highest point in order to
overcome the wall and then ﬂies down to the target position.
The orientation of the box is represented in terms of RPY
angles in the plot at the bottom of Fig. 6. We can see that the
pitch and roll are always zero (showing that the attitude of
box is kept stable during the aerial transportation) while the
yaws goes monotonically from 0 deg to about 45 deg, which
is the desired ﬁnal orientation.
Figure 7a is meant to represent the human translational
command (top plot) and the corresponding actual object
execution (bottom plot) in terms of linear velocity. A dead
zone (delimited by the dashed black horizontal lines) has
been introduced in order to facilitate the human task. We
4339
0 1 2 3 4 5 6 7 8
?0.04
?0.02
0
0.02
0.04
0.06
0.08
time [s]
hand position [m]
0 1 2 3 4 5 6 7 8
?4
?2
0
2
4
6
8
time [s]
object linear velocity [m/s]
(a) Top: displacement vector o
h
of the po-
sition of the hand centroid with respect to
the rest position (blue! x, green! y, red
! z). Horizontal dashed black lines are used
to delimitate the dead zone. Bottom: velocity
of the centroid of the teleoperated box.
0 1 2 3 4 5 6 7 8
?80
?60
?40
?20
0
20
40
time [s]
hand rotation [deg]
0 1 2 3 4 5 6 7 8
?80
?60
?40
?20
0
20
40
time [s]
object yaw rate [deg/s]
(b) Top: angle w
h
representing the rotation
of the ﬁngers with respect to the rest posi-
tion. Horizontal dashed black lines are used
to denote a dead zone. Bottom: yaw rate of
the teleoperated box.
0 1 2 3 4 5 6 7 8
0.075
0.08
0.085
0.09
0.095
0.1
0.105
0.11
time [s]
hand radius [m]
0 1 2 3 4 5 6 7 8
?1
0
1
2
3
4
time [s]
radial forces [N]
(c) Top: Hand radius. Bottom: radial forces
exerted by the UA Vs on the objects (right).
Fig. 7
can see how the human ﬁrst commands a positive vertical
velocity (red line) in order to lift the object. Then a positive
horizontal velocity along the x axis is also commanded in
order to overcome the wall and reach the target. Finally a
negative vertical velocity and a zero horizontal velocity is
commanded. The actual velocity of the object (bottom plot)
follows the commanded one. The smother behavior of this
second one with respect to the commanded velocity is due to
the presence of the 4-th order ﬁlter described before. Clearly,
the integral of this velocity signal is the position of Fig. 6.
As per the rotational commands we artiﬁcially limited
the rotation to happen only about the vertical axis, being
the task only to rotate the yaw of the object. Figure 7b
represents the corresponding rotation rate commanded by the
human (top) and the actual yaw rate executed by the object
under the action of the teleoperated quadrotors. Also in this
case we used a dead zone to facilitate the human task. The
human ﬁrst commands a strong negative rotation rate, then
stops and then completes the motion with a another negative
rotation-rate command. It is possible to appreciate how the
actual rotational rate follows the commanded one. Clearly,
the integral of this yaw rate signal is the yaw-rate of Fig. 6.
Finally, Fig. 7c represents the radius commanded by the
human operator (top) and the average norm of the corre-
sponding internal forces exerted by the quadrotors onto the
object (bottom) which are fundamental in order to perform
a stable grasp.
In order to show the behaviors of the four quadrotors
during the teleoperated grasping, we present in Fig. 8a the
tracking errors of the tooltip and in Fig. 8b the quadrotor
orientation in terms of RPY angles. At the beginning of the
simulation the errors are zero because the quadrotors are not
pushing against the box yet. As soon as the push action starts
the coordinate of the error which is parallel to the pushing
axes of each quadrotor starts to increase in absolute value
(i.e., coordinate x for quadrotor 1 and 3, and coordinate y for
2 and 4). When the grasping is performed, the z component
of the error becomes positive for each quadrotor, which in
turn generates the lifting force. At the same time the pitch
of all the quadrotors becomes negative in order to provide
the needed grasping and lifting forces (Fig. 8b top). In the
plot roll and pitch angles of all the four quadrotors are
represented. All the UA Vs keep the same roll and pitch
angles during the task apart from small variations due to
the changes in the operator commands. Given the operator
command the quadrotors also start a coordinated yaw rotation
(Fig. 8b bottom). The yaw rotation happens because the
quadrotor are also asked, as an additional task, to regulate
the yaw in order to maintain their tooltip perpendicular to
the box faces. In fact, as explained in Sec. III-C, the yaw can
be commanded independently from the force that is exerted
on the object.
Finally, the coordinated rotation stops and the tracking
errors become again zero when the object is released. Notice
how the pitch saturates before 20 deg due to a saturation
function that as been introduced in order to fulﬁll real-world
requirements (see discussion in Sec. III-B).
The reader is encouraged to watch the video clip attached
to the paper where a HIL simulation can be fully appreciated.
V. CONCLUSIONS
An approach to teleoperate a swarm of UA Vs, able to grasp
and manipulate objects, using the free motion of the human
hand has been proposed. Grasping by multi-ﬁngered hands
represents the theoretical background of this work since we
assumed that each UA V was able to establish a single contact
with the object thus simulating a ﬂying hand performing a
precision grasp where each quadrotor carries a single ﬁnger
represented by a tool ﬁxed to the body frame. To the best
of our knowledge this is the ﬁrst time in which a swarm of
quadrotors has been telecontrolled by a human hand.
In the HIL simulations we showed the effectiveness of
the proposed approach in a realistic virtual environment.
4340
0 1 2 3 4 5 6 7 8
?0.4
?0.2
0
0.2
0.4
0.6
time [s]
pos. error quadrotor 1[m]
0 1 2 3 4 5 6 7 8
?0.4
?0.2
0
0.2
0.4
0.6
time [s]
pos. error quadrotor 2 [m]
0 1 2 3 4 5 6 7 8
?0.4
?0.2
0
0.2
0.4
0.6
time [s]
pos. error quadrotor 3 [m]
0 1 2 3 4 5 6 7 8
?0.4
?0.2
0
0.2
0.4
0.6
time [s]
pos. error quadrotor 4 [m]
(a) Tracking errors y
d
i
  y
i
for each quadrotor (blue! x, green! y, red! z).
0 1 2 3 4 5 6 7 8
?20 
 0 
 20 
time [s]
Roll Pitch [deg]
0 1 2 3 4 5 6 7 8
?180 
 ?90 
 0 
 90 
 180 
time [s]
Yaw [deg]
(b) Rotation angles of the 4 quadrotors. Top: roll (blue)
and pitch angles (green). Bottom: yaw angles (red).
Fig. 8
As future work decentralized control techniques will be
investigated to improve the stability of the grasp. Real force
feedback provided by haptic interfaces will be also intro-
duced to substitute the visual feedback used in the current
version of the work. We are also working to improve the
ergonomics of the setup in order to make it more comfortable
for the human operator and to reduce tiredness effects. In
parallel we are also working on the real implementation of
a ﬂying hand, with encouraging results [13].
ACKNOWLEDGMENTS
The research leading to these results has received funding
from the European Union Seventh Framework Programme
FP7/2007-2013 under grant agreement n 601165 of the
project WEARHAP WEARable HAPtics for humans and
robots.
REFERENCES
[1] R. Spica, A. Franchi, G. Oriolo, H. H. B¨ ulthoff, and P. Robuffo
Giordano, “Aerial grasping of a moving target with a quadrotor UA V,”
in 2012 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems,
Vilamoura, Portugal, Oct. 2012, pp. 4985–4992.
[2] J. Thomas, J. Polin, K. Sreenath, and V . Kumar, “Avian-Inspired
Grasping for Quadrotor Micro UA Vs,” in 2013 ASME Int. Design
Engineering Technical Conf. and Computers and Information in En-
gineering Conf., Portland, OR, Aug. 2013.
[3] K. Kondak, K. Krieger, A. Albu-Sch¨ affer, M. Schwarzbach, M. La-
iacker, I. Maza, A. Rodriguez-Castano, and A. Ollero, “Closed-loop
behavior of an autonomous helicopter equipped with a robotic arm for
aerial manipulation tasks,” International Journal of Advanced Robotic
Systems, vol. 10, pp. 1–9, 2013.
[4] D. Mellinger, M. Shomin, N. Michael, and V . Kumar, “Cooperative
grasping and transport using multiple quadrotors,” in 10th Int. Symp.
on Distributed Autonomous Robotic Systems, Lausanne, Switzerland,
Nov. 2010.
[5] J. Fink, N. Michael, S. Kim, and V . Kumar, “Planning and control
for cooperative manipulation and transportation with aerial robots,”
The International Journal of Robotics Research, vol. 30, no. 3, pp.
324–334, 2010.
[6] M. Manubens, D. Devaurs, L. Ros, and J. Cort´ es, “Motion planning for
6-D manipulation with aerial towed-cable systems,” in 2013 Robotics:
Science and Systems, Berlin, Germany, May 2013.
[7] M. B. Srikanth, A. Soto, A. Annaswamy, E. Lavretsky, and J.-J.
Slotine, “Controlled manipulation with multiple quadrotors,” in AIAA
Conf. on Guidance, Navigation and Control, Portland, OR, Aug. 2011.
[8] V . Parra-Vega, A. Sanchez, C. Izaguirre, O. Garcia, and F. Ruiz-
Sanchez, “Toward aerial grasping and manipulation with multiple
UA Vs,” Journal of Intelligent & Robotics Systems, vol. 70, pp. 575–
593, 2012.
[9] A. Franchi, C. Secchi, H. I. Son, H. H. B¨ ulthoff, and P. Robuffo
Giordano, “Bilateral teleoperation of groups of mobile robots with
time-varying topology,” IEEE Trans. on Robotics, vol. 28, no. 5, pp.
1019–1033, 2012.
[10] A. Franchi, C. Secchi, M. Ryll, H. H. B¨ ulthoff, and P. Robuffo Gior-
dano, “Shared control: Balancing autonomy and human assistance with
a group of quadrotor UA Vs,” IEEE Robotics & Automation Magazine,
Special Issue on Aerial Robotics and the Quadrotor Platform, vol. 19,
no. 3, pp. 57–68, 2012.
[11] G. Gioioso, G. Salvietti, M. Malvezzi, and D. Prattichizzo, “Mapping
synergies from human to robotic hands with dissimilar kinematics: an
approach in the object domain,” IEEE Trans. on Robotics, vol. 29,
no. 4, pp. 825–837, 2013.
[12] N. Michael and V . Kumar, “Opportunities and challenges with au-
tonomous micro aerial vehicles,” The International Journal of Robotics
Research, vol. 31, no. 11, pp. 1279–1291, 2012.
[13] G. Gioioso, M. Ryll, D. Prattichizzo, H. H. B¨ ulthoff, and A. Franchi,
“Turning a near-hovering controlled quadrotor into a 3D force effec-
tor,” in 2014 IEEE Int. Conf. on Robotics and Automation, Hong Kong,
China, May. 2014.
[14] T. Iberall, “The nature of human prehension: Three dextrous hands in
one,” in 1989 IEEE Int. Conf. on Robotics and Automation, Scottsdale,
AZ, May 1989, pp. 396–401.
[15] D. Prattichizzo and J. C. Trinkle, “Grasping,” in Springer Handbook
of Robotics, B. Siciliano and O. Khatib, Eds. Springer, 2008, pp.
671–700.
[16] R. M. Murray, Z. Li, and S. S. Sastry, A mathematical introduction to
robotic manipulation. CRC, 1994.
[17] A. Bicchi, “Force distribution in multiple whole-limb manipulation,”
in 1993 IEEE Int. Conf. on Robotics and Automation, Atlanta, May
1993, pp. 196–201.
[18] G. Salvietti, L. Meli, G. Gioioso, M. Malvezzi, and D. Prattichizzo,
“Object-based bilateral telemanipulation between dissimilar kinematic
structures,” in 2013 IEEE/RSJ Int. Conf. on Intelligent Robots and
Systems, Tokyo, Japan, Nov 2013.
[19] D. Prattichizzo and A. Bicchi, “Consistent task speciﬁcation for
manipulation systems with general kinematics,” ASME Journal on
Dynamic Systems, Measurement, and Control, vol. 119, pp. 760–767,
1997.
[20] C. Borst, M. Fischer, and G. Hirzinger, “A fast and robust grasp
planner for arbitrary 3D objects,” in 1999 IEEE Int. Conf. on Robotics
and Automation, Detroit, MI, May 1999, pp. 1890–1896.
[21] T. Lee, M. Leokyand, and N. H. McClamroch, “Geometric tracking
control of a quadrotor UA V on SE(3),” in 49th IEEE Conf. on Decision
and Control, Atlanta, GA, Dec. 2010, pp. 5420–5425.
[22] D. J. Lee and C. Ha, “Mechanics and control of quadrotors for tool
operation,” in 2012 ASME Dynamic Systems and Control Conference,
Fort Lauderdale, FL, Oct. 2012.
[23] D. J. Lee, A. Franchi, H. I. Son, H. H. B¨ ulthoff, and P. Robuffo
Giordano, “Semi-autonomous haptic teleoperation control architecture
of multiple unmanned aerial vehicles,” IEEE/ASME Trans. on Mecha-
tronics, Focused Section on Aerospace Mechatronics, vol. 18, no. 4,
pp. 1334–1345, 2013.
4341
