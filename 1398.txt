  
  
Abstract— A wearable robot secured on the shoulder of a 
human is developed for assisting its wearer in the execution of 
tasks in the overhead workspace. Installing a ceiling panel is an 
example of such a task. During this task the robot can hold the 
panel or collaborate actively with the human in order to fix the 
panel with the appropriate equipment. This wearable robot, 
termed “Supernumerary Robotic Limbs”, works closely with 
the human to streamline the operation and reduce the human 
workload. First, the design concept of the Robot-on-the-
Shoulder system is described, and a new approach to the 
coordinated control between the wearable robot and the human 
is presented. A graphical task process representation based on 
Coloured Petri Nets (CPN) is used to model the concurrent and 
distributed nature of the human-robot system, which comprises 
two human hands and two robot hands. The CPN framework is 
then extended to a type of hybrid control system by imbedding 
local dynamic controllers in the Transition nodes of the CPN 
model. Each local dynamic controller collects sensor signals 
relevant to the target transition and makes a predictive control 
decision. This allows the robot on the shoulder to take a 
proactive and preemptive action as well as to confirm a 
successful Transition. The control parameters for these 
algorithms are tuned based on “teaching-by-showing” 
techniques using human demonstration data. Partial Least 
Squares is used for extracting significant sensor signals from 
high-dimensional sensor data in order to do real time 
predictions and control. A prototype robot-on-the-shoulder 
system is built, and the CPN hybrid control is implemented and 
tested for a ceiling panel installation task.  
 
 
I. INTRODUCTION 
In our daily chores we often need an extra hand to hold an 
object temporarily while both of our hands are busy. When 
installing a lighting fixture on the ceiling, for example, we 
wish to have a third arm to hold the fixture while securing it 
with screws using a screwdriver. Such tasks in the overhead 
workspace result very laborious for the worker since the 
workspace between the shoulder and the waist is the comfort 
zone of the human. Installation, inspection, and repair of 
ceiling panels, cable wires, air ducts, and lighting fixtures are 
just a few examples of tasks that are ergonomically 
 
*Research supported by The Boeing Company. 
B. Llorens – Bonilla is with the Department of Mechanical Engineering, 
Massachusetts Institute of Technology, Cambridge, MA 02139 USA 
(corresponding author to provide phone: 617-233-0515; e-mail: 
llorensb@mit.edu).  
H. H. Asada is with the Department of Mechanical Engineering, 
Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-
mail: asada@mit.edu). 
 
challenging for the human. The arm’s strength as well as 
precision and dexterity of hand performance deteriorate 
sharply as the work area is elevated to the overhead space. 
Prolonged work in an inefficient and uncomfortable posture 
often leads to injuries at the neck, arms, and the back.  
 The goal of this work is to develop a wearable robot that 
is secured to the shoulder of a human and that assists the 
human in executing a task. Figure 1 shows a robot on the 
shoulder, called Supernumerary Robotic Limbs (SRL), 
assisting the wearer in installing a ceiling panel. It is expected 
that, with the extra arms coordinated with the human arms, 
the human can perform the installation task in an effective 
and productive manner with a reduced physical effort. One of 
the many ways the robot can aid the human is by holding the 
ceiling panel. This allows the human to use his or her, now 
free, hands to pick up a screw and a powered screwdriver in 
order to secure the panel. Tasks such as this one usually 
require two workers to perform the task, but the SRL on the 
shoulder allows a single worker to execute the task 
individually.  
 
 
Figure 1.   SRL prototype that is worn on the shoulders and aids the human 
worker in overhead tasks. 
In the robotics and rehabilitation literature, there are two 
types of wearable robots that have been studied. One is 
prostheses for amputees [1, 2], and the other is exoskeletons 
for extending the strength of human limbs [3, 4]. SRL is a 
third type of wearable robots that has distinct features and 
functionality. Unlike prostheses, SRL provides the wearer 
with extra limbs rather than replacing lost limbs. Unlike 
exoskeletons, where powered joints are attached to the 
corresponding joints of the wearer, the SRL moves 
independently from them wearer’s limbs. These properties 
allow the SRL to open up new possibilities for wearable 
robots, while creating unique technical issues and challenges. 
A Robot on the Shoulder: Coordinated Human-Wearable Robot 
Control using Coloured Petri Nets and Partial Least Squares 
Predictions* 
Baldin Llorens – Bonilla, H. Harry Asada, Member, IEEE 
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 119
  
 Communication and coordination between the SRL and 
its operator is the key to successful implementation of the 
SRL. The robot must understand the intentions of its wearer 
and plan its actions accordingly. Human-robot 
communication and coordination have been studied over the 
last 30 years, resulting in a number of effective techniques 
and methodologies. These include visual gesture 
understanding [5, 6, 7], voice recognition [8, 9], and other 
monitoring techniques [10]. Various human – robot 
communication techniques have also been developed in the 
fields of robotic surgery, tele – operation and virtual reality 
[11, 12, 13]. Other studies have focused on developing 
methods for modeling collaborative task processes. These 
methods include Markov processes [14, 15], probabilistic 
state machines [16], and a network of discrete events 
connected by indicators [17], all of which seek to achieve 
coordinated control of the robot and human activities. 
 These prior studies have primarily focused on a human-
robot system where the robot is physically separated from the 
human. In contrast, SRL is attached to the human body, 
which leads to the creation of a communication channel that 
substantially differs from the cases with separate robots. The 
reaction force and moment induced by the robot’s motion, as 
well as the ones that the environment exerts on the robot’s 
end-effector, are transmitted to the human body as haptic 
feedback. For example, considering the scenario in Figure 1: 
as the robot holds the ceiling panel and presses it against the 
ceiling frame, the human feels the reaction force on his or her 
shoulder. In fact, the human can use this haptic feedback as a 
measure of how securely the ceiling panel is being held and 
to coordinate better his or her motion with the robot. 
 Having the SRL working in close proximity to the 
human, both pairs of human and robotic arms concurrently 
share the same workspace. One of the new technical 
challenges of this problem is the coordination and 
synchronization among the four arms, which is essential for 
the successful and efficient implementation of the SRL. It is 
imperative to employ an effective modeling method for 
representing a collaborative task process, where multiple 
arms and hands are working concurrently. 
 This paper presents the novel design concept of the 
wearable supernumerary robotic limbs and also addresses the 
coordination and communication issues between the robot 
and its human wearer. Coordination will be planned using 
Coloured Petri Nets (CPN) [18, 19, 20] and communication 
will be handled using Partial Least Squares (PLS) [25] based 
predictions. Petri Net is a powerful tool for representing 
distributed, concurrent, and discrete event processes [21]. 
The tasks performed by four arms, including two human 
arms and two SRLs robot arms, are a distributed system 
consisting of both human and robot control systems. It is also 
a concurrent event system, since the four arms make state 
transitions in parallel. However, describing the discrete 
nature of state transitions alone cannot capture important 
dynamics of the system required to perform real-time 
coordination control. To address this issue, local dynamic 
controllers are built for a few selected transitions, where the 
robot takes proactive and preemptive actions by observing 
the human and the task process status. This can increase the 
level of human-robot coordination from static step-by-step 
transitions to dynamic, prediction-based transitions.  
 This paper presents the basic design concept description 
of the Robot-on-the-Shoulder system, followed by CPN 
modeling of human-robot-task processes. Using PLS and 
teach by demonstration data, we embed local, dynamic 
controllers in several transitions of the CPN model. This 
extends the CPN model to a real-time hybrid system.  We use 
a prototype system to demonstrate the features of the 
proposed approach. 
II. PROTOTYPE DESIGN 
The primary objective of developing a Robot-on-the-
Shoulder system is to alleviate the overhead workload. The 
robot can assist the human in performing laborious work such 
as raising, holding, and securing an object in the overhead 
area, thus reducing fatigue and the possibility of injuries. 
Furthermore, the Robot-on-the-Shoulder can streamline the 
task execution by serving as a co-worker. As described 
previously in the installation of a ceiling panel, the human 
and the robot can work concurrently to expedite the 
execution of multi-step operations.  
 
Figure 2.  Example of a person carrying a heavy load through the use of a 
rod aligned across the shoulders. 
Wearing a robot may represent an additional burden for 
the human. However, carrying a robot on the shoulder is 
indeed not a significant load. As long as the center of mass of 
the robot is aligned with or near the centerline of the human 
spine, the weight of the robot is directly borne by the 
backbone thus decreasing the bending moment acting on the 
back. For example, people can carry heavy loads, such as 
buckets of water, with a rod aligned with the shoulder (Figure 
2). Figure 3 shows the overall view of the Robot-on the-
Shoulder system. It consists of two arms, each with five 
degrees of freedom (DOF), a shoulder mount, and an 
electronics unit and various sensor modules that are attached 
to the back of the mount. Details of our prototype 
implementation are included in Chapter V, Section A. 
III. COLOURED PETRI NET AND TASK MODEL 
We consider the ceiling panel installation task as an 
exemplary case study to build a task model, since it contains 
many steps of actions that need high-level skills and 
coordination. In the case in which the SRL performs the roles 
of a helping worker, we will consider a ceiling panel 
installation task. Figure 4 illustrates each step of this case: 
120
  
 
 
Figure 3.  The top figure shows the front view of the robot mount. The 
shoulder mount is placed directly over the shoulder line, just like in Figure 
2. When in rest position, the robot arms are folded backward and inward. 
This minimizes its inertia and keeps the robot outside the range of motion of 
its wearer. The bottom figure shows the robot’s right arm. Red arrows and 
axes indicate the arm’s five degrees of freedom. 
• Step 1: The human worker picks up and positions the 
panel.  
• Step 2: After the panel is properly positioned, the 
SRL holds the panel for the human.  
• Step 3: The human proceeds to pick up and place a 
screw and operate a powered screwdriver (PSD) to 
fix the panel.  
• Step 4: Human can repeat step 3 until the panel has 
been secured. Note that as more screws are fixed to 
the panel, the robot’s load is decreased and 
eventually it can remove one or even both arms from 
the panel.  
This scenario presents ideal circumstances since the human 
performs the complex parts, e.g. placing the screw and 
operating the PSD, while the robot holds the panel once its 
wearer has placed it. Note that the panel must be held against 
the ceiling frame at all times, and two hands, either the 
human or the robot hands, or even a combination of these is 
needed to securely hold it. However, assignment of hands, 
i.e. which hands to hold the panel, may change dynamically 
as such is the nature of collaboration between coworkers. For 
example, the human worker may have difficulty using the 
PSD due to environmental constrains. He can exploit the 
SRL’s unconstrained arm mobility by switching roles with 
the SRL. That way he can hold the ceiling panel while the 
SRL takes care of fine – positioning and operating the PSD.  
This section will focus on showing the advantages brought on 
by using a CPN model to plan the robot’s actions. For 
detailed information on how to design, model and analyze a 
CPN, please refer to [18]. 
                                        
Figure 4.  Schematic for each of the task’s steps of transitions. The red 
color represents the human wearer, black represents the SRL, blue is the 
ceiling panel, green is a screw and purple represents the powered screw 
driver.  
We consider a case relevant to the ceiling panel 
installation task and to the possible scenarios presented in the 
previous paragraph. This task process contains multiple 
events that occur concurrently and in a distributed manner. 
For the ceiling panel installation task, the following discrete 
static states, termed Places P, and dynamic state paths, 
termed Transitions T, are defined: 
P = {Standby, SRL holds panel in position, Screw in 
Position, PSD in position, Fastened Screws} 
T ={Prepare screw and PSD, Fastened screw} 
The Places or Transitions occurring on the system are 
controlled by Tokens, which represent the availability of 
resources. In the human-robot system, multiple types of 
resources, e.g. human arms vs SRLs, must be distinguished. 
We use Coloured Petri Nets (CPN) in order to employ Colour 
Tokens containing diverse information that allows us to 
identify them and assign them their roles accordingly. The 
Colours, or properties for the system’s tokens, used for our 
model are: 
∑ = {No, Re, NoxRe, status, TR}, 
where the colours No, Re, NoxRe, status and TR are integer, 
string, product of integer and string, Boolean and a list of 
integer and strings products respectively. These colours serve 
as labels that are used to distinguish each token and allow us 
to group and use them as we want. Using state space analysis 
for our CPN model we concluded that it has only one 
possible end state (completed task) but various manners of 
achieving it. Figure 5 shows a simplified CPN model 
representation of one part of the panel installation task. 
Numbers in the circles represent their respective Places P, 
Letters in the rectangles represent their respective Transitions 
T and the small circles in the Places are the Tokens. This 
schematic will be used to explain how CPN modeling 
matches our system’s concurrent and collaborative demands.  
Given that the SRL is holding the panel (red tokens are 
assigned to place 2) the human worker can proceed to grab a 
screw from the standby place, move it into place (Place 3) 
and use his or her other hand to grab the PSD (Place 4). After 
both tools are in position the worker can proceed to fix the 
screw and repeat the process. After the first screw has been 
positioned, both hands of the human worker return to 
standby. At this point, the robot can choose to keep holding 
the panel with both arms, or, since the load has been greatly 
reduced, can choose to bear the entire load on one arm and 
return the other to standby. This arm can assume the role of 
positioning the screw or operating the PSD given the right 
conditions, or even doing both in case the human worker 
121
  
decides to hold the panel and switch roles entirely with the 
SRL. Some conditions that may be evaluated by the CPN 
when deciding which tokens should proceed to Places 3 and 4 
might be muscle fatigue, environmental constrain, poor 
performance, etc. By adding sensors to the system or 
establishing certain criteria we can decide how to establish 
our priorities when assigning roles. One example would be 
the implementation of electromyography to determine when 
the worker’s fatigue levels are high enough. The CPN model 
would proceed to assign the passive tasks to the worker and 
the more demanding ones to the SRL.  
 
Figure 5.  Simplified section of our CPN model’s places and transitions. 
Places 1 through 5 correspond to Places P, transitions C through D to 
Transitions T. Tokes have been simplified as to be identifiable using the 
colors shown in the legend to the right of the schematic. 
It is important to clarify that the CPN model shown in 
Figure 5 is a simplified version of our complete model that is 
used to show how the places and transitions are connected. 
Each arc has a unique expression that determines the 
behavior of the tokens during the execution of each 
transition. Each transition has guard expressions that also 
indicate which tokens are needed in order to trigger each 
transition. This results in a very complex model that is not 
included in this paper. The CPN-based task modeling helps 
examine whether transitions can occur properly without 
leading the task process into a dead lock. The CPN Tools 
software was used for validating the execution of the task 
process.  
 
IV. PARTIAL LEAST SQUARES PREDICTIONS AND CONTROL 
Experienced co-workers execute the ceiling panel 
installation task in a much more dynamic and efficient 
manner than the step-by-step discrete transitions depicted by 
the standard CPN model. Each worker observes the co-
worker’s action and makes a proactive, or even preemptive, 
action based on “prediction”. This prediction-based, 
proactive/preemptive transition can be generated by 
combining the CPN model with “teach through 
demonstration” data.  
Human skill acquisition is an effective method for 
extracting human skills from teaching-by-showing data [5, 
22, 23, 24]. In this work we observe coordinated actions of 
two co-workers and extract dynamic predictors for triggering 
transitions that are made in a proactive/preemptive manner. 
The identified dynamic predictors are embedded in the CPN 
model and used for controlling SRLs in real-time. 
A challenge in applying human skill acquisition is the 
difficulty in finding the right “structure” of skill model to 
predict the observed behavior. It is not clear which signals 
and what sort of traits the human uses in his/her skills. To 
ease this fundamental difficulty, we employ a multivariate 
data analysis technique, referred to as Partial Least Squares 
(PLS) regression. PLS is an extension of Principal 
Component Analysis to input-output data, where most 
significant latent variables are determined to maximize the 
correlation for predicting outputs. In this section we describe 
the methods, experiments and equipment used to prove that 
the PLSR algorithm can be successfully used to find a 
relationship between the sensors worn by the human worker 
and the preemptive action taken by his or her coworker. 
In this work, a number of signals are observed from 
human demonstrations, and a high-dimensional input space is 
formed by aggregating the data over a certain time window. 
Human demonstration will take place at a test rig that 
resembles a ceiling structure (Figure 6). During these tests 
the main worker (wearing the SRL) will pick up a ceiling 
panel, each time from a different location, and proceed to 
position the panel with the correct orientation against the test 
rig. During this time, the second worker will be manually 
operating the SRL in order to help the main worker position 
and hold the panel. The goal of this second worker is to 
minimize the amount of time that the main worker has to 
hold the panel. With that in mind, for this experiment the 
second worker cannot assume leadership and cannot interfere 
directly with the main worker’s workspace.   The test subject 
that wears the SRL will wear 3 inertial measurement units 
(IMUs). Each will record gyro and accelerometer 
information.  These are worn in two different locations: one 
is worn in each wrist (also shown in Figure 6) and one at the 
base of the robot’s shoulder mount. This last IMU ensures 
that the overall orientation and motion of the robot’s base 
(affected by human motion) is taken into account for our 
prediction algorithm. In its simplest form the input data space 
recorded at any time t contains the gyro and accelerometer 
data from 3 IMU’s. This accounts for 18 data points. 
However, given the spacial-temporal aspect of the task at 
hand, we’ll use the PLSR algorithm to make predictions 
based on time-series data.   
The output space is formed by observed data of the co-
worker’s movements, which can be used for controlling the 
SRL. In these tests, the co-worker’s actions are presented as 
all the joint angles for the robot, therefore the output matrix 
at any time t consists of 10 readings corresponding to the 5 
joints of both robot arms. The objective of PLS data analysis 
is to find the most significant factors of both data groups and 
identify which factors from the input data can describe best 
the output [25, 26, 27, 28]. 
Taking into consideration all data recorded at time t we 
can proceed to build the input and output matrices used for 
the PLSR predictions. We had 5 test subjects that, arranged in 
5 different worker #1 – worker #2 configurations, each 
repeated the experiment 25 times under different conditions. 
122
  
  
Figure 6.  Figure to the left shows the test rig where the ceiling panel has to 
be installed. Figure to the right shows one of the trials where test subject #2 
(wearing red) helps test subject #1 during the ceiling panel instalation task. 
As shown in the figure to the right, two IMUs are located at the test subject 
#1’s wrists.  
  
Out of these trials, 20 were used as training data for the 
algorithm and 5 were used for data validation. The trials used 
for data validation were chosen at random and excluded from 
the training algorithm. From each trial we will evaluate a 
number of 160 data points, which corresponds to the amount 
of samples during the slowest task completion + 20 samples. 
Using data recovered at time t we can build: 
x(t)=(x
1
(t),x
2
(t),...,x
18
(t))
y(t)=(?
1
(t),?
2
(t),...,?
10
(t))
                               
(1)
(2)
 
Storing data from time t – 19 up to t we can build the 
following matrices: 
X
n
(t)=[x(t?19),x(t?18),...,x(t)]
Y
n
(t)= y(t)
                  
(3)
(4)
 
Where n = t and goes from 1 to 160 correspond to the 
respective time sample in the trial data point. Notice that for 
point n = 1 from the trial data, (3) will include up to the x(t – 
19)th term. In order to include such data in our analysis we 
prohibited the test subject from starting to perform the task 
until a period of 5 seconds has gone by. During this time the 
test subjects are free to do as they will as long as the helping 
worker maintains the SRL’s arms at the rest position. This 
allows us to have a variety of initial conditions for our 
evaluated data sets. The output matrix (4) is straightforward 
to build as it only contains the joint angles at time t. Taking 
into account our M = 20 training data sets, and J = 5 
configurations of test subjects, we can build our X and Y 
matrices which will be used for the PLSR algorithm: 
 ??
!
=
?
!
?
?
?
!
?
,???
!
=
??
!
?
?
??
!
?
,?=
???
!
?
?
???
!
?
      (5)  
 ??
!
=  
?
!
?
?
?
!
?
  ,???
!
=  
??
!
?
?
??
!
?
,?=
???
!
?
?
???
!
?
        (6)  
Before implementing the PLSR algorithm, both X and Y 
were column normalized. In order to mimic using the PLSR 
for real-time predictions, the validating data sets were 
normalized using the averages and standard deviation from 
the training data sets. Using the normalized matrices as our 
input and output matrices respectively for the PLSR 
algorithm allowed us to calculate the actions that the robot 
must make at time t in order to help its wearer in a proactive 
and preemptive manner. By predicting the joint angles for the 
SRL at each instant in time, then we can employ Adaptive or 
Sliding control to ensure that the SRL follows our predicted 
trajectory [29]. 
Integrating these predictions with the CPN model brings 
another interesting aspect to how the tasks are executed. 
Instead of having executed every task after the previous has 
been completed, now we can have both the robot and the 
human working concurrently and in the same workspace. 
This behavior is not a part of the CPN model, and forces our 
CPN model to form a super transition where concurrent 
actions of this nature can happen. This concept is shown in 
Figure 7.  
                                                                                   
Figure 7.  Figure on the left is a simplified version of the CPN model of the 
task section performed in the experiment. Figure on the right shows sensor 
data of both X (top) and Y (bottom). The black square in the top graph 
represents transition a and the one in the bottom graph transition b.While a 
CPN model would not allow the robot to start moving until the human has 
finished positioning the panel, PLSR allows the robot to act in a preemptive 
manner.  
Using traditional Transitions and Places does not allow the 
robot to act preemptively. However, in order to employ the 
PLSR algorithm to perform highly coordinated and 
concurrent actions we can combine various transitions and 
places into a single, broader transition: a super transition. In 
this case, the super transition would absorb transitions a and 
b, as well as Place 2. The resulting super transition would 
then utilize the PLSR algorithm to control the robot. 
V. PROTOTYPING AND EXPERIMENTAL RESULTS 
A.  Prototyping  
We wanted this robot to be tightly attached to its 
operator’s upper body. Also, the human’s current arm 
movement should interfere as little as possible with this 
mount. Because of this, the part that is in direct contact with 
the human should be as close as possible to the cervical 
region, resting on the trapezius. This is the area that remains 
most static when a person extends his or her arms upward. 
The contact area is extended to the clavipectoral triangle in 
the front and to the middle of the trapezius in the back. We 
use hiking backpack straps to ensure that the mount is 
strongly attached to the user. These straps are attached to the 
bottom part of the mount and also serve as padding. They are 
123
  
in direct contact with the wearer from the trapezius to the 
pectoral muscles and the axillary fold. The straps are crossed 
in the back as to ensure that the mount remains attached to 
the upper chest and back area and does not shift its support to 
the shoulders and arms. This design aligns the center of mass 
of the robot with that of its operator. This adds the weight of 
the robot directly to the human’s, thus this added weight does 
not create a torque about the back. This greatly reduces the 
possibility of wear-induced back problems as the load goes 
directly to the legs. In addition, having the arms produce 
reaction torques directly to the operator’s upper body is more 
natural to the human. This aids in achieving the goal of 
having this robot feel as an extension of the human body.  
Each of the SRL’s arms has five degrees of freedom 
(DOF), which enables us to have full position control in 3D 
space and allows us to control two DOF for the orientation of 
the endpoint. The servoed joints are torque-controllable and 
provide position, speed, and torque information. These servos 
are also fully back drivable, a property that is used to obtain 
the “teach by demonstration” training data. We also include 
sensors for monitoring the state of a tool, such as the ones for 
measuring the spindle speed and torque of a powered 
screwdriver. All the data was recorded using LabView as our 
interface. 
B.  PLSR Experimental Results 
In this section we will present the results obtained from 
using PLSR to determine the SRL’s joint angles in real time. 
Figure 8 shows the percent variance of the data sets as a 
function of components to be used for the PLS regression. 
Using the first 3 components we can describe 85% of the 
output data variance.   
 
Figure 8.  Variance of the data sets in percent as a function of the number 
of components used for the PLS regression. 
Using these three components for our algorithm results 
in the desired behavior of having the SRL act proactively in 
aiding the human worker. By further examining the loading 
vectors from our input matrix we can determine which 
sensor data is the most important when making predictions. 
The first component relies heavily on the acceleration data 
from all three axis of the IMUs, this is related to both motion 
and posture of the human worker. By examining the 
temporal aspects of the input matrix we can see that the 
acceleration values from our IMUs remain equally important 
without regards to that point’s place in time (wether it is x(t-
19) or x(t-3)). However, when taking a look at the loadings 
of the second and third principal component we see complex 
combinations of both acceleration and velocity 
measurements that vary with time. Considering these three 
components we can determine that the sensor information 
most relevant to making the predictions are the 
accelerometer readings from all 3 IMUs, the angular 
velocities in z from the IMUs located in the wrists and the 
velocity in y from the IMU at the base of the robot.  An 
average result of our prediction on the validating data set is 
shown in Figure 9. We can see that the joint prediction (only 
showing 3 joint angles for simplicity) is quite accurate, as 
this algorithm can be used to control the general behavior of 
the SRL while implementing other systems, such as vision, 
to perform fine positioning in the environment. One of the 
advantages of using PLSR as our control algorithm is that 
although the human operator is active before the execution 
of the task, this algorithm does not recognize that moving 
pattern as the one that corresponds to the transition in 
question. Another advantage is that the input matrix X used 
for the PLSR algorithm is taking into account the orientation 
and movement properties of the robot’s base, thus the joint 
angles predicted are already accounting for a part of human 
induced disturbances.  
Combining PLSR predictions together with super 
transitions in a CPN model can be used in order to control 
the SRL in a proactive manner. Using these super transitions 
removes idle time, thus accelerating the execution of the 
process and greatly reducing the worker’s fatigue.  
 
Figure 9.  Top figure shows the accelerometer data recorded from the IMU 
located in the robot’s shoulder mount. Figure in the middle shows all the 
recorded data from all IMUs in our validating data set. The figure in the 
bottom shows the recorded position joint angles for the SRL’s right arm 
(solids) and our PLSR predicted values (dashed). 
One interesting aspect is that this algorithm cannot only 
be used to determine the robot’s actions based on the 
human’s behavior, but also vice versa. For example, after the 
robot hands are confirmed to be holding the panel, the 
human’s hands are free and briefly resume the standby 
position. By changing our X and Y matrices, we can use this 
PLS to predict the human’s actions based on the torque 
exerted by the joints of the SRL. This is the equivalent of 
using the reaction forces felt by the human operator to 
predict when the human has determined that the SRL has 
secured its hold on the ceiling panel. Once again, this 
algorithm can be used to preemptively trigger other dynamic 
transitions that require the SRL and its operator to work in a 
0 2 4 6 8 10
60
65
70
75
80
85
90
95
Number of Principal Components and % Variance Explained
0 2 4 6 8 10 12
?20
?10
0
10
20
Gyro and Accelerometer data for all 3 Sensors
Magnitude
0 2 4 6 8 10 12
?1.5
?1
?0.5
0
Accelerometer Data ? Robot Mount Orientation
G
 
 
0 2 4 6 8 10 12
50
100
150
200
250
300
Robot Joint Angle Prediction
Time [s]
Degrees
 
 
Theta 1
Theta 2
Theta 3
Theta 1 estimate
Theta 2 estimate
Theta 3 estimate
Z
Y
X
124
  
coordinate manner while executing concurrent tasks that are 
dependant of one another. 
VI. CONCLUSION AND FUTURE WORKS 
A wearable robot secured to a human’s shoulders is 
developed and presented. The design constraints were 
presented in order to have a wearable robot that does not 
create torques about its operator’s back and that minimally 
interferes with the operator’s actions. This wearable robot, 
termed Supernumerary Robotic Limbs (SRL), works in a 
highly coordinated manner with the human during the 
execution of such tasks. Coloured Petri Nets (CPN) is used 
to model the concurrent and distributed nature of a ceiling 
panel installation task. This model assigns to the robot the 
roles that result more laborious for the human, thus lessening 
the workload on the human worker. The CPN framework is 
then extended to a hybrid control system by grouping several 
transitions and places into a super transition, which is then 
imbedded with dynamic controllers. Each dynamic 
controller collects sensor signals from the human operator 
and the robot mount. Using this data and a Partial Least 
Squares Regression (PLSR), we are able to predict the 
SRL’s trajectory based on the human’s actions in a 20 
sample window. Using a rough prototype and teach by 
demonstration experimental data we tuned the PLS 
algorithm to allow the SRL to take proactive action during 
the execution of the task. This algorithm proved to be a very 
strong solution to both determining when the robot can start 
to execute its role and for determining in real time the 
trajectory that the SRL has to follow during each transition.  
REFERENCES 
[1] S. Bitzer and P. van der Smagt, "Learning EMG control of a robotic 
hand: towards active prostheses." IEEE International Conference on 
Robotics and Automation (ICRA), pp. 2819-2823, 2006. 
[2] G. S. Dhillon and K. W. Horch, “Direct neural sensory feedback and 
control of a prosthetic arm”. IEEE Transactions on Neural Systems 
and Rehabilitation Engineering, pp. 468-472, 2005. 
[3] S. Kousidou, N. Tsagarakis, D. G. Caldwell, and C. Smith, “Assistive 
exoskeleton for task based physiotherapy in 3-dimensional space”. 
The First IEEE/RAS-EMBS International Conference on Biomedical 
Robotics and Biomechatronics, pp. 266- 271, February 2006. 
[4] A. Dollar and H. Herr, “Lower extremity exoskeletons and active 
orthoses: challenges and state-of-the-art” IEEE Transactions on 
Robotics, vol. 24, no. 1, pp. 144–158, 2008. 
[5] Chao Hu; Meng, M.Q.; Liu, P.X.; Xiang Wang, "Visual gesture 
recognition for human-machine interface of robot teleoperation", 
IEEE/RSJ International Conference on Intelligent Robots and Systems 
(IROS). vol.2, pp.1560-1565, 2003. 
[6] F.A. Bertsch and V.V. Hafner, "Real-time dynamic visual gesture 
recognition in human-robot interaction," 9th IEEE-RAS International 
Conference on Humanoid Robots. pp. 447-453, 2006. 
[7] Y. Sato, K. Bernardin, H. Kimura and K. Ikeuchi, "Task analysis 
based on observing hands and objects by vision," IEEE/RSJ 
International Conference on Intelligent Robots and Systems, vol.2, pp. 
1208-1213, 2002. 
[8] J.M. Lee, J.S. Choi and M. Park, "Design of the robotic system for 
human-robot interaction using sound source localization, mapping 
data and voice recognition" in IEEE’s ICCAS-SICE, pp. 1143-1147, 
2009. 
[9] R. Brueckmann, A. Scheidig and H. Gross, "Adaptive Noise 
Reduction and Voice Activity Detection for improved Verbal Human-
Robot Interaction using Binaural Data," IEEE International 
Conference on Robotics and Automation, pp. 1782-1787, 2007. 
[10] B. Gleeson, K. MacLean, A. Haddadi, E. Croft and J. Alcazar, 
"Gestures for industry Intuitive human-robot communication from 
human observation," 8th ACM/IEEE International Conference on 
Human-Robot Interaction (HRI), pp. 349-356, 2013. 
[11] T.B. Sheridan, "Space teleoperation through time delay: review and 
prognosis," IEEE Transactions on Robotics and Automation, vol.9, 
no.5, pp. 592-606. 
[12] S. Kamuro, K. Minamizawa, N. Kawakami and S. Tachi, 
"Ungrounded kinesthetic pen for haptic interaction with virtual 
environments," The 18th IEEE International Symposium on Robot 
and Human Interactive Communication, pp. 436-441, 2009. 
[13] P. Berkelman and J. Ma, "A Compact, Modular, Teleoperated Robotic 
Minimally Invasive Surgery System", The First IEEE/RAS-EMBS 
International Conference on Biomedical Robotics and 
Biomechatronics, pp. 702-707, 2006. 
[14] M. Hiratsuka and H. H. Asada, “Detection of human mistakes and 
misperception for human perceptive augmentation: Behavior 
monitoring using hybrid hidden markov models”, In IEEE 
International Conference on Robotics and Automation (ICRA). Vol. 1, 
pp. 577-582, 2000. 
[15] A.B. Karami, L. Jeanpierre, and A.I. Mouaddib. "Human-robot 
collaboration for a shared mission." 5th ACM/IEEE International 
Conference on Human-Robot Interaction (HRI), pp. 155-156, 2010.  
[16]  M. Awais, and D. Henrich, “Human-robot collaboration by intention 
recognition using probabilistic state machines”. IEEE 19th 
International Workshop In Robotics in Alpe-Adria-Danube Region 
(RAAD), pp. 75-80, June 2010. 
[17] G. A. Pereira, B. S. Pimentel, L. Chaimowicz and M. F. Campos, 
“Coordination of multiple mobile robots in an object carrying task 
using implicit communication”. IEEE International Conference on 
Robotics and Automation, Vol. 1, pp. 281-286, 2002. 
[18] K. Jensen, and L. M. Kristensen, “Coloured Petri Nets: modelling and 
validation of concurrent systems”, Springer, Chapter 1-7, 2009.  
[19] A. A. Desrochers, and R. Y. Al-Jaar , “Applications of Petri nets in 
manufacturing systems: modeling, control, and performance analysis” 
New York IEEE press, Vol. 70, 1995. 
[20] N. Viswanadham and Y. Narahari, “Coloured Petri net models for 
automated manufacturing systems”, IEEE International Conference on 
Robotics and Automation (ICRA), Vol. 4, pp. 1985- 1990, 1987. 
[21] B.J. McCarragher and H. Asada, "A Discrete Event Controller Using 
Petri Nets Applied to Robotic Assembly: The Desired Velocity 
Commands," American Control Conference (ACC), pp. 2473-2478, 
1992. 
[22] H. Asada and H. Izumi, "Direct teaching and automatic program 
generation for the hybrid control of robot manipulators," IEEE 
International Conference on Robotics and Automation (ICRA), vol.4, 
no., pp. 1401-1406, 1987. 
[23] H. Asada and S. Liu, "Transfer of human skills to neural net robot 
controllers," IEEE International Conference on Robotics and 
Automation (ICRA), vol. 3, pp. 2442-2448, 1991. 
[24] B. Llorens-Bonilla, F. Parietti, and H. H. Asada. "Demonstration-
based control of supernumerary robotic limbs." IEEE/RSJ 
International Conference on Intelligent Robots and Systems (IROS), 
pp. 3936-3942, 2012. 
[25] V.E. Vincenzo, W.W. Chin, J. Henseler and H. Wang, “Handbook of 
partial least squares: Concepts, methods and applications”. Springer, 
2010. 
[26] H. Abdi, “Partial least squares regression and project on latent 
structure regression (pls resgression)”, Wiley Interdisciplinary 
Reviews: Computational Statistics, vol. 2, pp. 97–106, 2010.  
[27] S. Wold, M. Sjöström, and L. Eriksson, "PLS-regression: a basic tool 
of chemometrics." Chemometrics and Intelligent Laboratory 
Systems”, vol. 58, no. 2 , pp.109-130, 2001. 
[28] A.R. McIntosh, W.K. Chau, and A.B. Protzner. "Spatiotemporal 
analysis of event-related fMRI data using partial least 
squares." Neuroimage, vol. 23, no. 2, pp.764-775, 2004. 
[29] J.J.E. Slotine and W. Li, “Applied nonlinear control”. Vol. 199, no. 1. 
New Jersey: Prentice hall, 1991. 
125
