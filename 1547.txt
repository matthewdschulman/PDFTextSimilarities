Outdoor Place Recognition in Urban Environments Using Straight Lines
Jin Han Lee, Sehyung Lee, Guoxuan Zhang, Jongwoo Lim, Wan Kyun Chung, and Il Hong Suh
Abstract— In this paper, we propose a visual place recognition
algorithm which uses only straight line features in challenging
outdoor environments. Compared to point features used in most
existing place recognition methods, line features are easily found
in man-made environments and more robust to environmental
changes such as illumination, viewing direction, or occlusion
because they are more likely to be extracted from structures.
Candidate matches are found using a vocabulary tree and
their geometric consistency is veriﬁed by a motion estimation
algorithm using line segments. The proposed algorithm operates
in real-time, and it is tested with a challenging real-world
dataset with more than 10,000 database images acquired in
urban driving scenarios.
I. INTRODUCTION
Place recognition algorithms are used in several applica-
tions such as simultaneous localization and mapping (SLAM)
and autonomous robot navigation. In robotics, the geometry-
based localization process often exhibit scalability problems
because, during pose estimation, small errors accumulate and
eventually it becomes too large to be corrected via geomet-
ric reasoning. Therefore, many recent robotics applications
utilize appearance-based techniques for the localizations [1]–
[4].
Currently, most visual place recognition systems use point
features, such as the scale-invariant feature transform (SIFT)
[5] or speeded-up robust features (SURF) [6]. In SLAM
researches, however, there has been some approaches using
lines as landmarks [7]–[11], because lines can effectively
convey structural information with fewer number of them,
as a line spans over a one-dimensional space, rather than a
single point in a space (see Figure 1). However, they have
not been widely adopted because tracking lines are harder
than tracking points and recognizing them is difﬁcult due to
the lack of reliable feature descriptors.
Previously, we proposed a system [12] that reliably rec-
ognizes places in structured indoor environments with only
Jin Han Lee and Sehyung Lee are with the Department of Electronics
and Computer Engineering, Hanyang University, Seoul, Korea. fjhlee,
shlg@incorl.hanyang.ac.kr
Guoxuan Zhang is with the Information Systems Technology and Design
Pillar, Singapore University of Technology and Design, Singapore. Cur-
rently, he is a postdoc at Princeton University, New Jersey, United States.
guoxuan zhang@sutd.edu.sg
Jongwoo Lim is with the Division of Computer Science and En-
gineering, College of Engineering, Hanyang University, Seoul, Korea.
jlim@hanyang.ac.kr
Wan Kyun Chung is with the Department of Mechanical Engineering,
Pohang University of Science and Technology (POSTECH), Pohang, Korea.
wkchung@postech.ac.kr
Il Hong Suh is with the the Department of Electronics
and Computer Engineering, Hanyang University, Seoul, Korea.
ihsuh@hanyang.ac.kr. All correspondences should be addressed
to Il Hong Suh.
Fig. 1. An outdoor scene, and two ﬁgures containing 498 line features and
1758 SURF features, respectively, extracted from the scene. Straight lines
are preferred over points, because they represent structural information more
effectively.
line features, and we aim to extend the method to outdoors
in this work. In [12], we used a Bayesian ﬁltering framework
to reduce the inﬂuence of the noisy responses from the
vocabulary tree. However, the approach has a limit that all
scenes in both of the query and the database need to be
in sequential orders because the Bayesian ﬁltering works
under that assumption. In this work, however, we utilize
a geometric veriﬁcation algorithm instead of the ﬁltering,
and the scenes do not need to be in sequential orders.
This allows applications an incremental construction of the
database. To the best of our knowledge, there has been
no visual place recognition system using only line features
in outdoor environments. We utilize the vocabulary tree
presented by Nister et al. [13] that we train for ﬁnding
matching hypotheses. Then, for geometric veriﬁcation of the
hypotheses, we adopt an idea from Zhang [14] to estimate
a relative motion between two scenes with line segments.
We show that the retrieval performance of a vocabulary tree
built with line descriptors works better than a tree built
with state-of-the-art point descriptors in a structured outdoor
environment, and the potential of using line descriptors in
practical visual place recognition systems. We utilize the
mean standard-deviation line descriptor (MSLD) proposed
by Wang et al. [15] as a descriptor for line segments.
The main contributions of this paper are as follows:
 A geometric veriﬁcation algorithm using line segments
 A real-time implementation and experimental validation
of a place recognition algorithm that uses only line
features under challenging environmental conditions
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 5550
The remainder of this paper is organized as follows.
Section II describes algorithms for ﬁnding matching hypothe-
ses using a vocabulary tree, and presents an experimental
evaluation of the retrieval performance of the tree trained
with line descriptors, by comparing it with another tree
trained with SIFT. Section III presents a motion estimation
algorithm used to verify candidate matches. In Section IV,
we provide results obtained from experiments conducted in
urban driving environments containing several environmental
changes. This paper concludes in Section V.
II. SCENE REPRESENTATION WITH LINE SEGMENTS
A. Line Extraction and Description
In order to extract line segments, we devised a simple but
reliable extractor inspired from [16]. Given an image, Canny
edges are detected ﬁrst and the system extracts line segments
as follows: At an edge pixel the extractor connects a straight
line with a neighboring one, and continues ﬁtting lines and
extending to the next edge pixel until it satisﬁes co-linearity
with the current line segment. If the extension meets a high
curvature, the extractor returns the current segment only if
it is longer than 20 pixels, and repeats the same steps until
all the edge pixels are consumed. Then with the segments,
the system incrementally merges two segments with length
weight if they are overlapped or closely located and the
difference of orientations is sufﬁciently small.
Descriptor vectors for the segments are generated using
MSLD [15]. For each segment, the MSLD ﬁrst identiﬁes
the perpendicular direction d
?
with its average gradient
direction, and parallel direction d
k
rotated 90 degrees from
d
?
in clockwise. For every pixel on the segment, it sets c
subregions each with a size ofrr along to thed
?
in a non-
overlapping manner. If a line segment consists ofl pixels, it
resultscl subregions on the segment. In this work we use
the same settingsc = 9,r = 5 as in [15]. In each subregion,
accumulating distributed gradients along the direction d
?
,
d
k
and their opposite directions results a histogram with four
bins. With the mean and standard deviation of the histograms
calculated along thed
k
results (4+4)9 = 72 dimensional
vectors. This statistical representation allows robust matching
between two line segments with noisy locations of end
points.
B. Vocabulary Tree
The visual bag-of-words approach maps an arbitrary fea-
ture to a visual word using a pre-built dictionary, and repre-
sents the scene with the set of words to recognize it. In other
words, to get a dictionary, it divides the feature space by
clustering given huge number of training features. Then for
each of arbitrary features, it assigns one of the cluster index
to the feature to efﬁciently represent scenes. The vocabulary
tree [13] is one of the most popular algorithms among
the visual bag-of-words family. It hierarchically divides the
feature space to offer more efﬁcient and effective way in
both of training and querying phases, and it enables online
database insertion and querying when it is utilized with an
inverted ﬁle mechanism.
We extracted eight million MSLD descriptors from eight
tourism videos of historical buildings in Europe, and used
them as the training set to build a vocabulary tree. Then,
we performed hierarchical k-means clustering of branching
factor k = 50, number of levels l = 3, with the training set
resulting a tree with 127551 nodes. Following the analysis
of the authors of [13], we use only 125000 leaf nodes in
this work. In Section II-C, we experimentally evaluate the
retrieval performance of the tree.
In the phase of database construction, every descriptor
vector in the scenes inserts the ID of the image to the
corresponding leaf node. Similarly, when querying a scene,
also every descriptor vector in the query image traverses
through the tree to reach a leaf node, then images of the ID
listed in the node represent potential candidate matches and
receive votes. In this voting scheme, we use the normalized
difference with term frequency-inverted document frequency
(TF-IDF) weighting [17] in the L
1
-norm [13].
We deﬁne the query q and the database d vectors as
follows:
q
k
=n
k
w
k
; (1)
d
k
=m
k
w
k
: (2)
Here,
n
k
=
number of word k
number of total words in query scene
(3)
is the term frequency of the wordk in the query image, and
m
k
=
number of word k
number of total words in the database scene
(4)
is the term frequency of word k in a database image.
Moreover, w
k
is the inverted-document frequency given by
w
k
= ln
N
N
k
; (5)
where N is the total number of database images, and N
k
is the number of database images containing the word k.
Then, if the wordk is observed in the query scene, the score
assigned to the database image i is given by
s
i
= 2+
X
kjq
k
6=0;d
k
6=0
 
jq
k
 d
k
j q
k
 d
k

: (6)
In this scoring scheme, the words with high “term frequency”
(i.e., they frequently appear in an image) receive higher
scores. Meanwhile, words with high “inverted-document
frequency” (i.e., they also frequently appear in other images)
are penalized.
C. Evaluation of the Vocabulary Tree in Outdoor Environ-
ments
In order to verify the retrieval performance of the vo-
cabulary tree trained with MSLD line descriptors, we per-
formed experimental comparisons with another vocabulary
tree trained with SIFT in identical environments. We used
a standard implementation of the SIFT from [20]. For
the evaluation, we acquired images with a robot-equipped
5551
(c) MSLD (97.81%)                       (d) SIFT (92.25%) 
frame # of db image 
frame # of query 
image 
(b) samples of the scenes used in the vocabulary tree evaluation 
(a) trajectory followed for the sequence acquisition 
Fig. 2. Experimental evaluation of the vocabulary trees
camera in Myung-dong, Seoul, while travelling a 235 meter-
long loop twice. In building the vocabulary tree with the
SIFT features, we used the same settings that are used to
build the vocabulary tree with the MSLD features (i.e., the
same eight videos, eight million SIFT features, k = 50, and
l = 3).
In the sequence, the 715 scenes acquired from the ﬁrst
travel are used as a database, and the 684 scenes from the
second travel are used as queries. In Figure 2, (a) shows the
trajectory followed for the sequence acquisition, (b) shows
some examples of the scenes, and (c) and (d) are the results
of the evaluation. The top ﬁve scored returns from the query
are represented by points darkened according to their scores.
Since it is difﬁcult to obtain the actual trajectories of the
robot, we tried to maintain the velocity of the robot to be
constant, and to follow almost the same trajectories. Then,
we can approximate its retrieval accuracy by following two
steps: First, we adjust the scale of the vertical axis to be equal
to the scale of the horizontal axis. Then, with an assumption
that correct matches should be on the diagonal line, we
consider a query is successful if at least one of the top
ﬁve returns is not farther than ten frames from the diagonal
line. With the strategy, we performed the evaluation varying
parameters of line extraction and SIFT keypoint extraction
(i.e., a minimum length threshold in line extraction and a
maximum number of retaining keypoints in SIFT). The SIFT
(a) samples of the scenes used in this evaluation 
(b) MSLD (71.87%) 
frame # of db image 
frame # of query 
image 
(c) SIFT (53.02%) 
Fig. 3. Experimental evaluation of the vocabulary trees under illumination
and season changes. (a) The images in the left column are samples from
the morning-fall sequence and the images in the right column are samples
from the noon-winter sequence.
keypoint detector allows scoring each keypoints according to
local contrast values, and the system can determine the num-
ber of features to retain according to the scores. Therefore,
to evaluate the retrieval performance of the vocabulary trees
with respect to the number of features, we controlled the
minimum length threshold in line extraction for the case of
MSLD and the number of retaining features for the case of
SIFT in this evaluation.
In the case of MSLD, the number of successful returns
was counted as 672 when the minimum line length threshold
was set to 30, and it was 631 with the parameter for the
number of retaining features set to 500 in the case of the
SIFT. As shown in Figure 2.(c) and (d), we observe that the
vocabulary tree built with the SIFT descriptors shows a little
more spread distribution of the points along the diagonal line
than the case of the MSLDs. Because the tested area was very
structured, it is more reasonable to attribute this result to the
experimented environment and not the performances of the
SIFT or the MSLD.
Additionally, we did another experiment for a comparison
of the vocabulary trees under strong environmental changes.
With a black box camera equipped in a vehicle, we acquired
an image sequence, at an early morning in fall driving in
the campus of Hanyang University, to use them as database
scenes, and we gathered another sequence at around noon
in winter to use them as query scenes. Therefore, in scenes
5552
TABLE I
EVALUATION RESULTS OF THE VOCABULARY TREES
  MSLD SIFT 
# retain feat . . . 700 500 300 
line length 10 20 30 . . . 
travelling loop twice 
# database 715 
# query 684 
# success 
return 
668 669 672 612 631 625 
% success 97.66 97.81 98.25 89.47 92.25 91.37 
# feature 366K 119K 60K 478K 342K 205K 
season & illumination change 
# database 670 
# query 647 
# success 
return 
303 465 443 335 343 333 
% success 59.20 71.87 68.47 51.78 53.02 51.00 
# feature 303K 102K 50K 448K 321K 193K 
From top to bottom, each row indicates # retain feat: 
number of best features to retain, line length: threshold of 
minimum length in line segment extraction, # database: 
number of scenes stored in the database, # query: number of 
query scenes, # success return: number of scenes counted as 
successful return, % success: percentage of the successful 
returns to the database, # feature: total number of features 
used in this evaluation 
of the image sequences, there are illumination and seasonal
changes. In Figure 3, (a) shows some example images used
for this evaluation, and (b)-(c) show the result plots. As can
be seen in the result plots, in both cases the vocabulary
trees returned far more noisy responses than the previous
evaluation. We analyze this as an effect of concurrence of the
two environmental changes: illumination and weather. As can
be seen in Figure 3. (a), there are strong illumination changes
as well as difference in the shape of trees along streets. In
the case of MSLD, the number of successful returns was 465
among 647 queries with the minimum line length threshold
of 20, and it was 343 in the case of SIFT with the parameter
for number of retained features set to 500. The results of the
evaluations are given in Table I.
III. MOTION ESTIMATION USING LINE SEGMENTS
More scenes in the database, higher ambiguity in the
best hypothesis selection is unavoidable if the vocabulary
tree is used alone for ﬁnding the best match because it
does not take into account any geometric information of the
features. Therefore, a geometric veriﬁcation step is employed
to overcome the scalability bottleneck, In most visual place
recognition systems that use point features, multiple-view
l 
e 
s 
e? 
s? 
e?? 
s?? 
l?
? 
l?
? 
image 1 image 2 
l? 
Fig. 4. Two line segments in correspondence. In this case, the overlap
length is deﬁned as the length of the line segment l
0
.
geometry such as epipolar constraint is used to ﬁnd only
consistent matches using ﬁve point algorithm [18] or eight
point algorithm [19]. Under the assumption that correspond-
ing feature points in two views come from the same rigid
3D scene, it veriﬁes the correspondences using geometric
constraints. In case of line matches, it is well known that
a relative motion between two views cannot be determined
from any number of line matches [8]. However, there has
been some algorithms which computes the relative motion by
maximizing the overlap of the matched line segments. In this
work, we use a similar approach as Zhang [14] to estimate
motion between a query and a hypothesis images using line
segments. However, the following different techniques are
adopted for our objective.
 Instead of using the downhill simplex method for op-
timization in [14], we utilize a nonlinear least square
method to guarantee real-time performance.
 In the design of the cost function, we use a much
simpler cost function and utilize a robust loss function to
reduce the effects of outliers in feature correspondences.
A. Maximizing Overlap Length of the Matched Segments
under Epipolar Geometry
Figure 4 shows the deﬁnition of the overlap length of
the matched segments. We denote a line segment in the
ﬁrst image as l and a corresponding line segment in the
second image asl
0
. The rotation matrix and translation vector
between two images are denoted as R and t, respectively.
The essential matrix E becomes [t]

R where [t]

denotes
the skew symmetric matrix of t. The epipolar line l
0
p
in the
second image of a point p in the ﬁrst image can be written
as
l
0
p
=E~ p; (7)
where ~ p is the homogeneous coordinate of the point p. For
the epipolar linesl
0
e
andl
0
s
of the two end pointse ands of
l, we can then compute their intersections with the matched
linel
0
as~ e
00
=l
0
l
0
e
; ~ s
00
=l
0
l
0
s
, respectively. Because the
line segments are oriented, and if the motions are relatively
small, the possible combinations of the two segments can
be considered as shown in Figure 5. We denote the overlap
length in the second image as L
0
and it can be calculated
simply by the following equation.
5553
e? 
s? 
e?? 
s?? 
e?? 
s?? 
e? 
s? 
e?? 
s? 
e? 
s?? 
e? 
s?? 
e?? 
s? 
s?? 
e? 
e?? 
s? 
s? 
e?? 
e? 
s?? 
(a) (b) (c) (d) (e) (f) 
Fig. 5. All possible cases of the two line segments. In each case of (a)-(d),
the overlap length is deﬁned as the length of the thick line. In (e) and (f),
the overlap lengths are deﬁned as the gap represented by the dotted lines.
L
0
=
1
2
 
kv
e
00
s
00k+kv
e
0
s
0k kv
e
00
e
0k kv
s
00
s
0k

; (8)
where v
ab
represents a vector between points a and b, and
e
0
and s
0
denote the end points of l
0
. The overlap length L
0
is calculated only if v
e
0
s
0v
e
00
s
00 > 0.
We should consider a symmetric role of the both images.
Therefore, we denote the overlap lengthL in the ﬁrst image
and calculate it in the same way. Moreover, the overlap
lengths L
0
i
;L
i
are divided by l
i
;l
0
i
, respectively, to remove
the inﬂuence of the length of the line segments, where l
i
and l
0
i
denote the lengths of the line segments l
i
and l
0
i
,
respectively.
Then, if a sufﬁcient number of correspondences are given,
by maximizing the overlap lengths deﬁned by the whole
matched segments the relative motion between the two views
can be determined [14]. Finally, the motion estimation prob-
lem can be deﬁned as minimizing following cost function
for all i-th correspondences.
X
i

(1 L
i
=l
i
)
2
+(1 L
0
i
=l
0
i
)
2

: (9)
However, if a line segment has small angles (or parallel)
with its corresponding epipolar lines, the overlap length
would be unstable, and it might cause a fail in the opti-
mization. Therefore, in the calculation of the costs in our
implementation, the system discards a line segment if it has
an angle difference with its corresponding epipolar line less
than 1 degree.
B. Optimization using a Nonlinear Least Square
For the optimization, we have implemented the Levenberg-
Marqardt (LM) iterative method to achieve real-time per-
formance. The LM is a widely used nonlinear least square
method which shows good results by augmenting its normal
equation so that transitions between Gauss-Newton and gra-
dient methods occur according to its convergence. In order to
reduce the inﬂuence of outliers in feature correspondences,
we adopt the Cauchy loss function given by
(c) =s
2
log(1+c
2
=s
2
); (10)
l?
? 
l?
? 
l? 
Fig. 6. The epipolar geometry reduces the search region for a line segment
l. The matching line segment l
0
should have at least one of the its endpoints
in the region.
where c is the cost and s is some constant. This function
approximates c
2
for small values of c, and s determines the
range of the approximation. In this work, we empirically set
s to 0.3. The resulted cost function is as follows.
C =
X
i


(1 L
i
=l
i
)
2
+(1 L
0
i
=l
0
i
)
2

: (11)
Since the problem is nonlinear, initial guesses are impor-
tant to obtain an acceptable solution. Similar to [14], we
use icosahedrons to get uniformly distributed initial samples.
For translation vector t2 R
3
, we get 40 samples from a
hemisphere of a tessellated icosahedron because if t is a
solution, so is t. Since the scale of the translation t is
inherently unrecoverable, we assume t of unit length and
use it in the spherical coordinate system in the optimization.
Therefore, t would be (;) in R
2
. For the rotation vector
r2 R
3
, we also sample 20 unit vectors from the faces of
the original icosahedron (i.e. not tessellated). Since the angle-
axis representation of ther has its norm as the rotation angle,
we multiply each sample with

6
;

3
, resulting 40 samples for
r. Adding a zero vector to the set of rotation samples results
total 4041=1640 samples. With those initial samples, the
system calculates initial costs using Equation (11). Then,
only 10 samples which yield the smallest cost are used to
carry out the optimization process independently. The ﬁnalr
andt resulting the minimum cost are accepted as the motion
between the two scenes.
C. Geometric Veriﬁcation of Two Scenes
As shown in Figure 6, the epipolar geometry reduces the
search space for line segments. Furthermore, if we assume
long distances of the 3D line segments in the world from the
camera, we can warp their imaged segments from one image
to another by treating their endpoints as in inﬁnite depths.
Then, the angle difference between the warped segment and
the matching segment should be small. Therefore, the system
searches line segments which holds those two constraints,
and returns the matches if the distance of the two descriptor
vectors of the segments are closer than a given threshold,

d
, and its nearest neighbor distance ratio is smaller than a
threshold, 
r
. Figure 7 shows two examples of the warping
of the line segments with our motion estimation implementa-
tion. The images in column Figure 7. (a) show the reference
5554
(a) (b) (c) 
Fig. 7. Examples of warping of line segments. With an assumption of
inﬁnite depth of endpoints, the line segments in (a) are warped onto (b) and
(c) with the relative motions estimated by the proposed algorithm.
image and extracted line segments, and the columns Figure 7.
(b) and (c) show the target images and warped line segments
from (a).
The hypothesis with the maximum number of the matches
or with the minimum cost of the matches can be chosen as
the recognized scene. We tested each scheme, and it returned
some false positives which were not generated in the case of
using the other scheme. Therefore, we deﬁne a score of the
hypothesis i, g
i
, and it is calculated as following equation.
g
i
=
X
j
1
d
j
q
1+
 
1
dj

2
; (12)
whered
j
denotes the distance of the descriptor vectors of the
j-th match. The scoring scheme takes into account both of
the distance of descriptor vectors as well as the number of the
matches. Finally, the hypothesis of the top score is returned
as the recognized scene if the scoreg is higher than a given
threshold, 
g
. All the parameters and thresholds mentioned
so far are given in Table II.
IV. EXPERIMENTS
In this section, we present the experimental results con-
ducted under three environmental changes: season, illumina-
tion, and weather. For image acquisition, we used a black box
camera (DBL-100, Dabonda, 130-degree of FOV .) equipped
in a vehicle, and the optical distortions were removed before
the experiments. The database contains 10,439 images gath-
ered in three different days at around noon in the middle
of September, 2013, which were in the fall, with driving
scenarios in Seoul. About half of the scenes were acquired
on roads, and the rest were gathered in the campus of
Hanyang University. The trajectory followed in the sequence
acquisition was about seven kilometers long.
Three sequences were also gathered in different days to
use them as queries. The ﬁrst sequence is gathered in summer
in order to use it for the experiment under season change,
and the second one is gathered in an early morning before
sunrise for the experiment under illumination change. The
last sequence is gathered in a rainy day for the experiment
under weather change. According to the result from Section
II-C, we set the minimum length threshold in line extraction
Fig. 8. The trajectory followed for the acquisition of the sequences used
in the experiments.
to 20. All the experiments were performed in real-time using
an Intel i7-2600K @ 3.40GHz processor with 16GB DDR3
memory. The results are given in Table II, and demo videos
can be seen athttp://youtu.be/0KYd8yV8A1E?hd=
1.
The ﬂow of the system in this experiment is as follows:
 The current input scene is queried to the vocabulary
tree, and the tree returns the top m hypotheses.
 For each of the returns, features in the current scene
and the hypothesis scene are matched with a distance
threshold,
di
of descriptor vectors and a ratio threshold,

ri
of the nearest neighbor distances. If the ratio of
the number of matches to the number of features in
the query scene is higher than a threshold, 
a
, the
hypothesis takes further steps, or is discarded.
 For each of the hypotheses come from the previous step,
the system estimates motions between the query and the
hypotheses scenes.
 With the motions, the system matches features again
between the two scenes with weaker thresholds, than in
the second step (i.e. 
d
>
di
;
r
>
ri
), and calculates
the score g
i
for each hypothesis.
 The top scored hypothesis i is returned as the recog-
nized scene if g
i
is higher than a threshold, 
g
.
A. Experiment under Season Change
In this experiment, we used a sequence gathered in a
summer while database scenes were gathered in a fall. Figure
9. (a) shows examples of the query and recognized scenes,
and a motion-guided MSLD matching result between the two
scenes is also given on the ﬁrst row. As shown in Table
II, total 872 scenes are queried and 356 and 205 scenes
are recognized with different thresholds 
a
= 0:05 and

a
= 0:10, respectively.
B. Experiment under Illumination Change
For this experiment, we gathered a sequence starting at
AM 6:01, 11 September, 2013, which is eight minutes earlier
from the sunrise in Seoul. We can observe motion blurs on
the sides of the images because the camera maximized its
exposure. As shown in Table II, however, it results the least
number of false negatives. We analyze this as an effect of
the uncrowded roads. When the threshold 
a
= 0:05, this
5555
(a) Season change 
(b) Illumination change 
(c) Weather change 
Fig. 9. Experimental results under three environmental changes: (a) season
change, (b) illumination change, and (c) weather change. In each case,
the left-bottom image is a query scene, and the right-bottom image is a
recognized scene. The result of the motion-guided feature matching is shown
above the query and recognized scenes.
Fig. 10. An example of false positives. The repeated pattern in the
database (right) scene satisﬁes both of close distance of descriptor vectors
and geometric conﬁgurations with a pattern on the query (left) scene, and
this leads to false positives.
TABLE II
PARAMETER SETTINGS AND PERFORMANCES
A B C 
resolution 720?405 
line length 20 
# database 10,439 
# msld db 1,546,231 
m 5 
? ??
,? ?? 0.4, 0.6 
? ? 0.05 0.10 0.05 0.10 0.05 0.10 
? ? ,? ? 0.7, 0.7 
? ? 5 
# query 872 1573 1917 
# msld query 144,229 235,704 286,012 
# recog 356 205 1199 966 1227 771 
# false pos 0 0 8 0 14 0 
# false neg 516 667 374 607 690 1146 
avg time line [ms] 22.65 19.06 18.40 
avg time query tree [ms] 6.13 6.05 5.25 5.32 5.52 5.54 
avg time opt [ms] 10.87 12.57 11.55 13.33 18.69 18.39 
avg time init match [ms] 2.63 2.39 2.32 2.31 2.34 2.31 
avg time init cost [ms] 4.86 5.98 5.75 6.93 5.09 6.64 
avg time epi match [ms] 2.82 2.61 2.35 2.28 2.65 2.73 
avg time query [ms] 129.39 106.63 222.60 193.69 133.18 114.54 
From top to bottom, each row indicates resolution: image resolution 
used, line length: minimum length threshold in line extraction, # 
database: number of scenes in the database, # msld db: total number of 
MSLD descriptors in the database, ? ?? , ? ? ? , ? ? ,? ? ,? ? ,? ? : please refer 
to the main text, # query: number of queried scenes, # msld query: total 
number of MSLD descriptors in the query scenes, # recog: number of 
recognized scenes, # false pos: number of false positives, # false neg: 
number of false negatives, avg time line: average elapsed time for line 
segments extraction, avg time query tree: average elapsed time in 
querying to the vocabulary tree, avg time opt: average elapsed time of 
optimization for motion estimation, avg time init match: average 
elapsed time for initial MSLD matching, avg time init cost: average 
elapsed time for calculation of the initial costs, avg time epi match: 
average elapsed time for the motion-guided MSLD matching, avg time 
query: average elapsed time for a query. 
experiment shows eight false positives, and an example of the
false positives is shown in Figure 10. As shown in the ﬁgure,
the query and the database scenes commonly have a repeated
pattern on the roads satisfying both of the close distances of
the descriptor vectors and the geometric conﬁguration of the
features, and this leads to the false positive. However, the
false positives are removed with a stronger threshold 
a
=
0:10 because it discards the hypothesis, but this also increases
the number of true negatives.
C. Experiment under Weather Change
For this experiment, we acquired a sequence in a rainy
day, while the windshield wipers of the vehicle were in
operation. This experiment generates 14 false positives in
1,227 recognitions. By adjusting the threshold 
a
from 0.05
5556
to 0.10, the false positives are removed, but it also increases
the number of the false negatives as in the other experiments.
Figure 9. (c) shows an example of this experiment. Although
raindrops on the windshield and the wipers made blur and
occlusions, the system was not much affected.
D. Experimental Results
We evaluated the proposed algorithm in three different
conditions. The precisions of the three experiments were
99.76%, 99.33%, and 98.86% in the same thresholds, re-
spectively. The thresholds were set so that false-positive
results are minimized. The experimental results revealed
that our method can robustly recognize the place in signif-
icantly changed environments. It also denotes that implying
the thresholds can be generalized to various environmental
changes. In addition, the computational time measured as
averagely 150 ms makes the demonstration real-time.
V. CONCLUSION
In this paper, we proposed an outdoor place recognition
algorithm using only straight line features. A vocabulary
tree built with line descriptors is used to ﬁnd candidate
matches, and a motion estimation algorithm is used to verify
them. In order to evaluate the retrieval performance of the
vocabulary tree built with MSLD line descriptors, we per-
formed experimental comparisons with other tree built with
SIFT, and the vocabulary tree trained with the line features
exhibits better results in urban environments. We tested our
algorithm with three challenging environmental changes such
as season, weather, and illumination. The database scenes
consist of more than 10,000 images, and the experimental
results demonstrated the real-time performance and reliable
accuracy of the precision rate higher than 98%.
ACKNOWLEDGMENT
This research was supported by the Global Frontier R&D
Program on “Human-centered Interaction for Coexistence”
funded by the National Research Foundation of Korea
grant funded by the Korean Government (MEST) (NRF-
M1AXA003- 2011-0028353). This work was also supported
by the Industrial Strategic Technology Development Program
(10044009) funded by the Ministry of Knowledge Economy
(MKE, Korea).
REFERENCES
[1] K. Konolige, J. Bowman, J. D. Chen, P. Mihelich, M. Calonder, V .
Lepetit, and P. Fua, “View-based Maps,” The International Journal of
Robotics Research (IJRR), vol.29, no.8, pp.941-957, July 2010.
[2] M. Cummins and P. Newman, “FAB-MAP: Probabilistic Localization
and Mapping in the Space of Appearance,” The International Journal
of Robotics Research (IJRR), vol.27, no.6, pp.647-665, June 2008.
[3] A. Angeli, D. Filliat, S. Doncieux, and J. -A. Meyer, “Fast and
Incremental Method for Loop-Closure Detection using Bags of Visual
Words,” IEEE Transactions on Robotics (TRo), vol.24, no.5, pp.1027-
1037, Oct. 2008.
[4] D. Filliat, “A Visual Bag of Words Method for Interactive Qualitative
Localization and Mapping,” in Proc. of IEEE International Conference
on Robotics and Automation (ICRA), pp.3921-3926, April 2007.
[5] D. G. Lowe, “Distinctive Image Features from Scale-Invariant Key-
points,” International Journal of Computer Vision (IJCV), vol.60, no.2,
pp.91-110, Nov. 2004.
[6] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “SURF: Speeded-Up
Robust Features,” Computer Vision - ECCV 2006, vol.3951, pp.404-
417, Jan. 2006.
[7] P. Smith, L. Reid, and A. Davison,“Real-Time Monocular SLAM with
Straight Lines, 2 in Proc. of 2006 British Machine Vision Conference
(BMVC), pp.17-26, Sep. 2006.
[8] R. I. Hartley, “A Linear Method for Reconstruction from Lines and
Points,” in Proc. of the Fifth International Conference on Computer
Vision (ICCV), pp.882-887, June 1995.
[9] G. Klein, D. Murray, “Improving the Agility of Keyframe-based
SLAM,” in Proc. of the tenth European Conference on Computer
Vision: Part II (ECCV)”, pp.802-815, Jan. 2008.
[10] M. Chandraker, J. Lim, and D. Kriegman, “Moving in Stereo: Efﬁ-
cient Structure and Motion using Lines,” in Proc. of the IEEE 12th
International Conference on Computer Vision (ICCV), pp.1741-1748,
Sept.29 2009-Oct. 2 2009.
[11] G. Zhang and I. H. Suh, “A Vertical and Floor Line-based Monocular
SLAM System for Corridor Environments,” International Journal of
Control, Automation and Systems (IJCAS), vol.10, no., pp.547-557,
June 2012.
[12] J. H. Lee, G. Zhang, J. Lim, and I. H. Suh, “Place Recognition using
Straight Lines for Vision-based SLAM,” in Proc. of IEEE International
Conference on Robotics and Automation (ICRA), May 2013.
[13] D. Nister and H. Stewenius, “Scalable Recognition with a V ocabulary
Tree,” in Proc. of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR), vol.2, no., pp.2161-2168, June
2006.
[14] Z. Zhang, “Estimating motion and structure from correspondences of
line segments between two perspective images,” IEEE Transactions
on Pattern Analysis and Machine Intelligence (PAMI), vol.17, no.12,
pp.1129-1139, 1995.
[15] Z. Wang, F. Wu, and Z. Hu, “MSLD: A Robust Descriptor for Line
Matching,” Pattern Recognition, vol.42, no.5, pp.941-953, May 2009.
[16] H. Bay, V . Ferraris, and L. Van Gool, “Wide-Baseline Stereo Matching
with Line Segments,” in Proc. of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), vol.1, no., pp.329-336, June
2005.
[17] J. Sivic and A. Zisserman, “Video Google: a Text Retrieval Approach
to Object Matching in Videos,” in Proc. of the IEEE 9th International
Conference on Computer Vision (ICCV), vol.2, no., pp.1470-1477, Oct.
2003.
[18] D. Nister, “An efﬁcient solution to the ﬁve-point relative pose prob-
lem,” IEEE Transactions on Pattern Analysis and Machine Intelligence
(PAMI), vol.26, no.6, pp.756-770, 2004.
[19] R. I. Hartley, “In defense of the eight-point algorithm.” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence (PAMI), vol.19,
no.6, pp.580-593, 1997.
[20] http://opencv.willowgarage.com/wiki/
5557
