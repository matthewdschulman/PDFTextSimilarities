Online Approximate Model Representation of Unknown Objects
Kiho Kwak, Jun-Sik Kim, Daniel F. Huber, and Takeo Kanade
Abstract? Object representation is useful for many computer
vision tasks, such as object detection, recognition, and tracking.
Computer vision tasks must handle situations where unknown
objects appear and must detect and track some object which
is not in the trained database. In such cases, the system must
learn or, otherwise derive, descriptions of new objects. In this
paper, we investigate creating a representation of previously
unknown objects that newly appear in the scene. The repre-
sentation creates a viewpoint-invariant and scale-normalized
model approximately describing an unknown object with multi-
modal sensors. Those properties of the representation facilitate
3D tracking of the object using 2D-to-2D image matching. The
representation has both benets of an implicit model (referred
to as a view-based model) and an explicit model (referred to
as a shape-based model). Experimental results demonstrate the
viability of the proposed representation and outperform the
existing approaches for 3D-pose estimation.
I. INTRODUCTION
Object recognition and tracking are well-studied problems
in computer vision [1] [2]. Such algorithms frequently learn
a representation of an object or object category from a
database of labeled training examples [3] [4] [5], though
some methods can recognize objects with few examples or
even just one [6] [7]. In situations where novel objects need
to be detected and tracked, learning methods that require
training examples are not applicable. For example, if an
autonomous vehicle has a vision system trained to detect
and track people and other vehicles, how will the system
perform if it encounters a cow crossing the road? While it
may be possible to recognize the cow as an obstacle without
any training examples, it would be benecial to be able to
reliably track the object from a distance and to predict its
future motion.
This paper describes a method for creating a model of
an object online as it is observed and then demonstrates
how this model can improve detection and tracking of the
object when it is observed again in the future. The choice
of representation of the model can have a signicant impact
on the capabilities and performance of a vision algorithm.
A representation should describe the shape and appearance
of 3D objects [8]. Historically, object representations follow
one of two extremes for encoding geometry. Explicit repre-
sentations model the 3D shape of an object, for example, by
creating a point cloud or surface mesh model of the object [9]
Kiho Kwak is with Agency for Defense Development, Republic of Korea
kkwak.add@gmail.com
Jun-Sik Kim is with Korea Institute of Science and Technology, Republic
of Korea junsik.kim@kist.re.kr
Daniel F. Huber and Takeo Kanade are with Carnegie Mellon
University, Pittsburgh, PA 15213, USA fdhuber@ri.cmu.edu,
tk@cs.cmu.edug
Approximate model (geometry and appearance)
Matched observation
Approximate model
Geometrically normalized 
observation
Observation
Registration
(a)
(b)
(c)
Sensor input
Fig. 1. Tracking using the proposed approximate model. (a) The ap-
proximate model is created online from a sequence of observations of the
target object. (b) A new view of the object is observed and automatically
segmented from the image. (c) The new observation is matched to the
approximate model to determine the matching location and orientation of
the target object.
[10]. Implicit representations model shape indirectly without
explicitly encoding 3D shape, for example, by storing a set
of representative images of an object taken from different
viewpoints [11] [12]. Explicit representations have the ad-
vantage that they can compactly represent an object and can
handle novel viewpoints, but creating an accurate 3D model
is in itself a challenging task. Implicit representations can be
easier to create, but they require signicant storage and can
have difculty with novel viewpoints.
In this paper, we advocate an alternative representation
that takes an intermediate position between explicit and im-
plicit representations, which we call an approximate model.
Rather than explicitly attempting to model all the details
of an objects shape, we distill the geometry down to a
small number of planar, vertically-oriented patches, each of
which is coupled with an appearance model for that patch.
Figure 1 shows an overview of tracking an object using an
approximate model representation.
2014 IEEE International Conference on Robotics & Automation (ICRA)
Hong Kong Convention and Exhibition Center
May 31 - June 7, 2014. Hong Kong, China
978-1-4799-3684-7/14/$31.00 ©2014 IEEE 1725
Object region
Unwrapped object region
Input image
Matching
Appearance model
Appearance model
(a)
(b)
Matching
Fig. 2. Appearance matching. The green box shows the Region Of
Interest (ROI) of the tracked object in a frame. The blue box shows the
appearance model of the object which is obtained from the rst frame. (a)
The dashed line describes that the image matching between the model and
the observation is difcult. (b) The line shows that the unwrapped image
makes the image matching easy.
We demonstrate approximate modeling with an algorithm
that builds an approximate model online from a sequence
of observations of a moving object taken from a moving
platform. Our modeling approach uses a combination of 2D
imagery and and range imaging (from a single-line LIDAR).
The images provide dense appearance information but cannot
easily provide accurate shape information, while the LIDAR
provides sparse shape information but limited appearance
data. The combination of the two sensors gains the benets
of both.
Creating an approximate model from a sequence of obser-
vations is challenging for several reasons. First, the model
must be created from scratch. In order to relate the current
observation to previous ones, it is necessary to determine
the correspondence between the new observation and the
in-progress model. Our approach operates incrementally,
incorporating new observations as they arrive, continuously
improving the approximate model. Segmentation algorithms
are used to automatically disambiguate the target object from
background regions.
Second, we observe that nding correspondences between
views can be challenging due to viewpoint-dependent ap-
pearance changes, scale changes, and illumination variations
from shadows (Figure 2). Our approximate model repre-
sentation addresses these challenges using an unwrapped
image, which is a scale-normalized, fronto-parallel image of
the object (Figure 1(a)). The image is created using coarse
geometry provided by 3D imaging to warp the observed
image to approximate how it would appear from a frontal
viewpoint. In this way, unwrapped images from different
viewpoints are transformed to become more similar (Fig-
ure 1(b)). To obtain robustness to illumination changes, we
match images using a combination of color similarity and
image gradients (edge strength)(Figure 1(c)). The unwrapped
image improves image matching under appearance variations
due to illumination, viewpoint, and partial occlusion and
improves detection and tracking performance.
We illustrate the benets of approximate models through
experiments that compare detection and tracking perfor-
mance to alternative methods. Our experiments show that
we can track novel objects more accurately and at longer
distances using the approximate model.
The contributions of this paper are threefold. 1) We
introduce the concept of approximate modeling. 2) We
demonstrate approximate modeling with an online algorithm
for creating approximate models of moving objects observed
from a moving platform. 3) We demonstrate the benet
of approximate modeling in real-world experiments using a
system integrated onto an autonomous vehicle testbed.
II. APPROXIMATE REPRESENTATION IN A SINGLE FRAME
In a single frame image, we can approximately represent
the object with the corresponding geometry. Our representa-
tion approximates the object shape with a set of piecewise
planar patches, and each patch contains a scale-normalized
and fronto-parallel image of the object as a descriptor. Two
preliminary works, extrinsic calibration of a lidar and a cam-
era and segmentation and detection of moving object regions,
are achieved by using the algorithms in [13] and [14],
respectively.
Our approximate representation approach in a single frame
consists of two steps: shape representation with a piecewise
linear model and appearance representation using the ap-
proximate shape. These steps are described in the following
subsections.
A. Shape Representation with Piecewise Linear Model
Given raw lidar measurements of an object (Figure 3 (a)),
we rst lter out some unreliable measurements using the
Rahmer-Douglas-Peucker algorithm (referred to as Iterative
End Point Fit algorithm) [15] [16]. The algorithm, given a
curve composed of a set of points, nds a similar curve with
fewer points. Using the algorithm, we roughly t the raw
lidar measurements to a piecewise linear curve L composed
of line segments.
Once we obtain L from the raw lidar measurements,
we compute the number of line segments in L. Each line
segment implies a 3D vertical planar patch composing the
object surface, as we approximate the shape with a set
of vertical planar patches. Estimating the object geometry
(
?
L =f
?
L
i
g; i = 1;;n) composed of n line segments is
achieved by using the Douglas-Peucker algorithm (Figure 3
(b)).
B. Appearance Representation using Approximate Shape
Each tted line
?
L
i
of a lidar geometry directly corresponds
to a plane of an object's surface. Once we know the corre-
sponding image region that is the object ROI I with the lidar
geometry, we divide the object ROI into n sub-image regions
I
i
.
In order to unwrap the object image I (a perspective
distorted image) using
?
L, we should estimate a homography
matrix H which is a transformation from I to the unwrapped
object image plane
?
I (Figure 4). Building
?
I composed of n
sub-unwrapped image planes, we compute each homography
H
i
from I
i
to
?
I
i
. Let I
i
have four vertices p
j
i
= (u
j
i
;v
j
i
;1)
T
; j=
1726
(a)
(b)
?
1
?
2
?
3
?
4
?
5
Fig. 3. Fitting lidar measurements. (a) The raw lidar measurement is
obtained from a moving car. The red dots show the return points from the
surface of the car. (b) Each blue dashed line between the estimated corner
points (the end points of each line segment) shows the approximated plane
segment that composes the surface of the car. The number of line segments
means that the object surface consists of ve planes. The numbers on each
line segments show the angle difference between the normal direction vector
(blue arrows) of each line and the viewing direction vector (red arrows) from
the sensor.
1;;4 in homogeneous coordinates and the corresponding
vertices q
j
i
= (x
j
i
;y
j
i
;1)
T
on
?
I
j
exist. H
i
is estimated as [17]:
q
j
i
= H
i
p
j
i
: (1)
To decide the corresponding vertices q
j
i
, it is required to
know the size of the unwrapped object image plane
?
I. It
is decided by the spatial resolution per pixel a (mm=pixel)
and the height of the object region. In this paper, we set
the object height to a predened height h because we do
not know the real object height yet. Let l
i
be the real length
of the line segment. The number of pixels in a row and a
column of
?
I
i
are computed by
h
a
and
l
i
a
, respectively. The
spatial resolution a is dependent on the working distance D
that can be decided by the sensor's accuracy. If D increases,
a also increases:
a =
2Dtan(H
FOV
=2)
H
res
; (2)
?
1
?
2
?
3
?
4
?
5
?
1
?
4
?
5
Unwrapping
?
2
?
3
Fig. 4. Unwrapped image planes using the corresponding lidar measure-
ments. The sub-image planes (I
1
, I
2
, I
3
, I
4
, I
5
) in the object ROI are estimated
with the tted lines (
?
L
1
,
?
L
2
,
?
L
3
,
?
L
4
,
?
L
5
) obtained by piecewise linear tting,
respectively. Height of the imaged planes is xed to a predened height (6
m). The unwrapped image
?
I is obtained by stitching the unwrapped images
(
?
I
1
,
?
I
2
,
?
I
3
,
?
I
4
,
?
I
5
).
where H
FOV
is the horizontal camera eld of view (radians)
and H
res
is the horizontal camera resolution (pixels).
III. APPROXIMATE MODELING FROM A SEQUENCE OF
OBSERVATIONS
Once we create the unwrapped object image while keeping
a track of a previously unseen object, the approximate model
of the object is created by incrementally registering the
unwrapped object images. To register the images, we track
the object over time, and then align the tracked object
images. The tracked object images are extracted by the
boundary detection algorithm in [14] that is formulated as
a classication problem that determines whether an object
boundary exists between two consecutive range measure-
ments.
We align a current unwrapped image of an object onto
the reference image updated with previous images. Since
the unwrapped images are invariant to scale and viewpoint
changes, the unwrapped images are registered by 2D-to-
2D appearance matching (Figure 5). In order to minimize
the difference between the reference image and the current
image, we update the reference image by integration of
measurements using a new mosaicing approach that applies
higher weights to measurements from the closet and the most
frontal viewpoint.
Our approximate modeling from a sequence of observation
of a moving object is achieved with three steps: registration
of unwrapped images, temporal integration of unwrapped
images, and foreground region estimation. These steps are
described in the following subsections.
A. Registration of Unwrapped Images
Registering two unwrapped object images is ideally a
simple 1D matching problem in the horizontal direction
because the unwrapped images are scale-normalized at xed
vertical location of the lidar scan plane. In practice, because
the ground is not perfectly at, we allow a little vertical
movement in the registration.
To estimate quality of alignment of the two unwrapped
images, we ensure the weighted sum of correlation values of
1727
Fig. 5. Updating the reference image using integrated mosacing. Red boxes show the reference images that we obtained at each frame. Each reference
image consists of color appearance and edge-gradient.
50 100 150 200 250
50
100
150
200
250
300
50 100 150 200 250
50
100
150
200
250
300
50 100 150 200 250
50
100
150
200
250
300
50 100 150 200 250
50
100
150
200
250
300
Matching score (Color)
50 100 150 200 250
50
100
150
200
250
300
Matching score (Gradient)
50 100 150 200 250
50
100
150
200
250
300
Matching score (Weighted)
50 100 150 200 250
50
100
150
200
250
300
0.5 0.6 0.7 0.8
Fig. 6. Correlation of two unwrapped object images. Both the test
images are obtained with two-frame difference (15 frames/second). In the
correlation score, blue is low ( < 0:5 ) and red is high values ( > 0:8 ).
The three correlation scores indicate that both of the images are almost
overlapped ; the point with the highest score is at the center of the score
map. The score map of the color appearances has a large variance along the
horizontal direction. Although the edge-gradients seem to be well correlated
with a smaller variance, the background edges affect to the correlation score.
The weighted sum of both the two score maps shows that the problems of
the appearance and edge-gradient matching are solved. The weight of the
correlation score of the color appearance is 0:3.
color appearance and edge-gradient, as shown in Figure 6.
In outdoor environments, object's appearance is inuenced
by various factors, such as illumination, brightness, and
shadows. Specically, the color appearance is more seriously
affected than the edge-gradient by those factors [18]. Thus,
we apply different weights to the alignment of the color
appearance and the edge-gradient.
Given a current image
?
I
t
(x) sampled at a discrete pixel
location x
i
=(x
i
;y
i
), registration is to nd where it is located
in the previous image
?
I
t 1
(x). The correlation value is
computed as following [19]:
S(u)=
Œ
i
f
?
I
t 1
(x
i
) 
?
I
t 1
gf
?
I
t
(x
i
+ u) 
?
I
t
g
q
Œ
i
f
?
I
t 1
(x
i
) 
?
I
t 1
g
2
f
?
I
t
(x
i
+ u) 
?
I
t
g
2
; (3)
where u=(u;v) is the displacement, and
?
I
t 1
=
1
N
Œ
i
?
I
t 1
(x
i
) (4)
?
I
t
=
1
N
Œ
i
?
I
t
(x
i
+ u):
are the mean intensity of the corresponding patches and N
is the number of pixels in the patch. Once we compute the
correlation scores of the color appearance S
c
(u) and S
g
(u) of
?
I
t
(x) and
?
I
t 1
(x), the best alignment is made at the position
of which the weighted sum of both correlation scores is
maximized:
x

= arg max
x;y
fwS
c
(u)+(1 w)S
g
(u)g; 8(u;v)2(x;y): (5)
B. Building Reference Image: Temporal Integration of Un-
wrapped Images
Registration aligns the corresponding unwrapped images
from multiple views of a tracked object, and building a single
reference image can be made by integrating them properly.
In order to integrate the aligned unwrapped images, we
incrementally build a mosaic by assigning the column images
of the selected views onto the mosaic image plane. Since the
part of the appearance model are usually observed integrated
mosaic is typically visible in multiple frames, column images
of each of these assigned images are potential candidates in
the integrated mosaic. We have two challenges in creating the
integrated mosaic incrementally: (1) how to consider an ar-
tifact problem occurred by different illumination conditions,
(2) how to minimize the difference between the integrated
mosaic and a new unwrapped image when we register both
images.
Compositing the integrated mosaic M is performed by
optimally choosing source images for the mosaic using a
graph cut optimization [20]. Let
?
I
1
,,
?
I
m
be the set of
aligned unwrapped images to the mosaic image plane. The
graph cut estimates a label image F in which the label x at
1728
column q denoted by F
q
indicates that an image
?
I
x
should
be used as the source for qth column in the mosaic.
The energy function that we minimize is denoted by
E(F) that is the sum of a data penalty term expressing the
quality of the set of aligned unwrapped images used in the
mosaic and a pairwise interaction penalty term explaining
the discontinuities of all pairs of neighboring columns in the
mosaic. LetP be the set of all columns in M andN be
the set of all adjacent column pairs in M:
E(F)=
Œ
q2P
D
q
(F
q
)+
Œ
(q;r)2N
V
q;r
(F
q
;F
r
); (6)
where the data penalty term denoted by D
q
(F
q
) is the cost of
assigning label F
q
to column q and is dened by the plane
weights as:
D
q
(F
q
)= w
F
q
(q); (7)
The weight w of the unwrapped image
?
I is obtained by
estimating the anglef between the sensor's viewing direction
and the normal direction of the object surface using the
corresponding lidar data
?
L, so that the plane image captured
at the most frontal viewpoint is preferable. Let
?
I be composed
of n plane segments and the size of
?
I be r c. The weight
w
k
; k = 1;;c of each column is estimated by using the
angle f [21] [22]:
w
k
= sin
2
f
k
: (8)
The pairwise interaction penalty term V
q;r
(F
q
;F
r
) is the
cost of assigning label F
q
and F
r
to neighboring columns
q and r in the label image. This is to avoid selecting too
different images for nearby columns. Note that, if F
q
= F
r
then V
q;r
(F
q
;F
r
)= 0.
V
q;r
(F
q
;F
r
)=kF
q
  F
r
k; (9)
C. Foreground Region Estimation
The integrated mosaic benets from registering the un-
wrapped images incrementally. However, the mosaic is not
directly able to be used as a model because we do not
estimate the foreground in the mosaic and there are many
artifacts produced by stitching the unwrapped object images
obtained from different lighting condition and vignette ef-
fects.
To remove these artifacts and estimate the foreground
region in the object ROI, we obtain the mean appearance
by computing the average values of colors and gradient
magnitudes of the registered unwrapped images (see Figure 7
(a)). The mean appearance benets from estimating the
foreground region. As the object moves, the background
in the object ROI will typically change more dramatically
than the foreground object appearance. Therefore, the fore-
ground region is estimated by detecting the point where the
background variability begins to occur. This also means that
the consistency of edge gradient in the background region
is not guaranteed but that in the foreground is consistent
(a)
(b)
(c)
Fig. 7. Horizontal boundary estimation using an MRF optimization. (a)
shows the mean color appearance obtained with the registered unwrapped
object images. The object is tracked for 65 frames. (b) describes the
condence of edge gradient. Blue denotes values no greater than zero and
other colors describe positive condence values. Closer to red corresponds
to higher condence, closer to blue to lower condence. (c) shows the
estimated horizontal boundaries (green) in the mean color appearance. The
foreground rows in the image are labeled as 1 and the others are the
background as 0. In the right gure, x-axis is the label values and y-axis is
the row index of vertical pixels. (Best viewed in color)
(see Figure 7 (b)). We pose the foreground region detection
as a horizontal boundary detection problem using an MRF
optimization. The horizontal boundary detection is a kind of
binary labeling problem which determines whether a row is
the foreground or not (see Figure 7 (c)).
1) Gradient Condence of Registered Images: Suppose
we have two unwrapped object images (
?
I
i
,
?
I
j
) which are
already registered. The consistency of edge gradient between
?
I
i
and
?
I
j
is achieved by measuring the condence of gradient
vector eld [18]. Let G
i
and G
j
be the gradient vector
elds of
?
I
i
and
?
I
j
respectively. Given the matching position
x

= (x

;y

) , the gradient condence C of the intersection
region between the both images is estimated by:
C(
?
I
i
\
?
I
j
) =
1
2
(kG
i
(x

;y

)k+kG
j
(x

;y

)k)
 kG
i
(x

;y

)  G
j
(x

;y

)k; (10)
the condences except the intersection region are computed
by:
C(
?
I
i
 (
?
I
i
\
?
I
j
))=
1
2
kG
i
(x

;y

)k; (11)
C(
?
I
j
 (
?
I
i
\
?
I
j
))=
1
2
kG
j
(x

;y

)k: (12)
1729
2) Horizontal Boundary Estimation: We use graph cut
optimization to detect the horizontal boundaries in the mean
appearance. The graph cut optimization returns the label
(e.g.,fForeground = 1;Background = 0g) of each row min-
imizing an energy function with two energy terms. The
rst energy term corresponds to the consistency of edge
gradient in the mean appearance and the second energy
term relates with the color similarity between row pixels
in the mean appearance. Let V be the set of all rows in
the gradient condence map C and N be the set of all
adjacent row pairs in the mean color appearance
ø
I
c
. This
binary labeling problem assigns a label F
p
to pth row. The
labeling variables F is obtained by minimizing the energy
function as Equation 6.
The data penalty term D
p
(F
p
) is dened as the negative
log likelihood of the foreground and background condence
distributions of the row of C. Let the gradient condence
map C be m n and
?
C
+
and
?
C
 
be the sum of the number
of positive and negative condences in C. The foreground
and background condence distribution are obtained as:
p(
?
CjForeground) =
1
n
?
C
+
p(
?
CjBackground) =
1
n
?
C
 
: (13)
Then, we use the distributions to set D
p
(F
p
) as negative log
likelihoods:
D
p
(F
p
)=

 ln p(
?
CjF
p
= 1) if F
p
is foreground
 ln p(
?
CjF
p
= 0) if F
p
is background
(14)
The pairwise term V
p;s
(F
p
;F
s
) is penalizes the appearance
differences between adjacent rows p and s in
ø
I
c
. This term
is dened as:
V
p;s
(F
p
;F
s
)=jF
p
  F
s
j exp( D
p;s
): (15)
where D
p;s
is the similarity of the two image pairs computed
by using Histogram Intersection [19]. Given a pair of his-
tograms, H
p
and H
s
, of pth and sth images respectively,
each containing k bins, the histogram intersection of the
normalized histogram is dened as follows:
D
p;s
=
k
Œ
i=1
min(H
p
(i); H
s
(i)); (16)
where, we map the colors in the image channel into a discrete
color space containing k bins.
IV. EXPERIMENTS
A SICK LMS-221 lidar and a PointGrey Flea2 camera
were used to acquire the data sets for our experiments. The
lidar has a 180 degree horizontal eld of view, with a line
scanning frequency of 75 Hz and a 0:5

angular resolution.
The camera has a 60

horizontal eld of view, with a frame
rate of 15 Hz and a resolution of 1024 by 768 pixels. These
sensors were mounted on front of a vehicle. The lidar was
-25 -20 -15 -10 -5 0 5 10
-15
-10
-5
0
5
10
15
20
25
30
35
X(m)
Y(m)
Sensor position (Ground truth)
Sensor position (Estimation): 2 frames
Model geometry
Model center
0 20 40 60 80 100 120
0
5
10
Angle error
frames
deg
0 20 40 60 80 100 120
-2
0
2
Translation errors (X, Y direction)
frames
m
X
Y
0 20 40 60 80 100 120
-0.2
0
0.2
Estimated translation (Z direction)
frames
m
0 20 40 60 80 100 120
0
0.5
Distance error
frames
m
Fig. 8. Evaluation of 3D sensor pose estimate. The frame rate is 7.5
frames/second; that is, we use 65 frames of data with 2 frame intervals
out of 130 data frames. The initial few frames are articially generated for
evaluating the robustness of our algorithm.
placed at a height of 70 cm, and the camera was installed
30 cm above the lidar. The data from each sensor was time-
stamped to enable synchronization.
Using the framework described above, we performed a
series of experiments to evaluate the effectiveness of our
approximate representation approach. In the rst experiment,
we evaluated the accuracy of the 3D sensor-pose estimation
using our approximate representation algorithm. We then
compared our representation algorithm to two state-of-the-
art methods: iterative closet point (ICP) [23] and the color
ICP approach [24]. Finally, we evaluated the detection and
tracking performance of our approximate model.
A. Evaluation of 3D Pose Estimation Accuracy
We evaluated the accuracy of the 3D sensor pose estimate.
The groundtruth pose data for comparison has been obtained
by manual alignment. The performance was evaluated in
three aspects: angle error, translation error on the XY-ground
plane, and distance error between the ground-truth position
and the estimated position. Additionally, we estimated the
translation in the Z direction. The testing was achieved with
the two sets running on different frame rates, i.e., two- and
ve-frame intervals. The object views vary more dramatically
as the frame interval is increased.
Figure 8 shows the results of 3D sensor pose estimation.
Our approach accurately estimates the sensor position in-
dependent of the frame rate. As shown in Figure 8, the
average of the angle error is 1:2

at the two frame rate.
The translation errors in the Y direction on the XY-ground
plane increase increases at the specic frames. Angular error
causes a large error in the Y translation direction because the
angle error increases at the specic frames where the sensor
position moves on the Y axis. Since we use lidar geometry for
the modeling, the maximum distance error is less than 0:3m.
The largest angle errors occur when the object moves on the
slanted ground because our approximate representation does
not estimate roll and pitch motion.
1730
Fig. 9. Comparison with ICP and color ICP. The color ICP algorithm
performs better than the ICP algorithm in the case of abrupt view change,
but color ICP also does not overcomes the estimation error of the sensor
position in the case of featureless data sets.
B. Comparison against Existing Methods
In our second experiment, we compared the performance
of our algorithm with two existing algorithms; the iterative
closet points (ICP) approach and color ICP. Both algorithms
are state-of-the-art approaches to register or align data,
though there are many variants of these. The color ICP
considers not only point data, but color as well. For the
color ICP approach, the texture information was used with
small image patches centered on the projection of lidar
measurements.
The results, shown in Figure 9, show that our algorithm
outperforms both algorithms for the initial frame section of
1 to 40 frames. These frames consist of challenging cases
such as abrupt view changes and featureless geometry data.
The misalignment of the ICP algorithm was caused by some
featureless geometry data (e.g., lines without corners). The
ICP algorithm requires a good initial transformation in order
to converge to the globally optimal solution, otherwise only
a local optimum is achieved. The color ICP is robust with re-
spect to featureless data, but the corresponding image patches
are not discriminative because most cars have textureless
surfaces and similar colors at the same height.
C. Evaluation of Detection and Tracking Performance
We test the performance of our detector and the tracking
accuracy with 3D pose estimation of unknown object using
our approximate model. For the experiment, we rst create
the approximate model of an object of interest using half of
the frames in a full frame data set and then detect and track
the object in the full frames. Figure 10 shows the tracking
result of an object in 3D space. For these experiments, we
detect an object at 3 frame intervals in 130 frame data
and estimate the 3D pose of the object. The tracking was
performed by detecting the object and estimating the 3D
X (pixel)
Y (pixel)
0 100 200 300 400 500 600
0
50
100
-15 -10 -5 0 5 10 15
0
5
10
15
20
25
X(m)
Y(m)
Sensor origin
Lidar scan points
Object center
Tracking trajectory
Fig. 10. Object tracking with 3D pose estimation. In the scene image,
the green box represents the object image region that is detected using the
vertical boundary detection approach. The geometric model of the tracked
object is shown as a red in the right image. The image in the lower left
corner shows the matching region on the model of the tracked object. The
red shows the edge gradients of the object model, the green and yellow
describe the edge gradient matched with high probability.
pose. The tracking trajectory was obtained by tracking the
estimated center of mass of the object. We predicted an
accurate trajectory in all frames.
V. SUMMARY AND FUTURE WORK
We have presented an algorithm approximately represent-
ing unknown objects in outdoor environments using lidar
measurements and corresponding imagery. The success of
the proposed method lies in three central stages which
consist of approximate representation in a single frame,
online approximate modeling from a sequence of observation
of a moving object, and real-world experiments using the
approximate model.
The approximate model consists of the unwrapped mosaics
(i.e, color and edge-gradient) and the approximate geometry
of an object. These model components are created by an
incremental mosaicing with unwrapped images which are
invariant to viewpoint and scale changes. With the approx-
imate model, we detect and track an object in a scene, and
then estimate its 3D pose by 2D-to-2D matching between
the unwrapped observation and its approximate model. The
proposed algorithm is tested with the datasets we obtained
in an outdoor environment. The approximate representation
outperforms both ICP and color ICP alignment algorithms
for estimating the sensor position in 3D. The detecting
and tracking results shows that the proposed representation
improves on the performance achieving the both benets
of the 2D and 3D model-based approaches by detecting
the object and estimating its 3D pose through 2D image
matching.
In future research, we hope to address a number of issues.
First, we would like to evaluate the detection and tracking
performance of the proposed approach on more complicated
paths of unknown objects. Second, we plan to extract models
for other classes such as pedestrians or bicycles using the
proposed algorithm.
1731
REFERENCES
[1] J. Mundy, ?Object Recognition in the Geometric Era: A Retrospec-
tive,? in Toward Category-Level Object Recognition, ser. Lecture
Notes in Computer Science, J. Ponce, M. Hebert, C. Schmid, and
A. Zisserman, Eds. Springer Berlin Heidelberg, 2006, vol. 4170, pp.
3?28.
[2] A. Yilmaz, O. Javed, and M. Shah, ?Object tracking: A survey,? ACM
Computer Survey, vol. 38, no. 4, pp. 13+, Dec. 2006.
[3] N. Dalal and B. Triggs, ?Histograms of oriented gradients for human
detection,? in Computer Vision and Pattern Recognition, IEEE Com-
puter Society Conference on, 2005, pp. 886?893.
[4] B. Leibe, K. Schindler, N. Cornelis, and L. Van Gool, ?Coupled object
detection and tracking from static cameras and moving vehicles,?
Pattern Analysis and Machine Intelligence, IEEE Transactions on,
vol. 30, pp. 1683?1698, October 2008.
[5] A. Ess, K. Schindler, B. Leibe, and L. Van Gool, ?Object detection
and tracking for autonomous navigation in dynamic environments,?
The International Journal of Robotics Research, vol. 29, pp. 1707?
1725, December 2010.
[6] N. Dowson and R. Bowden, ?Simultaneous modeling and tracking
(smat) of feature sets,? in Computer Vision and Pattern Recognition,
IEEE Computer Society Conference on, vol. 2, june 2005, pp. 99 ?
105 vol. 2.
[7] Z. Yin and R. Collins, ?On-the-y object modeling while tracking,?
in Computer Vision and Pattern Recognition. IEEE Computer Society
Conference on, june 2007, pp. 1 ?8.
[8] Y . Sato, M. D. Wheeler, and K. Ikeuchi, ?Object shape and reectance
modeling from observation,? in Proceedings of the 24th annual
conference on Computer graphics and interactive techniques, ser.
SIGGRAPH '97, 1997, pp. 379?387.
[9] D. Koller, K. Danilidis, and H.-H. Nagel, ?Model-based object tracking
in monocular image sequences of road trafc scenes,? International
Journal of Computer Vision, vol. 10, pp. 257?281, June 1993.
[10] F. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce, ?3d object
modeling and recognition using afne-invariant patches and multi-
view spatial constraints,? in Computer Vision and Pattern Recognition.
IEEE Computer Society Conference on, vol. 2, june 2003, pp. II ?
272?7 vol.2.
[11] H. Schneiderman and T. Kanade, ?A statistical method for 3d object
detection applied to faces and cars,? in Computer Vision and Pattern
Recognition, Proceedings. IEEE Conference on, vol. 1, 2000, pp. 746?
751 vol.1.
[12] A. Torralba, K. Murphy, and W. Freeman, ?Sharing features: efcient
boosting procedures for multiclass object detection,? in Computer
Vision and Pattern Recognition, IEEE Computer Society Conference
on, vol. 2, 2004, pp. II?762?II?769 V ol.2.
[13] K. Kwak, D. Huber, H. Badino, and T. Kanade, ?Extrinsic calibration
of a single line scanning lidar and a camera,? in IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS), 2011.
[14] K. Kwak, D. Huber, J. Chae, and T. Kanade, ?Boundary detection
based on supervised learning,? in Robotics and Automation, IEEE
International Conference on, 2010.
[15] D. H. Douglas and T. K. Peucker, ?Algorithms for the reduction of
the number of points required to represent a digitized line or its
caricature,? Cartographica The International Journal for Geographic
Information and Geovisualization, vol. 10, no. 2, pp. 112?122, 1973.
[16] V . Nguyen, S. G¬ achter, A. Martinelli, N. Tomatis, and R. Siegwart,
?A comparison of line extraction algorithms using 2d range data for
indoor mobile robotics,? Autonomous Robots, vol. 23, no. 2, pp. 97?
111, Aug. 2007.
[17] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision, 2nd ed. Cambridge University Press, 2004.
[18] D. Scharstein, ?Matching images by comparing their gradient elds,?
in The International Conference on Pattern Recognition, 1994, pp.
572?575.
[19] R. Szeliski, Computer Vision : Algorithms and Applications,
R. Szeliski, Ed. Springer-Verlag New York Inc, 2010.
[20] Y . Y . Boykov and M. P. Jolly, ?Interactive graph cuts for optimal
boundary & region segmentation of objects in n-d images,? in Inter-
national Conference on Computer Vision, vol. 1. IEEE Comput. Soc,
2001, pp. 105 ? 112.
[21] V . S. Lempitsky and D. V . Ivanov, ?Seamless mosaicing of image-
based texture maps,? in Computer Vision and Pattern Recognition.
IEEE Computer Society Conference on. IEEE Computer Society,
2007.
[22] S. N. Sinha, D. Steedly, R. Szeliski, M. Agrawala, and M. Pollefeys,
?Interactive 3d architectural modeling from unordered photo collec-
tions,? ACM Transactions on Graphics, vol. 27, no. 5, pp. 159:1?
159:10, Dec. 2008.
[23] Z. Zhang, ?Iterative point matching for registration of free-form curves
and surfaces,? International Journal of Computer Vision, vol. 13, no. 2,
pp. 119 ? 152, Oct. 1994.
[24] A. E. Johnson and S. B. Kang, ?Registration and integration of textured
3-d data,? in Proceedings of the International Conference on Recent
Advances in 3-D Digital Imaging and Modeling, ser. NRC '97. IEEE
Computer Society, 1997, pp. 234?.
1732
